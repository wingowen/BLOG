{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"https://wingowen.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-07-29T10:17:24.000Z","updated":"2022-07-29T10:18:07.390Z","comments":true,"path":"categories/index.html","permalink":"https://wingowen.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-07-29T10:18:35.000Z","updated":"2022-07-29T10:18:54.011Z","comments":true,"path":"tags/index.html","permalink":"https://wingowen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"网络相关","slug":"运维/网络相关","date":"2022-09-01T02:45:43.000Z","updated":"2022-09-01T05:58:03.222Z","comments":true,"path":"2022/09/01/运维/网络相关/","link":"","permalink":"https://wingowen.github.io/2022/09/01/%E8%BF%90%E7%BB%B4/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/","excerpt":"iptables / netfilter","text":"iptables / netfilter iptablesiptables 是 Linux 防火墙的管理工具而已，位于 /sbin/iptables；真正实现防火墙功能的是 netfilter，它是 Linux 内核中实现包过滤的内部结构。 iptables 传输数据包的过程： 当一个数据包进入网卡时，它首先进入 PREROUTING 链，内核根据数据包目的 IP 判断是否需要转送出去。 如果数据包就是进入本机的，它就会沿着图向下移动，到达 INPUT 链。数据包到了 INPUT 链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过 OUTPUT 链，然后到达 POSTROUTING 链输出。 如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出。 iptables的规则表和链： 表（tables）提供特定的功能，iptables 内置了 4 个表 filter 表，包过滤； nat 表，网络地址转换； mangle 表，包重构、修改； raw 表，数据跟踪处理。 链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一 条或数条规则。当一个数据包到达一个链时，iptables 就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则 iptables 将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。 五个链为： PREROUTING：路由选择前； INPUT：路由选择后，进入到主机中； FORWARD：路由选择后，转发； OUTPUT：路由选择后（判断用哪张网卡发出包）,流出； POSTROUTING：最后的数据流出。 常用命令： 123456789101112131415161718192021222324252627282930313233343536373839# 查看规则iptables -t 表名 -Liptables -t nat --line -nvL PREROUTING # --line 显示规则的行号# -n 不解析IP# -v 显示详细内容# 添加规则iptables -t filter -A INPUT -s 192.168.1.146 -j DROPiptables -t filter -I INPUT -s 192.168.1.146 -j ACCEPT# 指定位置iptables -t filter -I INPUT 5 -s 192.168.1.146 -j REJECT # 设置指定表的指定链的默认策略（默认动作），并非添加规则。iptables -t filter -P FORWARD ACCEPT# -I 插入到第一行# -A 插入到最后# 删除规则iptables -t filter -D INPUT 3iptables -t filter -D INPUT -s 192.168.1.146 -j DROPiptables -t filter -F INPUTiptables -t filter -F# -F 清空# 删除自定义链iptables -X WEB# 修改规则iptables -t filter -R INPUT 3 -s 192.168.1.146 -j ACCEPT# 清除包的计数iptabls -t nat -Z PREROUTING# 清除nat表所有链的计数iptabls -t nat -Z# 保存service iptables saveiptables-save &gt; /etc/sysconfig/iptables","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"网络相关","slug":"网络相关","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"}]},{"title":"服务安装","slug":"运维/服务安装","date":"2022-08-31T02:41:25.000Z","updated":"2022-09-01T02:44:34.288Z","comments":true,"path":"2022/08/31/运维/服务安装/","link":"","permalink":"https://wingowen.github.io/2022/08/31/%E8%BF%90%E7%BB%B4/%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/","excerpt":"JDK","text":"JDK JDK12345678wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27;yum install jdk-8u202-linux-x64.rpm -ycat &gt;&gt; ~/.bash_profile &lt;&lt; EOFJAVA_HOME=/usr/java/jdk1.8.0_202-amd64/EOFsource ~/.bash_profile MySQL12345678910111213141516171819# mysql 服务下载启动yum install -y http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpmyum install mysql-community-server.x86_64 -y --nogpgchecksystemctl start mysqldsystemctl enable mysqld# mysql 服务配置# 获取初始密码cat /var/log/mysqld.log | grep password# 登陆控制台mysql -p# 设置密码及权限&gt; set global validate_password_policy=0;&gt; SET PASSWORD = PASSWORD(&#x27;wefe2022&#x27;);&gt; grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;wefe2022&quot;;&gt; flush privileges;","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"网站收集","slug":"网站收集","date":"2022-08-29T08:57:30.000Z","updated":"2022-08-29T09:06:24.800Z","comments":true,"path":"2022/08/29/网站收集/","link":"","permalink":"https://wingowen.github.io/2022/08/29/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/","excerpt":"收集一些有用的网站。","text":"收集一些有用的网站。 JAVASringhttps://github.com/wuyouzhuguli/SpringAll","categories":[{"name":"日常","slug":"日常","permalink":"https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"}],"tags":[{"name":"网站收集","slug":"网站收集","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"}]},{"title":"TED 第一期","slug":"TED/TED 第一期","date":"2022-08-27T00:28:40.000Z","updated":"2022-08-29T09:05:21.479Z","comments":true,"path":"2022/08/27/TED/TED 第一期/","link":"","permalink":"https://wingowen.github.io/2022/08/27/TED/TED%20%E7%AC%AC%E4%B8%80%E6%9C%9F/","excerpt":"Keep Learngin.","text":"Keep Learngin. Keep your goal to yourselfEveryone, please think of your biggest personal goal. For real — you can take a second. You’ve got to feel this to learn it. Take a few seconds and think of your personal biggest goal, okay? Imagine deciding right now that you’re going to do it. Imagine telling someone that you meet today what you’re going to do. Imagine their congratulations, and their high image of you. Doesn’t it feel good to say it out loud?Don’t you feel one step closer already, like it’s already becoming part of your identity? Well, bad news: you should have kept your mouth shut,because that good feeling now will make you less likely to do it. The repeated psychology tests have proven that telling someone your goal makes it less likely to happen. Any time you have a goal, there are some steps that need to be done, some work that needs to be done in order to achieve it. Ideally you would not be satisfied until you’d actually done the work. But when you tell someone your goal and they acknowledge it, psychologists have found that it’s called a “social reality”. The mind is kind of tricked into feeling that it’s already done. And then because you’ve felt taht satisfaction, you’re less motivated to do the actual hard work necessary. So this goes against conventianal wisdom that we should tell our friends our goals, right? So they hold us to it. So, let’s look at the proof. 1926: Kurt Lewin, founder of social psychology, called this “subsituation”. 1933: Wera Mahler found when it was acknowledged by others, it felt real in the mind. 1982, Peter Gollwitzer wrote a whole book about this, and in 2009, he did some new tests that were published. It goes like this: 163 people across four separate tests. Everyone wrote down their personal goal. Then half of them announced their commitment to this goal to the room, and half didn’t. Then everyone was given 45 minutes of work that would directly lead them towards their goal, but they were told that they could stop at any time. Now, those who kept their mouths shut worked the entire 45 minutes on average, and when asked afterward, said that they felt that they had a long way to go still to achieve their goal. But those who had announced it quit after only 33 minutes, on average, and when asked afterward, said that they felt much closer to achieving their goal. So if this is true, what can we do? Well, you could resist the temptation to announce your goal. You can delay the gratification that the social acknowledgement brings, and you can understand that your mind mistakes the talking for the doing. But if you do need to talk about something, you can state it in a way that gives you no satisfication, such as, “I really want to run this marathon, so I need to train five times a week and kick my ass if I don’t, okay?” So audience, next time you’re tempted to tell someone your goal, what will you say? Exactly! Well done.","categories":[{"name":"英语","slug":"英语","permalink":"https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"英语演讲","slug":"英语演讲","permalink":"https://wingowen.github.io/tags/%E8%8B%B1%E8%AF%AD%E6%BC%94%E8%AE%B2/"}]},{"title":"计算机科学","slug":"编程/计算机科学","date":"2022-08-15T09:16:17.000Z","updated":"2022-08-29T09:05:56.912Z","comments":true,"path":"2022/08/15/编程/计算机科学/","link":"","permalink":"https://wingowen.github.io/2022/08/15/%E7%BC%96%E7%A8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/","excerpt":"协程。","text":"协程。 协程操作系统在线程等待 IO 的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待 IO 的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。 协程刚好可以解决上述 2 个问题。协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。 协程只有在等待 IO 的过程中才能重复利用线程。 假设协程运行在线程之上，并且协程调用了一个阻塞 IO 操作，这时候会发生什么？实际上操作系统并不知道协程的存在，它只知道线程，因此在协程调用阻塞 IO 操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥其作用。","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}]},{"title":"降维","slug":"算法/降维","date":"2022-08-14T08:48:21.000Z","updated":"2022-08-29T09:04:45.843Z","comments":true,"path":"2022/08/14/算法/降维/","link":"","permalink":"https://wingowen.github.io/2022/08/14/%E7%AE%97%E6%B3%95/%E9%99%8D%E7%BB%B4/","excerpt":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。","text":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。 低维嵌入介绍在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个低维嵌入 embedding。 缓解维数灾难的一个重要途径是降维 dimension reduction。它是通过某种数学变换将原始高纬度属性空间转变为一个低维子空间，在这个子空间中样本密度大幅提高，计算距离也变得更为容易。低维嵌入的目的是解决 k 邻近学习方法可操作性弱的问题。 将一个三维问题垂直投影，变成一个二维问题。 这种方法叫做多维缩放 Multiple Dimensional Scaling，简称 MDS，这是一种经典的降维方法。 MDS 12345678910111213141516# 导入包import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasets,manifoldfrom collections import Counterdef load_data(): # 使用 scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() return iris.data,iris.target# 产生用于降维的数据集X, y=load_data()print(X.shape)print(Counter(y))","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"特征选择","slug":"算法/特征选择","date":"2022-08-13T14:41:12.000Z","updated":"2022-08-29T09:05:47.711Z","comments":true,"path":"2022/08/13/算法/特征选择/","link":"","permalink":"https://wingowen.github.io/2022/08/13/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。","text":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。 过滤式选择先对数据集进行特征选择，然后再训练分类器，特征选择过程与后续训练无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。 Relief 选择法Relief Relevant Features，该方法设计了一个相关统计量来度量特征的重要性，并且其是一个向量，其每个分量分别对应一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。最终只需要确定一个阈值 r，然后选择比 r 大的相关统计量分量所对应的特征即可。也可以指定选取相关统计量分量最大的前 k 个特征。 FInsher 选择法计算数据集中每个类别样本的类内特征方差与类间特征方差。类内特征方差越小，类间特征方差越大，越有利于后续的分类训练，即该特征需要保留，反之该特征需要滤除。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#导入包import pandas as pdimport numpy as np#创建示例样本sample = [[1,1,5],[10,0,1],[2,5,6]]label = [1, 0, 1]print(&quot;sample:&quot;,sample)print(&quot;label:&quot;,label)#判断样本长度与类标长度是否匹配if len(sample) != len(label): print(&#x27;Sample does not match label&#x27;) exit()#创建并保存计算过程中的变量df1 = pd.DataFrame(sample)df2 = pd.DataFrame(label, columns=[&#x27;label&#x27;])data = pd.concat([df1, df2], axis=1) # 合并成为一个dataframeprint(&quot;data:\\n&quot;,data,&#x27;\\n&#x27;)data0 = data[data.label == 0]#对标签分类，分成包含0和1的两个dataframedata1 = data[data.label == 1]n = len(label)#标签长度n1 = sum(label)#1类标签的个数n0 = n - n1#0类标签的个数lst = []#用于返回的列表features_list = list(data.columns)[:-1]print(&quot;特征维数:&quot;)print(features_list)#fisher score计算for feature in features_list: print(&#x27;\\nfeature&#x27;,feature,&#x27;:&#x27;) # 算关于类标0 m0_feature_mean = data0[feature].mean() # 0 类标签在第 m 维上的均值 print(&quot;m0_feature_mean&quot;,m0_feature_mean) m0_SW=sum((data0[feature] -m0_feature_mean )**2) # 0类在第 m 维上的类内方差 print(&quot;m0_SW&quot;,m0_SW) # 算关于类标1 m1_feature_mean = data1[feature].mean() # 1 类标签在第 m 维上的均值 print(&quot;m1_feature_mean&quot;,m1_feature_mean) m1_SW=sum((data1[feature] -m1_feature_mean )**2)# 1 类标签在第 m 维上的类内方差 print(&quot;m1_SW&quot;,m1_SW) # 算关于 data m_all_feature_mean = data[feature].mean() # 所有类标签在第 m 维上的均值 print(&quot;m_all_feature_mean&quot;,m_all_feature_mean) m0_SB = n0 / n * (m0_feature_mean - m_all_feature_mean) ** 2 # 0 类标签在第 m 维上的类间方差 print(&quot;m0_SB&quot;,m0_SB) m1_SB = n1 / n * (m1_feature_mean - m_all_feature_mean) ** 2 # 1 类标签在第 m 维上的类间方差 print(&quot;m1_SB&quot;,m1_SB) m_SB = m1_SB + m0_SB # 计算SB print(&quot;m_SB&quot;,m_SB) m_SW = (m0_SW + m1_SW) / n # 计算 SW print(&quot;m_SW&quot;,m_SW) if m_SW == 0: # 0/0类型也是返回nan m_fisher_score = np.nan else: # 计算Fisher score m_fisher_score = m_SB / m_SW #计算第m维特征的Fisher score #Fisher score值添加进列表 print(&quot;m_fisher_score&quot;,m_fisher_score) lst.append(m_fisher_score) 包裹式选择包裹式特征选择直接将最终要使用的学习器的性能作为特征子集的评价准则，包裹式特征选择的目的就是为给定的学习器选择最有利于其性能而量身定做的特征子集。 一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，比过滤式特征选择更好，但由于在特征选择过程中要多次训练学习器，因此包裹式特征选择的计算开销比过滤式特征选择大得多。 LVW 是一个经典的包裹式特征选择方法，它在拉斯维加斯方法框架下使用随机策略进行子集搜索，以最终分类器误差作为特征子集评价标准。 除了 LVW 包裹式特征选择之外，RFE(递归特征消除)也是一种常见的包裹式特征选择方法，RFE特征选择使用模型准确率来判断哪些特征（或特征组合）对预测结果贡献较大，并且递归地去除贡献小的特征。 除了 RFE 之外，还有一种选择算法称为 RFECV，其是以 RFE 为基础进行改进得到的。 RFECV 通过交叉验证的方式执行 RFE，以此来选择最佳数量的特征，即不手动设置保留的特征数量。对于一个数量为 d 的 feature 的集合，他的所有的子集的个数是 2 的 d 次方减 1 (包含空集)。指定一个外部的学习算法，比如 SVM 之类。通过该算法计算所有子集的validation error。选择 error 最小的那个子集作为所挑选的特征。 123456789101112131415161718192021222324252627282930313233343536373839404142#导入包from sklearn.feature_selection import RFE,RFECVfrom sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn import model_selection&#x27;&#x27;&#x27;class RFE(BaseEstimator, MetyuanaEstimatorMixin, SelectorMixin): 参数： BaseEstimator: 基模型 n_features_to_select：目标特征数量 return：经过选择后的特征 比较经过特征选择和未经特征选择的数据集，对 LinearSVC 的预测性能的区别&#x27;&#x27;&#x27;### 加载数据iris = load_iris()X, y = iris.data, iris.target### 特征提取estimator = LinearSVC()selector = RFE(estimator=estimator, n_features_to_select=2)X_t = selector.fit_transform(X, y) #对样本进行特征选择，最终保留n_features_to_select个特征。print(&quot;\\n特征选择结果显示:&quot;)print(&quot;原数据样本X：&quot;,X[1])print(&quot;特征选择后样本X_t：&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0, stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))clf_t.fit(X_train_t, y_train_t)print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t))) 嵌入式选择嵌入式特征选择是将特征选择过程与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。 正则化嵌入式选择 $L1$ 范数与 $L2$ 范数都有利于降低过拟合，但前者还会带来一个额外的好处，即 $L1$ 范数比 $L2$ 范数更容易获得稀疏解，即它求得的 $w$ 会有更少的非零分量。 其中，基于 $L1$ 正则化的学习方法是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。 123456789101112131415161718192021222324252627282930313233#导入包from sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectFromModelfrom sklearn import model_selection#导入数据集并打印示例iris = load_iris()X, y = iris.data, iris.targetprint(&quot;原始数据特征维数：&quot;,len(X[1])) # (150, 4)print(&quot;原始数据样本：&quot;,X[1])#特征选择lsvc = LinearSVC(C=0.01, penalty=&quot;l1&quot;, dual=False).fit(X, y) #设置分类器model = SelectFromModel(lsvc, prefit=True) #设置模型为特征选择X_t = model.transform(X) #获取经过筛选的数据print(&quot;特征选择数据特征维数&quot;,len(X_t[1])) #(150, 3)print(&quot;特征选择后数据样本&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0,stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)clf_t.fit(X_train_t, y_train_t)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t)))","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性回归与逻辑回归","slug":"算法/线性回归与逻辑回归","date":"2022-08-12T02:35:16.000Z","updated":"2022-08-13T14:34:12.821Z","comments":true,"path":"2022/08/12/算法/线性回归与逻辑回归/","link":"","permalink":"https://wingowen.github.io/2022/08/12/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等","text":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等 线性回归线性回归分析 Linear Regression Analysis 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。 所以线性的定义为：自变量之间只存在线性关系，即自变量只能通过相加、或者相减进行组合。 监督学习 如果现在有一个房子 H1，面积是 S，监督学习如何估算它的价格？ 监督学习从训练集中找到面积最接近 S 的房子 H2，预测 H1 的价格等于 H2 的价格。 监督学习根据训练集，找到一个数学表达式，对任意的面积的房子都可以估算出其价格。 h 代表假设函数：Training Set → Learning Algorithm → h；Size of House + h → Estimated Price。 线性回归的假设模型 h_{\\theta}(x)=\\theta_{0}+\\theta_{1} x如何求解模型，有以下两种思路。 尝试一些 $\\theta{0}$ 和 $\\theta{1}$ 的组合，选择能使得画出的直线正好穿过训练集的 $\\theta{0}$ 和 $\\theta{1}$ 。 尝试一些 $\\theta{0}$ 和 $\\theta{1}$ 的组合，然后在训练集上进行预测，选能使得预测值与真实的房子价格最接近的 $\\theta{0}$ 和 $\\theta{1}$ 。 选择最佳的 $\\theta{0}$ 和 $\\theta{1}$，使得 $h_{\\theta}(x)$ 对所有的训练样本 $(x, y)$ 来说，尽可能的接近 $y$。 损失函数 Train Set: $\\left{\\left(x^{(1)}, y^{(1)}\\right),\\left(x^{(2)}, y^{(2)}\\right), \\cdots,\\left(x^{(m)}, y^{(m)}\\right)\\right}$ \\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}最小化损失函数，得到的 $\\theta{0}$ 和 $\\theta{1}$ 是最佳的。 12345678910# 房屋的价格和面积数据import numpy as npdata = np.array([[2104, 460], [1416, 232], [1534, 315], [852,178]])# 使用线性回归模型计算预测值def get_predict(x, theta0, theta1): h = theta0 + theta1 * x #todo return h 梯度下降算法梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合 $(\\theta{0},\\theta{1},……,\\theta_{n})$ 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合， 可能会找到不同的局部最小值。 实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，需要同时更新 $\\theta{0}$ 和 $\\theta{1}$，实现方法是：你应该计算公式右边的部分，通过那一部分计算出 $\\theta{0}$ 和 $\\theta{1}$的值，然后同时更新 $\\theta{0}$ 和 $\\theta{1}$。 \\text { temp0 }:=\\theta_{0}-\\alpha \\frac{\\partial}{\\partial \\theta_{0}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\text { temp1 }:=\\theta_{1}-\\alpha \\frac{\\partial}{\\partial \\theta_{1}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\theta_{0}:=\\text { temp0 } \\\\ \\theta_{1}:=\\text { temp1 }逻辑回归二分类问题下，采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。它适用于标签 y 取值离散的情况。 逻辑函数 Logistic Function，一个最常用的逻辑函数是 Sigmoid Function，以 Z=0 为决策界限，公式如下： g(z)=\\frac{1}{1+e^{-z}}123import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 逻辑回归模型假设 h_\\theta(x)=g(\\theta^TX)$h\\theta(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性 （estimated probablity），即 $ h\\theta(x) = P(y=1|x;\\theta)$。 例如，如果对于给定的 x，通过已经确定的参数计算得出 $h_\\theta(x)$=0.7，则表示有 70% 的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7 = 0.3。 损失函数 线性回归模型的代价函数是所有模型误差的平方和，若逻辑回归的假设模型沿用这个定义，得到的函数将是一个非凸函数 non-convex function。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)}) \\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{aligned} -\\log \\left(h_{\\theta}(x)\\right) & \\text { if } y=1 \\\\ -\\log \\left(1-h_{\\theta}(x)\\right) & \\text { if } y=0 \\end{aligned}\\right. Cost(h_\\theta(x), y)=-y\\times{log(h_\\theta(x))}-(1-y)\\times{log(1-h_\\theta(x))} J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) return np.sum(first - second) / (len(X)) 当实际的 y=1 且 $h\\theta(x)$ 也为1 时误差为 0，当 y=1 但 $h\\theta(x)$ 不为 1 时误差随着 $h_\\theta(x)$ 的变小而变大； 当实际的 y=0 且 $h\\theta(x)$ 也为 0 时代价为 0，当 y=0 但 $h\\theta(x)$ 不为 0 时误差随着 $h_\\theta(x)$ 的变大而变大。 同样使用梯度下降法对参数进行更新： \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j123456789import numpy as np# 返回某一轮训练中的梯度def _gradient(X, Y_label, theta): # _f用来计算 y 的值 y_pred = _f(X, theta, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) return w_grad 多元分类 我们将多个类中的一个类标记为正向类 y=1，然后将其他所有类都标记为负向类，这个模型记作 $h^{(1)\\theta(x)}$。接着，类似地第我们选择另一个类标记为正向类 y=2，再将其它类都标记为负向类，将这个模型记作 $h^{(2)\\theta(x)}$，依此类推。 最后我们得到一系列的模型简记为： h^{(i)_\\theta(x)} = p(y=i|x;\\theta)最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h^{(i)\\theta(x)}$， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 $h^{(i)\\theta(x)}$ 最大的 i，即 $\\maxih^{(i)\\theta(x)}$。 过拟合化和正则化过拟合在训练数据上的表现非常好；对于非训练的数据点，过拟合的模型表现与我们的期望有较大的偏。 减少拟合化的方法： 减少选取变量的数量：选取最重要的参数； 正则化：一种减小参数大小的办法。 正则化 回归：岭回归。 分类：L1 正则化，L2 正则化。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"英语单词","slug":"考研/英语单词","date":"2022-08-02T01:16:03.000Z","updated":"2022-09-09T10:27:57.236Z","comments":true,"path":"2022/08/02/考研/英语单词/","link":"","permalink":"https://wingowen.github.io/2022/08/02/%E8%80%83%E7%A0%94/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/","excerpt":"第一次学习时间：2022年08月02日。","text":"第一次学习时间：2022年08月02日。 UNIT 01state n. 状态 情况 国 州 v. 陈述 说明 The current state of affairs may have been encouraged — though not justified — by the lack of legal penalty (in American, bur not in Europe) for data leakage. As a condition of receiving state approval for the sale, the company agreed to seek permission from state regulators to operate past 2012. statute n. 法令 法规 manifestation n. 显示 表现 示威运动 statistic n. 统计数值 statistical adj. 统计学的 a statistical population distribution among age peers. stationary adj. 固定的 静止的 不动的 statement n. 陈述 声明 表达 A string of accidents, including the partial collapse of a colling tower in 2007 and the discovery of an underground pipe system leakage, raised serious questions about both A’s safety and B’s management especically after company made misleading statements about the pipe. understatement n. 保守陈述 轻描淡写 overstate v. 夸大陈述 It was banks that were on the wrong planet, with account that vastly overvalued assets. Today they argue that market prices overstate losses, because they largely reflect the temporary illiquidity of markets, not the likely extent of bad debts. statesman n. 政治家 estate n. 房地产 身份 财产 devastate v. 毁灭 毁坏 devastating adj. 毁灭的 极具破坏力的 The wear-and-tear that come from these longer relationships can be quite devastating. workstation n. 工作台 status n. 地位 情形 状态 assert v. 坚称 断言 表明 The administration was in essence asserting that because it didn’t want to carry out Congress’s immigration wishes, no state should be allowed to do so either. affirm v. 断言 肯定 证实 public adj. 公众的 公共的 n. 民众 publication n. 出版物 republican adj. 共和国的 共和党的 n. 共和主义者 publicity n. 公众信息 宣传 公之于众的状况 publicized adj. 公开的 publicly adv. 公共得 in public 公开地 mass n. 民众 大量 civil adj. 公民的 civil servant 公务员 law n. 法律 法规 claw n. 爪 爪形器具 v. 用爪挖 lawful adj. 合法的 法定的 lawsuit n. 诉讼 诉讼案件 flaw n. 缺点 v. 使破裂 lawyer n. 律师 legislation n. 法律 法规 That may change fast: lots of proposed data-security legislation is now doing the rounds in Washington.D.C. mean v. 意味着 adj. 吝啬的 刻薄的 n. 平均数 Clearly, intelligence encompasses more than a score on a test, Just What does it mean to be smart? The development of “cloud computing”, meanwhile, means that police officers could conceivably access even more information with a few swipes on a touchscreen. meaning n. 意义 含义 well-meaning adj. 好心的 善意的 meaningless adj. 没有意义的 不重要的 无价值的 meaningfully adv. 有意义地 And the best way to learn how to encoding information meaningfully, Tom determined. was a process konwn as deliberate pracitce. Deliberate practice entails more than simply repeating a task. Rather, it involves setting specific goles,obtaining immediate feedback and concentrating as much on technique as on outcome. means n. 方法 手段 meanwhile adv. 同时 其间 Meanwhile, as the recession is looming large, people are getting anxious. They tend to keep a tighter hold on their purse and considereating at home a realistic alternative. by means of 用 依靠 by no means 绝不 一点也不 indicate v. 表明 暗示 象征 反映 implication n. 可能引发的后果 暗示 含义 Scholars, policymakers, and critics of all stripes have debated the social implications of these changes. influence n. 影响 权势 v. 影响 influential adj. 有影响力的 n. 影响者 有势力的人 impact n. 撞击 碰撞 影响 v. 撞击 对…产生影响 碰撞 Though serveral fast-fashion companies have made efforts to curb their impact on labor and the environment - including H&amp;M, with its green Conscious Collection Line - Tom believes lasting change can only be effected by the customer. live v. 居住 n. 生命 adj. 活的 现场演出的 Devoted concertgoers who reply that recordings are no subtitute for live performance are missing the point. alive adj. 有生气的 活着的 deliver v. 发表 递送 接生 delivery n. 分娩 投递 讲话方式 outlive v. 比…活的久 Steelworkers, airline employees, and now thoes in ths auto industry are joining millions of families who must worry about interest rates, stock market fluctuation, and the hash reality that they may outlive their retirement money. liveliness n. 活力 livelihood n. 生计 生活 live off 依赖…生活 dewell v. 居住于 reside v. 定居于 survive v. 幸存 比…活得长 survival n. 生存 幸存 inhabit v. 居住于 settle v. 使定居 federal adj. 联邦的 A few premiers are suspicious of any federal-provincial deal-making. federation n. 联邦 The idea is to create a federation of private online identity systems. FBI Federal Bureau of Investigation CIA Central Intelligence Agency large adj. 大的 大规模的 largely adv. 主要地 大体的 enlarge v. 扩大 放大 at large 一般 普遍地 bulky adj. 庞大的 体积大的 outsize adj. 特大的 huge adj. 庞大的 mark v. 做标记 n. 痕迹 America American 人 America’s new plan to buy up toxic assets will not work unless banks mark assets to levels which buyers find attractive. market n. 市场 行情 v. 推销 营销 The rough guide to marketing success used to be that you got what you paid for. marketplace n. 市场 商场 市集 marked adj. 显著的 明显的 有记号的 There is a marked different between the education which every one gets from living with others, and the deliberate educating of the young. marketer n. 市场商人 市场营销人员 we media 自媒体 paid and owned media 付费媒体与自有媒体 Paid and owned media are controlled by marketers promoting their own products. remark v. 评论 n. 评论 remarkable adj. 卓越的 非凡的 辉煌的 landmark n. 地标 里程碑 转折点 adj. 有重大意义的 blot n. 污点 spot n. 点 stain n. 污渍 污迹 symbol n. 象征 标志 system n. 系统 体制 systematic adj. 有系统的 系统化的 systematically adv. 系统地 有条理地 Databases used by some companies don’t rely on data conllected systematically but rather lump together information from different research projects. regime n. 政权 政体 管理制度 organization n. 组织 机构 团体 structure n. 结构 构造 组织 stress v. 强调 着重 n. 压力 They show kone how to deal with setbacks, stresses and feelings of inadequacy. Because representative government presupposes an informed citizency, the report supports full literacy; stresses the study of history and government, particularly American history and American government; and encourages the use of new digital technologies. stressed-out 因心理紧张而被压垮的 highlight v. 强调 突出 emphasize v. 强调 starin v. 扭伤 不堪重负 n. 压力 负担 重负 UNIT 02peer n. 同龄人 同等地位的人 贵族 v. 仔细看 费力地看 contemporary n. 同代人 Several massive leakages of customer and employee data this year … have left managers hurriedly peering into their intricate IT systems and business processes in search of potential vulnerabilities. For less certain, however, is how successfully experts and bureaucrats can select our peer groups and steer their activities in virtuous direction. gaze at 凝视 注视 start at 盯 glare at 瞪着 怒视 issue v. 发行 发表 n. 问题 争端 报刊号 tissue n. 组织 面巾纸 claim v. 主张 要求 断言 n. 主张 要求 proclaim v. 宣布 明确表示 disclaim v. 放弃 否认 acclaim v. 向…欢呼 为…喝彩 At any rate, this change will ultimately be acclaimed by an ever-growing number of both domestic and international customers, regardless of how long the current consumer pattern will take hold. lay claim to 对…提出所有权要求 allege v. 断言 声称 contend v. 声称 主张 assert v. 声称 断言 patent v, 授予专利 adj. 专利的 n. 专利 Over the past decade, thousands of patents have been granted for what are called business methods. intellectual property 知识产权 court n. 法院 球场 庭院 courteous adj. 有礼貌的 line n. 行 路线 界限 队伍 v. 排队 hardline adj. 强硬的 underline v. 在…下划线 强调 突出 decline v. n. 下降 减少 衰退 online adj. 在线的 Only if the jobless arrive at the jobcentre with a CV, register for a online job search, and start looking for work will they be eligible for benefit - and then they should report weekly rather than fortnightly. offline adj. 离线的 coastline n. 海岸线 deadline n. 最后期限 截止日期 Assign responsibilities around the house and make sure homework deadlines are met. incline v. 倾向 倾斜 易于 baseline n. 基线 基准 airline n. 航线 航空公司 outline n. 轮廓 大纲 概要 Be flexible, Your outline should smoothly conduct you from one point to next, but do not permit it to railroad you. If a relevant and important idea occurs to you now, work it into the draft. lineage n. 血统 家系 家族 pipeline n. 管线 管道 in the pipeline 在筹备中 guideline n. 指导原则 in line with 和… 成直线 与…一致 按照 series n. 一连串 一系列 His first experiment, nearly 30 years ago, invilved momory: training a person to hear and then repeat a radom series iof numbers. row n. 排 行 boundary n. 边界 分界线 bound n. 界限 限制 route n. 线路 succession n. 继承 连续 procession n. 队伍 队列 value n. 价格 实用性 重要性 v. 评价 估计 重视 overvalue v. 对…估价过高 过分重视 It was banks that were on the wrong planet, with accounts that vastly overvalued assets. devalued adj. 贬值的 worth n. 价值 财产 significance n. 意义 重要性 view v. 观察 看 n. 观点 景色 眼界 interview n. 面试 采访 v. 采访 viewer n. 电视观众 观看者 指示器 review n. 评论 评审 v. 回顾 复习 reviewer n. 批评家 评论家 worldview n. 世界观 in view of 由于 考虑到 in view 在考虑中 在能看见的范围内 on view 在展出 在容易看见的地方 take sth. in view = take sth. into account 考虑 perspective n. 观点 思考方法 角度 individual adj. 个人的 单独的 独特的 n. 个人 个体 individually adv. 个别地 单独地 collective adj. 集体的 But it takes collective scrutiny and acceptance to transform a discovery claim into a mature discovery. economic adj. 经济的 经济学的 We have to suspect that continuing economic growth promotes the development of education even when government don’t force it. uneconomic adj. 不经济的 不赢利的 economics n. 经济学 economically adv. 节俭地 socioeconomic adj. 社会经济的 fiscal cliff 财政悬崖 IMF International Monetary Fund 国际货币基金组织 financial adj. 金融的 财政的 A smartphone may contain an arrestee’s reading history, financial history, medical history and comprehensive records of recent correspondence. fiscal adj. 财政的 国库的 monetary adj. 货币的 create v. 创造 创作 建立 creature n. 人 生物 动物 creative adj. 创造的 有创造力的 recreate v. 重现 再建 creativity n. 创造力 creation n. 创造物 物品 So it seems paradoxical to talk about habits in the same context as creativity and innovation. creationism n. 创世说 creationist n. 创始者 procreation n. 生育 生殖 In a society that so persistently celebrates procreations, is it any wonder that admitting you regret having children is equivalent to admitting you support kitten-killing? legal adj. 法律的 合法的 illegal adj. 非法的 违法的 n. 非法移民 lawful adj. 合法的 法定的 legitimate adj. 合法的 v. 使合法化 In effect, the White House claimed that it could invalidate any otherwise legitimate state law that it disagrees with. official adj. 官方的 法定的 n. 官员 行政人员 consider v. 认为 把…看作 consideration n. 体贴 关心 reconsider v. 重新 考虑 considerable adj. 相当多的 considering prep. 考虑到 就…而言 鉴于 consider…as… 把…当作… No shock there, considering how much work it is to raise a kid withour a partner to lean on. UNIT 03subject v. 使遭受 使服从 n. 主题 学科 adj. 受…支配 At the very least, the court should make itself subject to the code of conduct that applied to the rest of the federal judiciary. subjective adj. 主观的 be subject to 遭受 承受 In addition, the computer programs a company uses to estimate relationships may be patented and not subject to peer review or outside evaluation. lead v. 领导 带来 促进 n. 带领 Science and technology would cure all the ills of humanity, leading to lives of fulfillment and opportunity for all. leadership n. 领导能力 mislead v. 误导 leader n. 领导者 Research has found IQ predicted leadership skills when the tests were given under low-stress conditions, but under high-stress conditions, IQ was negatively correlated with leadership - that is, it predicted the opposite. plead v. 请求 提出…为理由 First, the object of our study pleads for definition. “Dare to be different, please don’t smoke!” pleads one billboard campaign aimed at reducing smoking among teenagers - teenagers, who desire nothing more than fitting in. leading adj. 卓越的 最重要的 However, many leading American universities want their undergraduates to have a grounding in the basic canon of ideas that every educated person should possess. lead someone on 误导某人 蒙骗某人 lead up to 导致 转向 为…作准备 take the lead 带头 领导 intend v. 想要 打算 企图 intend to do / inteng doing sth. sector n. 部门 But many within the public sector suffer under the current system, too. account v. 说明 解释 把…视作 n. 账目 账户 描述 解释 It is painful to read these roundabout accounts today. It was bank that were on the wrong planet, with accounts that vastly overvalued assets. Many, like the Fundamental Physics Prize, are funded from the telephone-number-sized banks accounts of Internet entrepreneurs. accountant n. 会计师 会计的 account for 占据 解释 说明 What might account for this strange phenomenon. take into account 考虑到 体谅 leave sth. out of account 不考虑 on account of 因为 由于 The need of training is too evident; the pressure to accomplish a change in their attitude and habits is too urgent to leave these consequency wholly out of account. description n. 描述 形容 explanation n. 解释 说明 consumer n. 顾客 消费者 These labels encourage style-conscious consumer to see clothes as disposable - meant to last only a wash or two, although they don’t advertise taht - and to renew their wardrobe every few weeks. consumption n. 消耗 消费 But despite some claim to the contrary, laughing probably has little influence on physical fitness. Laughter does produce short-term changes in the function of the heart and its blood vessels, boosting heart rate and oxygen consupmtion. But because hard laughter is difficult to sustain, a good laugh is unlikely to have manageable benefits the way, say, walking or jogging does. consuming adj. 消耗的 消费的 consumerism n. 消费主义 消费者权益保护 client n. 客户 顾客 委托人 But there are few places where clients have more grounds for complaint than American. environmental adj. 环境的 有关环境的 environmentally adv. 有关环境地 environmentallist n. 环境保护论者 surroundings n. 周边 周围环境 atmosphere n. 大气层 氛围 situation n. 状况 形势 essay n. 论文 article n. 文章 论文 matter n. 物质 v. 有关系 Anyone who has toiled through SAT will testify that test-taking skill also matters, whether it’s knowing when to guess or what questions to skip. as a matter of fact 其实 实际上 substance n. 物质 重要性 实质 stuff n. 材料 本质 要素 material n. 物质 材料 原料 fund v. 拨款 为… 提供资金 n. 专款 基金 资金 Many, like the Fundamental Physics Prize, are funded from the telephone-number-sized bank accounts of internet entrepreneurs. fundamental adj. 基本的 根本的 fundamentally adv. 从根本上地 基础地 The traditional rule was it’s safer to stay where you are, but that’s been fundamentally inverted, says one headhunter. underfund v. 对…资金提供不足 hold v. 保持 n. 船舱 Home prices are holding steady in most regions. household n. 家庭 一家人 同住在一所房子里 adj. 家庭的 家喻户晓的 众人皆知的 My salary barely covered her household expendes. holder n. 支持物 支持者 持有人 The Federal Circuit’s action comes in the wake of a series of recent decisions by the Supreme Court that has narrowed the scope of protections for patent holders. withhold v. 扣留 拒绝 给予 shareholder n. 股东 As boards scrutinize succession plans in response to shareholder pressure, executives who don’t get the node also may wish to move on. shareholding n. 股权 stockholder n. 股权持有者 股东 stakeholder n. 利益相关人 股东 Such hijacked media are the opposite of earned media: an asset or campaign becomes hostage to consumer, other stakeholders, or activists who make negative allegations about a brand or product. hold back 退缩 阻止 抑制 hold on 等一等 别挂电话 坚持 hold out 伸出 递出 hold up 举起 支持 拦截 cling v. 紧紧抓住 抱紧不放 withstand v. 经守住 承受 顶住 Moreover, even though humans have been upright for millions of years, our feet and back continnue to struggle with bipedal posture and cannot easily withstand repeated strain inflicted by over size limbs. seize v. 抓住 控制 function v. 起作用 n. 功能 作用 functional adj. 功能性的 运转正常的 work v. 使工作 employ v. 使用 利用 雇用 His function is analogous to that of a judge, who must accept the obligation of revealing in as abvious a manner as possible the course of reasoning which lead him to this dicision. evidence n. 证据 迹象 evident adj. 明显的 Even though there is plenty of evidence that the quality of the teachers is the most important variable, teachers’ unions have fought against getting rid of bad ones and promoting good ones. The need of training is too evident; the pressure to accomplish a change in their attitude and habits is too urgent to leave these consequences wholly out of account. proof n. 证明 confirmation n. 确认 证实 Little reward accompanies duplication and confirmation of what is already known and believed. practice v. / n. 练习 训练 in practice 在实践中 实际上 事实上 perform v. 执行 履行 But in the everyday practice of science, discovery frequently follows an ambiguous an complicated route. perform an impressive variety of interesting compositions. note v. 记下 n. 笔记 denote v. 代表 表示 northworthy 值得注意的 显著的 重要的 take / make notes 作笔记 If you were to examine in the birth certificates of every soccer player in 2006’s World Cup tournament you would most likely find a noteworthy quirk. UNIT 04degree n. 程度 读书 学位 登记 I struggled a lot to get the college degree. agreement n. 协定 一致 同意 There is the so-called big deal, where institutional subscribers pay for access to a collection of online journal titles through sitelicensing agreement. Greek n. 希腊人 希腊文 adj. 希腊的 希腊人的 Ancient Greek philosopher Aristotle viewed laughter as “a bodily exercise precious to health.” concern n. 关心 v. 涉及 Certainly, there are valid concerns about the patchwork regulations that could result if every state sets its own rules. As Nature has pointed out before, there are some legitimate concerns about how science prizes - both new and old - are distribute. concerned adj. 关心的 担心的 认为重要的 unconcerned adj. 不关心的 无忧虑的 concern oneslef with 关心 as / so far as … be concerned 就…来说 allow v. 允许 In the last decade or so, advances in technology have allowed mass-market labels such as Zara, H&amp;M, and Uniqlo to react to trends more quickly and anticipate demand more precisely. However, the Justices said that Arizona police would be allowed to verify the legal status of people who come in contact with law enforcement. shallow v. 变浅 adj. 浅的 allow for 考虑到 估计 允许有 product n. 产品 production n. 产品 作品 productivity n. 生产力 生产率 生产能力 invention n. 发明物 Only when humanity began to get its food in a more productive way was there time for other things. As education improved humanity’s productivity protential, they could in turn afford more education. level n. 水平 adj. 水平的 v. 弄平 low-level adj. 低水平的 次要的 At the state level their influence can be even more fearsome. Left, until now, to odd, low-level IT staff to put right，and seen as a concern only of data-rich industries such as banking, telecoms and air travel, information protection is now hight on the boss’s agenda in business of every variety. effort n. 努力 艰难的尝试 Though serveral fast-fashion companies have made efforts to curb their compact on labor and environment - including H&amp;M, with its green conscious collection line - cline believes lasting change can only be effected by the customer. effortless adj. 不费力气的 infer v. 推论 推断 inferiority n. 劣势 下等 次级 自卑感 inferior adj. 低等的 n. 部下 inference n. 推论 推断 deduce v. 推论 推断 conclude v. 得出结论 推断出 This seems a justification for neglect of those in need,and a rationalization of exploitation, of the superiority of those at the top and the inferiority of those at the bottom. professional n. 专业人士 adj. 职业的 If you then examined the European national youth teams that feed the World Cup and professional ranks, you would find this strange phenomenon to be even more pronounced. professionalize v. 使…职业化 Besides professionalizing the professions by this separation, top American universities have professionlized the professor. professionalization n. 职业化 professionlism n. 敬业精神 Professionlism has turned the acquisition of a doctoral degress into a prerequisite for a successful academic career: as late as 1696, a third of American professors did not possess one. provide v. 提供 provided conj. 倘若 假如 provider n. 提供者 供应者 provision n. 供应 条款 The program keep track of your progress and provides detailed feedback on your performance amd improvement. The same drematic technological changes that have provide marketers with more conmunications choices have also increased the risk that passionate consumers will voice thier opinions in quicker, more visible, and much more damaging ways. This trend, which we believe is still in its infancy, effectively began with retailers and travel providers such as airlines and hotels will no doubt go further. supply v. 供给 补充 offer v. 提供 给予 You are now not wanted; you are now exclued from the work environmrent that offers purpose and structure in your life. challenge n. / v. 挑战 怀疑 unchallended adj. 没有挑战的 没有异议的 deal n. 数量 v. 处理","categories":[{"name":"英语","slug":"英语","permalink":"https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"英语单词","slug":"英语单词","permalink":"https://wingowen.github.io/tags/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/"}]},{"title":"Python 编程","slug":"编程/Python-编程","date":"2022-08-01T03:17:02.000Z","updated":"2022-08-17T02:41:37.879Z","comments":true,"path":"2022/08/01/编程/Python-编程/","link":"","permalink":"https://wingowen.github.io/2022/08/01/%E7%BC%96%E7%A8%8B/Python-%E7%BC%96%E7%A8%8B/","excerpt":"Python 编程开发查漏补缺。 gRPC Redis","text":"Python 编程开发查漏补缺。 gRPC Redis GRPCGoogle 开发的基于 HTTP/2 和 Protocol Buffer 3 的 RPC 框架。 Protocol Buffers, protobuf：结构数据序列化机制。 gRPC 默认使用 Brotocol Buffers，用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。 定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 GRPC 服务器来处理客户端调用。在客户端拥有一个存根 Stub，存根负责接收本地方法调用，并将它们委派给各自的具体实现对象（在远程服务器上）。 简单实现实现一个简单的 gRPC HelloWorld。 proto file定义 Protocol Buffers 规则文件。 123456789101112131415161718syntax = &quot;proto3&quot;;package helloworld;service Greeter &#123; // 定义方法参数和返回类型 rpc SayHello (HelloRequest) returns (HelloResponse) &#123;&#125;&#125;// 请求结构声明message HelloRequest &#123; string name = 1;&#125;// 响应结构声明message HelloResponse &#123; string message = 1;&#125; 运行 grpc_tools 工具。 1python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto 生成 python 代码。 helloworld_pb2.py 为 Protocol Buffers 的 Python 实现。 1234567891011121314151617181920# helloworld_pb2_grpc.py 用于 gRPC 实现的 Python 方法实现# 客户端存根class GreeterStub(object): def __init__(self, channel): self.SayHello = channel.unary_unary( &#x27;/helloworld.Greeter/SayHello&#x27;, request_serializer=helloworld__pb2.HelloRequest.SerializeToString, response_deserializer=helloworld__pb2.HelloResponse.FromString, )# 服务端服务class GreeterServicer(object): def SayHello(self, request, context): context.set_code(grpc.StatusCode.UNIMPLEMENTED) context.set_details(&#x27;Method not implemented!&#x27;) raise NotImplementedError(&#x27;Method not implemented!&#x27;) def add_GreeterServicer_to_server(servicer, server): # ...... server自定义 gRPC 服务端。 1234567891011121314151617181920212223import grpcimport randomfrom concurrent import futuresimport helloworld_pb2import helloworld_pb2_grpc# 实现定义的方法，继承并实现方法class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloResponse(message=&#x27;Hello &#123;msg&#125;&#x27;.format(msg=request.name))def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) # 绑定处理器 helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) # 未使用 SSL，所以是不安全的 server.add_insecure_port(&#x27;[::]:50054&#x27;) server.start() print(&#x27;gRPC 服务端已开启，端口为 50054...&#x27;) server.wait_for_termination()if __name__ == &#x27;__main__&#x27;: serve() client自定义客户端。 1234567891011121314import grpcimport helloworld_pb2, helloworld_pb2_grpcdef run(): # 本次不使用 SSL，所以 channel 是不安全的 channel = grpc.insecure_channel(&#x27;localhost:50054&#x27;) # 客户端实例 stub = helloworld_pb2_grpc.GreeterStub(channel) # 调用服务端方法 response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#x27;World&#x27;)) print(&quot;Greeter client received: &quot; + response.message)if __name__ == &#x27;__main__&#x27;: run() RedisREmote DIctionary Server, Redis 是一个 key-value 存储系统，是跨平台的非关系型数据库。 123456789101112131415pip install redisimport redis # 导入redis 模块# 获取连接r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True) # Redis 实例会维护一个自己的连接池，建立连接池，从连接池获取连接pool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)r = redis.Redis(connection_pool=pool)r.set(&#x27;name&#x27;, &#x27;runoob&#x27;, nx, xx) # 设置 name 对应的值, 当 nx = Ture 则只有 Key 不存在才执行插入; xx 相反print(r[&#x27;name&#x27;])print(r.get(&#x27;name&#x27;), px, ex) # 取出键 name 对应的值, px 毫秒 ex 秒 为过期时间print(type(r.get(&#x27;name&#x27;))) # 查看类型 在使用中，Redis 存储可分为两大类： set 即 k v，这里的 v 通常是一个字符串。 hset 即 k Hash-v，这里的 v 是一个 Redis Hash，是一个 string 类型的 field（字段）和 value（值）的映射表。 缓存技术缓存就是利用编程技术将数据存储在临时位置，而不是每次都从源数据去检索。","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"}]},{"title":"决策树算法","slug":"算法/决策树算法","date":"2022-07-31T02:58:15.000Z","updated":"2022-08-10T15:03:29.129Z","comments":true,"path":"2022/07/31/算法/决策树算法/","link":"","permalink":"https://wingowen.github.io/2022/07/31/%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/","excerpt":"基本概念。","text":"基本概念。 决策树的基本概念决策树节点： 叶节点表示一份类别或者一个值； 非叶节点表示一个属性划分。 决策树的有向边代表了属性划分的不同取值： 当属性是离散时，可将属性的每一个取值用一条边连接到子结点； 当属性是连续时，需要特殊处理。 决策树是一种描述实例进行分类的树形结构。 对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。 决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略；决策树的学习过程本质上是从训练数据中寻找一组分类规则；决策树学习也可以看做是由训练数据集估计条件概率模型。 由上述描述可以得知，决策树学习是一个递归过程，有三种情况会导致递归返回： 当前结点包含的所有样本属于同一类别。 当前属性结合为空，或所有样本在所有属性上的取值都相同：将当前结点标记为叶结点，其类别为该节点包含的样本最多的类别。 当前结点包含的样本基本为空：将当前结点标记为叶结点，其类别为父节点包含样本最多的类别 决策树的学习结果为：树结构 + 叶节点的取值（类别） 信息增益熵，又称信息熵，是信息论的重要概念。熵是度量样本集合纯度的指标，熵越大，样本的纯度越低。假设当前样本集合 $D$ 中第 $i$ 类样本所占比例的为 $p_i(i = 1,2,…,C)$，则 $D$ 的熵定义为： H(D)=-\\sum_{i=1}^{C} p_{i} \\log _{2} p_{i}信息增益表示特征对于当前样本集纯度提升的程度。某属性的信息增益越大，说明使用改属性进行划分获得的纯度提升越大。因此使用信息增益进行决策树属性选择时，选择属性信息增益最大的作为当前节点。 G(D, a)=H(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} H\\left(D^{v}\\right)123456789101112131415161718192021222324252627282930# 信息增益计算def get_G(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data 中所有样本属于同一类，由于 xlnx 在 x = 1和 x-&gt;0 是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE return H - Eprint(&#x27;各个属性的信息增益为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_G(data,i,len(data.columns)-1)) 信息增益比以信息增益作为划分数据集的特征，会导致对可取值数目较多的属性有所偏好。为了缓解这种不良影响，采用信息增益比作为选择属性的准则。 123456789101112131415161718192021222324252627282930def get_GR(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益比 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益比, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 IV = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data中所有样本属于同一类，由于xlnx 在x = 1和x-&gt;0是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE IV = IV - new_data.shape[0] / data.shape[0] * (np.log2(new_data.shape[0] / data.shape[0])) G = H - E return G / IVprint(&#x27;各个属性的信息增益比为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_GR(data,i,len(data.columns)-1)) 基尼系数纯度使用基尼指数来度量，在使用基尼系数作为指标时，应该选择基尼指数最小的属性。 1234567891011121314151617181920212223def get_Gini(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的基尼系数 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的基尼系数, type:float, e.g.:0.32 &#x27;&#x27;&#x27; Gini = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] Gini_v = 1 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] Gini_v = Gini_v - p * p Gini = Gini + new_data.shape[0] / data.shape[0] * Gini_v return Giniprint(&#x27;各个属性的基尼指数为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_Gini(data,i,len(data.columns)-1)) ID3ID3 算法的核心是在决策树各结点上使用信息增益准则选择特征：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，根据特征的不同取值建立子结点。递归地调用以上方法，构建决策树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def build_tree_id3(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用 ID3 算法在 data 数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_G(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_id3(new_data, nu, new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)root_id3 = build_tree_id3(data, None, ori_ppt, len(data.columns)-1)# 可视化createPlot(root_id3) C4.5C4.5 算法对 ID3 算法进行了改进，即使用信息增益比来选择特征，其余和 ID3 算法基本相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def build_tree_c45(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用C4.5算法在data数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_GR(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_c45(new_data, nu,new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)# print(data)root_c45 = build_tree_c45(data, None ,ori_ppt, len(data.columns)-1)createPlot(root_c45) 损失函数与剪枝决策树的剪枝往往通过最小化决策树的损失函数实现。 123456789101112131415161718192021def cal_loss(root, alpha): &#x27;&#x27;&#x27; 计算以root为根结点的决策树的损失值 :param root: 根结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 以root为根结点的决策树的损失值, type:float, e.g.:0.24 &#x27;&#x27;&#x27; loss = 0 for leaf in root.leaf_ch: data = leaf.data for v in np.unique(data.iloc[:,len(data.columns)-1]): ntk = data[data.iloc[:,len(data.columns)-1] == v].shape[0] loss = loss - ntk * np.log2(ntk / data.shape[0]) loss = loss + alpha * len(root.leaf_ch) return lossori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)cal_loss(root_c45, 0.3) 决策树生成算法递归地产生决策树，直到无法继续。这种做法会带来过拟合问题。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，构建过于复杂的决策树。因此，一种解决方法是考虑决策树的复杂程度，从而对决策树进行简化。对决策树进行简化的过程称为剪枝。即从决策树中裁掉一些子树或叶结点，将其根节点或父节点作为新的叶结点。 剪枝算法的实现 计算每个节点的经验熵。 递归地从树的叶结点向上回缩，若回缩后的损失值 &gt; 回缩前的损失值，则进行剪枝，父节点变为叶节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def tree_pruning(root, leaf, alpha): &#x27;&#x27;&#x27; 决策树剪枝 :param root: 根结点, type:Node :param leaf: 叶结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 剪枝后的决策树根结点, type:Node &#x27;&#x27;&#x27; new_root = copy.deepcopy(root) pre_loss = cal_loss(root, alpha) flag = 1 for nl in leaf.fa.leaf_ch.copy(): nl.can_delete = 1 new_set = set() for leaf_ch in root.leaf_ch: if leaf_ch.can_delete != 1: new_set.add(leaf_ch) root.leaf_ch = new_set leaf.fa.set_leaf(leaf.fa.data.iloc[:,len(root.data.columns)-1].value_counts().keys()[0]) root.add_leaf_ch(leaf.fa) after_loss = cal_loss(root, alpha) if after_loss &gt;= pre_loss: #不剪枝 root = new_root flag = 0 return root, flag ori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)# createPlot(root_c45)#设置超参数alpha = 0.3update = 1while update == 1: update = 0 for leaf in root_c45.leaf_ch.copy(): root_c45,flag = tree_pruning(root_c45, leaf, alpha) if flag: update = 1# root_c45createPlot(root_c45) 连续值处理 缺失值处理","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算机组成","slug":"考研/计算机组成","date":"2022-07-30T07:15:43.000Z","updated":"2022-09-06T15:14:32.406Z","comments":true,"path":"2022/07/30/考研/计算机组成/","link":"","permalink":"https://wingowen.github.io/2022/07/30/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/","excerpt":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线","text":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线 计算机系统概论计算机系统层次结构 微程序机器 M0 微指令系统： 由硬件直接执行微命令； 实际机器 M1 机器语言机器：用微程序解释机器指令； 虚拟机器 M2 操作系统机器：用机器语言解释操作系统； 虚拟机器 M3 汇编语言机器：用汇编程序翻译成机器语言程序； 虚拟机器 M4 高级语言机器：用编译程序翻译成汇编语言程序或其它中间语言程序。 计算机的基本组成冯·诺依曼提出存储程序的概念，以此概念为基础的计算机通称为冯·诺依曼计算机，其具有如下特点： 计算机由运算器、存储器、控制器、输入设备和输出设备五大部件组成； 指令和数据以同地位存放于存储器内，可按地址寻访； 指令和数据均用二进制数表示； 指令由操作码和地址码组成，操作码用来表示操作的性质，地址码用来表示操作数在存储器中的位置； 指令在存储器内按顺序存放。通常，指令是顺序执行的，在特定条件下，可根据运算结果或根据设定的条件改变执行顺序。 机器以运算器为中心，输入输出设备与存储器间的数据传送通过运算器完成。 各部件的功能如下： 运算器用来完成算术运算和逻辑运算，并将运算的中间结果暂存在运算器内； 存储器用来存放数据和程序； 控制器用来控制、指挥程序的运行、程序的输入输出以及处理运算结果； 运算器和控制器在逻辑关系和电路结构上联系十分紧密，两大部件往往集成在同一芯片上，因此通常将它们合起来统称为中央处理器 CPU, Central Processing Unit。 现代计算机组成：CPU, I/O 以及 主存储器 Main Memory, MM。 CPU + MM 称为主机；I/O 有称为外部设备。 Arithmetic Logic Unit, ALU 算术逻辑部件，用来完成算术逻辑运算；Control Unit, CU 控制单元，用来解释存储器中的指令，并发出各种操作命令来执行指令。 ALU 和 CU 是 CPU 的核心部件； I/O 设备也受 CU控制，用来完成相应的输入、输出操作。 计算机的工作步骤TODO 系统总线计算机系统的五大部件之间的互连方式有两种： 各部件之间使用单独的连线，成为分散连接； 另一种是各部件连到一组公共信息传输线上，成为总线连接。 总线是连接多个部件的信息传输线，是各部件共享的传输介质。当多个部件与总线相连时，如果出现两个或两个以上部件同时向总线发送信息，势必导致信号冲突，传输无效。因此，在某一时刻，只允许有一个部件向总线发送信息，而多个部件可以同时从总线上接收相同信息。 以运算器为中心的结构。I/O 设备与主存交换信息时仍然要占用 CPU，因此还会影响 CPU 的工作效率。 单总线（系统总线）。I/O 设备与主存交换信息时，原则上不影响 CPU 的工作，CPU 仍可继续处理不访问主存或 I/O 设备的操作，提高了 CPU 的工作效率。 但当某一时刻各部件都要占用总线时，就会发生冲突，必须设置总线判优逻辑，让各部件按优先级高低来占用总线，这也会影响整机的工作速度。 以存储器为中心的双总线结构框图。存储中线只供主存与 CPU 之间传输信息，即提高了传输效率，又减轻了系统总线的负担，还保留了 I/O 设备与存储器交换信息不经过 CPU 的特点。 总线分类片内总线：芯片内部总线； 系统总线：各大部件之间的信息数据信息；按系统总线传输信息的不同，又可分为三类：数据总线、地址总线和控制总线。 通信总线：计算机系统之间或与其他系统之间的通信。 总线特性及性能指标TODO 总线结构TODO 总线控制判优控制（仲裁逻辑）和通信控制。 总线判优控制 主设备：对总线有控制选；从设备：只能响应从主设备发来的总线命令，对总线没有控制权。 若多个主设备同时要使用总线时，由总线的判优、仲裁逻辑按一定的优先等级顺序确定哪个主设备能使用总线，只有获得总线使用权的主设备才能开始传送数据。 总线判优控制可分集中式和分布式两种： 将控制逻辑集中在一处； 将控制逻辑分散在与总线连接的各个部件或设备上。 集中控制优先仲裁方式链式查询方式 控制总线中有 3 根线用于总线控制：BS 总线忙、BR 总线请求、BG 总线同意。其中 BG 是串行从一个 I/O 接口送到下一个 I/O 接口。如果 BG 到达的接口有总线请求 BR，BG 信号就不再往下传，意味着该接口获得了总线使用权，并建立总线忙 BS 信号，表示它占用了总线。 离总线控制部件最近的设备具有最高优先级。 只需很少几根线就能按一定有限次序实现总线控制，并且很容易扩充设备，但对电路故障很敏感，且优先级别低的设备可能很难获得请求。 计数器定时查询方式 与链式查询方式相比，多了一组设备地址线，少了一根总线同意线 BG。 总线控制部件接到由 BR 送来的总线请求信号后，在 BS=0 时，总线控制部件中的计数器开始计数，并通过设备地址线向各设备发出一组地址信号。当某个请求占用总线的设备地址与计数值一致时，便获得总线使用权，此时终止计数查询。 初始值可由程序设置；终止计数后可以重头开始，也可以从上一次计数终点开始。 对电路故障敏感度小于链式查询方式，但增加了控制线数（设备地址）目，控制也较复杂。 独立请求方式 每一台设备均有一对总线请求线和总线同意线。当设备要求使用总线时，便发出改设备的请求信号。总线控制部件中有一排队电路，可根据优先次序确定响应哪一台设备的请求。 响应快，优先次序控制灵活（根据程序改变），控制线数量多，总线控制更复杂。 通信控制通常将完成一次总线操作的时间称为总线周期，可分为以下 4 各阶段。 申请分配阶段：主模块提出申请，总线仲裁机构决定下一传数周期的总线使用权授予某一申请者； 寻址阶段：取得了使用权的主模块通过总线发出本次要访问的从模块的地址及有关命令。启动参与本次传数的从模块； 传数阶段：主模块与从模块进行数据交换，数据由源模块发出，经数据总线流入目的模块； 结束阶段：主模块的有关信息均从系统总线上撤除，让出总线使用权。 总线通信控制主要解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调如何配合。 同步通信 读命令：CPU 在 T1 上升沿发出地址信息；在 T2 的上升沿发出读命令；与地址信号相符合的输入设备按命令进行一系列内部操作，且必须在 T3 的上升沿到来之前将 CPU 所需数据送到数据总线上；CPU 在 T3 时钟周期内将数据上的信息传送到其内部寄存器；CPU 在 T4 上升沿撤销读命令，输入设备不再向数据总线上传送数据，撤销它对数据总线的驱动。 规定明确、统一，模板间配合简单一致。 缺点是主、从模块时间配合属于强制性“同步”，必须在限定时间内完成规定的要求。并且对所有从模块都用统一时限，各模块速度不同，必须以最慢速度的部件来设计公共时钟，严重影响总线工作效率，给设计带来局限性，缺乏灵活性。 异步通信 (a) 不互锁：主模块发出请求信号后，不必等待接到从模块的回答信号，而是经过一段时间，确认从模块已收到请求信号后，便撤销其请求信号；从模块接到请求信号后，在条件允许时发出回答信号，并且经过一段时间确认主模块已收到回答信号后，自动撤销回答信号。 (b) 半互锁：主模块发出请求信号，必须接待到从模块的回答信号后再撤销其请求信号，有旧互锁关系；而从模块接到请求信号后发出回答信号，但不必等待获知主模块的请求信号，而是隔一段时间后自动撤销其回答信号，无互锁关系。 (c) 全互锁：皆需获得回答信号后撤销。在网络通信中，通信双方采用的就是全互锁方式。 半同步通信 保留了同步通信的基本特点，同时又像异步通信那样允许不同速度的模块和谐地工作。 增设了一条等待响应信号线，采用插入等待周期的措施来协调通信双方的配合问题。 分离式通信 将一个传输周期（总线周期）分解为两个子周期，两个传输子周期都只有单方面的信息流，每个模块都变成了主模块。 存储器概述存储器分类 a) 按存储介质分类。 半导体存储器 由半导体器件组成，用超大规模集成电路工艺制成芯片，体积小、功耗低、存取时间短。当电源消失时，所存信息也随即像丢失，是一种易失性存储器。 非挥发性材料制成的半导体存储器，克服了信息易失的弊病。 双极型 TTL 半导体存储器：高速。 MOS 半导体存储器：高集成度，制造简单，成本低，故被广泛应用。 磁表面存储器 在金属或塑料基体的表面上涂一层磁性材料作为记录介质，工作时磁层随载磁体高速运转，用磁头在磁层上进行读、写操作。 用具有矩形磁滞回线特性的材料作次表面物质，按其剩磁状态的不同而区分 0 或 1，而且剩磁状态不会轻易丢失，故这类存储器具有非易失性的特点。 磁芯存储器 被半导体存储器取代。 光盘存储器 用激光在磁光材料上进行读、写的存储器，具有非易失性的特点。 记录密度高、耐用性好、可靠性高和可互换性强等特点。 b) 按存取方式分类 随机存储器 Random Access Memory RAM 是一种可读、写存储器，其特点是存储器的任何一个存储单元的内容都可以随机存取，且存取时间与存储单位的物理位置无关。计算机系统中的主存都采用这种随机存储器。 静态 RAM，以触发器原理寄存信息。 动态 RAM，以电容充放电原理寄存信息。 只读存储器 Read Only Memory ROM 掩模型只读存储器 Masked ROM，MROM；可编程只读存储器 Programmable ROM，PROM；可擦除可编程只读存储器 Erasable Programmable ROM，EPROM；用电可擦除可编程只读存储器 Electrically Erasable Programmable ROM， EEPROM；Flash Memory。 串行访问存储器 对存储单元进行读写操作时，需按其物理位置的先后顺序寻找地址，则这种存储器称为串行访问存储器。 由于信息所在位置不同，读写时间均不相同。 c) 按在计算机中的作用分类 主存储器 可以和 CPU 直接交换信息。 速度快、容量小、每位价格高。 辅助存储器 是主存储器的后援存储器，用来存放当时暂时不用的程序和书，不能与 CPU 直接交换信息。 速度慢、容量大、每位价格低。 缓冲存储器 用在两个速度不同的部件之中。 层次结构 由上至下，位价越来越低，速度越来越慢，容量越来越大。 寄存器中的数直接在 CPU 内部参与运算。 主存用来存放将要参与运行的程序和数据，其速度与 CPU 速度差距较大，为使它们匹配，在主存与 CPU 之间插入了一种比主存速度更快、容量跟更小的高速缓冲存储器 Cache。 寄存器、缓存、主存这三类存储器都是由速度不同、位价不等的半导体存储材料制成的，它们都设在主机内。 磁盘、磁带属于辅助存储器，其容量比主存大得多，大都用来存放暂时未用到的程序和数据文件。 CPU 不能直接访问辅存，辅存只能与主存交换信息，因此辅存的速度可以比主存慢很多。 缓存 - 主存层次主要解决 CPU 与主存速度不匹配的问题。 主存 - 辅存层次主要解决存储系统的容量问题。形成了虚拟存储系统，在这个系统中，程序员变成的地址范围与虚拟存储器的地址空间相对应。 程序员编程时，可用的地址空间远远大于主存空间，使程序员以为自己占有一个容量极大的主存（虚拟存储器）。其逻辑地址转变为物理地址的工作由计算机系统的硬件和操作系统自动完成的，对程序员是透明的。 当虚地址的内容在主存时，机器便可立即使用；若虚地址的内容不在主存，则必须先将此虚地址内容传递到主存的合适单元后再为机器所用。 主存储器主存的基本组成。 根据 MAR 储存器地址寄存器中的地址访问某个存储单元时，还需要经过地址译码、驱动等电路，才能找到所需要访问的单元。 读出时，需经过读出放大器，才能将被选中单元的存储字送到 MDR 主存数据寄存器。 写入时，MDR 中的数据也必须经过写入电路才能真正写入到被选中的单元中。 现代计算机的主存都由半导体集成电路构成，驱动器、译码器和读写电路均制作在存储芯片中，而 MAR 和 MDR 制作在 CPU 芯片中。存储芯片和 CPU 芯片可通过总线连接。 当要从存储器读出来某一信息字时，首先由 CPU 将该字的地址送到 MAR，经地址总线送至主存，然后发出读命令，主存接到读命令后将该单元内容读至数据总线上。 若要向主存存入一个信息字时，首先 CPU 将该字所在主存单元的地址经 MAR 送到地址总线，并将信息字送入 MDR，然后向主存发出写命令，主存接到写命令后，便将数据线上的信息写入到对应地址线指出的主存单元中。","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}]},{"title":"模型评估与选择","slug":"算法/模型评估与选择","date":"2022-07-29T10:24:58.000Z","updated":"2022-08-02T01:19:57.843Z","comments":true,"path":"2022/07/29/算法/模型评估与选择/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。","text":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。 错误率和精度123456789101112131415161718import numpy as np# 真实的数据标签real_label = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]) \\ \\ \\# 分类器的预测标签classifier_pred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])compare_result = (real_label == classifier_pred)compare_result = compare_result.astype(np.int)# m 为样本数量 b 为预测错误样本m = len(real)b = m - np.sum(cmp)# 错误率error_rate = (b / m)*100# 精确度 accaccuracy = (1 - b / m)*100 误差模型在训练样本上的误差称为训练误差或经验误差；模型在新样本上的误差称为泛化误差。 过拟合模型：虽然训练误差接近 0，泛化误差非常大。 欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。 偏差和方差偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。 偏差 bias，与真实值的偏离程度； 方差 variance，该随机变量在其期望值附近的波动程度。 评估方法评估：对学习器的泛化误差进行评估并进而做出选择。 留出法以一定比例划分训练集和测试集。 1234567891011121314151617181920212223# 导入包import numpy as npfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): &#x27;&#x27;&#x27; return: 返回随机生成 200 个点的坐标 &#x27;&#x27;&#x27; dots = 200 # 样本数 dim = 2 # 数据维度 X = np.random.randn(dots,dim) # 建立数据集，shape(200,2) # 建立样本 X 的类别 Y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): Y[i] = 1 return X, Y# 加载数据X,Y = load_pts()# 使用train_test_split划分训练集和测试集train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0) 交叉验证交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。 12345678910111213# 导入包from sklearn.model_selection import KFoldimport numpy as np# 生成数据集，随机生成40个点data = np.random.randn(40,2)# 交叉验证法kf = KFold(n_splits = 4, shuffle = False, random_state = None) for train, test in kf.split(data): print(train) print(test,&#x27;\\n&#x27;) 自助法有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ： 每次随机从 D 中挑选一个样本； 将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中； 重复执行 m 次该过程； 最后得到包含 m 个样本数据集 D’。 由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。 123456789101112131415161718# 导入包import numpy as np# 任意设置一个数据集X = [1,4,3,23,4,6,7,8,9,45,67,89,34,54,76,98,43,52]# 通过产生的随机数获得抽取样本的序号 bootstrapping = []for i in range(len(X)): bootstrapping.append(np.random.randint(0,len(X),(1)))# 通过序号获得原始数据集中的数据D_1 = []for i in range(len(X)): print(int(bootstrapping[i])) D_1.append(X[int(bootstrapping[i])]) print(D_1) 总结 采样方法 与原始数据集的分布是否相同 相比原始数据集的容量 是否适用小数据集 是否适用大数据集 是否存在估计偏差 留出法 分层抽样 否 变小 否 是 是 交叉验证法 分层抽样 否 变小 否 是 是 自助法 放回抽样 否 不变 是 否 是 性能度量性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。 性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。 回归任务常用性能度量：MSE mean square error，均方差。 分类任务常用性能度量：acc accuracy，精度；错误率 查准率、查全率、F1对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为： 真正例 true positive 假正例 false positive 真反例 true negative 假反例 false negative P(\\text { Precision })=\\frac{T P}{T P+F P} \\\\R(\\text { Recall })=\\frac{T P}{T P+F N}Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。 Precision，查准率：在预测为正的样本中，实际为正的概率。 P-R 曲线，BRP，Break Even Point：平衡单 P = R。 由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。BRP 还是过于简单，更常用的是 F1 度量。 F 1=\\frac{2 \\times P \\times R}{P+R}=\\frac{2 T P}{n+T P-T N}F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。 当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。 F_{\\beta}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R}β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。 如果使用了类似交叉验证法，我们会得到多个 confusion matrix： 宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1； 微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as np# 加载数据集def generate_data(random_state=2021): &quot;&quot;&quot; :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] &quot;&quot;&quot; noise_rate = 0.1 # 噪声比例 sample_num = 4096 # 总样本数 noise_sample_num = int(sample_num*noise_rate) # 噪声样本数 np.random.seed(random_state) Pred_Score = np.random.uniform(0,1,sample_num) GT_label = (Pred_Score&gt;0.5).astype(np.int) noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=False, p=None) for index in noise_ids: GT_label[index] = 1 if GT_label[index] == 0 else 0 return GT_label, Pred_ScoreGT_label, Pred_Score = generate_data()# 请你补全以下代码，计算查准率与查全率def get_PR(GT_label, Pred_Score, threshold, random_state=2021): &quot;&quot;&quot; 计算错误率和精度 :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] :threshold: 评估阈值 :random_state: 随机种子 :返回值: P: 查准率 R: 查全率 &quot;&quot;&quot; Pred_Label = list(map(lambda x: 1 if x &gt; threshold else 0, Pred_Score)) from sklearn.metrics import precision_score, recall_score P = precision_score(GT_label, Pred_Label) R = recall_score(GT_label, Pred_Label) &quot;&quot;&quot; TODO &quot;&quot;&quot; return P, R P, R = get_PR(GT_label, Pred_Score, 0.55, random_state=2021)print(&quot;查准率P ：&#123;:.2f&#125;&quot;.format(P))print(&quot;查全率R ：&#123;:.2f&#125;&quot;.format(R)) ROC 与 AUC 原理ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。 \\begin{aligned}T P R(\\text { Precision }) &=\\frac{T P}{T P+F N} \\\\F P R(\\text { Precision }) &=\\frac{F P}{F P+T N}\\end{aligned}TPR 真正率，真正样本与实际为正的样本的比率； FPR 假正率，加正样本与实际为负的样本的比率。 若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两 个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC Area Under ROC Curve。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): dots = 200 # 点数 X = np.random.randn(dots,2) * 15 # 建立数据集，shape(200,2)，坐标放大15倍 # 建立 X 的类别 y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): if X[i,0] &gt; -15 and X[i,0] &lt; 15 and X[i,1] &gt; -15 and X[i,1] &lt; 15: # 矩形框内的样本都是目标类（正例） y[i] = 1 if 0 == np.random.randint(i+1) % 10: # 对数据随机地插入错误，20 个左右 y[i] = 1 - y[i] # 数据集可视化 plt.scatter(X[np.argwhere(y==0).flatten(),0], X[np.argwhere(y==0).flatten(),1],s = 20, color = &#x27;blue&#x27;, edgecolor = &#x27;k&#x27;) plt.scatter(X[np.argwhere(y==1).flatten(),0], X[np.argwhere(y==1).flatten(),1],s = 20, color = &#x27;red&#x27;, edgecolor = &#x27;k&#x27;) plt.xlim(-40,40) plt.ylim(-40,40) plt.grid(False) plt.tick_params( axis=&#x27;x&#x27;, which=&#x27;both&#x27;, bottom=False, top=False) return X, yX, y = load_pts()plt.show()### 训练模型 ###from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.svm import SVC# 将数据集拆分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) # 建立模型 clf1 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=4,min_samples_split=4)clf2 = GradientBoostingClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=10)clf3 = SVC(kernel=&#x27;rbf&#x27;, gamma=0.001, probability=True)# 训练模型clf1.fit(X_train, y_train)clf2.fit(X_train, y_train)clf3.fit(X_train, y_train)### 评估模型 ###from sklearn.metrics import roc_curve# 模型预测y_score1 = clf1.predict_proba(X_test)y_score2 = clf2.predict_proba(X_test)y_score3 = clf3.predict_proba(X_test)# 获得 FPR、TPR 值fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,1])fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,1])fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,1])### 绘制 ROC 曲线 ###from sklearn.metrics import aucplt.figure()# 绘制 ROC 函数def plot_roc_curve(fpr, tpr, c, name): lw = 2 roc_auc = auc(fpr,tpr) plt.plot(fpr, tpr, color=c,lw=lw, label= name +&#x27; (area = %0.2f)&#x27; % roc_auc) plt.plot([0,1], [0,1], color=&#x27;navy&#x27;, lw=lw, linestyle=&#x27;--&#x27;) plt.xlim([0, 1.0]) plt.ylim([0, 1.05]) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.ylabel(&#x27;True Positive Rate&#x27;) #plt.title(&#x27;&#x27;) plt.legend(loc=&quot;lower right&quot;) plot_roc_curve(fpr1, tpr1, &#x27;red&#x27;,&#x27;DecisionTreeClassifier &#x27;) plot_roc_curve(fpr2, tpr2, &#x27;navy&#x27;,&#x27;GradientBoostingClassifier &#x27;) plot_roc_curve(fpr3, tpr3, &#x27;green&#x27;,&#x27;SVC &#x27;) plt.show() 比较检验（TODO）模型性能比较的重要因素： 实验评估得到的性能不等于泛化性能； 测试集上的性能与测试集本身的选择有很大关系； 很多机器学习算法本身有一定的随机性。 统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。 假设检验由样本推测总体的方法。 交叉验证 t 检验McNemar 检验Friedman 检验与 Nemenyi 后续检验类别不平衡在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。 如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。 解决不平衡分类问题的策略可以分为两大类： 从数据层面入手 , 通过改变训练集样本分布降低不平衡程度； 从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。 数据层面解决类别不平衡扩大数据样本。 重采样：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。 过采样：复制稀有样本； 123456789101112131415161718192021222324252627282930313233# 导入包from sklearn.datasets import make_classificationfrom collections import Counterfrom imblearn.over_sampling import RandomOverSampler# 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)# 打印每个类别样本数print(Counter(y))# 过采样ros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)# 打印过采样后每个类别样本数print(sorted(Counter(y_resampled).items()))# 生成新的稀有样本# 导入包from imblearn.over_sampling import SMOTE# 过采样sm = SMOTE(random_state=42)X_res, y_res = sm.fit_resample(X, y)# 打印过采样后每个类别样本数print(&#x27;Resampled dataset shape %s&#x27; % Counter(y_res)) 欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。 123456789# 导入包from imblearn.under_sampling import RandomUnderSampler# 欠采样rus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)# 打印欠采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。 因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。 12345678# 导入包from imblearn.combine import SMOTEENN# 过采样与欠采样结合smote_enn = SMOTEENN(random_state=0)X_resampled, y_resampled = smote_enn.fit_resample(X, y)# 打印采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 算法层面解决类别不平衡惩罚项方法：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，错分稀有类的样本需要付出更大的代价。 通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。 例如，调整 SVM 以惩罚稀有类别的错误分类。 1234567# LABEL 0 4000# LABEL 1 200# 导入相关包from sklearn.svm import SVC# 添加惩罚项clf = SVC(C=0.8, probability=True, class_weight=&#123;0:0.25, 1:0.75&#125;) 特征选择方法 样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。 选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"贝叶斯算法","slug":"算法/贝叶斯算法","date":"2022-07-29T10:24:58.000Z","updated":"2022-08-02T01:27:32.849Z","comments":true,"path":"2022/07/29/算法/贝叶斯算法/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/","excerpt":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。","text":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。 条件概率P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率： P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率： P(B \\mid A)=\\frac{P(A \\cap B)}{P(A)}那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出贝叶斯公式： P(A \\mid B)=\\frac{P(B \\mid A) \\times P(A)}{P(B)}{\\scriptsize }贝叶斯基础思想： 已知类条件概率密度参数表达式和先验概率； 利用贝叶斯公式转换成后验概率； 根据后验概率大小进行决策分类。 根据以上基本思想，可以得到贝叶斯概率计算公式表达为：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）。 优点： 贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断； 它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信； 如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了； 它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。 缺点： 它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出； 有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。 扩展阅读： 一文读懂概率论学习：贝叶斯理论 贝叶斯决策论&amp;朴素贝叶斯算法 朴素贝叶斯法讲解 sklearn 贝叶斯方法 贝叶斯推断：广告邮件自动识别的代码实现若邮件包含某个关键词，求此邮件是广告的概率。 12345678910111213141516171819# 广告邮件数量ad_number = 4000# 正常邮件数量normal_number = 6000# 所有广告邮件中，出现 “红包” 关键词的邮件的数量ad_hongbao_number = 1000# 所有正常邮件中，出现 “红包” 关键词的邮件的数量normal_hongbao_number = 6# 广告的先验概率 P(A)P_ad = ad_number / (ad_number + normal_number)# 包含红包的先验概率 P(B)P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)# 广告 包含红包的似然概率 P(B|A)P_hongbao_ad = ad_hongbao_number / ad_number# 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbaoprint(P_ad_hongbao) 10.9940357852882705 极大似然估计极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。 极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 极大似然估计与贝叶斯推断是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。 极大似然估计的简单计算一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。 \\begin{array}{c} \\frac{d}{d p}\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{61}(1-p)^{39}=\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right)\\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\\right) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61-100 p) \\\\ =0 \\end{array}当 $P = \\frac{61}{100}, 0$ 时，导数为零。因为 1 &lt; P &lt; 0，所以 $P = \\frac{61}{100}$。 极大似然估计的简单应用求极大似然估计 MLE 的一般步骤： 由总体分布导出样本的联合概率函数（或联合密度）； 把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 $θ$ 看作自变量，得到似然函数 $l(θ)$； 求似然函数 $l(θ)$ 的最大值点，常常转化为求 $lnl(θ)$ 的最大值点，即 $θ$ 的 MLE； 在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。 若随机变量 $x$ 服从一个数学期望为 $μ$、方差为 $σ^2$ 的正态分布，记为 $N(μ,σ^2)$，假设 $μ=30, σ=2$。 1234567891011import numpy as npfrom scipy.stats import normimport matplotlib.pyplot as pltμ = 30 # 数学期望σ = 2 # 方差x = μ + σ * np.random.randn(10000) # 正态分布plt.hist(x, bins=100) # 直方图显示plt.show()print(norm.fit(x)) # 返回极大似然估计，估计出参数约为 30 和 2 朴素贝叶斯分类器朴素贝叶斯分类器是一系列假设特征之间强（朴素）独立条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。 MNIST 手写体数字识别123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import warningswarnings.filterwarnings(&quot;ignore&quot;)# numpy 库import numpy as np# tensorflow 库中的 mnist 数据集import tensorflow as tfmnist = tf.keras.datasets.mnist# sklearn 库中的 BernoulliNBfrom sklearn.naive_bayes import BernoulliNB# 绘图工具库 pltimport matplotlib.pyplot as pltprint(&quot;读取数据中 ...&quot;)# 载入数据(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量train_images = train_images.reshape(len(train_images),784)test_images = test_images.reshape(len(test_images),784)print(&#x27;读取完毕!&#x27;)def plot_images(imgs): &quot;&quot;&quot;绘制几个样本图片 :param show: 是否显示绘图 :return: &quot;&quot;&quot; sample_num = min(9, len(imgs)) img_figure = plt.figure(1) img_figure.set_figwidth(5) img_figure.set_figheight(5) for index in range(0, sample_num): ax = plt.subplot(3, 3, index + 1) ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;) ax.grid(False) plt.margins(0, 0) plt.show()plot_images(train_images)print(&quot;初始化并训练贝叶斯模型...&quot;)# 定义 朴素贝叶斯模型classifier_BNB = BernoulliNB()# 训练模型classifier_BNB.fit(train_images,train_labels)print(&#x27;训练完成!&#x27;)print(&quot;测试训练好的贝叶斯模型...&quot;)# 分类器在测试集上的预测值test_predict_BNB = classifier_BNB.predict(test_images)print(&quot;预测完成!&quot;)# 计算准确率accuracy = classifier_BNB.score(test_images, test_labels)print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy) 对结果进行统计比较分析。 12345678910111213141516171819202122# 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 class_num = &#123;&#125;# 每个类别预测为 0-9 类别的个数，predict_num = []# 每个类别预测的准确率class_accuracy = &#123;&#125;for i in range(10): # 找到类别是 i 的下标 class_is_i_index = np.where(test_labels == i)[0] # 统计类别是 i 的个数 class_num[i] = len(class_is_i_index) # 统计类别 i 预测为 0-9 各个类别的个数 predict_num.append( [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)]) # 统计类别 i 预测的准确率 class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100 print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % ( i, class_num[i], predict_num[i][i], class_accuracy[i])) 用热力图对结果进行分析。 123456789101112import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) np.random.seed(0)uniform_data = predict_numax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)ax.set_xlabel(&#x27;真实值&#x27;)ax.set_ylabel(&#x27;预测值&#x27;)plt.show()","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"考研","slug":"考研/考研","date":"2022-04-21T06:19:23.000Z","updated":"2022-08-03T05:41:56.366Z","comments":true,"path":"2022/04/21/考研/考研/","link":"","permalink":"https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/%E8%80%83%E7%A0%94/","excerpt":"考研院校信息整理汇总。","text":"考研院校信息整理汇总。 报考专业 深大 - 人工智能与金融科技 初试科目 101 思想政治理论 201 英语一 301 数学一 408 计算机学科专业基础综合 复试科目 FSX8 机器学习 计算机考研 408 包括（150） 数据结构 45 计算机组成原理 45 操作系统 35 计算机网络 25","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]}],"categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"日常","slug":"日常","permalink":"https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"},{"name":"英语","slug":"英语","permalink":"https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"},{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"网络相关","slug":"网络相关","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"},{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"},{"name":"网站收集","slug":"网站收集","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"},{"name":"英语演讲","slug":"英语演讲","permalink":"https://wingowen.github.io/tags/%E8%8B%B1%E8%AF%AD%E6%BC%94%E8%AE%B2/"},{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"英语单词","slug":"英语单词","permalink":"https://wingowen.github.io/tags/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/"},{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]}