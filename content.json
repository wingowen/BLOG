{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"https://wingowen.github.io","root":"/"},"pages":[],"posts":[{"title":"考研","slug":"考研","date":"2022-04-21T06:19:23.000Z","updated":"2022-04-21T06:20:44.732Z","comments":true,"path":"2022/04/21/考研/","link":"","permalink":"https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/","excerpt":"","text":"报考专业 ​ 深大 - 人工智能与金融科技 初试科目 ​ [101]思想政治理论 ​ [201]英语一 ​ [301]数学一 ​ [408]计算机学科专业基础综合 复试科目 ​ [FSX8]机器学习 计算机考研 408 包括（150） ​ 数据结构 45 ​ 计算机组成原理 45 ​ 操作系统 35 ​ 计算机网络 25","categories":[],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"Python","slug":"Python","date":"2022-04-21T03:30:00.000Z","updated":"2022-04-21T05:52:32.704Z","comments":true,"path":"2022/04/21/Python/","link":"","permalink":"https://wingowen.github.io/2022/04/21/Python/","excerpt":"","text":"记录常用的 Python Packages。 PandasindexDataFrame 导出为 CSV 时默认 index&#x3D;True 会把 index 列作为一列数据输出到 CSV 文件中，此时 index 这一信息就消失了。 而工具包在处理 DataFrame 数据并不把 index 作为数据列处理。","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"}]},{"title":"Kubernetes","slug":"Kubernetes","date":"2022-04-15T01:06:13.000Z","updated":"2022-04-15T01:06:13.068Z","comments":true,"path":"2022/04/15/Kubernetes/","link":"","permalink":"https://wingowen.github.io/2022/04/15/Kubernetes/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Docker","slug":"Docker","date":"2022-04-07T08:45:18.000Z","updated":"2022-04-11T05:52:34.775Z","comments":true,"path":"2022/04/07/Docker/","link":"","permalink":"https://wingowen.github.io/2022/04/07/Docker/","excerpt":"","text":"以下 docker-compose 的版本若未特殊说明，皆为 3.0。 网络12345678# 默认创 bridge 网络docker network create default_network# 查看网络内部信息docker network inspect default_network# 查看所有网络docker network ls# 移除指定的网络docker network rm default_network 1234networks: default: external: name: self-define-network 资源Docker 的容器资源限制，CPU 按百分比进行限制，使用 docker stats [container] 查看容器的 CPU 使用率，此使用率显示的是占用宿主机的百分比。 1234567# CPU 打满的脚本#!/bin/bashwhile true;do openssl speed;done 12345deploy: resources: limits: cpus: 0.1 memory: 16G 打包123docker run -d [image] /bin/bashdocker commit [container]","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wingowen.github.io/tags/Docker/"}]},{"title":"BigData","slug":"BigData","date":"2022-04-06T10:23:01.000Z","updated":"2022-04-15T01:06:33.542Z","comments":true,"path":"2022/04/06/BigData/","link":"","permalink":"https://wingowen.github.io/2022/04/06/BigData/","excerpt":"","text":"Spark一个较为广泛的定义是：Spark 是一个类 Hadoop MapReduce 的通用并行框架，专为大规模数据处理而设计的快速通用的大数据引擎及轻量级的大数据处理统一平台。 发展到现在，Spark 这个词已经不仅仅只代表 Spark 了，也代表着 Spark 生态。 Spark SQL 提供 HiveQL ( Apache Hive 的 SQL 变体，Hive 查询语言 ) 与 Spark 进行交互的 API；每个数据库表被当做一个 RDD，Spark SQL 被转换为 Spark 操作。 Spark Steaming 对实时数据流进行处理和控制，允许程序能够像处理普通 RDD 数据一样处理实时数据。 MapReduce VS. Spark： MapReduce 只提供 Map 和 Reduce 两个操作，复杂的计算需要大量的 Job 才能完成；其中间结果放置在 HDFS 文件系统中，迭代计算的效率很低；适用 Batch 数据处理，对交互式、实时数据处理支持不够；开发需要写大量的底层代码。 Spark 提供了丰富的算子；中间结果存储在内存中。 由于 MapReduce 每一步操作的结果都会被存入磁盘中，故在计算出现错时可以很好的从磁盘中恢复；Spark 则需要根据 RDD 中的信息进行数据的重新计算，会耗费一定的资源。 Spark 故障恢复的两种方式： 通过数据的血缘关系再执行一遍前面的处理。 Checkpoint 将数据存储到持久化储存中。 Spark 运行方式： 在 Spark 集群中由一个节点作为 driver 端创建 SparkContext。Spark 应用程序的入口负责调度各个运算资源，协调各个 WorkerNode上 的 Executor。根据用户输入的参数会产生若干个 worker，worker 节点运行若干个 executor，一个 executor 是一个进程，运行各自的 task，每个 task 执行相同的代码段处理不同的数据。 DAG Scheduler 划分作业，依次提交 stage 对应的 taskSet 给 task 作业调度器 TaskScheduleImpl，Task 作业调度器 submite taskSet 给 driver 端的 CoarseGrainedExecutorBackend ( 与 Executor 通信的 )，CoarseGrainedExecutorBackend 接收到 task 提交 even 后，会调用 Executor 执行 task，最终 task 是在 TaskRunner 的 run 方法内运行。 Spark 根据 RDD 之间的不同点依赖关系切分成不同的阶段（Stage）；没有依赖关系的 Stage 是可以并行执行的，但是对于 job，Spark是串行执行的，如果想要并行执行 Job，可以在 Spark 程序中进行多线程编程。 在这个 DAG 图中，Spark 能够充分了解数据之间的血缘关系，这样某些任务失败后可以根据血缘关系重新执行计算获取失败了的 RDD。 宽依赖和窄依赖 窄依赖指父 RDD 的每个分区只被子 RDD 的一个分区所使用。 宽依赖指父 RDD 的每个分区可能被多个子 RDD 的分区所使用。 Spark 的资源管理架构： Master 是 Spark 的 主控节点，在实际的生产环境中会有多个 Master，只有一个 Master 处于 active 状态。 Worker 是 Spark 的工作节点，向 Master 汇报自身的资源、Executor 执行状态的改变，并接受 Master 的命令启动 Executor 或 Driver。 Driver 是应用程序的驱动程序，每个应用包括许多小任务，Driver 负责推动这些小任务的有序执行。 Executor 是 Spark 的工作进程，由 Worker 监管，负责具体任务的执行。 在 Spark 和 Yarn 两边的角色对比中：Master 和 ResourceManager 对应，Worker 和 NodeManager 对应，Driver 和 AppMaster 对应，Executor 和 Container 对应。架构相似，因此 Spark 很容易构建在 Yarn 之上。 部署模式： Local 模式：部署在同一个进程上，只有 Driver 角色。接受任务后创建 Driver 负责应用的调度执行，不涉及 Master 和 Worker； Local-Cluster 模式：部署在同一个进程上，存在 Master 和 Worker 角色，它们作为独立线程存在于这个进程内； Standalone 模式：Spark 真正的集群模式，在这个模式下 Master 和 Worker 是独立的进程； 第三方部署模式：构建于 Yarn 或 Mesos 之上，由它们提供资源管理。 Spark on YarnSpark on Yarn 对 Job 的处理过程： 客户端提交一个任务给 Yarn ResourceManager 后，AppManager 接受任务并找到一个 Container 创建 AppMaster，此时 AppMaster 上运行的是 Spark Driver。之后 AppMaster 申请 Container 并启动，Spark Driver 在 Container 上启动 Spark Executor，并调度 Spark Task 在 Spark Executor 上运行，等到所有的任务执行完毕后，向 AppManager 取消注册并释放资源。 Spark on Yarn-Client： 客户端在提交完任务之后不会将 Spark Driver 托管给 Yarn，而是在客户端运行。AppMaster 申请完 Container 之后同样也是由 Spark Driver 去启动 Spark Executor，执行任务。 YarnYarn，Yet Another Resource Negotiator，是一个工作调度集群资源管理框架。 在 Yarn 问世前，Hadoop 使用 JobTracker 进行资源管理和作业调度。存在以下瓶颈：JobTracker 同时部署多个时只有一个是处于 active 状态；应用程序相关和资源管理相关的逻辑全部放在 JobTracker 中；MapReduce 计算模型与 JobTracker 的耦合过高。 Yarn 采用 Master &#x2F; Slave 结构，整体采用双层调度架构：第一层调度是 ResourceManager 和 NodeManager。 ResourceManager 包含 Scheduler 和 AppManager 两个组件，分管资源调度和应用管理。（进行拆分，粒度比 JobTracker 更细） NodeManager 可部署在独立机器上，用于管理机器上的资源。 第二层调度指的是 NodeManager 和 Container，NodeManger 会将资源抽象成一个个 Container 并管理它们的生命周期。 Yarn 运作流程： 客户端向 ResourceManager 的 AppManager 提交应用并请求一个 AppMaster 实例； ResourceManager 找到可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 AppMaster 实例； AppMaster 向 ResourceManager 注册，注册之后，客户端就可以查询 ResourceManager 获得自己 AppMaster 的详情以及直接和 App Master 交互； 接着 AppMaster 向 Resource Manager 请求资源，即 Container； 获得 Container 后，AppMaster 启动 Container，并执行 Task； Container 执行过程中会把运行进度和状态等信息发送给 AppMaster； 客户端主动和 App Master 交流应用的运行状态、进度更新等信息； 所有工作完成 App Master 向 RM 取消注册然后关闭，同时所有的 Container 也归还给系统。 从以上流程可以了解到，AppMaster 是作为 Job 的驱动角色，它驱动了 Job 任务的调度执行。在这个运作流程中，AppManager 只需要管理 AppMaster 的生命周期以及保存它的内部状态，而 AppMaster 这个角色的抽象使得每种类型的应用都可以定制自己的 AppMaster，这样其他的计算模型就可以相对容易地运行在 Yarn 集群上。 Yarn HA假如 Container 故障 Resource Manager 可以分配其他的 Container 继续执行，当运行 AppMaster 的 Container 故障后也将分配新的 Container，AppMaster 可以从 AppManager 获取信息恢复。当 NodeManager 故障的时候系统可以先把这个节点移除，在其他 NodeManager 重启再继续任务。 ResourceManager 可以启动多台，只有其中一台是 active 状态的，其他都处于待命状态。这台 active 状态的 ResourceManager 执行的时候会向 ZooKeeper 集群写入它的状态，当它故障的时候这些 RM 首先选举出另外一台正常运行的 RM 变为 active 状态，然后从 ZooKeeper 集群加载出现故障 ResourceManager 的状态。在转移的过程中它不接收新的 Job，转移完成后才接收新 Job。 资源分配方式FIFO Scheduler 如果没有配置策略的话，所有的任务都提交到一个 default 队列，根据它们的提交顺序执行。富裕资源就执行任务，若资源不富裕就等待前面的任务执行完毕后释放资源，这就是 FIFO Scheduler 先入先出的分配方式。 在 Job1 提交时占用了所有的资源，不久后 Job2 提交了，但是此时系统中已经没有资源可以分配给它了。加入 Job1 是一个大任务，那么 Job2 就只能等待一段很长的时间才能获得执行的资源。所以先入先出的分配方式存在一个问题就是大任务会占用很多资源，造成后面的小任务等待时间太长而饿死，因此一般不使用这个默认配置。 Capacity Scheduler Capacity Scheduler 是一种多租户、弹性的分配方式。每个租户一个队列，每个队列可以配置能使用的资源上限与下限（譬如 50%，达到这个上限后即使其他的资源空置着，也不可使用），通过配置可以令队列至少有资源下限配置的资源可使用。 队列 A 和队列 B 分配了相互独立的资源。Job1 提交给队列 A 执行，它只能使用队列 A 的资源。接着 Job2 提交给了队列 B 就不必等待 Job1 释放资源了。这样就可以将大任务和小任务分配在两个队列中，这两个队列的资源相互独立，就不会造成小任务饿死的情况了。 Fair Scheduler Fair Scheduler 是一种公平的分配方式，所谓的公平就是集群会尽可能地按配置的比例分配资源给队列。 Job1 提交给队列 A，它占用了集群的所有资源。接着 Job2 提交给了队列 B，这时 Job1 就需要释放它的一半的资源给队列 A 中的 Job2 使用。接着 Job3 也提交给了队列 B，这个时候 Job2 如果还未执行完毕的话也必须释放一半的资源给 Job3。这就是公平的分配方式，在队列范围内所有任务享用到的资源都是均分的。","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wingowen.github.io/tags/Spark/"},{"name":"Yarn","slug":"Yarn","permalink":"https://wingowen.github.io/tags/Yarn/"}]},{"title":"MySQL","slug":"MySQL","date":"2022-04-06T09:07:16.000Z","updated":"2022-04-21T06:14:15.009Z","comments":true,"path":"2022/04/06/MySQL/","link":"","permalink":"https://wingowen.github.io/2022/04/06/MySQL/","excerpt":"","text":"基本使用基本概念： 数据库 database：保存有组织的数据的容器（通常是一个文件或一组文件）。 表 table：某种特定类型数据的结构化清单。 模式 schema：关于数据库和表的布局及特性的信息。 列 column：表中的一个字段。所有表都是由一个或多个列组成的。 数据类型 datatype：所容许的数据的类型。每个表列都有相应的数据类型，它限制（或容许）该列中存储的数据。 行 row：表中的一个记录。 主键 primary key：一列（或一组列），其值能够唯一区分表中每个行。 MySQL 连接信息： IP:PORT USERNAME PASSWORD (optional) 常用命令： 123456789101112131415# 库SHOW DATABASES;USE [database];# 表SHOW TABLES;SHOW COLUMNS FROM [table];= DESCRIBE [table];# SHOW 相关HELP SHOW;SHOW STATUS; # 服务器状态信息SHOW CREATE [database]; # 显示建库语句SHOW CREATE [table]; # 显示建表语句SHOW GRANTS; # 显示授予用户的安全权限SHOW ERRORS;SHOW WARNINGS; 子句 clause： SQL 语句由子句构成，有些子句是必需的，而有的是可选的。 检索数据一般检索，去重检索，范围检索； 排序检索。 SELECT1234567891011SELECT [column] FROM [table];SELECT [column01], [column02], [column03] FROM [table];SELECT * FROM [table];SELECT DISTINCT [column] FROM table; # 去重，应用于所有列SELECT [column] from table LIMIT 5; # 前五行SELECT [column] from table LIMIT 5.5; # 五行之后的五行= LIMIT 5 OFFSET 5SELECT [table].[column] FROM [database].[table]; # 完全限定 排序1234567SELECT [column] FROM [table] ORDER BY [column];SELECT [column] FROM [table] ORDER BY [column01], [column02]; # 01 相同则则按02SELECT [column] FROM [table] ORDER BY [column] DESC; # 降序SELECT [column] FROM [table] ORDER BY [column01] DESC, [column02]; # 只作用于 01SELECT [column] FROM [table] ORDER BY [column] DESC LIMIT 1; 大小写与排序： MySQL 默认为字典 dictionary 排序，A 与 a 相同。数据管理员能够在需要的时候改变这种行为。 子句位置： 在给出 ORDER BY 子句时，应该保证它位于 FROM 子句之后。如果使用 LIMIT，它必须位于ORDER BY之后。使用子句的次序不对将产生错误消息。 过滤只检索所需数据需要指定搜索条件 search criteria，搜索条件也称为过滤条件 filter condition。 MySQL 在执行匹配时默认不区分大小写。 1SELECT [column] FROM [table] WHERE a=b; 子句位置： 在同时使用 ORDER BY 和 WHERE子 句时，应该让 ORDER BY 位于 WHERE 之后，否则将会产生错误。 WHERE 子句操作符 1SELECT [column] FROM [table] between low AND high; 空值检测1SELECT [column] FROM [table] WHERE column IS NULL; NULL 与 匹配： NULL 具有特殊的含义，数据库不知道它们是否匹配，所以在匹配过滤时不返回它们。 组合 WHERE 子句操作符 operator： 用来联结或改变 WHERE 子句中的子句的关键字。也称为逻辑操作符 logical operator。 AND 操作符： 1SELECT [column] FROM [table] WHERE a=b AND c=d; OR 操作符： 1SELECT [column] FROM [table] WHERE a=b OR c=d; WHERE 可包含任意数目的 AND 和 OR 操作符。允许两者结合以进行复杂和高级的过滤。 计算次序： AND 操作符的优先级高于 OR 操作符。可使用圆括号来明确的分组相应的操作符。 1SELECT [column] FROM [table] WHERE (a=b OR c=d) AND e&gt;f; 任何时候使用具有 AND 和 OR 操作符的 WHERE 子句，都应该使用圆括号明确地分组操作符，消除歧义。 IN 操作符： 功能与 OR 相当，但更加简介与快速，并且能包含其它 SELECT 语句，动态建立 WHERE 子句。 12SELECT [column] FROM [table] a IN (b, c);= WHERE a=b OR a=c; NOT 操作符： WHERE 子句后的 NOT 只用来否定它之后跟的任何条件。 通配符通配符 wildcard，用来匹配值的一部分的特殊字符。 搜索模式 search pattern，由字面值、通配符或两者组合构成的搜索条件。 通配符本身实际是 SQL 的 WHERE 子句中有特殊含义的字符，SQL 支持几种通配符。为在搜索子句中使用通配符，必须使用 LIKE 操作符。LIKE 指示 MySQL，后跟的搜索模式利用通配符匹配而不是直接相等匹配进行比较。 LIKE 操作符： % 通配符：匹配多个字符。 1SELECT [column] FROM [table] WHERE a LIKE &#x27;%abc%&#x27;; 注意，通配符 % 匹配不了 NULL。 _ 通配符：匹配单个字符。 注意，通配符搜索的处理更费事，不要滥用。 正则表达式正则表达式的相关知识可参考：《正则表达式必知必会》 123SELECT [column] from [table] WHERE a REGEXP &#x27;1000&#x27; ORDER BY b; 正则表达式默认不区分大小写。 12REGEXP [exp];REGEXP BiNARY [exp]; # 区分大小写 MySQL 支持一小部分正则表达式，以下列出大部分： 123456&#x27;abcd&#x27; # 包含 abcd&#x27;.000&#x27; # . 表示任意字符&#x27;a|b&#x27; # or&#x27;[123] abc&#x27; = &#x27;[1|2|3] abc&#x27; # 匹配特定字符 123&#x27;[a-z]abc&#x27; # 匹配范围任意一个字符&#x27;\\\\.&#x27; # 匹配特殊字符 123456&#x27;\\\\([0-9] sticks?\\\\)&#x27; # s? 表示 s 可选result(1 stick)(2 sticks)&#x27;[[:digit:]&#123;4&#125;]&#x27; # 匹配最少四个任意数字 1&#x27;^[0-9\\\\.]&#x27; # 以数字或 . 开头的字符 可以用带文字串的 REGEXP 来测试表达式。 1SELECT &#x27;hello&#x27; REGEXP &#x27;[0-9]&#x27;; 计算与函数创建计算字段计算字段并不实际存在于数据库表中，计算字段是运行时在 SELECT 语句内创建的。 只有数据库知道 SELECT 语句中哪些列是实际的表列，哪些列是计算字段。从客户机（如应用程序）的角度来看，计算字段的数据是以与其他列的数据相同的方式返回的。 客户机与服务器的格式 可在 SQL 语句内完成的许多转换和格式化工作都可以直接在客户机应用程序内完成。但一般来说，在数据库服务器上完成这些操作比在客户机中完成要快得多，因为 DBMS 是设计来快速有效地完成这种处理的。 使用 SELECT 提供的测试和试验函数与计算。 123SELECT 3*2SELECT Trim(&#x27; abc&#x27;)SELECT Now() 别名： 新计算的列没有名字，客户端是没有办法应引用的，MySQL 使用别名 alias 来解决这个问题。 别名还可以用于重命名先存的列名。 1SELECT a, b AS ab FROM [tables]; 函数测试： 使用 SELECT 提供的测试和试验函数与计算。 123SELECT 3*2SELECT Trim(&#x27; abc&#x27;)SELECT Now() 算术计算12345SELECT a, b*c AS d FROM [table]# + 加# - 减# * 乘# \\ 除 函数多数的 SQL 语句是可移植的，在 SQL 实现之间有差异时，这些差异通常不那么难处理。而函数的可移植性却不强，DBMS 对函数的支持和实现差异很大。 若使用函数，应该保证做好代码注释，以便日后能确切知道所编写 SQL 代码的含义。 文本处理函数拼接字段： 1SELECT Concat(a, &#x27;(&#x27;, &#x27;b&#x27;, &#x27;)&#x27;) FROM [table] ORDER BY c; 去掉空格： 1SELECT Concat(a, Rtrim(&#x27;( &#x27;), trim(b), Ltrim(&#x27; )&#x27;) FROM [table] ORDER BY c; 日期和时间处理函数： 日期和时间采用相应的数据类型和特殊的格式存储，以便能快速和有效地排序或过滤，并且节省物理存储空间。 不管是插入新值还是过滤，首选的格式是 yyyy-mm-dd 1234# Date 指示 MySQL 仅提取列的日期部分SELECT a, b FROM [table] WHERE Date(date)=&#x27;2022-7-21&#x27;# Time 指示 MySQL 仅提取列的时间部分SELECT a, b FROM [table] WHERE Time(date)=&#x27;11:11:11&#x27; 数值处理函数 汇总数据使用聚集函数汇总表的数据，以便分析和报表生成。 聚集函数 聚集函数 aggregate function 运行在行组上，计算和返回单个值的函数。 AVG() AVG() 函数忽略列值为 NULL 的行。 123SELECT AVG(price) AS avg_price FROM products;# 只计算 id=1 的产品价格SELECT AVG(price) AS avg_price FROM products where = 1; COUNT() 1234# 对表中行的数目进行计数，不管列中是否包含 NULLSELECT COUNT(*) FROM [table]# 对特定列进行计数时会忽略 NULLSELECT COUNT([column]) FROM [table] MAX()、MIN() 忽略 NULL。 SUM() 1SELECT SUM(price*quantity) AS total_price FROM products where = 1; 在多个列上进行计算 如本例所示，利用标准的算术操作符，所有聚集函数都可用来执行多个列上的计算。 忽略 NULL。 聚集函数组合使用 12345SELECT COUNT(*) AS num MIN(price) AS price_min MAX(price) AS price_max AVG(price) AS price_avgFROM products; DISTINCT仅 MySQL 5 以及后期版本支持。 聚集函数可以对所有的行执行计算，也可以指定 DISTINCT 参数，只对不同的值进行计算，即去重后再计算。 DISTINCT 必须使用列名，不能用于计算或表达式。 1SELECT AVG(DISTINCT price) AS avg_price FROM products where = 1; 分组数据分组允许把数据分为多个逻辑组，以便能对每个组进行聚集计算。 GROUP BY 子句指示 MYSQL 分组数据，然后对每个组而不是整个结果集进行聚集。 12345SELECT cust_id, COUNT(*) AS order_num,FROM ordersGROUB BY cust_id; 使用规则 GROUP BY 子句可以包含任意数目的列。这使得能对分组进行嵌套，为数据分组提供更细致的控制。 如果在 GROUP BY 子句中嵌套了分组，数据将在最后规定的分组上进行汇总。换句话说，在建立分组时，指定的所有列都一起计算（所以不能从个别的列取回数据）。 GROUP BY 子句中列出的每个列都必须是检索列或有效的表达式（但不能是聚集函数）。 如果在 SELECT 中使用表达式，则必须在 GROUP BY 子句中指定相同的表达式。不能使用别名。 除聚集计算语句外，SELECT 语句中的每个列都必须在 GROUP BY 子句中给出。❑ 如果分组列中具有 NULL 值，则 NULL 将作为一个分组返回。如果列中有多行 NULL 值，它们将分为一组。 GROUP BY 子句必须出现在 WHERE 子句之后，ORDER BY 子句之前。 ROLL UPROLLUP 生成考虑此层次结构有意义的所有分组集。 12345SELECT cust_id, COUNT(*) AS order_num,FROM ordersGROUB BY cust_id WITH ROLLUP / ROLLUP(cust_id); # 多得到一行总和的订单数 过滤分组WHERE 过滤的是行，不是分组，使用 HAVING 过滤分组。 HAVING 支持所有 WHERE 操作符。 123456SELECT cust_id, COUNT(*) AS order_num,FROM ordersGROUB BY cust_idHAVING COUNT(*) &gt;= 2; 排序分组分组的排序需要使用 ORDER BY 来保证。 123456SELECT cust_id, COUNT(*) AS order_num,FROM ordersGROUB BY cust_idORDER BY order_num; 子查询查询 query 任何 SQL 语句都是查询。但此术语一般指 SELECT 语句。SQL 还允许创建子查询 subquery，即嵌套在其他查询中的查询。 最常用的是在 WHERE 子句中使用 IN。 1234567891011121314151617181920212223242526272829# EXAMPLE 01SELECT prod_itermFROM productsWHERE prod_id IN ( SELECT prod_id FROM orders WHERE cust_id = &#x27;1001&#x27; ); # EXAMPLE 02SELECT cust_name, cust_state, (SELECT COUNT(*) FROM orders WHERE orders.cust_id == customers.cust_id) AS order_numFROM customersORDER BY cust_name; 联结表","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wingowen.github.io/tags/MySQL/"}]}],"categories":[],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"},{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"Docker","slug":"Docker","permalink":"https://wingowen.github.io/tags/Docker/"},{"name":"Spark","slug":"Spark","permalink":"https://wingowen.github.io/tags/Spark/"},{"name":"Yarn","slug":"Yarn","permalink":"https://wingowen.github.io/tags/Yarn/"},{"name":"MySQL","slug":"MySQL","permalink":"https://wingowen.github.io/tags/MySQL/"}]}