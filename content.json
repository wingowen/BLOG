{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"Docker","slug":"Docker","date":"2022-04-07T08:45:18.000Z","updated":"2022-04-08T08:16:45.477Z","comments":true,"path":"2022/04/07/Docker/","link":"","permalink":"http://example.com/2022/04/07/Docker/","excerpt":"","text":"以下 docker-compose 的版本若未特殊说明，皆为 3.0。 网络12345678# 默认创 bridge 网络docker network create default_network# 查看网络内部信息docker network inspect default_network# 查看所有网络docker network ls# 移除指定的网络docker network rm default_network 1234networks: default: external: name: self-define-network 资源Docker 的容器资源限制，CPU 按百分比进行限制，使用 docker stats [container] 查看容器的 CPU 使用率，此使用率显示的是占用宿主机的百分比。 1234567# CPU 打满的脚本#!/bin/bashwhile true;do openssl speed;done 12345deploy: resources: limits: cpus: 0.1 memory: 16G 打包1docker run -d [image] /bin/bash","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://example.com/tags/Docker/"}]},{"title":"Spark","slug":"Spark","date":"2022-04-06T10:23:01.000Z","updated":"2022-04-08T21:49:34.226Z","comments":true,"path":"2022/04/06/Spark/","link":"","permalink":"http://example.com/2022/04/06/Spark/","excerpt":"","text":"Spark一个较为广泛的定义是：Spark 是一个类 Hadoop MapReduce 的通用并行框架，专为大规模数据处理而设计的快速通用的大数据引擎及轻量级的大数据处理统一平台。 发展到现在，Spark 这个词已经不仅仅只代表 Spark 了，也代表着 Spark 生态。 Spark SQL 提供 HiveQL ( Apache Hive 的 SQL 变体，Hive 查询语言 ) 与 Spark 进行交互的 API；每个数据库表被当做一个 RDD，Spark SQL 被转换为 Spark 操作。 Spark Steaming 对实时数据流进行处理和控制，允许程序能够像处理普通 RDD 数据一样处理实时数据。 MapReduce VS. Spark： MapReduce 只提供 Map 和 Reduce 两个操作，复杂的计算需要大量的 Job 才能完成；其中间结果放置在 HDFS 文件系统中，迭代计算的效率很低；适用 Batch 数据处理，对交互式、实时数据处理支持不够；开发需要写大量的底层代码。 Spark 提供了丰富的算子；中间结果存储在内存中。 由于 MapReduce 每一步操作的结果都会被存入磁盘中，故在计算出现错时可以很好的从磁盘中恢复；Spark 则需要根据 RDD 中的信息进行数据的重新计算，会耗费一定的资源。 Spark 故障恢复的两种方式： 通过数据的血缘关系再执行一遍前面的处理。 Checkpoint 将数据存储到持久化储存中。 Spark 运行方式： 在 Spark 集群中由一个节点作为 driver 端创建 SparkContext。Spark 应用程序的入口负责调度各个运算资源，协调各个 WorkerNode上 的 Executor。根据用户输入的参数会产生若干个 worker，worker 节点运行若干个 executor，一个 executor 是一个进程，运行各自的 task，每个 task 执行相同的代码段处理不同的数据。 DAG Scheduler 划分作业，依次提交 stage 对应的 taskSet 给 task 作业调度器 TaskScheduleImpl，Task 作业调度器 submite taskSet 给 driver 端的 CoarseGrainedExecutorBackend ( 与 Executor 通信的 )，CoarseGrainedExecutorBackend 接收到 task 提交 even 后，会调用 Executor 执行 task，最终 task 是在 TaskRunner 的 run 方法内运行。 Spark 根据 RDD 之间的不同点依赖关系切分成不同的阶段（Stage）；没有依赖关系的 Stage 是可以并行执行的，但是对于 job，Spark是串行执行的，如果想要并行执行 Job，可以在 Spark 程序中进行多线程编程。 在这个 DAG 图中，Spark 能够充分了解数据之间的血缘关系，这样某些任务失败后可以根据血缘关系重新执行计算获取失败了的 RDD。 宽依赖和窄依赖 窄依赖指父 RDD 的每个分区只被子 RDD 的一个分区所使用。 宽依赖指父 RDD 的每个分区可能被多个子 RDD 的分区所使用。 Spark 的资源管理架构： Master 是 Spark 的 主控节点，在实际的生产环境中会有多个 Master，只有一个 Master 处于 active 状态。 Worker 是 Spark 的工作节点，向 Master 汇报自身的资源、Executor 执行状态的改变，并接受 Master 的命令启动 Executor 或 Driver。 Driver 是应用程序的驱动程序，每个应用包括许多小任务，Driver 负责推动这些小任务的有序执行。 Executor 是 Spark 的工作进程，由 Worker 监管，负责具体任务的执行。 在 Spark 和 Yarn 两边的角色对比中：Master 和 ResourceManager 对应，Worker 和 NodeManager 对应，Driver 和 AppMaster 对应，Executor 和 Container 对应。架构相似，因此 Spark 很容易构建在 Yarn 之上。 部署模式： Local 模式：部署在同一个进程上，只有 Driver 角色。接受任务后创建 Driver 负责应用的调度执行，不涉及 Master 和 Worker； Local-Cluster 模式：部署在同一个进程上，存在 Master 和 Worker 角色，它们作为独立线程存在于这个进程内； Standalone 模式：Spark 真正的集群模式，在这个模式下 Master 和 Worker 是独立的进程； 第三方部署模式：构建于 Yarn 或 Mesos 之上，由它们提供资源管理。 Spark on YarnSpark on Yarn 对 Job 的处理过程： 客户端提交一个任务给 Yarn ResourceManager 后，AppManager 接受任务并找到一个 Container 创建 AppMaster，此时 AppMaster 上运行的是 Spark Driver。之后 AppMaster 申请 Container 并启动，Spark Driver 在 Container 上启动 Spark Executor，并调度 Spark Task 在 Spark Executor 上运行，等到所有的任务执行完毕后，向 AppManager 取消注册并释放资源。 Spark on Yarn-Client： 客户端在提交完任务之后不会将 Spark Driver 托管给 Yarn，而是在客户端运行。AppMaster 申请完 Container 之后同样也是由 Spark Driver 去启动 Spark Executor，执行任务。 YarnYarn，Yet Another Resource Negotiator，是一个工作调度集群资源管理框架。 在 Yarn 问世前，Hadoop 使用 JobTracker 进行资源管理和作业调度。存在以下瓶颈：JobTracker 同时部署多个时只有一个是处于 active 状态；应用程序相关和资源管理相关的逻辑全部放在 JobTracker 中；MapReduce 计算模型与 JobTracker 的耦合过高。 Yarn 采用 Master &#x2F; Slave 结构，整体采用双层调度架构：第一层调度是 ResourceManager 和 NodeManager。 ResourceManager 包含 Scheduler 和 AppManager 两个组件，分管资源调度和应用管理。（进行拆分，粒度比 JobTracker 更细） NodeManager 可部署在独立机器上，用于管理机器上的资源。 第二层调度指的是 NodeManager 和 Container，NodeManger 会将资源抽象成一个个 Container 并管理它们的生命周期。 Yarn 运作流程： 客户端向 ResourceManager 的 AppManager 提交应用并请求一个 AppMaster 实例； ResourceManager 找到可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 AppMaster 实例； AppMaster 向 ResourceManager 注册，注册之后，客户端就可以查询 ResourceManager 获得自己 AppMaster 的详情以及直接和 App Master 交互； 接着 AppMaster 向 Resource Manager 请求资源，即 Container； 获得 Container 后，AppMaster 启动 Container，并执行 Task； Container 执行过程中会把运行进度和状态等信息发送给 AppMaster； 客户端主动和 App Master 交流应用的运行状态、进度更新等信息； 所有工作完成 App Master 向 RM 取消注册然后关闭，同时所有的 Container 也归还给系统。 从以上流程可以了解到，AppMaster 是作为 Job 的驱动角色，它驱动了 Job 任务的调度执行。在这个运作流程中，AppManager 只需要管理 AppMaster 的生命周期以及保存它的内部状态，而 AppMaster 这个角色的抽象使得每种类型的应用都可以定制自己的 AppMaster，这样其他的计算模型就可以相对容易地运行在 Yarn 集群上。 Yarn HA假如 Container 故障 Resource Manager 可以分配其他的 Container 继续执行，当运行 AppMaster 的 Container 故障后也将分配新的 Container，AppMaster 可以从 AppManager 获取信息恢复。当 NodeManager 故障的时候系统可以先把这个节点移除，在其他 NodeManager 重启再继续任务。 ResourceManager 可以启动多台，只有其中一台是 active 状态的，其他都处于待命状态。这台 active 状态的 ResourceManager 执行的时候会向 ZooKeeper 集群写入它的状态，当它故障的时候这些 RM 首先选举出另外一台正常运行的 RM 变为 active 状态，然后从 ZooKeeper 集群加载出现故障 ResourceManager 的状态。在转移的过程中它不接收新的 Job，转移完成后才接收新 Job。 资源分配方式FIFO Scheduler 如果没有配置策略的话，所有的任务都提交到一个 default 队列，根据它们的提交顺序执行。富裕资源就执行任务，若资源不富裕就等待前面的任务执行完毕后释放资源，这就是 FIFO Scheduler 先入先出的分配方式。 在 Job1 提交时占用了所有的资源，不久后 Job2 提交了，但是此时系统中已经没有资源可以分配给它了。加入 Job1 是一个大任务，那么 Job2 就只能等待一段很长的时间才能获得执行的资源。所以先入先出的分配方式存在一个问题就是大任务会占用很多资源，造成后面的小任务等待时间太长而饿死，因此一般不使用这个默认配置。 Capacity Scheduler Capacity Scheduler 是一种多租户、弹性的分配方式。每个租户一个队列，每个队列可以配置能使用的资源上限与下限（譬如 50%，达到这个上限后即使其他的资源空置着，也不可使用），通过配置可以令队列至少有资源下限配置的资源可使用。 队列 A 和队列 B 分配了相互独立的资源。Job1 提交给队列 A 执行，它只能使用队列 A 的资源。接着 Job2 提交给了队列 B 就不必等待 Job1 释放资源了。这样就可以将大任务和小任务分配在两个队列中，这两个队列的资源相互独立，就不会造成小任务饿死的情况了。 Fair Scheduler Fair Scheduler 是一种公平的分配方式，所谓的公平就是集群会尽可能地按配置的比例分配资源给队列。 Job1 提交给队列 A，它占用了集群的所有资源。接着 Job2 提交给了队列 B，这时 Job1 就需要释放它的一半的资源给队列 A 中的 Job2 使用。接着 Job3 也提交给了队列 B，这个时候 Job2 如果还未执行完毕的话也必须释放一半的资源给 Job3。这就是公平的分配方式，在队列范围内所有任务享用到的资源都是均分的。","categories":[],"tags":[{"name":"Big Data","slug":"Big-Data","permalink":"http://example.com/tags/Big-Data/"}]},{"title":"MySQL 必知必会","slug":"MySQL 必知必会","date":"2022-04-06T09:07:16.000Z","updated":"2022-04-06T09:29:04.459Z","comments":true,"path":"2022/04/06/MySQL 必知必会/","link":"","permalink":"http://example.com/2022/04/06/MySQL%20%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/","excerpt":"","text":"","categories":[],"tags":[{"name":"Database","slug":"Database","permalink":"http://example.com/tags/Database/"}]}],"categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://example.com/tags/Docker/"},{"name":"Big Data","slug":"Big-Data","permalink":"http://example.com/tags/Big-Data/"},{"name":"Database","slug":"Database","permalink":"http://example.com/tags/Database/"}]}