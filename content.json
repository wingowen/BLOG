{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"https://wingowen.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-07-29T10:17:24.000Z","updated":"2022-07-29T10:18:07.390Z","comments":true,"path":"categories/index.html","permalink":"https://wingowen.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-07-29T10:18:35.000Z","updated":"2022-07-29T10:18:54.011Z","comments":true,"path":"tags/index.html","permalink":"https://wingowen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Pandas 练习","slug":"编程/Pandas-练习","date":"2023-09-19T07:11:03.000Z","updated":"2023-09-19T07:24:53.797Z","comments":true,"path":"2023/09/19/编程/Pandas-练习/","link":"","permalink":"https://wingowen.github.io/2023/09/19/%E7%BC%96%E7%A8%8B/Pandas-%E7%BB%83%E4%B9%A0/","excerpt":"","text":"12import numpy as npimport pandas as pd 12data = &#123;&quot;grammer&quot;:[&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,&quot;R&quot;,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;], &quot;score&quot;:[1,2,np.nan,4,5,6,7,10]&#125; 1df = pd.DataFrame(data=data) 1df[df[&quot;grammer&quot;]==&#x27;Python&#x27;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; grammer score 0 Python 1.0 7 Python 10.0 1df.columns Index(['grammer', 'score'], dtype='object') 1df.rename(columns=&#123;&#x27;score&#x27;:&#x27;popularity&#x27;&#125;, inplace=True) 1df[&#x27;grammer&#x27;].value_counts() Python 2 GO 1 Java 1 C 1 R 1 PHP 1 SQL 1 Name: grammer, dtype: int64 12# 线性插值填充df[&#x27;popularity&#x27;] = df[&#x27;popularity&#x27;].fillna(df[&#x27;popularity&#x27;].interpolate()) 1df[df[&#x27;popularity&#x27;]&gt;3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; grammer popularity 3 GO 4.0 4 R 5.0 5 SQL 6.0 6 PHP 7.0 7 Python 10.0 1df.drop_duplicates(subset=[&#x27;grammer&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; grammer popularity 0 Python 1.0 1 C 2.0 2 Java 3.0 3 GO 4.0 4 R 5.0 5 SQL 6.0 6 PHP 7.0 1df[&#x27;popularity&#x27;].mean() 4.75 1df[&#x27;grammer&#x27;].to_list() ['Python', 'C', 'Java', 'GO', 'R', 'SQL', 'PHP', 'Python'] 1df.shape (8, 2) 123# 改变列的顺序col = df.columns[[1,0]]df = df[col] 1df[df[&#x27;popularity&#x27;] == df[&#x27;popularity&#x27;].max()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; popularity grammer 7 10.0 Python 1df[-5:df.shape[0]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; popularity grammer 3 4.0 GO 4 5.0 R 5 6.0 SQL 6 7.0 PHP 7 10.0 Python 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; popularity grammer 3 4.0 GO 4 5.0 R 5 6.0 SQL 6 7.0 PHP 7 10.0 Python 1df.drop(index=len(df)-1, inplace=True) 12row = &#123;&#x27;popularity&#x27;: 6.6, &#x27;grammer&#x27; : &#x27;Perl&#x27;&#125;df = df.append(row, ignore_index=True) 1df = df.sort_values(&quot;popularity&quot;) 1df[&#x27;grammer&#x27;].map(lambda x: len(x)) 0 6 1 1 2 4 3 2 4 1 5 3 7 4 6 3 Name: grammer, dtype: int64 PART 02 1df = pd.read_excel(&quot;pandas120.xlsx&quot;) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 2020-03-16 11:30:18 本科 20k-35k 1 2020-03-16 10:58:48 本科 20k-40k 2 2020-03-16 10:46:39 不限 20k-35k 3 2020-03-16 10:45:44 本科 13k-20k 4 2020-03-16 10:20:41 本科 10k-20k 1234def cal_ave(x): min, max = x.split(&#x27;-&#x27;) return int((int(min[:-1])+int(max[:-1]))/2*1000)df[&#x27;salary&#x27;] = df[&#x27;salary&#x27;].apply(cal_ave) 1df.groupby(df[&#x27;education&#x27;]).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; salary education 不限 19600.000000 大专 10000.000000 本科 19361.344538 硕士 20642.857143 1df[&#x27;createTime&#x27;] = df[&#x27;createTime&#x27;].map(lambda x : x.strftime(&#x27;%m-%d&#x27;)) 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 135 entries, 0 to 134 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 createTime 135 non-null object 1 education 135 non-null object 2 salary 135 non-null int64 dtypes: int64(1), object(2) memory usage: 3.3+ KB 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; salary count 135.000000 mean 19159.259259 std 8661.686922 min 3500.000000 25% 14000.000000 50% 17500.000000 75% 25000.000000 max 45000.000000 123bins = [0,5000, 20000, 50000]group_names = [&#x27;低&#x27;, &#x27;中&#x27;, &#x27;高&#x27;]df[&#x27;categories&#x27;] = pd.cut(df[&#x27;salary&#x27;], bins, labels=group_names) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary categories 0 03-16 本科 27500 高 1 03-16 本科 30000 高 2 03-16 不限 27500 高 3 03-16 本科 16500 中 4 03-16 本科 15000 中 ... ... ... ... ... 130 03-16 本科 14000 中 131 03-16 硕士 37500 高 132 03-16 本科 30000 高 133 03-16 本科 19000 中 134 03-16 本科 30000 高 135 rows × 4 columns 1df.sort_values(&#x27;salary&#x27;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary categories 53 03-16 本科 45000 高 37 03-16 本科 40000 高 101 03-16 本科 37500 高 16 03-16 本科 37500 高 131 03-16 硕士 37500 高 ... ... ... ... ... 123 03-16 本科 4500 低 126 03-16 本科 4000 低 110 03-16 本科 4000 低 96 03-16 不限 3500 低 113 03-16 本科 3500 低 135 rows × 4 columns 1df.loc[32] createTime 03-16 education 硕士 salary 22500 categories 高 Name: 32, dtype: object 1np.median(df[&#x27;salary&#x27;]) 17500.0 1df.salary.plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel='Frequency'&gt; 12# KED Kernel Density Estimationdf.salary.plot(kind=&#x27;kde&#x27;) &lt;AxesSubplot:ylabel='Density'&gt; 1del df[&#x27;categories&#x27;] 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 03-16 本科 27500 1 03-16 本科 30000 2 03-16 不限 27500 3 03-16 本科 16500 4 03-16 本科 15000 ... ... ... ... 130 03-16 本科 14000 131 03-16 硕士 37500 132 03-16 本科 30000 133 03-16 本科 19000 134 03-16 本科 30000 135 rows × 3 columns 1df.salary.max() - df.salary.min() 41500 1pd.concat([df[:1], df[-2:-1]]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 03-16 本科 27500 133 03-16 本科 19000 1df.append(df.loc[7]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 03-16 本科 27500 1 03-16 本科 30000 2 03-16 不限 27500 3 03-16 本科 16500 4 03-16 本科 15000 ... ... ... ... 131 03-16 硕士 37500 132 03-16 本科 30000 133 03-16 本科 19000 134 03-16 本科 30000 7 03-16 本科 12500 136 rows × 3 columns 1df.set_index(&quot;createTime&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; education salary createTime 03-16 本科 27500 03-16 本科 30000 03-16 不限 27500 03-16 本科 16500 03-16 本科 15000 ... ... ... 03-16 本科 14000 03-16 硕士 37500 03-16 本科 30000 03-16 本科 19000 03-16 本科 30000 135 rows × 2 columns 1df_r = pd.DataFrame(np.random.randint(1, 10, 135), columns=[&#x27;random&#x27;]) 1df = pd.concat([df, df_r], axis=1) 1df[&#x27;sub&#x27;] = df.salary - df.random 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary random sub 0 03-16 本科 27500 2 27498 1 03-16 本科 30000 6 29994 2 03-16 不限 27500 5 27495 3 03-16 本科 16500 3 16497 4 03-16 本科 15000 8 14992 ... ... ... ... ... ... 130 03-16 本科 14000 1 13999 131 03-16 硕士 37500 5 37495 132 03-16 本科 30000 8 29992 133 03-16 本科 19000 7 18993 134 03-16 本科 30000 6 29994 135 rows × 5 columns 1df.isna().sum() createTime 0 education 0 salary 0 random 0 sub 0 dtype: int64 1df.isna().values.any() False 1df.salary.astype(float) 0 27500.0 1 30000.0 2 27500.0 3 16500.0 4 15000.0 ... 130 14000.0 131 37500.0 132 30000.0 133 19000.0 134 30000.0 Name: salary, Length: 135, dtype: float64 1len(df[df.salary &gt; 10000].salary) 119 12df.education.value_counts() 本科 119 硕士 7 不限 5 大专 4 Name: education, dtype: int64 1df.education.unique() array(['本科', '不限', '硕士', '大专'], dtype=object) 1df.education.nunique() 4 1df[(df.salary + df[&#x27;sub&#x27;]) &gt; 60000].tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary random sub 92 03-16 本科 35000 6 34994 101 03-16 本科 37500 4 37496 131 03-16 硕士 37500 5 37495 金融数据处理 1df = pd.read_excel(&#x27;./600000.SH.xls&#x27;) WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero 1df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 0 600000.SH 浦发银行 2016-01-04 16.1356 16.1444 16.1444 15.4997 15.7205 42240610 754425783 -0.4151 -2.5725 17.8602 0.2264 3.320318e+11 3.320318e+11 1.865347e+10 6.5614 1 600000.SH 浦发银行 2016-01-05 15.7205 15.4644 15.9501 15.3672 15.8618 58054793 1034181474 0.1413 0.8989 17.8139 0.3112 3.350163e+11 3.350163e+11 1.865347e+10 6.6204 2 600000.SH 浦发银行 2016-01-06 15.8618 15.8088 16.0208 15.6234 15.9855 46772653 838667398 0.1236 0.7795 17.9307 0.2507 3.376278e+11 3.376278e+11 1.865347e+10 6.6720 1df.isna().sum() 代码 1 简称 2 日期 2 前收盘价(元) 2 开盘价(元) 2 最高价(元) 2 最低价(元) 2 收盘价(元) 2 成交量(股) 2 成交金额(元) 2 涨跌(元) 2 涨跌幅(%) 2 均价(元) 2 换手率(%) 2 A股流通市值(元) 2 总市值(元) 2 A股流通股本(股) 2 市盈率 2 dtype: int64 1df[df[&#x27;日期&#x27;].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 327 NaN NaN NaT NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 328 数据来源：Wind资讯 NaN NaT NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 12345data = dffor columname in data.columns: if data[columname].count() != len(data): loc = data[columname][data[columname].isnull().values==True].index.tolist() print(&#x27;列名：&quot;&#123;&#125;&quot;, 第&#123;&#125;行位置有缺失值&#x27;.format(columname,loc)) 列名：&quot;代码&quot;, 第[327]行位置有缺失值 列名：&quot;简称&quot;, 第[327, 328]行位置有缺失值 列名：&quot;日期&quot;, 第[327, 328]行位置有缺失值 列名：&quot;前收盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;开盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;最高价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;最低价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;收盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;成交量(股)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;成交金额(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;涨跌(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;涨跌幅(%)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;均价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;换手率(%)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;A股流通市值(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;总市值(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;A股流通股本(股)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;市盈率&quot;, 第[327, 328]行位置有缺失值 1df.dropna(axis=0, how=&#x27;any&#x27;, inplace=True) 12# df = df.set_index(&#x27;日期&#x27;)df[&#x27;收盘价(元)&#x27;].plot(kind=&#x27;line&#x27;) &lt;AxesSubplot:&gt; 1df[[&#x27;收盘价(元)&#x27;, &#x27;开盘价(元)&#x27;]].plot(kind=&#x27;line&#x27;) &lt;AxesSubplot:&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 25910 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 25910 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0, flags=flags) 1df[&#x27;涨跌幅(%)&#x27;].plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel='Frequency'&gt; 1df[&#x27;涨跌幅(%)&#x27;].plot(kind=&#x27;hist&#x27;, bins=30) &lt;AxesSubplot:ylabel='Frequency'&gt; 12345temp = pd.DataFrame(columns = data.columns.to_list())for i in range(len(data)): if type(data.iloc[i,13]) != float: temp = temp.append(data.loc[i])temp.head(1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 26 600000.SH 浦发银行 2016-02-16 16.2946 16.2946 16.2946 16.2946 16.2946 -- -- 0.0 0.0 -- -- 3.441565e+11 3.441565e+11 1.865347e+10 6.801 1i = df[df[&#x27;换手率(%)&#x27;].isin([&#x27;--&#x27;])].index 1i Int64Index([26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], dtype='int64') 1df = df.drop(index=i) 1df[&#x27;换手率(%)&#x27;].plot(kind=&#x27;kde&#x27;) &lt;AxesSubplot:ylabel='Density'&gt; 1df[&#x27;收盘价(元)&#x27;].diff() 0 NaN 1 0.1413 2 0.1237 3 -0.5211 4 -0.0177 ... 322 -0.0800 323 -0.1000 324 -0.0600 325 -0.0600 326 -0.1000 Name: 收盘价(元), Length: 309, dtype: float64 1df[&#x27;收盘价(元)&#x27;] - df[&#x27;收盘价(元)&#x27;].shift(1) 0 NaN 1 0.1413 2 0.1237 3 -0.5211 4 -0.0177 ... 322 -0.0800 323 -0.1000 324 -0.0600 325 -0.0600 326 -0.1000 Name: 收盘价(元), Length: 309, dtype: float64 1df[&#x27;收盘价(元)&#x27;].pct_change() 0 NaN 1 0.008988 2 0.007799 3 -0.032598 4 -0.001145 ... 322 -0.005277 323 -0.006631 324 -0.004005 325 -0.004021 326 -0.006729 Name: 收盘价(元), Length: 309, dtype: float64 1df = data.set_index(&#x27;日期&#x27;) 1df[&#x27;收盘价(元)&#x27;].rolling(5).mean() 日期 2016-01-04 NaN 2016-01-05 NaN 2016-01-06 NaN 2016-01-07 NaN 2016-01-08 15.69578 ... 2017-05-03 15.14200 2017-05-04 15.12800 2017-05-05 15.07000 2017-05-08 15.00000 2017-05-09 14.92000 Name: 收盘价(元), Length: 327, dtype: float64 1df[&#x27;收盘价(元)&#x27;].rolling(5).sum() 日期 2016-01-04 NaN 2016-01-05 NaN 2016-01-06 NaN 2016-01-07 NaN 2016-01-08 78.4789 ... 2017-05-03 75.7100 2017-05-04 75.6400 2017-05-05 75.3500 2017-05-08 75.0000 2017-05-09 74.6000 Name: 收盘价(元), Length: 327, dtype: float64 1234temp = pd.DataFrame()temp[&#x27;m5&#x27;] = df[&#x27;收盘价(元)&#x27;].rolling(5).mean()temp[&#x27;m20&#x27;] = df[&#x27;收盘价(元)&#x27;].rolling(20).mean()temp.plot() &lt;AxesSubplot:xlabel='日期'&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) 1df[&#x27;收盘价(元)&#x27;].resample(rule=&#x27;W&#x27;).max() 日期 2016-01-10 15.9855 2016-01-17 15.8265 2016-01-24 15.6940 2016-01-31 15.0405 2016-02-07 16.2328 ... 2017-04-16 15.9700 2017-04-23 15.5600 2017-04-30 15.2100 2017-05-07 15.1600 2017-05-14 14.8600 Freq: W-SUN, Name: 收盘价(元), Length: 71, dtype: float64 12df[&#x27;收盘价(元)&#x27;].plot()df[&#x27;收盘价(元)&#x27;].resample(&#x27;7D&#x27;).max().plot() &lt;AxesSubplot:xlabel='日期'&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) 1data[&#x27;开盘价(元)&#x27;].expanding(min_periods=1).mean() 0 16.144400 1 15.804400 2 15.805867 3 15.784525 4 15.761120 ... 322 16.055594 323 16.052552 324 16.049159 325 16.045266 326 16.041122 Name: 开盘价(元), Length: 327, dtype: float64 12df[&#x27;expanding Open mean&#x27;]=df[&#x27;开盘价(元)&#x27;].expanding(min_periods=1).mean()df[[&#x27;开盘价(元)&#x27;, &#x27;expanding Open mean&#x27;]].plot(figsize=(16, 6)) &lt;AxesSubplot:xlabel='日期'&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0, flags=flags) pandas &amp; numpy 123456# 随机df1 = pd.DataFrame(np.random.randint(0, 100, 20))# 等步长df2 = pd.DataFrame(np.arange(0, 100, 5))# 正态分布df3 = pd.DataFrame(np.random.normal(0, 1, 20)) 1df3.plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel='Frequency'&gt; 1df = pd.concat([df1,df2,df3],ignore_index=True) 1df = pd.concat([df1,df2,df3], axis=1, ignore_index=True) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 1 2 0 25 0 0.573071 1 86 5 -0.474405 2 34 10 -0.910378 3 18 15 0.713983 4 9 20 0.732289 5 67 25 -2.087864 6 5 30 -1.126019 7 51 35 -1.512201 8 3 40 -0.697655 9 10 45 -1.082615 10 59 50 -0.809815 11 28 55 -0.158042 12 21 60 -1.753240 13 96 65 0.687153 14 77 70 -1.534696 15 25 75 1.169113 16 26 80 -0.015986 17 52 85 0.517278 18 60 90 1.045848 19 35 95 0.856043 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 1 2 count 20.000000 20.000000 20.000000 mean 39.350000 47.500000 -0.293407 std 27.642978 29.580399 1.034218 min 3.000000 0.000000 -2.087864 25% 20.250000 23.750000 -1.093466 50% 31.000000 47.500000 -0.316223 75% 59.250000 71.250000 0.693860 max 96.000000 95.000000 1.169113 1df.columns = [&#x27;col1&#x27;, &#x27;col2&#x27;, &#x27;col3&#x27;] 1df[&#x27;col1&#x27;][~df[&#x27;col1&#x27;].isin(df[&#x27;col2&#x27;])] 1 86 2 34 3 18 4 9 5 67 7 51 8 3 10 59 11 28 12 21 13 96 14 77 16 26 17 52 Name: col1, dtype: int64 1df[&#x27;col1&#x27;].append(df[&#x27;col2&#x27;]).value_counts().head(3) 25 3 60 2 35 2 dtype: int64 1np.where(df[&#x27;col1&#x27;] % 5==0) (array([ 0, 6, 9, 15, 18, 19]),) 1df[df.columns[::-1]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; col3 col2 col1 0 0.573071 0 25 1 -0.474405 5 86 2 -0.910378 10 34 3 0.713983 15 18 4 0.732289 20 9 5 -2.087864 25 67 6 -1.126019 30 5 7 -1.512201 35 51 8 -0.697655 40 3 9 -1.082615 45 10 10 -0.809815 50 59 11 -0.158042 55 28 12 -1.753240 60 21 13 0.687153 65 96 14 -1.534696 70 77 15 1.169113 75 25 16 -0.015986 80 26 17 0.517278 85 52 18 1.045848 90 60 19 0.856043 95 35 1df[&#x27;col1&#x27;].take([1,10,15]) 1 86 10 59 15 25 Name: col1, dtype: int64 12tem = np.diff(np.sign(np.diff(df[&#x27;col1&#x27;])))np.where(tem == -2)[0] + 1 array([ 1, 5, 7, 10, 13, 18]) 1df.mean(axis=1) 0 8.524357 1 30.175198 2 14.363207 3 11.237994 4 9.910763 5 29.970712 6 11.291327 7 28.162600 8 14.100782 9 17.972462 10 36.063395 11 27.613986 12 26.415587 13 53.895718 14 48.488435 15 33.723038 16 35.328005 17 45.839093 18 50.348616 19 43.618681 dtype: float64 1np.convolve(df[&#x27;col2&#x27;], np.ones(3)/3, mode=&#x27;valid&#x27;) array([ 5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60., 65., 70., 75., 80., 85., 90.]) 1df[&#x27;col2&#x27;].rolling(window=3).mean() 0 NaN 1 NaN 2 5.0 3 10.0 4 15.0 5 20.0 6 25.0 7 30.0 8 35.0 9 40.0 10 45.0 11 50.0 12 55.0 13 60.0 14 65.0 15 70.0 16 75.0 17 80.0 18 85.0 19 90.0 Name: col2, dtype: float64 1df.sort_values(&quot;col3&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; col1 col2 col3 5 67 25 -2.087864 12 21 60 -1.753240 14 77 70 -1.534696 7 51 35 -1.512201 6 5 30 -1.126019 9 10 45 -1.082615 2 34 10 -0.910378 10 59 50 -0.809815 8 3 40 -0.697655 1 86 5 -0.474405 11 28 55 -0.158042 16 26 80 -0.015986 17 52 85 0.517278 0 25 0 0.573071 13 96 65 0.687153 3 18 15 0.713983 4 9 20 0.732289 19 35 95 0.856043 18 60 90 1.045848 15 25 75 1.169113 1df.col1[df[&#x27;col1&#x27;] &gt; 50]= &#x27;高&#x27; &lt;ipython-input-97-d0c96ec6289a&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df.col1[df['col1'] &gt; 50]= '高' 1np.linalg.norm(df[&#x27;col2&#x27;]-df[&#x27;col3&#x27;]) 248.99392792632952 123456789df1= pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],&#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;],&#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;],&#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;, &#x27;B3&#x27;]&#125;)df2= pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],&#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;],&#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C1&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;],&#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D1&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;) 1pd.merge(df1, df2, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 1pd.merge(df1, df2, how=&#x27;inner&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 1pd.merge(df1, df2, how=&#x27;left&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 123df = pd.DataFrame(&#123;0: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0), 1: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0), 2: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0)&#125;) 1na = np.array(df) 1np.argwhere(na==1) array([[ 0, 0], [ 1, 1], [ 1, 2], [ 3, 0], [ 3, 1], [ 3, 2], [ 4, 2], [ 5, 0], [ 5, 1], [ 6, 0], [ 6, 2], [ 7, 0], [ 8, 0], [ 8, 2], [ 9, 2], [10, 2], [11, 2], [12, 0], [12, 1], [14, 0], [14, 1], [15, 0], [15, 2], [17, 0], [17, 2], [18, 2], [19, 0], [19, 2], [21, 1], [21, 2], [22, 0], [22, 1], [23, 2], [24, 0], [24, 1], [25, 0], [28, 1], [29, 1], [30, 0], [30, 1], [31, 2], [32, 0], [33, 0], [33, 1], [33, 2], [34, 0], [35, 0], [36, 1], [36, 2], [37, 1], [37, 2], [38, 0], [38, 1], [39, 0], [39, 1], [40, 0], [41, 1], [42, 1], [43, 0], [43, 1], [43, 2], [44, 2], [45, 2], [47, 1], [47, 2], [48, 1], [49, 0], [49, 1], [50, 0], [50, 1], [51, 2], [52, 2], [53, 1], [54, 0], [55, 0], [55, 2], [56, 0], [57, 2], [58, 0], [58, 1], [58, 2], [59, 2], [61, 1], [61, 2], [62, 1], [63, 0], [64, 1], [65, 0], [65, 1], [66, 0], [66, 1], [66, 2], [67, 2], [68, 0], [68, 1], [68, 2], [69, 0], [69, 2], [70, 2], [71, 0], [71, 1], [71, 2], [72, 1], [73, 1], [74, 2], [76, 1], [76, 2], [77, 1], [77, 2], [78, 1], [78, 2], [79, 0], [79, 2], [80, 1], [81, 0], [81, 1], [81, 2], [83, 1], [84, 0], [84, 1], [84, 2], [85, 0], [86, 0], [86, 1], [86, 2], [87, 2], [89, 0], [90, 0], [90, 2], [91, 1], [91, 2], [92, 1], [93, 0], [96, 0], [96, 2], [98, 0], [98, 1]]) 1pd.pivot_table(df,values=[0,1],index=0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 1 2 0 0 0.418182 0.490909 1 0.488889 0.444444 1df = pd.read_csv(&quot;./数据.csv&quot;, encoding=&#x27;gbk&#x27;) 1!file -i 数据.csv 数据.csv: regular file 1!yum install enca zsh:1: command not found: yum 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; positionId positionName companyId companyLogo companySize industryField financeStage companyLabelList firstType secondType ... plus pcShow appShow deliver gradeDescription promotionScoreExplain isHotHire count aggregatePositionIds famousCompany 0 6802721 数据分析 475770 i/image2/M01/B7/3E/CgoB5lwPfEaAdn8WAABWQ0Jgl5s... 50-150人 移动互联网,电商 A轮 ['绩效奖金', '带薪年假', '定期体检', '弹性工作'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 1 5204912 数据建模 50735 image1/M00/00/85/CgYXBlTUXeeAR0IjAABbroUk-dw97... 150-500人 电商 B轮 ['年终奖金', '做五休二', '六险一金', '子女福利'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] False 2 6877668 数据分析 100125 image2/M00/0C/57/CgqLKVYcOA2ADcFuAAAE8MukIKA74... 2000人以上 移动互联网,企业服务 上市公司 ['节日礼物', '年底双薪', '股票期权', '带薪年假'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 3 6496141 数据分析 26564 i/image2/M01/F7/3F/CgoB5lyGAQGAZeI-AAAdOqXecnw... 500-2000人 电商 D轮及以上 ['生日趴', '每月腐败基金', '每月补贴', '年度旅游'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] True 4 6467417 数据分析 29211 i/image2/M01/77/B8/CgoB5l1WDyGATNP5AAAlY3h88SY... 2000人以上 物流丨运输 上市公司 ['技能培训', '免费班车', '专项奖金', '岗位晋升'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 100 6884346 数据分析师 21236 i/image/M00/43/F6/CgqKkVeEh76AUVPoAAA2Bj747wU6... 500-2000人 移动互联网,医疗丨健康 C轮 ['技能培训', '年底双薪', '节日礼物', '绩效奖金'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 101 6849100 商业数据分析 72076 i/image2/M01/92/A4/CgotOV2LPUmAR_8dAAB_DlDMiXA... 500-2000人 移动互联网,电商 C轮 ['节日礼物', '股票期权', '带薪年假', '年度旅游'] 市场|商务类 市场|营销 ... NaN 0 0 0 NaN NaN 0 0 [] False 102 6803432 奔驰·耀出行-BI数据分析专家 751158 i/image3/M01/64/93/Cgq2xl48z2mAeYRoAAD6Qf_Jeq8... 150-500人 移动互联网 不需要融资 [] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] False 103 6704835 BI数据分析师 52840 i/image2/M00/26/CA/CgoB5lofsguAfk9ZAACoL3r4p24... 2000人以上 电商 上市公司 ['技能培训', '年底双薪', '节日礼物', '绩效奖金'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] True 104 6728058 数据分析专家-LQ(J181203029) 2474 i/image2/M01/14/4D/CgoB5lyq5fqAAHHzAAAa148hbk8... 2000人以上 汽车丨出行 不需要融资 ['弹性工作', '节日礼物', '岗位晋升', '技能培训'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] True 105 rows × 53 columns 1pd.pivot_table(df,values=[&quot;companyId&quot;,&quot;salary&quot;,&quot;score&quot;],index=&quot;positionId&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; companyId salary score positionId 5203054 329 30000 4.0 5204912 50735 15000 176.0 5269002 50576 37500 1.0 5453691 166666 30000 4.0 5519962 50735 37500 14.0 ... ... ... ... 6882983 7461 27500 15.0 6884346 21236 25000 0.0 6886661 321001 37500 5.0 6888169 751158 42500 1.0 6896403 285786 30000 3.0 95 rows × 3 columns 1pip install nbconvert pandoc Requirement already satisfied: nbconvert in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (6.0.7) Collecting pandoc Downloading pandoc-2.3.tar.gz (33 kB) Requirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.5.1) Requirement already satisfied: traitlets&gt;=4.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (5.0.5) Requirement already satisfied: nbformat&gt;=4.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (5.0.8) Requirement already satisfied: pygments&gt;=2.4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (2.7.2) Requirement already satisfied: defusedxml in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.6.0) Requirement already satisfied: jupyter-core in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (4.6.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (1.4.3) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.8.4) Requirement already satisfied: jupyterlab-pygments in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.1.2) Requirement already satisfied: testpath in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.4.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.3) Requirement already satisfied: bleach in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (3.2.1) Requirement already satisfied: jinja2&gt;=2.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (2.11.2) Collecting plumbum Downloading plumbum-1.8.2-py3-none-any.whl (127 kB) \u001b[K |████████████████████████████████| 127 kB 14 kB/s eta 0:00:01 \u001b[?25hRequirement already satisfied: ply in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from pandoc) (3.11) Requirement already satisfied: async-generator in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (1.10) Requirement already satisfied: jupyter-client&gt;=6.1.5 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (6.1.7) Requirement already satisfied: nest-asyncio in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (1.4.2) Requirement already satisfied: ipython-genutils in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from traitlets&gt;=4.2-&gt;nbconvert) (0.2.0) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.4-&gt;nbconvert) (3.2.0) Requirement already satisfied: webencodings in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (0.5.1) Requirement already satisfied: six&gt;=1.9.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (1.15.0) Requirement already satisfied: packaging in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (20.4) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jinja2&gt;=2.4-&gt;nbconvert) (1.1.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (2.8.1) Requirement already satisfied: pyzmq&gt;=13 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (19.0.2) Requirement already satisfied: tornado&gt;=4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (6.0.4) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (0.17.3) Requirement already satisfied: setuptools in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (50.3.1.post20201107) Requirement already satisfied: attrs&gt;=17.4.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (20.3.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from packaging-&gt;bleach-&gt;nbconvert) (2.4.7) Building wheels for collected packages: pandoc Building wheel for pandoc (setup.py) ... \u001b[?25ldone \u001b[?25h Created wheel for pandoc: filename=pandoc-2.3-py3-none-any.whl size=33271 sha256=22e3e077eb44a4ba73332321d2656d5d67e118abef8947040a2719d096f363f4 Stored in directory: /Users/wingo.wen/Library/Caches/pip/wheels/90/3a/a8/3237a93e3a6261bd24edabf3277ca59f64c1710b3d8c7c72a0 Successfully built pandoc Installing collected packages: plumbum, pandoc Successfully installed pandoc-2.3 plumbum-1.8.2 Note: you may need to restart the kernel to use updated packages. 1!jupyter nbconvert --to markdown pandas.ipynb","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://wingowen.github.io/tags/Pandas/"}]},{"title":"SQL 练习","slug":"数据库/SQL-练习","date":"2023-09-19T07:03:55.000Z","updated":"2023-09-19T07:25:20.863Z","comments":true,"path":"2023/09/19/数据库/SQL-练习/","link":"","permalink":"https://wingowen.github.io/2023/09/19/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL-%E7%BB%83%E4%B9%A0/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381CREATE TABLE `course` ( `c_id` text, `c_name` text, `t_id` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `score` ( `s_id` text, `c_id` text, `s_score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `student` ( `s_id` text, `s_name` text, `s_birth` text, `s_sex` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `teacher` ( `t_id` text, `t_name` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `course` (`c_id`, `c_name`, `t_id`) VALUES(&#x27;02&#x27;, &#x27;数学&#x27;, &#x27;01&#x27;),(&#x27;03&#x27;, &#x27;英语&#x27;, &#x27;03&#x27;),(&#x27;01&#x27;, &#x27;语文&#x27;, &#x27;02&#x27;);INSERT INTO `score` (`s_id`, `c_id`, `s_score`) VALUES(&#x27;01&#x27;, &#x27;02&#x27;, &#x27;90&#x27;),(&#x27;01&#x27;, &#x27;03&#x27;, &#x27;99&#x27;),(&#x27;02&#x27;, &#x27;01&#x27;, &#x27;70&#x27;),(&#x27;02&#x27;, &#x27;02&#x27;, &#x27;60&#x27;),(&#x27;02&#x27;, &#x27;03&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;01&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;02&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;03&#x27;, &#x27;80&#x27;),(&#x27;04&#x27;, &#x27;01&#x27;, &#x27;50&#x27;),(&#x27;04&#x27;, &#x27;02&#x27;, &#x27;30&#x27;),(&#x27;04&#x27;, &#x27;03&#x27;, &#x27;20&#x27;),(&#x27;05&#x27;, &#x27;01&#x27;, &#x27;76&#x27;),(&#x27;05&#x27;, &#x27;02&#x27;, &#x27;87&#x27;),(&#x27;06&#x27;, &#x27;01&#x27;, &#x27;31&#x27;),(&#x27;06&#x27;, &#x27;03&#x27;, &#x27;34&#x27;),(&#x27;07&#x27;, &#x27;02&#x27;, &#x27;89&#x27;),(&#x27;07&#x27;, &#x27;03&#x27;, &#x27;98&#x27;),(&#x27;01&#x27;, &#x27;01&#x27;, &#x27;80&#x27;);INSERT INTO `student` (`s_id`, `s_name`, `s_birth`, `s_sex`) VALUES(&#x27;02&#x27;, &#x27;钱电&#x27;, &#x27;1990-12-21&#x27;, &#x27;男&#x27;),(&#x27;03&#x27;, &#x27;孙风&#x27;, &#x27;1990-05-20&#x27;, &#x27;男&#x27;),(&#x27;04&#x27;, &#x27;李云&#x27;, &#x27;1990-08-06&#x27;, &#x27;男&#x27;),(&#x27;05&#x27;, &#x27;周梅&#x27;, &#x27;1991-12-01&#x27;, &#x27;女&#x27;),(&#x27;06&#x27;, &#x27;吴兰&#x27;, &#x27;1992-03-01&#x27;, &#x27;女&#x27;),(&#x27;07&#x27;, &#x27;郑竹&#x27;, &#x27;1989-07-01&#x27;, &#x27;女&#x27;),(&#x27;08&#x27;, &#x27;王菊&#x27;, &#x27;1990-01-20&#x27;, &#x27;女&#x27;),(&#x27;01&#x27;, &#x27;赵雷&#x27;, &#x27;1990-01-01&#x27;, &#x27;男&#x27;);INSERT INTO `teacher` (`t_id`, `t_name`) VALUES(&#x27;02&#x27;, &#x27;李四&#x27;),(&#x27;03&#x27;, &#x27;王五&#x27;),(&#x27;01&#x27;, &#x27;张三&#x27;);# 查询&quot;01&quot;课程比&quot;02&quot;课程成绩高的学生的信息及课程分数select *from student st left join score sc01 on st.s_id = sc01.s_id and sc01.c_id = 01 left join score sc02 on st.s_id = sc02.s_id and sc02.c_id = 02where sc01.s_score &gt; sc02.s_score;# 查询平均成绩大于等于60分的同学的学生编号和学生姓名和平均成绩select *from student st left join ( select s_id,avg(s_score) as ms from score group by s_id ) st1 on st.s_id=st1.s_id WHERE st1.ms &gt; 60;# 查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩select *from student st left join ( SELECT s_id, count(1) as class_num, sum(s_score) score_sum FROM score group by s_id ) c on st.s_id = c.s_id;# 查询&quot;李&quot;姓老师的数量select count(1) from teacher where t_name like &quot;李%&quot;;# 查询学过&quot;张三&quot;老师授课的同学的信息select * FROM student stleft JOIN score sc on st.s_id = sc.s_idleft JOIN course c on c.c_id = sc.c_idleft JOIN teacher t on c.t_id = t.t_idwhere t.t_name=&quot;张三&quot;;# 查询学过编号为&quot;01&quot;并且也学过编号为&quot;02&quot;的课程的同学的信息select * FROM student stleft JOIN score sc on st.s_id = sc.s_idleft JOIN course c on c.c_id = sc.c_idwhere c.c_id=&#x27;01&#x27; or c.c_id=&#x27;02&#x27;;# 查询没有学全所有课程的同学的信息SELECT * FROM student st WHERE st.s_id in (select st.s_id as c_numFROM student st left JOIN score sc on st.s_id = sc.s_idGROUP BY st.s_idhaving count(sc.c_id) &lt; 3);select student.* from studentjoin (select count(c_id)num1 from course)tmp1left join( select s_id,count(c_id)num2 from score group by s_id)tmp2on student.s_id=tmp2.s_id and tmp1.num1=tmp2.num2where tmp2.s_id is null;# 查询至少有一门课与学号为&quot;01&quot;的同学所学相同的同学的信息select st.* from student stleft join score sc on sc.s_id = st.s_idwhere sc.c_id in (select c_id from score where s_id = 01)and st.s_id!=01group by st.s_id,s_name,s_birth,s_sex;select student.* from studentjoin (select c_id from score where score.s_id=01)tmp1join (select s_id,c_id from score)tmp2 on tmp1.c_id =tmp2.c_id and student.s_id =tmp2.s_idwhere student.s_id not in(&#x27;01&#x27;)group by student.s_id,s_name,s_birth,s_sex;select * from studentjoin (select c_id from score where score.s_id=01)tmp1join (select s_id,c_id from score)tmp2 on tmp1.c_id =tmp2.c_id and student.s_id =tmp2.s_id;# 查询和&quot;01&quot;号的同学学习的课程完全相同的其他同学的信息:# hive不支持group_concat方法,可用 concat_ws(’|’, collect_set(str)) 实现select * FROM(select st.*, GROUP_CONCAT(tmp.c_id SEPARATOR &#x27;;&#x27;) as all_c_id from student stjoin (select s_id,c_id from score)tmpon st.s_id =tmp.s_idWHERE st.s_id != 01group by st.s_id,s_name,s_birth,s_sex) aleft join (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01 group by score.s_id) tmp2on a.s_id = a.s_idwhere a.all_c_id = tmp2.all_c_id; select student.*,tmp1.course_id from studentjoin (select s_id ,GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id from score group by s_id having s_id not in (1))tmp1 on student.s_id = tmp1.s_idjoin (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id2 from score where s_id=1)tmp2 on tmp1.course_id = tmp2.course_id2;select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id2 from score where s_id=1;select * FROM(select st.*, GROUP_CONCAT(tmp.c_id SEPARATOR &#x27;;&#x27;) as all_c_id from student stjoin (select s_id,c_id from score)tmpon st.s_id =tmp.s_idWHERE st.s_id != 01group by st.s_id,s_name,s_birth,s_sex) aleft join (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01 group by score.s_id) tmp2on a.s_id = a.s_idwhere a.all_c_id = tmp2.all_c_id;select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01;# 查询没学过&quot;张三&quot;老师讲授的任一门课程的学生姓名SELECT st.* FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idLEFT JOIN course c ON c.c_id = sc.c_idLEFT JOIN teacher t ON t.t_id = c.t_idWHERE t.t_name=&#x27;张三&#x27;;# 查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩SELECT st.s_id FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idWHERE sc.s_score &lt; 60;SELECT st.s_id, st.s_name, round(avg(sc.s_score)) FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idWHERE sc.s_score &lt; 60GROUP BY s_id, s_nameHAVING count(sc.s_score) &gt;= 2;select student.s_id,student.s_name,tmp.avg_score from studentinner join (select s_id from score where s_score&lt;60 group by score.s_id having count(s_id)&gt;1)tmp2on student.s_id = tmp2.s_idleft join ( select s_id,round(AVG (score.s_score)) avg_score from score group by s_id)tmp on tmp.s_id=student.s_id;# 检索&quot;01&quot;课程分数小于60，按分数降序排列的学生信息:SELECT st.*, sc.s_score from student stLEFT JOIN score sc on sc.s_id = st.s_idwhere sc.c_id = 01 and sc.s_score &lt; 60order by sc.s_score DESC;# 按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩select a.s_id,tmp1.s_score as chinese,tmp2.s_score as math,tmp3.s_score as english, round(avg (a.s_score),2) as avgScorefrom score aleft join (select s_id,s_score from score s1 where c_id=&#x27;01&#x27;)tmp1 on tmp1.s_id=a.s_idleft join (select s_id,s_score from score s2 where c_id=&#x27;02&#x27;)tmp2 on tmp2.s_id=a.s_idleft join (select s_id,s_score from score s3 where c_id=&#x27;03&#x27;)tmp3 on tmp3.s_id=a.s_idgroup by a.s_id,tmp1.s_score,tmp2.s_score,tmp3.s_score order by avgScore desc;select * from course;SELECT sc.s_id, a.s_score as chinese, b.s_score as math, c.s_score as english, round(avg(a.s_score)) as avg_score FROM score scLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;01&#x27;) a on a.s_id=sc.s_idLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;02&#x27;) b on b.s_id=sc.s_idLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;03&#x27;) c on c.s_id=sc.s_idgroup by sc.s_id, a.s_score, b.s_score, c.s_scoreorder by avg_score desc;# 查询各科成绩最高分、最低分和平均分：以如下形式显示：课程ID，课程name，最高分，最低分，平均分，及格率，中等率，优良率，优秀率:# 及格为&gt;=60，中等为：70-80，优良为：80-90，优秀为：&gt;=90select c.c_id, c.c_name, max(s_score), min(s_score),round(avg(s_score)),round(sum(case when sc.s_score&gt;=60 and sc.s_score&lt;70 then 1 else 0 end)/count(1),2) &quot;及格率&quot;,round(sum(case when sc.s_score&gt;=70 and sc.s_score&lt;80 then 1 else 0 end)/count(1),2) &quot;中等率&quot;,round(sum(case when sc.s_score&gt;=80 and sc.s_score&lt;90 then 1 else 0 end)/count(1),2) &quot;优良率&quot;,round(sum(case when sc.s_score&gt;=90 then 1 else 0 end)/count(1)) &quot;优秀率&quot;from score scLEFT JOIN course c on c.c_id=sc.c_idgroup by c_id, c.c_name;# 按各科成绩进行排序，并显示排名select s1.*,row_number()over(order by s1.s_score desc) Ranking from score s1 where s1.c_id=&#x27;01&#x27;union all select s2.*,row_number()over(order by s2.s_score desc) Ranking from score s2 where s2.c_id=&#x27;02&#x27;union all select s3.*,row_number()over(order by s3.s_score desc) Ranking from score s3 where s3.c_id=&#x27;03&#x27;order by noRanking asc;SELECT sc.s_score, sc.s_id FROM score sc where sc.c_id = 1 GROUP BY sc.s_score, sc.s_id order by sc.s_score;# 查询学生的总成绩并进行排名select sum(s_score) as sum from scoregroup by s_idorder by sum;# 查询不同老师所教不同课程平均分从高到低显示(SELECT t_name, c.c_name, round(avg(sc.s_score)) from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_id where t.t_id=01 group by t_name, c.c_name )Union(SELECT t_name, c.c_name, round(avg(sc.s_score)) from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_id where t.t_id=02group by t_name, c.c_name;SELECT t_name, c.c_name, round(avg(sc.s_score)) avg from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_idgroup by t_name, c.c_nameorder by avg;# 查询所有课程的成绩第2名到第3名的学生信息及该课程成绩(SELECT * from (select * from score where c_id=&#x27;01&#x27; order by s_score desc limit 3)tmp1order by tmp1.s_score asc limit 2)UNION ALL(SELECT * from (select * from score where c_id=&#x27;02&#x27; order by s_score desc limit 3)tmp1order by tmp1.s_score asc limit 2);# 查询学生平均成绩及其名次SELECT @rownum := @rownum + 1 as rank, s.avgFROM (SELECT round(avg(s_score)) as avg FROM score GROUP by s_id order by avg desc) s,(SELECT @rownum := 0) r;# 查询各科成绩前三名的记录select score.c_id,course.c_name,student.s_name,s_score from scorejoin student on student.s_id=score.s_idjoin course on course.c_id=score.c_idWHERE score.c_id=&#x27;01&#x27;order by s_score desc limit 3;# 查询每门课程被选修的学生数SELECT c.c_name, count(1) from student stleft join score sc on sc.s_id = st.s_idleft join course c on c.c_id = sc.c_idgroup by c.c_name;SELECT * from student stleft join score sc on sc.s_id = st.s_idleft join course c on c.c_id = sc.c_id;# 查询所有学生的课程及分数情况select a.s_name, SUM(case c.c_name when &#x27;语文&#x27; then b.s_score else 0 end ) as chainese, SUM(case c.c_name when &#x27;数学&#x27; then b.s_score else 0 end ) as math, SUM(case c.c_name when &#x27;英语&#x27; then b.s_score else 0 end ) as english, SUM(b.s_score) as sumScore from student a join score b on a.s_id=b.s_id join course c on b.c_id=c.c_id group by s_name,a.s_id;# 查询每门课程成绩最好的前三名SELECT s_name, s_score FROM student stLEFT JOIN score sc on sc.s_id = st.s_id and sc.c_id=01order by s_score desc limit 3;# 统计每门课程的学生选修人数（超过5人的课程才统计）# 要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列select c.c_id, c_name, count(1) as num from course cleft join score sc on sc.c_id = c.c_idleft join student st on st.s_id = sc.s_idgroup by c_name, c.c_idhaving num&gt;5order by num,c.c_id;select distinct course.c_id,tmp.num from course join (select c_id,count(1) as num from score group by c_id)tmp where tmp.num&gt;=5 order by tmp.num desc ,course.c_id asc;# 查询各学生的年龄(周岁):# 按照出生日期来算，当前月日 &lt; 出生年月的月日则，年龄减一SELECT *,year(CURRENT_DATE())-year(s_birth) + (case when MONTH(CURRENT_DATE())&gt;MONTH(s_birth) then 1 when MONTH(CURRENT_DATE())=MONTH(s_birth) and DAY(CURRENT_DATE())&gt;DAY(s_birth) then 0 else 0end) as age FROM student;# 查询本周过生日的学生SELECT WEEKOFYEAR(STR_TO_DATE(year(CURRENT_DATE())+date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;),&#x27;%m-%d&#x27;));SELECT date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;);SELECT *, WEEKOFYEAR(concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(s_birth,&#x27;%m-%d&#x27;))) as a, WEEKOFYEAR(CURRENT_DATE()) as b,concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(s_birth,&#x27;%m-%d&#x27;)) a1,WEEKOFYEAR(s_birth) as cfrom student;# 细微差别SELECT WEEKOFYEAR(&#x27;2023-01-02&#x27;);SELECT WEEK(&#x27;2023-01-02&#x27;);select WEEKOFYEAR(concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;)));select s_name,s_sex,s_birth from student where MONTH(s_birth)=&#x27;12&#x27;;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://wingowen.github.io/tags/SQL/"}]},{"title":"高等数学","slug":"考研/高等数学","date":"2023-01-04T09:13:09.000Z","updated":"2023-09-18T07:41:57.997Z","comments":true,"path":"2023/01/04/考研/高等数学/","link":"","permalink":"https://wingowen.github.io/2023/01/04/%E8%80%83%E7%A0%94/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/","excerpt":"函数与极限","text":"函数与极限 函数与极限 函数 x,yx,yx,y 为连个变量 (x∈D)(x \\in D)(x∈D)，∀x∈D\\forall x \\in D∀x∈D 若存在唯一确定的 yyy 与 xxx 对应，称 yyy 为 xxx 的函数，记 y=f(x)y=f(x)y=f(x)。 R={y∣y=f(x),x∈D}R = \\{ y | y=f(x), x \\in D \\} R={y∣y=f(x),x∈D} DDD - 定义域；RRR - 值域。 特殊的函数 符号函数。 y=sng⁡x={−1,x&lt;00,x=01,x&gt;0y=\\operatorname{sng} x=\\left\\{ \\begin{array}{rr} -1, &amp; x&lt;0 \\\\ 0, &amp; x=0 \\\\ 1, &amp; x&gt;0 \\end{array}\\right. y=sngx=⎩⎪⎨⎪⎧​−1,0,1,​x&lt;0x=0x&gt;0​ 狄利克雷函数，QQQ 表示有理数。 f(x)={1x∈Q0x∉Qf(x)=\\left\\{\\begin{array}{ll} 1 &amp; x \\in Q \\\\ 0 &amp; x \\notin Q \\end{array}\\right.f(x)={10​x∈Qx∈/Q​ 左取整函数 [X][X][X]。 反函数 y=f(x)y=f(x)y=f(x) 严格单调，⇒x=arcf(y)\\Rightarrow x=arcf(y)⇒x=arcf(y)。 例题：求 y=ln(x+x2+1)y=ln(x+ \\sqrt{x^2 + 1})y=ln(x+x2+1​) 的反函数。 ⇒ey=x+x2+1\\Rightarrow e^y = x+ \\sqrt{x^2 + 1}⇒ey=x+x2+1​ ① ∵(x+x2+1)(−x+x2+1)=1\\because (x+ \\sqrt{x^2 + 1})(-x+ \\sqrt{x^2 + 1}) = 1∵(x+x2+1​)(−x+x2+1​)=1 ∴−x+x2+1=1ey=e−y\\therefore -x+ \\sqrt{x^2 + 1} = \\frac{1}{e^y} = e^{-y}∴−x+x2+1​=ey1​=e−y ② ① - ② ⇒2x=ey−e−y⇒x=ey−e−y2\\Rightarrow 2x=e^y-e^{-y} \\Rightarrow x = \\frac{e^y-e^{-y}}{2}⇒2x=ey−e−y⇒x=2ey−e−y​ 基本初等函数 初等函数 基本初等函数的四则运算及复合运算而形成的式子。 初等性质 奇偶性 DDD 关于原点对称。 奇函数：∀x∈D,f(−x)=−f(x)\\forall x \\in D, f(-x) = -f(x)∀x∈D,f(−x)=−f(x)。 偶函数：∀x∈D,f(−x)=f(x)\\forall x \\in D, f(-x) = f(x)∀x∈D,f(−x)=f(x)。 单调性 ∀x1,x2∈D,x1&lt;x2⇒f(x1)&lt;f(x2)\\forall x_1, x_2 \\in D, x1 &lt; x2 \\Rightarrow f(x1)&lt;f(x2)∀x1​,x2​∈D,x1&lt;x2⇒f(x1)&lt;f(x2)，单调递增。 ∀x1,x2∈D,x1&lt;x2⇒f(x1)&lt;f(x2)\\forall x_1, x_2 \\in D, x1&lt;x2 \\Rightarrow f(x1)&lt;f(x2)∀x1​,x2​∈D,x1&lt;x2⇒f(x1)&lt;f(x2)，单调递减。 有界性 ∃M&gt;0,∀x∈D⇒∣f(x)∣&lt;M\\exists M&gt;0, \\forall x \\in D \\Rightarrow |f(x)|&lt;M∃M&gt;0,∀x∈D⇒∣f(x)∣&lt;M，则称 f(x)f(x)f(x) 在 DDD 上有界，即有下界和上界。 周期性 ∃T&gt;0,∀x∈D(x+T∈D)⇒f(x+T)=f(x)\\exists T&gt;0, \\forall x \\in D (x+T \\in D) \\Rightarrow f(x+T)=f(x)∃T&gt;0,∀x∈D(x+T∈D)⇒f(x+T)=f(x)，f(x)f(x)f(x) 为周期为 T 的周期函数。 TminT_{min}Tmin​ 为最小正周期。 数列极限 (ε−N)(\\varepsilon - N)(ε−N) an{a_n}an​ 为数列，AAA 为常数。∀ε&gt;0,∃N&gt;0,n&gt;N，∣an−A∣&lt;ε\\forall \\varepsilon &gt;0, \\exists N&gt;0, n&gt;N ，|a_n -A|&lt;\\varepsilon∀ε&gt;0,∃N&gt;0,n&gt;N，∣an​−A∣&lt;ε。 lim⁡n→∞an=A/an→A(n→∞)\\lim_{n \\to \\infty} a_n= A \\quad / \\quad a_n → A (n → {\\infty}) n→∞lim​an​=A/an​→A(n→∞) 例题：证明 lim⁡n→∞n−1n+1=1\\lim_{n \\to \\infty} \\frac{n-1}{n+1}=1limn→∞​n+1n−1​=1。 ∣n−1n+1−1∣=2n+1&lt;ε|\\frac{n-1}{n+1} -1| = \\frac{2}{n+1} &lt; \\varepsilon∣n+1n−1​−1∣=n+12​&lt;ε n&gt;2ε−1n &gt; \\frac{2}{\\varepsilon} - 1n&gt;ε2​−1 取 N=[2ε−1]N=[\\frac{2}{\\varepsilon} - 1]N=[ε2​−1]，当 n&gt;N→2n+1&lt;εn&gt;N \\to \\frac{2}{n+1} &lt; \\varepsilonn&gt;N→n+12​&lt;ε 成立。 数列计算性质 唯一性，使用反证法证明 假设 lim⁡n→∞an=A\\lim_{n \\to \\infty} a_n = Alimn→∞​an​=A 且 lim⁡n→∞an=B\\lim_{n \\to \\infty} a_n = Blimn→∞​an​=B 成立。 设 ε&lt;A−B2&gt;0\\varepsilon &lt; \\frac{A-B}{2} &gt; 0ε&lt;2A−B​&gt;0 ∃Na&gt;0\\exists N_a &gt; 0∃Na​&gt;0 使之成立，∣an−A∣&lt;A−B2⇒A+B2&lt;an&lt;3A−B2|a_n - A| &lt; \\frac{A-B}{2} \\Rightarrow \\frac{A+B}{2}&lt;a_{n}&lt;\\frac{3 A-B}{2}∣an​−A∣&lt;2A−B​⇒2A+B​&lt;an​&lt;23A−B​ ① ∃Nb&gt;0\\exists N_b &gt; 0∃Nb​&gt;0 使之成立，∣an−B∣&lt;A−B2⇒3B−A2&lt;an&lt;A+B2|a_n - B| &lt; \\frac{A-B}{2} \\Rightarrow \\frac{3 B-A}{2}&lt;a_{n}&lt;\\frac{A+B}{2}∣an​−B∣&lt;2A−B​⇒23B−A​&lt;an​&lt;2A+B​ ② ① 与 ② 冲突，故假设不成立。 有界性 iflim⁡n→∞an=Aif \\lim_{n \\to \\infty} a_n = Aiflimn→∞​an​=A，则 ∃M\\exists M∃M 使得 ∣an∣≤M|a_n|≤M∣an​∣≤M 成立。反之不成立。 证：有极限则一定有界。 取 ε=1\\varepsilon = 1ε=1，则 ∃N&gt;0\\exists N&gt;0∃N&gt;0，当 n&gt;Nn&gt;Nn&gt;N 时，∣an−A∣&lt;1|a_n - A| &lt; 1∣an​−A∣&lt;1 ∵∣∣an∣−∣A∣∣≤∣an−A∣\\because ||a_n|-|A|| ≤ |a_n-A|∵∣∣an​∣−∣A∣∣≤∣an​−A∣ ∴∣∣an∣−∣A∣∣&lt;1→∣an∣&lt;1+A\\therefore ||a_n|-|A|| &lt; 1 \\to |a_n| &lt; 1 + A∴∣∣an​∣−∣A∣∣&lt;1→∣an​∣&lt;1+A，注意，这是在 n&gt;Nn&gt;Nn&gt;N 的条件下成立的。 取 M=MAX{∣a1∣,∣a2∣,∣a3∣,...,∣an∣,1+A}M = MAX\\{|a_1|,|a_2|,|a_3|,...,|a_n|,1+A\\}M=MAX{∣a1​∣,∣a2​∣,∣a3​∣,...,∣an​∣,1+A} 则 ∀n,∣an∣≤M\\forall n, |a_n| ≤ M∀n,∣an​∣≤M 成立 保号性 iflim⁡n→∞an=A&gt;0if \\lim_{n \\to \\infty} a_n = A &gt; 0iflimn→∞​an​=A&gt;0，则 ∃N&gt;0\\exists N &gt; 0∃N&gt;0 当 n&gt;Nn&gt;Nn&gt;N 时，an&gt;0a_n&gt;0an​&gt;0 iflim⁡n→∞an=A&gt;0if \\lim_{n \\to \\infty} a_n = A &gt; 0iflimn→∞​an​=A&gt;0，则 ∃N&gt;0\\exists N &gt; 0∃N&gt;0 当 n&gt;Nn&gt;Nn&gt;N 时，an&lt;0a_n&lt;0an​&lt;0 证：取 ε=A2&gt;0\\varepsilon = \\frac{A}{2} &gt; 0ε=2A​&gt;0 函数极限 ∀ε&gt;0,∃δ&gt;0\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0∀ε&gt;0,∃δ&gt;0，当 0&lt;∣x−a∣&lt;δ0&lt;|x-a|&lt;\\delta0&lt;∣x−a∣&lt;δ 时，∣f(x)−A∣&lt;ε|f(x)-A|&lt;\\varepsilon∣f(x)−A∣&lt;ε。称 f(x)f(x)f(x) 当 x→ax \\to ax→a 时，以 AAA 为极限。 lim⁡n→af(x)=A/f(x)→A(x→a)\\lim_{n \\to a} f(x)= A \\quad / \\quad f(x) → A (x → a) n→alim​f(x)=A/f(x)→A(x→a) Case ∀ε&gt;0,∃X&gt;0,x&gt;X,∣f(x)−A∣&lt;ε→limx→∞+\\forall \\varepsilon&gt;0, \\exists X&gt;0, x&gt;X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty^+}∀ε&gt;0,∃X&gt;0,x&gt;X,∣f(x)−A∣&lt;ε→limx→∞+​ ∀ε&gt;0,∃X&lt;0,x&gt;−X,∣f(x)−A∣&lt;ε→limx→∞−\\forall \\varepsilon&gt;0, \\exists X&lt;0, x&gt;-X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty^-}∀ε&gt;0,∃X&lt;0,x&gt;−X,∣f(x)−A∣&lt;ε→limx→∞−​ ∀ε&gt;0,∃X&gt;0,∣x∣&gt;X,∣f(x)−A∣&lt;ε→limx→∞\\forall \\varepsilon&gt;0, \\exists X&gt;0, |x|&gt;X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty}∀ε&gt;0,∃X&gt;0,∣x∣&gt;X,∣f(x)−A∣&lt;ε→limx→∞​ Note x→ax \\to ax→a 包含 x→a−x \\to a^-x→a− 表示左邻域；x→a+x \\to a^+x→a+ 表示右邻域。 u˚(a,δ)\\mathring{u}(a, \\delta)u˚(a,δ) 表示 aaa 的去心 δ\\deltaδ 邻域。 limn→af(x)lim_{n \\to a} f(x)limn→a​f(x) 与 f(a)f(a)f(a) 无关。 函数存在分段时，需要分开讨论，即分为左极限 x→a−x \\to a^-x→a− 与右极限 x→a+x \\to a^+x→a+。 唯一性 局部有界性 无穷小与无穷大 无穷小 0 为无穷小，但无穷小不一定为 0。 无穷小余自变量趋向有关。（去心邻域） 运算法则 α→0,β→0⇒α±β→0\\alpha \\rightarrow 0, \\beta \\rightarrow 0 \\Rightarrow \\alpha \\pm \\beta \\rightarrow 0α→0,β→0⇒α±β→0 α→0,k×α→0\\alpha \\rightarrow 0, k \\times \\alpha \\rightarrow 0α→0,k×α→0 α→0,β→0⇒α×β→0\\alpha \\rightarrow 0, \\beta \\rightarrow 0 \\Rightarrow \\alpha \\times \\beta \\rightarrow 0α→0,β→0⇒α×β→0 lim⁡x→x0f(x)=A⇔f(x)=A+α,α→0(x→x0)\\lim_{x \\to x_0}f(x)=A \\Leftrightarrow f(x)=A+\\alpha, \\alpha \\rightarrow 0 (x \\rightarrow x_0)limx→x0​​f(x)=A⇔f(x)=A+α,α→0(x→x0​) 无穷大 ∀M&gt;0,∃δ&gt;0,0&lt;∣x−x0∣&lt;δ,∣f(x)∣&gt;M⇒limx→x0f(x)=∞\\forall M&gt;0, \\exists \\delta&gt;0, 0&lt;|x-x_0|&lt;\\delta , |f(x)|&gt;M \\Rightarrow lim_{x \\to x_0}f(x)=\\infty∀M&gt;0,∃δ&gt;0,0&lt;∣x−x0​∣&lt;δ,∣f(x)∣&gt;M⇒limx→x0​​f(x)=∞ 无穷小与无穷大互为倒数。 极限地运算法则 四则运算 limx→x0f(x)=A,limx→x0g(x)=Blim_{x \\to x_0}f(x) = A, lim_{x \\to x_0}g(x) = Blimx→x0​​f(x)=A,limx→x0​​g(x)=B limx→x0[f(x)±g(x)]=limx→x0f(x)±limx→x0g(x)=A±Blim_{x \\to x_0}[f(x) \\pm g(x)] = lim_{x \\to x_0}f(x) \\pm lim_{x \\to x_0}g(x) = A \\pm Blimx→x0​​[f(x)±g(x)]=limx→x0​​f(x)±limx→x0​​g(x)=A±B limx→x0kf(x)=klimx→x0f(x)lim_{x \\to x_0}kf(x) = klim_{x \\to x_0}f(x)limx→x0​​kf(x)=klimx→x0​​f(x) limx→x0f(x)g(x)=limx→x0f(x)limx→x0g(x)=ABlim_{x \\to x_0}f(x)g(x) = lim_{x \\to x_0}f(x)lim_{x \\to x_0}g(x) = ABlimx→x0​​f(x)g(x)=limx→x0​​f(x)limx→x0​​g(x)=AB limx→x0f(x)g(x)=limx→x0f(x)limx→x0g(x)=AB,limx→x0g(x)≠0lim_{x \\to x_0}\\frac{f(x)}{g(x)} = \\frac{lim_{x \\to x_0}f(x)}{lim_{x \\to x_0}g(x)} = \\frac{A}{B}, lim_{x \\to x_0}g(x) \\ne 0limx→x0​​g(x)f(x)​=limx→x0​​g(x)limx→x0​​f(x)​=BA​,limx→x0​​g(x)=0 Note P(x)P(x)P(x) 为最大次数为 nnn 的队列，Q(x)Q(x)Q(x) 为最大次数为 mmm 的队列 若 n&gt;m,limx→x0P(x)Q(x)=∞n&gt;m, lim_{x \\to x_0}\\frac{P(x)}{Q(x)} = \\inftyn&gt;m,limx→x0​​Q(x)P(x)​=∞ 若 n&lt;m,limx→x0P(x)Q(x)=0n&lt;m, lim_{x \\to x_0}\\frac{P(x)}{Q(x)} = 0n&lt;m,limx→x0​​Q(x)P(x)​=0 复合函数极限（套娃）。 极限存在准则 准则一：收敛定理 / 夹逼定理 f(x)≤g(x)≤h(x),limf(x)=limg(x)=A⇒limg(x)=Af(x) \\le g(x) \\le h(x), limf(x)=limg(x)=A \\Rightarrow limg(x)=Af(x)≤g(x)≤h(x),limf(x)=limg(x)=A⇒limg(x)=A 准则二：单调有界函数必有极限 重要极限 limx→0sinxx=1lim_{x \\to 0}\\frac{sinx}{x}=1 limx→0​xsinx​=1 limx→0(1+1n)n=elim_{x \\to 0}(1+\\frac{1}{n})^n=e limx→0​(1+n1​)n=e 无穷小的比较","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"高等数学","slug":"高等数学","permalink":"https://wingowen.github.io/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"}]},{"title":"常用命令","slug":"运维/常用命令","date":"2023-01-04T09:03:22.000Z","updated":"2023-01-04T09:04:26.430Z","comments":true,"path":"2023/01/04/运维/常用命令/","link":"","permalink":"https://wingowen.github.io/2023/01/04/%E8%BF%90%E7%BB%B4/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"BASH 脚本 12345678910111213141516# 条件判断if [ $1 = &quot;test&quot; ]; then UNION=else UNION=fi# 判断文件是否存在if [[ ! -f [file_path] ]]; then echo &#x27;文件不存在&#x27;fi# 判断文件夹是否存在if [[ ! -d [dir_path] ]]; then echo &#x27;文件夹不存在&#x27;fi find 123find . | grep -v volumns | grep -v &quot;\\.tar&quot; | cpio -pdm ../wefe-v4/ 用户与用户组 12345678910111213141516171819# 查看系统中所有用户# 用户名:用户密码:用户 ID:群组 ID:用户信息:家目录:shell 类型cat /etc/passwd# 查看用户组信息# 用户组名:用户组密码:用户组 ID:用户列表cat /etc/group# 新建用户useradd username# 设置密码passwd username# 将普通用户放入管理员组提升为管理员usermod -G wheel username# 限制 su - 命令只有 wheel 组成员可以运行vi /etc/pam.d/su Swap 12345678910111213141516171819# 查看系统 swap 大小free -g# 查看系统的挂载盘df -h# 创建 swap 文件夹fallocate -l 48G /data/swap# 创建 swap 区域mkswap -L swap /data/swapchmod 600 /data/swap# 挂载 swap 分区swapon /data/swap# 卸载 swap 分区swapoff /data/swap 环境变量 12# 登陆系统时 shell 读取的顺序/etc/profile -&gt;/etc/enviroment --&gt;$HOME/.profile --&gt;$HOME/.env 开机自启 12345# 开机自启脚本# /etc/rc.d/rc.local 文件会在 Linux 系统各项服务都启动完毕之后再被运行chmod +x /etc/rc.d/rc.localchmod +x auto_run_script.shvi /etc/rc.d/rc.local SSH 免密登陆过程 场景：A 机器通过 SSH 免密登陆到机器 B。 条件： A 生成了私钥、公钥； B 拥有 A 的公钥（在 authorized_keys 中）。 过程： A → B：发送连接请求，信息包括用户名以及 IP 等 B 在 authorized_keys 中查找相应用户名和 IP 对应的 公钥；若有符合的公钥，则生成一个随机字符串 R，并使用 A 的公钥对 R 进行加密生成 F®； B → A：发送 F® A 对 F® 进行解密得到 R； A → B：发送 R B 检查 R=R，则允许此次登陆。 1234ssh-keygen -t rsa -P &quot;&quot;# /etc/ssh/sshd_configPermitRootLogin yes","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"脚本命令","slug":"脚本命令","permalink":"https://wingowen.github.io/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/"}]},{"title":"网络相关","slug":"运维/网络相关","date":"2022-09-01T02:45:43.000Z","updated":"2023-01-04T08:53:33.149Z","comments":true,"path":"2022/09/01/运维/网络相关/","link":"","permalink":"https://wingowen.github.io/2022/09/01/%E8%BF%90%E7%BB%B4/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/","excerpt":"iptables / netfilter","text":"iptables / netfilter iptables iptables 是 Linux 防火墙的管理工具而已，位于 /sbin/iptables；真正实现防火墙功能的是 netfilter，它是 Linux 内核中实现包过滤的内部结构。 iptables 传输数据包的过程： 当一个数据包进入网卡时，它首先进入 PREROUTING 链，内核根据数据包目的 IP 判断是否需要转送出去。 如果数据包就是进入本机的，它就会沿着图向下移动，到达 INPUT 链。数据包到了 INPUT 链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过 OUTPUT 链，然后到达 POSTROUTING 链输出。 如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出。 iptables的规则表和链： 表（tables）提供特定的功能，iptables 内置了 4 个表 filter 表，包过滤； nat 表，网络地址转换； mangle 表，包重构、修改； raw 表，数据跟踪处理。 链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一 条或数条规则。当一个数据包到达一个链时，iptables 就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则 iptables 将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。 五个链为： PREROUTING：路由选择前； INPUT：路由选择后，进入到主机中； FORWARD：路由选择后，转发； OUTPUT：路由选择后（判断用哪张网卡发出包）,流出； POSTROUTING：最后的数据流出。 常用命令： 123456789101112131415161718192021222324252627282930313233343536373839# 查看规则iptables -t 表名 -Liptables -t nat --line -nvL PREROUTING # --line 显示规则的行号# -n 不解析IP# -v 显示详细内容# 添加规则iptables -t filter -A INPUT -s 192.168.1.146 -j DROPiptables -t filter -I INPUT -s 192.168.1.146 -j ACCEPT# 指定位置iptables -t filter -I INPUT 5 -s 192.168.1.146 -j REJECT # 设置指定表的指定链的默认策略（默认动作），并非添加规则。iptables -t filter -P FORWARD ACCEPT# -I 插入到第一行# -A 插入到最后# 删除规则iptables -t filter -D INPUT 3iptables -t filter -D INPUT -s 192.168.1.146 -j DROPiptables -t filter -F INPUTiptables -t filter -F# -F 清空# 删除自定义链iptables -X WEB# 修改规则iptables -t filter -R INPUT 3 -s 192.168.1.146 -j ACCEPT# 清除包的计数iptabls -t nat -Z PREROUTING# 清除nat表所有链的计数iptabls -t nat -Z# 保存service iptables saveiptables-save &gt; /etc/sysconfig/iptables","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"网络相关","slug":"网络相关","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"}]},{"title":"服务安装","slug":"运维/服务安装","date":"2022-08-31T02:41:25.000Z","updated":"2023-09-18T09:36:54.365Z","comments":true,"path":"2022/08/31/运维/服务安装/","link":"","permalink":"https://wingowen.github.io/2022/08/31/%E8%BF%90%E7%BB%B4/%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/","excerpt":"JDK","text":"JDK JDK 12345678wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27;yum install jdk-8u202-linux-x64.rpm -ycat &gt;&gt; ~/.bash_profile &lt;&lt; EOFJAVA_HOME=/usr/java/jdk1.8.0_202-amd64/EOFsource ~/.bash_profile MySQL 123456789101112131415161718192021222324252627282930313233343536# mysql 服务下载启动# 方式一：若速度过慢采用离线下载方式yum install -y &lt;http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm&gt;yum install mysql-community-server.x86_64 -y --nogpgcheck# 方式二：离线下载mkdir /opt/mysqlcd /opt/mysqlwget &lt;http://mirrors.ustc.edu.cn/mysql-ftp/Downloads/MySQL-5.7/mysql-5.7.38-1.el7.x86_64.rpm-bundle.tar&gt;tar -xvf mysql-5.7.38-1.el7.x86_64.rpm-bundle.tarrm -rf mysql-5.7.38-1.el7.x86_64.rpm-bundle.taryum install createrepo -ycreaterepo ./cat &gt;&gt; /etc/yum.repos.d/mysql.repo &lt;&lt; EOF [mysql]name=mysqlbaseurl=file:///opt/mysql/gpgcheck=0enabled=1 EOFyum install mysql-server -y# 方式二结束systemctl start mysqldsystemctl enable mysqld# mysql 服务配置# 获取初始密码进行登陆并修改密码cat /var/log/mysqld.log | grep password# set global validate_password_policy=0;SET PASSWORD = PASSWORD(&#x27;12341234&#x27;);grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;12341234&quot;;flush privileges; 123456789# 获取初始密码cat /var/log/mysqld.log | grep password# 登陆控制台mysql -p# 设置密码及权限&gt; set global validate_password_policy=0;&gt; SET PASSWORD = PASSWORD(&#x27;wefe2022&#x27;);&gt; grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;wefe2022&quot;;&gt; flush privileges; YUM YUM 源替换。 123456yum install wget -ycd /etc/yum.repos.dwget -O /etc/yum.repos.d/CentOS-Base.repo &lt;http://mirrors.aliyun.com/repo/Centos-7.repo&gt;wget -O /etc/yum.repos.d/epel.repo &lt;http://mirrors.aliyun.com/repo/epel-7.repo&gt;yum clean allyum makecache Nacos 12345678910111213141516mkdir /data/nacos &amp;&amp; cd /data/nacos# 检查依赖yum install -y whichjava -version# nacos 官网下载wget &lt;https://github.com/alibaba/nacos/releases/download/2.0.3/nacos-server-2.0.3.zip&gt;# 备用下载链接wget &lt;https://welab-wefe-release.oss-cn-shenzhen.aliyuncs.com/IAM/RES/nacos-server-2.0.3.zip&gt;yum install unzip -yunzip nacos-server-2.0.3.zipcd nacos/bin# 启动 nacos 单机版sh startup.sh -m standalone# 服务访问地址：ip:8848/nacos# 默认账号密码：nacos / nacos Nginx 12rpm -Uvh &lt;http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm&gt;yum install nginx -y Redis 123456yum install redis -ysystemctl start redis# 测试 redis 服务redis-cli ping# PONG K8S 部署 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 关闭防火墙、Swap、SELinuxsystemctl stop firewalldsystemctl disable firewalldswapoff -asetenforce 0cat /etc/selinux/config# SELINUX=disabled# 安装系统依赖以及 dockeryum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum list docker-ce --showduplicates | sort -ryum install docker-ce.x86_64 3:18.09.9 -ysystemctl start dockersystemctl enable docker# 需保证 docker 和 kubelet 的的 cgroupdriver 是相同的cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;graph&quot;: &quot;/data/docker-compose&quot;, &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOFsystemctl restart docker# 安装 kubelet、kubeadm、kubectlcat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetes Repositorybaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOFyum install -y kubelet-1.23.0-0.x86_64 kubeadm-1.23.0-0.x86_64 kubectl-1.23.0-0.x86_64 --disableexcludes=kubernetes # Kubernetes 集群网络有很多种实现，有很大一部分都用到了 Linux 网桥# 由于网桥是虚拟的二层设备，同节点的 Pod 之间通信直接走二层转发，跨节点通信才会经过宿主机 eth0# 创建/etc/sysctl.d/k8s.conf文件，添加如下内容。表示 bridge 设备在二层转发时也去调用 iptables 配置的三层规则 (包含 conntrack)cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF# 执行如下命令使修改生效modprobe br_netfiltersysctl -p /etc/sysctl.d/k8s.confsystemctl start kubeletsystemctl enable kubelet# Master 机器操作# 修改本机 IP、镜像拉取地址 registry.aliyuncs.com/google_containerskubeadm config print init-defaults &gt; init-config.yamlkubeadm config images pull --config=init-config.yamlkubeadm init --config=init-config.yamlcd ~mkdir .kubecp /etc/kubernetes/admin.conf config# Worker 按提示操作kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers[kubeadm join ...]# 若集群出问题则重置kubeadm reset# 安装网络插件 flannelkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlcat &gt; /run/flannel/subnet.env &lt;&lt; EOFFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOF# 注意！！！这里踩坑了，卡了好几天# flannel 启动后，会生成 cni.0 与 flannel.1，这两网卡需要在同一网段cni0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 10.244.0.1 netmask 255.255.255.0 broadcast 10.244.0.255 inet6 fe80::34b6:76ff:fe2c:7f58 prefixlen 64 scopeid 0x20&lt;link&gt; ether 36:b6:76:2c:7f:58 txqueuelen 1000 (Ethernet) RX packets 107164 bytes 8642460 (8.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 105234 bytes 9770864 (9.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.244.0.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::203e:21ff:fe5f:575b prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:3e:21:5f:57:5b txqueuelen 0 (Ethernet) RX packets 196206 bytes 60155180 (57.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 233768 bytes 30816014 (29.3 MiB) TX errors 0 dropped 8 overruns 0 carrier 0 collisions 0 Spark on Yarn 集群部署 假设三台机器的 hostname 以及 ip 如下： 1234# hostname ipnode0 10.11.0.2node1 10.11.0.3node2 10.11.0.4 以下操作如无特殊说明默认在 node0 机器上操作。 SSH 免密登陆配置 12345678910111213yum install ssh -y# 在三台机器分别生成钥匙，提示输入回车即可ssh-keygen -t rsa -P &quot;&quot;# 在三台机器分别添加 hosts 解析echo &quot;10.11.0.2 node010.11.0.3 node110.11.0.4 node2&quot; &gt;&gt; /etc/hosts;# 在 node0 上执行以下代码ssh-copy-id -i ~/.ssh/id_rsa.pub root@node0ssh-copy-id -i ~/.ssh/id_rsa.pub root@node1ssh-copy-id -i ~/.ssh/id_rsa.pub root@node2 JDK 安装 12345678910# 在 node0 节点上操作wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27; yum install jdk-8u202-linux-x64.rpm -y echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_202-amd64/PATH=\\$PATH:\\$JAVA_HOME/bin&quot; &gt; /etc/profile.d/jdk.shsource /etc/profile.d/jdk.shscp /etc/profile.d/jdk.sh node1:/etc/profile.d/jdk.shscp /etc/profile.d/jdk.sh node2:/etc/profile.d/jdk.sh 资源下载 12345678910mkdir -p /data/big-data-app/rescd /data/big-data-app/reswget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgzwget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gztar -xvf spark-3.1.3-bin-hadoop2.7.tgzmv spark-3.1.3-bin-hadoop2.7 ../spark3tar -xvf hadoop-2.7.7.tar.gzmv hadoop-2.7.7.tar.gz ../hadoop2 HDFS 1234567891011121314151617181920212223242526# 增量同步工具yum install -y ssh rsync# hadoop envecho &quot;export HADOOP_HOME=/data/big-data-app/hadoop2export HADOOP_INSTALL=\\$HADOOP_HOMEexport HADOOP_MAPRED_HOME=\\$HADOOP_HOMEexport HADOOP_HDFS_HOME=\\$HADOOP_HOMEexport HADOOP_COMMON_HOME=\\$HADOOP_HOMEexport HADOOP_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport YARN_HOME=\\$HADOOP_HOMEexport YARN_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport PATH=\\$PATH:\\$HADOOP_HOME/sbin:\\$HADOOP_HOME/bin&quot; &gt; /etc/profile.d/hadoop.shsource /etc/profile.d/hadoop.shscp /etc/profile.d/hadoop.sh node1:/etc/profile.d/hadoop.shscp /etc/profile.d/hadoop.sh node2:/etc/profile.d/hadoop.sh# spark envecho &quot;SPARK_HOME=/data/big-data-app/spark3PATH=\\$SPARK_HOME/bin:\\$PATH&quot; &gt; /etc/profile.d/spark.shsource /etc/profile.d/spark.shscp /etc/profile.d/spark.sh node1:/etc/profile.d/spark.sh************scp************ /etc/profile.d/spark.sh node2:/etc/profile.d/spark.sh 修改配置，配置文件在 $HADOOP_CONF_DIR 目录下。 capacity-scheduler.xml 12345&lt;!-- DefaultResourceCalculator only uses Memory --&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;&lt;/property&gt; core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node0:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node0:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;node0:50091&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 启动动态资源需依赖 jar 包资源，操作如下 12# 在每个节点上进行操作cp $SPARK_HOME/yarn/spark-3.1.3-yarn-shuffle.jar $HADOOP_HOME/share/hadoop/yarn/lib/ 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 根据节点内存设置节点分配给 YARN 的内存，为 1024 的倍数--&gt; &lt;!-- 32G 服务器，则可分配 32*0.75*1024=24576，一般分配比例为 0.75~0.85 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;32768&lt;/value&gt; &lt;/property&gt; &lt;!-- 每个 Container 能申请的资源最大值 --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;16384&lt;/value&gt; &lt;/property&gt; &lt;!-- 动态资源相关配置 START --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt; &lt;/property&gt; &lt;!-- 动态资源相关配置 END --&gt;&lt;/configuration&gt; slaves 12node1node2 Spark 配置 1234567891011121314151617181920echo &quot;node1node2&quot; &gt; $SPARK_HOME/conf/workersecho &quot;spark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs:///logsspark.yarn.historyServer.address node0:18080&quot; &gt;&gt; $SPARK_HOME/conf/spark-default.conf# 创建日志存储目录./bin/hdfs dfs -mkdir /logs# 声明 spark 日志服务启动参数export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 \\-Dspark.history.retainedApplications=3 \\-Dspark.history.fs.logDirectory=hdfs://node0:9000/logs&quot;# 启动日志服务cd $SPARK_HOME/sbinsh start-history-server.sh 分发大数据资源 12rsync -arv /data/big-data-app node1:/data/big-data-apprsync -arv /data/big-data-app node2:/data/big-data-app 格式化 NameNode 12cd $HADOOP_HOME/bin./hdfs namenode -format 启动服务并检查 1234567891011cd $HADOOP_HOME/binstart-all.sh# 到各节点检查进程jps# node0 NameNode SecondaryNode ResourceManager# node1 DataNode NodeManager# ndoe2 DataNode NodeManager# HDFS URL http://node0:50070# YARN URL http://node0:8088 提交集群任务测试 12345678cd $SPARK_HOME./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\\--master yarn \\\\--deploy-mode cluster \\\\--driver-memory 4g \\\\--executor-memory 2g \\\\--executor-cores 1 \\\\examples/jars/spark-examples*.jar \\\\","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"网站收集","slug":"网站收集","date":"2022-08-29T08:57:30.000Z","updated":"2023-09-18T09:31:31.175Z","comments":true,"path":"2022/08/29/网站收集/","link":"","permalink":"https://wingowen.github.io/2022/08/29/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/","excerpt":"收集一些有用的网站。","text":"收集一些有用的网站。 杂项 Hexo 博客主题 pure 使用说明 | Cofess - Web Developer &amp; Designer JAVA Spring All 刷题 HiveSQL 50 Pandas 120","categories":[{"name":"日常","slug":"日常","permalink":"https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"}],"tags":[{"name":"网站收集","slug":"网站收集","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"}]},{"title":"计算机科学","slug":"编程/计算机科学","date":"2022-08-15T09:16:17.000Z","updated":"2022-08-29T09:05:56.912Z","comments":true,"path":"2022/08/15/编程/计算机科学/","link":"","permalink":"https://wingowen.github.io/2022/08/15/%E7%BC%96%E7%A8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/","excerpt":"协程。","text":"协程。 协程 操作系统在线程等待 IO 的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待 IO 的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。 协程刚好可以解决上述 2 个问题。协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。 协程只有在等待 IO 的过程中才能重复利用线程。 假设协程运行在线程之上，并且协程调用了一个阻塞 IO 操作，这时候会发生什么？实际上操作系统并不知道协程的存在，它只知道线程，因此在协程调用阻塞 IO 操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥其作用。","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}]},{"title":"降维","slug":"算法/降维","date":"2022-08-14T08:48:21.000Z","updated":"2022-08-29T09:04:45.843Z","comments":true,"path":"2022/08/14/算法/降维/","link":"","permalink":"https://wingowen.github.io/2022/08/14/%E7%AE%97%E6%B3%95/%E9%99%8D%E7%BB%B4/","excerpt":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效 信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。","text":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效 信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。 低维嵌入介绍 在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个低维嵌入 embedding。 缓解维数灾难的一个重要途径是降维 dimension reduction。它是通过某种数学变换将原始高纬度属性空间转变为一个低维子空间，在这个子空间中样本密度大幅提高，计算距离也变得更为容易。低维嵌入的目的是解决 k 邻近学习方法可操作性弱的问题。 将一个三维问题垂直投影，变成一个二维问题。 这种方法叫做多维缩放 Multiple Dimensional Scaling，简称 MDS，这是一种经典的降维方法。 MDS 12345678910111213141516# 导入包import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasets,manifoldfrom collections import Counterdef load_data(): # 使用 scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() return iris.data,iris.target# 产生用于降维的数据集X, y=load_data()print(X.shape)print(Counter(y))","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"特征选择","slug":"算法/特征选择","date":"2022-08-13T14:41:12.000Z","updated":"2022-08-29T09:05:47.711Z","comments":true,"path":"2022/08/13/算法/特征选择/","link":"","permalink":"https://wingowen.github.io/2022/08/13/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。","text":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。 过滤式选择 先对数据集进行特征选择，然后再训练分类器，特征选择过程与后续训练无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。 Relief 选择法 Relief Relevant Features，该方法设计了一个相关统计量来度量特征的重要性，并且其是一个向量，其每个分量分别对应一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。最终只需要确定一个阈值 r，然后选择比 r 大的相关统计量分量所对应的特征即可。也可以指定选取相关统计量分量最大的前 k 个特征。 FInsher 选择法 计算数据集中每个类别样本的类内特征方差与类间特征方差。 类内特征方差越小，类间特征方差越大，越有利于后续的分类训练，即该特征需要保留，反之该特征需要滤除。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#导入包import pandas as pdimport numpy as np#创建示例样本sample = [[1,1,5],[10,0,1],[2,5,6]]label = [1, 0, 1]print(&quot;sample:&quot;,sample)print(&quot;label:&quot;,label)#判断样本长度与类标长度是否匹配if len(sample) != len(label): print(&#x27;Sample does not match label&#x27;) exit()#创建并保存计算过程中的变量df1 = pd.DataFrame(sample)df2 = pd.DataFrame(label, columns=[&#x27;label&#x27;])data = pd.concat([df1, df2], axis=1) # 合并成为一个dataframeprint(&quot;data:\\n&quot;,data,&#x27;\\n&#x27;)data0 = data[data.label == 0]#对标签分类，分成包含0和1的两个dataframedata1 = data[data.label == 1]n = len(label)#标签长度n1 = sum(label)#1类标签的个数n0 = n - n1#0类标签的个数lst = []#用于返回的列表features_list = list(data.columns)[:-1]print(&quot;特征维数:&quot;)print(features_list)#fisher score计算for feature in features_list: print(&#x27;\\nfeature&#x27;,feature,&#x27;:&#x27;) # 算关于类标0 m0_feature_mean = data0[feature].mean() # 0 类标签在第 m 维上的均值 print(&quot;m0_feature_mean&quot;,m0_feature_mean) m0_SW=sum((data0[feature] -m0_feature_mean )**2) # 0类在第 m 维上的类内方差 print(&quot;m0_SW&quot;,m0_SW) # 算关于类标1 m1_feature_mean = data1[feature].mean() # 1 类标签在第 m 维上的均值 print(&quot;m1_feature_mean&quot;,m1_feature_mean) m1_SW=sum((data1[feature] -m1_feature_mean )**2)# 1 类标签在第 m 维上的类内方差 print(&quot;m1_SW&quot;,m1_SW) # 算关于 data m_all_feature_mean = data[feature].mean() # 所有类标签在第 m 维上的均值 print(&quot;m_all_feature_mean&quot;,m_all_feature_mean) m0_SB = n0 / n * (m0_feature_mean - m_all_feature_mean) ** 2 # 0 类标签在第 m 维上的类间方差 print(&quot;m0_SB&quot;,m0_SB) m1_SB = n1 / n * (m1_feature_mean - m_all_feature_mean) ** 2 # 1 类标签在第 m 维上的类间方差 print(&quot;m1_SB&quot;,m1_SB) m_SB = m1_SB + m0_SB # 计算SB print(&quot;m_SB&quot;,m_SB) m_SW = (m0_SW + m1_SW) / n # 计算 SW print(&quot;m_SW&quot;,m_SW) if m_SW == 0: # 0/0类型也是返回nan m_fisher_score = np.nan else: # 计算Fisher score m_fisher_score = m_SB / m_SW #计算第m维特征的Fisher score #Fisher score值添加进列表 print(&quot;m_fisher_score&quot;,m_fisher_score) lst.append(m_fisher_score) 包裹式选择 包裹式特征选择直接将最终要使用的学习器的性能作为特征子集的评价准则，包裹式特征选择的目的就是为给定的学习器选择最有利于其性能而量身定做的特征子集。 一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，比过滤式特征选择更好，但由于在特征选择过程中要多次训练学习器，因此包裹式特征选择的计算开销比过滤式特征选择大得多。 LVW 是一个经典的包裹式特征选择方法，它在拉斯维加斯方法框架下使用随机策略进行子集搜索，以最终分类器误差作为特征子集评价标准。 除了 LVW 包裹式特征选择之外，RFE(递归特征消除)也是一种常见的包裹式特征选择方法，RFE特征选择使用模型准确率来判断哪些特征（或特征组合）对预测结果贡献较大，并且递归地去除贡献小的特征。 除了 RFE 之外，还有一种选择算法称为 RFECV，其是以 RFE 为基础进行改进得到的。 RFECV 通过交叉验证的方式执行 RFE，以此来选择最佳数量的特征，即不手动设置保留的特征数量。对于一个数量为 d 的 feature 的集合，他的所有的子集的个数是 2 的 d 次方减 1 (包含空集)。指定一个外部的学习算法，比如 SVM 之类。通过该算法计算所有子集的validation error。选择 error 最小的那个子集作为所挑选的特征。 123456789101112131415161718192021222324252627282930313233343536373839404142#导入包from sklearn.feature_selection import RFE,RFECVfrom sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn import model_selection&#x27;&#x27;&#x27;class RFE(BaseEstimator, MetyuanaEstimatorMixin, SelectorMixin): 参数： BaseEstimator: 基模型 n_features_to_select：目标特征数量 return：经过选择后的特征 比较经过特征选择和未经特征选择的数据集，对 LinearSVC 的预测性能的区别&#x27;&#x27;&#x27;### 加载数据iris = load_iris()X, y = iris.data, iris.target### 特征提取estimator = LinearSVC()selector = RFE(estimator=estimator, n_features_to_select=2)X_t = selector.fit_transform(X, y) #对样本进行特征选择，最终保留n_features_to_select个特征。print(&quot;\\n特征选择结果显示:&quot;)print(&quot;原数据样本X：&quot;,X[1])print(&quot;特征选择后样本X_t：&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0, stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))clf_t.fit(X_train_t, y_train_t)print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t))) 嵌入式选择 嵌入式特征选择是将特征选择过程与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。 正则化嵌入式选择 L1L1L1 范数与 L2L2L2 范数都有利于降低过拟合，但前者还会带来一个额外的好处，即 L1L1L1 范数比 L2L2L2 范数更容易获得稀疏解，即它求得的 www 会有更少的非零分量。 其中，基于 L1L1L1 正则化的学习方法是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。 123456789101112131415161718192021222324252627282930313233#导入包from sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectFromModelfrom sklearn import model_selection#导入数据集并打印示例iris = load_iris()X, y = iris.data, iris.targetprint(&quot;原始数据特征维数：&quot;,len(X[1])) # (150, 4)print(&quot;原始数据样本：&quot;,X[1])#特征选择lsvc = LinearSVC(C=0.01, penalty=&quot;l1&quot;, dual=False).fit(X, y) #设置分类器model = SelectFromModel(lsvc, prefit=True) #设置模型为特征选择X_t = model.transform(X) #获取经过筛选的数据print(&quot;特征选择数据特征维数&quot;,len(X_t[1])) #(150, 3)print(&quot;特征选择后数据样本&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0,stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)clf_t.fit(X_train_t, y_train_t)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t)))","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性回归与逻辑回归","slug":"算法/线性回归与逻辑回归","date":"2022-08-12T02:35:16.000Z","updated":"2022-09-09T11:25:52.979Z","comments":true,"path":"2022/08/12/算法/线性回归与逻辑回归/","link":"","permalink":"https://wingowen.github.io/2022/08/12/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等","text":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等 线性回归 线性回归分析 Linear Regression Analysis 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。 所以线性的定义为：自变量之间只存在线性关系，即自变量只能通过相加、或者相减进行组合。 监督学习 如果现在有一个房子 H1，面积是 S，监督学习如何估算它的价格？ 监督学习从训练集中找到面积最接近 S 的房子 H2，预测 H1 的价格等于 H2 的价格。 监督学习根据训练集，找到一个数学表达式，对任意的面积的房子都可以估算出其价格。 h 代表假设函数：Training Set → Learning Algorithm → h；Size of House + h → Estimated Price。 线性回归的假设模型 hθ(x)=θ0+θ1xh_{\\theta}(x)=\\theta_{0}+\\theta_{1} x hθ​(x)=θ0​+θ1​x 如何求解模型，有以下两种思路。 尝试一些 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 的组合，选择能使得画出的直线正好穿过训练集的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 。 尝试一些 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 的组合，然后在训练集上进行预测，选能使得预测值与真实的房子价格最接近的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 。 选择最佳的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​，使得 hθ(x)h_{\\theta}(x)hθ​(x) 对所有的训练样本 (x,y)(x, y)(x,y) 来说，尽可能的接近 yyy。 损失函数 Train Set: {(x(1),y(1)),(x(2),y(2)),⋯ ,(x(m),y(m))}\\left\\{\\left(x^{(1)}, y^{(1)}\\right),\\left(x^{(2)}, y^{(2)}\\right), \\cdots,\\left(x^{(m)}, y^{(m)}\\right)\\right\\}{(x(1),y(1)),(x(2),y(2)),⋯,(x(m),y(m))} 12m∑i=1m(hθ(x(i))−y(i))2\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2} 2m1​i=1∑m​(hθ​(x(i))−y(i))2 最小化损失函数，得到的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 是最佳的。 12345678910# 房屋的价格和面积数据import numpy as npdata = np.array([[2104, 460], [1416, 232], [1534, 315], [852,178]])# 使用线性回归模型计算预测值def get_predict(x, theta0, theta1): h = theta0 + theta1 * x #todo return h 梯度下降算法 梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合 (θ0,θ1,......,θn)(\\theta_{0},\\theta_{1},......,\\theta_{n})(θ0​,θ1​,......,θn​) 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合， 可能会找到不同的局部最小值。 实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，需要同时更新 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​，实现方法是：你应该计算公式右边的部分，通过那一部分计算出 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​的值，然后同时更新 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​。 temp0 :=θ0−α∂∂θ0J(θ0,θ1) temp1 :=θ1−α∂∂θ1J(θ0,θ1)θ0:= temp0 θ1:= temp1 \\text { temp0 }:=\\theta_{0}-\\alpha \\frac{\\partial}{\\partial \\theta_{0}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\text { temp1 }:=\\theta_{1}-\\alpha \\frac{\\partial}{\\partial \\theta_{1}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\theta_{0}:=\\text { temp0 } \\\\ \\theta_{1}:=\\text { temp1 } temp0 :=θ0​−α∂θ0​∂​J(θ0​,θ1​) temp1 :=θ1​−α∂θ1​∂​J(θ0​,θ1​)θ0​:= temp0 θ1​:= temp1 逻辑回归 二分类问题下，采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。它适用于标签 y 取值离散的情况。 逻辑函数 Logistic Function，一个最常用的逻辑函数是 Sigmoid Function，以 Z=0 为决策界限，公式如下： g(z)=11+e−zg(z)=\\frac{1}{1+e^{-z}} g(z)=1+e−z1​ 123import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 逻辑回归模型假设 hθ(x)=g(θTX)h_\\theta(x)=g(\\theta^TX) hθ​(x)=g(θTX) hθ(x)h_\\theta(x)hθ​(x) 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性 （estimated probablity），即 $ h_\\theta(x) = P(y=1|x;\\theta)$。 例如，如果对于给定的 x，通过已经确定的参数计算得出 hθ(x)h_\\theta(x)hθ​(x)=0.7，则表示有 70% 的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7 = 0.3。 损失函数 线性回归模型的代价函数是所有模型误差的平方和，若逻辑回归的假设模型沿用这个定义，得到的函数将是一个非凸函数 non-convex function。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 J(θ)=1m∑i=1mCost(hθ(x(i)),y(i))J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)}) J(θ)=m1​i=1∑m​Cost(hθ​(x(i)),y(i)) Cost⁡(hθ(x),y)={−log⁡(hθ(x)) if y=1−log⁡(1−hθ(x)) if y=0\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{aligned} -\\log \\left(h_{\\theta}(x)\\right) &amp; \\text { if } y=1 \\\\ -\\log \\left(1-h_{\\theta}(x)\\right) &amp; \\text { if } y=0 \\end{aligned}\\right. Cost(hθ​(x),y)={−log(hθ​(x))−log(1−hθ​(x))​ if y=1 if y=0​ Cost(hθ(x),y)=−y×log(hθ(x))−(1−y)×log(1−hθ(x))Cost(h_\\theta(x), y)=-y\\times{log(h_\\theta(x))}-(1-y)\\times{log(1-h_\\theta(x))} Cost(hθ​(x),y)=−y×log(hθ​(x))−(1−y)×log(1−hθ​(x)) J(θ)=−1m∑i=1m[y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i)))]J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))] J(θ)=−m1​i=1∑m​[y(i)log(hθ​(x(i)))+(1−y(i))log(1−hθ​(x(i)))] 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) return np.sum(first - second) / (len(X)) 当实际的 y=1 且 hθ(x)h_\\theta(x)hθ​(x) 也为1 时误差为 0，当 y=1 但 hθ(x)h_\\theta(x)hθ​(x) 不为 1 时误差随着 hθ(x)h_\\theta(x)hθ​(x) 的变小而变大； 当实际的 y=0 且 hθ(x)h_\\theta(x)hθ​(x) 也为 0 时代价为 0，当 y=0 但 hθ(x)h_\\theta(x)hθ​(x) 不为 0 时误差随着 hθ(x)h_\\theta(x)hθ​(x) 的变大而变大。 同样使用梯度下降法对参数进行更新： θj:=θj−α1m∑i=1m(hθ(x(i))−y(i))xj(i)\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j θj​:=θj​−αm1​i=1∑m​(hθ​(x(i))−y(i))xj(i)​ 123456789import numpy as np# 返回某一轮训练中的梯度def _gradient(X, Y_label, theta): # _f用来计算 y 的值 y_pred = _f(X, theta, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) return w_grad 多元分类 我们将多个类中的一个类标记为正向类 y=1，然后将其他所有类都标记为负向类，这个模型记作 h(1)θ(x)h^{(1)_\\theta(x)}h(1)θ​(x)。接着，类似地第我们选择另一个类标记为正向类 y=2，再将其它类都标记为负向类，将这个模型记作 h(2)θ(x)h^{(2)_\\theta(x)}h(2)θ​(x)，依此类推。 最后我们得到一系列的模型简记为： h(i)θ(x)=p(y=i∣x;θ)h^{(i)_\\theta(x)} = p(y=i|x;\\theta) h(i)θ​(x)=p(y=i∣x;θ) 最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：h(i)θ(x)h^{(i)_\\theta(x)}h(i)θ​(x)， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 h(i)θ(x)h^{(i)_\\theta(x)}h(i)θ​(x) 最大的 i，即 max⁡ih(i)θ(x)\\max_ih^{(i)_\\theta(x)}maxi​h(i)θ​(x)。 过拟合化和正则化 过拟合在训练数据上的表现非常好；对于非训练的数据点，过拟合的模型表现与我们的期望有较大的偏。 减少拟合化的方法： 减少选取变量的数量：选取最重要的参数； 正则化：一种减小参数大小的办法。 正则化 回归：岭回归。 分类：L1 正则化，L2 正则化。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Python 编程","slug":"编程/Python编程","date":"2022-08-01T03:17:02.000Z","updated":"2023-01-04T09:00:47.662Z","comments":true,"path":"2022/08/01/编程/Python编程/","link":"","permalink":"https://wingowen.github.io/2022/08/01/%E7%BC%96%E7%A8%8B/Python%E7%BC%96%E7%A8%8B/","excerpt":"Python 编程开发查漏补缺。 gRPC Redis","text":"Python 编程开发查漏补缺。 gRPC Redis GRPC Google 开发的基于 HTTP/2 和 Protocol Buffer 3 的 RPC 框架。 Protocol Buffers, protobuf：结构数据序列化机制。 gRPC 默认使用 Brotocol Buffers，用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。 定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 GRPC 服务器来处理客户端调用。在客户端拥有一个存根 Stub，存根负责接收本地方法调用，并将它们委派给各自的具体实现对象（在远程服务器上）。 简单实现 实现一个简单的 gRPC HelloWorld。 proto file 定义 Protocol Buffers 规则文件。 123456789101112131415161718syntax = &quot;proto3&quot;;package helloworld;service Greeter &#123; // 定义方法参数和返回类型 rpc SayHello (HelloRequest) returns (HelloResponse) &#123;&#125;&#125;// 请求结构声明message HelloRequest &#123; string name = 1;&#125;// 响应结构声明message HelloResponse &#123; string message = 1;&#125; 运行 grpc_tools 工具。 1python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto 生成 python 代码。 helloworld_pb2.py 为 Protocol Buffers 的 Python 实现。 1234567891011121314151617181920# helloworld_pb2_grpc.py 用于 gRPC 实现的 Python 方法实现# 客户端存根class GreeterStub(object): def __init__(self, channel): self.SayHello = channel.unary_unary( &#x27;/helloworld.Greeter/SayHello&#x27;, request_serializer=helloworld__pb2.HelloRequest.SerializeToString, response_deserializer=helloworld__pb2.HelloResponse.FromString, )# 服务端服务class GreeterServicer(object): def SayHello(self, request, context): context.set_code(grpc.StatusCode.UNIMPLEMENTED) context.set_details(&#x27;Method not implemented!&#x27;) raise NotImplementedError(&#x27;Method not implemented!&#x27;) def add_GreeterServicer_to_server(servicer, server): # ...... server 自定义 gRPC 服务端。 1234567891011121314151617181920212223import grpcimport randomfrom concurrent import futuresimport helloworld_pb2import helloworld_pb2_grpc# 实现定义的方法，继承并实现方法class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloResponse(message=&#x27;Hello &#123;msg&#125;&#x27;.format(msg=request.name))def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) # 绑定处理器 helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) # 未使用 SSL，所以是不安全的 server.add_insecure_port(&#x27;[::]:50054&#x27;) server.start() print(&#x27;gRPC 服务端已开启，端口为 50054...&#x27;) server.wait_for_termination()if __name__ == &#x27;__main__&#x27;: serve() client 自定义客户端。 1234567891011121314import grpcimport helloworld_pb2, helloworld_pb2_grpcdef run(): # 本次不使用 SSL，所以 channel 是不安全的 channel = grpc.insecure_channel(&#x27;localhost:50054&#x27;) # 客户端实例 stub = helloworld_pb2_grpc.GreeterStub(channel) # 调用服务端方法 response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#x27;World&#x27;)) print(&quot;Greeter client received: &quot; + response.message)if __name__ == &#x27;__main__&#x27;: run() Redis REmote DIctionary Server, Redis 是一个 key-value 存储系统，是跨平台的非关系型数据库。 123456789101112131415pip install redisimport redis # 导入redis 模块# 获取连接r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True) # Redis 实例会维护一个自己的连接池，建立连接池，从连接池获取连接pool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)r = redis.Redis(connection_pool=pool)r.set(&#x27;name&#x27;, &#x27;runoob&#x27;, nx, xx) # 设置 name 对应的值, 当 nx = Ture 则只有 Key 不存在才执行插入; xx 相反print(r[&#x27;name&#x27;])print(r.get(&#x27;name&#x27;), px, ex) # 取出键 name 对应的值, px 毫秒 ex 秒 为过期时间print(type(r.get(&#x27;name&#x27;))) # 查看类型 在使用中，Redis 存储可分为两大类： set 即 k v，这里的 v 通常是一个字符串。 hset 即 k Hash-v，这里的 v 是一个 Redis Hash，是一个 string 类型的 field（字段）和 value（值）的映射表。 缓存技术 缓存就是利用编程技术将数据存储在临时位置，而不是每次都从源数据去检索。 Flask 基于 Flask 实现自定义控制器规则。 TODO","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"}]},{"title":"决策树算法","slug":"算法/决策树算法","date":"2022-07-31T02:58:15.000Z","updated":"2023-01-04T07:19:42.822Z","comments":true,"path":"2022/07/31/算法/决策树算法/","link":"","permalink":"https://wingowen.github.io/2022/07/31/%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/","excerpt":"基本概念。","text":"基本概念。 决策树的基本概念 决策树节点： 叶节点表示一份类别或者一个值； 非叶节点表示一个属性划分。 决策树的有向边代表了属性划分的不同取值： 当属性是离散时，可将属性的每一个取值用一条边连接到子结点； 当属性是连续时，需要特殊处理。 决策树是一种描述实例进行分类的树形结构。 对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。 决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略；决策树的学习过程本质上是从训练数据中寻找一组分类规则；决策树学习也可以看做是由训练数据集估计条件概率模型。 由上述描述可以得知，决策树学习是一个递归过程，有三种情况会导致递归返回： 当前结点包含的所有样本属于同一类别。 当前属性结合为空，或所有样本在所有属性上的取值都相同：将当前结点标记为叶结点，其类别为该节点包含的样本最多的类别。 当前结点包含的样本基本为空：将当前结点标记为叶结点，其类别为父节点包含样本最多的类别 决策树的学习结果为：树结构 + 叶节点的取值（类别） 信息增益 熵，又称信息熵，是信息论的重要概念。熵是度量样本集合纯度的指标，熵越大，样本的纯度越低。假设当前样本集合 DDD 中第 iii 类样本所占比例的为 pi(i=1,2,...,C)p_i(i = 1,2,...,C)pi​(i=1,2,...,C)，则 DDD 的熵定义为： H(D)=−∑i=1Cpilog⁡2piH(D)=-\\sum_{i=1}^{C} p_{i} \\log _{2} p_{i} H(D)=−i=1∑C​pi​log2​pi​ 信息增益表示特征对于当前样本集纯度提升的程度。某属性的信息增益越大，说明使用改属性进行划分获得的纯度提升越大。因此使用信息增益进行决策树属性选择时，选择属性信息增益最大的作为当前节点。 G(D,a)=H(D)−∑v=1V∣Dv∣∣D∣H(Dv)G(D, a)=H(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} H\\left(D^{v}\\right) G(D,a)=H(D)−v=1∑V​∣D∣∣Dv∣​H(Dv) 123456789101112131415161718192021222324252627282930# 信息增益计算def get_G(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data 中所有样本属于同一类，由于 xlnx 在 x = 1和 x-&gt;0 是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE return H - Eprint(&#x27;各个属性的信息增益为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_G(data,i,len(data.columns)-1)) 信息增益比 以信息增益作为划分数据集的特征，会导致对可取值数目较多的属性有所偏好。为了缓解这种不良影响，采用信息增益比作为选择属性的准则。 123456789101112131415161718192021222324252627282930def get_GR(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益比 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益比, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 IV = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data中所有样本属于同一类，由于xlnx 在x = 1和x-&gt;0是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE IV = IV - new_data.shape[0] / data.shape[0] * (np.log2(new_data.shape[0] / data.shape[0])) G = H - E return G / IVprint(&#x27;各个属性的信息增益比为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_GR(data,i,len(data.columns)-1)) 基尼系数 纯度使用基尼指数来度量，在使用基尼系数作为指标时，应该选择基尼指数最小的属性。 1234567891011121314151617181920212223def get_Gini(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的基尼系数 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的基尼系数, type:float, e.g.:0.32 &#x27;&#x27;&#x27; Gini = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] Gini_v = 1 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] Gini_v = Gini_v - p * p Gini = Gini + new_data.shape[0] / data.shape[0] * Gini_v return Giniprint(&#x27;各个属性的基尼指数为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_Gini(data,i,len(data.columns)-1)) ID3 ID3 算法的核心是在决策树各结点上使用信息增益准则选择特征：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，根据特征的不同取值建立子结点。递归地调用以上方法，构建决策树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def build_tree_id3(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用 ID3 算法在 data 数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_G(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_id3(new_data, nu, new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)root_id3 = build_tree_id3(data, None, ori_ppt, len(data.columns)-1)# 可视化createPlot(root_id3) C4.5 C4.5 算法对 ID3 算法进行了改进，即使用信息增益比来选择特征，其余和 ID3 算法基本相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def build_tree_c45(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用C4.5算法在data数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_GR(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_c45(new_data, nu,new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)# print(data)root_c45 = build_tree_c45(data, None ,ori_ppt, len(data.columns)-1)createPlot(root_c45) 损失函数与剪枝 决策树的剪枝往往通过最小化决策树的损失函数实现。 123456789101112131415161718192021def cal_loss(root, alpha): &#x27;&#x27;&#x27; 计算以root为根结点的决策树的损失值 :param root: 根结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 以root为根结点的决策树的损失值, type:float, e.g.:0.24 &#x27;&#x27;&#x27; loss = 0 for leaf in root.leaf_ch: data = leaf.data for v in np.unique(data.iloc[:,len(data.columns)-1]): ntk = data[data.iloc[:,len(data.columns)-1] == v].shape[0] loss = loss - ntk * np.log2(ntk / data.shape[0]) loss = loss + alpha * len(root.leaf_ch) return lossori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)cal_loss(root_c45, 0.3) 决策树生成算法递归地产生决策树，直到无法继续。这种做法会带来过拟合问题。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，构建过于复杂的决策树。因此，一种解决方法是考虑决策树的复杂程度，从而对决策树进行简化。对决策树进行简化的过程称为剪枝。即从决策树中裁掉一些子树或叶结点，将其根节点或父节点作为新的叶结点。 剪枝算法的实现 计算每个节点的经验熵。 递归地从树的叶结点向上回缩，若回缩后的损失值 &gt; 回缩前的损失值，则进行剪枝，父节点变为叶节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def tree_pruning(root, leaf, alpha): &#x27;&#x27;&#x27; 决策树剪枝 :param root: 根结点, type:Node :param leaf: 叶结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 剪枝后的决策树根结点, type:Node &#x27;&#x27;&#x27; new_root = copy.deepcopy(root) pre_loss = cal_loss(root, alpha) flag = 1 for nl in leaf.fa.leaf_ch.copy(): nl.can_delete = 1 new_set = set() for leaf_ch in root.leaf_ch: if leaf_ch.can_delete != 1: new_set.add(leaf_ch) root.leaf_ch = new_set leaf.fa.set_leaf(leaf.fa.data.iloc[:,len(root.data.columns)-1].value_counts().keys()[0]) root.add_leaf_ch(leaf.fa) after_loss = cal_loss(root, alpha) if after_loss &gt;= pre_loss: #不剪枝 root = new_root flag = 0 return root, flag ori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)# createPlot(root_c45)#设置超参数alpha = 0.3update = 1while update == 1: update = 0 for leaf in root_c45.leaf_ch.copy(): root_c45,flag = tree_pruning(root_c45, leaf, alpha) if flag: update = 1# root_c45createPlot(root_c45) 连续值处理 缺失值处理","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算机组成","slug":"考研/计算机组成","date":"2022-07-30T07:15:43.000Z","updated":"2023-01-04T07:44:06.772Z","comments":true,"path":"2022/07/30/考研/计算机组成/","link":"","permalink":"https://wingowen.github.io/2022/07/30/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/","excerpt":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线","text":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线 计算机系统概论 计算机系统层次结构 微程序机器 M0 微指令系统： 由硬件直接执行微命令； 实际机器 M1 机器语言机器：用微程序解释机器指令； 虚拟机器 M2 操作系统机器：用机器语言解释操作系统； 虚拟机器 M3 汇编语言机器：用汇编程序翻译成机器语言程序； 虚拟机器 M4 高级语言机器：用编译程序翻译成汇编语言程序或其它中间语言程序。 计算机的基本组成 冯·诺依曼提出存储程序的概念，以此概念为基础的计算机通称为冯·诺依曼计算机，其具有如下特点： 计算机由运算器、存储器、控制器、输入设备和输出设备五大部件组成； 指令和数据以同地位存放于存储器内，可按地址寻访； 指令和数据均用二进制数表示； 指令由操作码和地址码组成，操作码用来表示操作的性质，地址码用来表示操作数在存储器中的位置； 指令在存储器内按顺序存放。通常，指令是顺序执行的，在特定条件下，可根据运算结果或根据设定的条件改变执行顺序。 机器以运算器为中心，输入输出设备与存储器间的数据传送通过运算器完成。 各部件的功能如下： 运算器用来完成算术运算和逻辑运算，并将运算的中间结果暂存在运算器内； 存储器用来存放数据和程序； 控制器用来控制、指挥程序的运行、程序的输入输出以及处理运算结果； 运算器和控制器在逻辑关系和电路结构上联系十分紧密，两大部件往往集成在同一芯片上，因此通常将它们合起来统称为中央处理器 CPU, Central Processing Unit。 现代计算机组成：CPU, I/O 以及 主存储器 Main Memory, MM。 CPU + MM 称为主机；I/O 有称为外部设备。 Arithmetic Logic Unit, ALU 算术逻辑部件，用来完成算术逻辑运算；Control Unit, CU 控制单元，用来解释存储器中的指令，并发出各种操作命令来执行指令。 ALU 和 CU 是 CPU 的核心部件； I/O 设备也受 CU控制，用来完成相应的输入、输出操作。 计算机的工作步骤 TODO 系统总线 计算机系统的五大部件之间的互连方式有两种： 各部件之间使用单独的连线，成为分散连接； 另一种是各部件连到一组公共信息传输线上，成为总线连接。 总线是连接多个部件的信息传输线，是各部件共享的传输介质。当多个部件与总线相连时，如果出现两个或两个以上部件同时向总线发送信息，势必导致信号冲突，传输无效。因此，在某一时刻，只允许有一个部件向总线发送信息，而多个部件可以同时从总线上接收相同信息。 以运算器为中心的结构。I/O 设备与主存交换信息时仍然要占用 CPU，因此还会影响 CPU 的工作效率。 单总线（系统总线）。I/O 设备与主存交换信息时，原则上不影响 CPU 的工作，CPU 仍可继续处理不访问主存或 I/O 设备的操作，提高了 CPU 的工作效率。 但当某一时刻各部件都要占用总线时，就会发生冲突，必须设置总线判优逻辑，让各部件按优先级高低来占用总线，这也会影响整机的工作速度。 以存储器为中心的双总线结构框图。存储中线只供主存与 CPU 之间传输信息，即提高了传输效率，又减轻了系统总线的负担，还保留了 I/O 设备与存储器交换信息不经过 CPU 的特点。 总线分类 片内总线：芯片内部总线； 系统总线：各大部件之间的信息数据信息；按系统总线传输信息的不同，又可分为三类**：数据总线、地址总线和控制总线**。 通信总线：计算机系统之间或与其他系统之间的通信。 总线特性及性能指标 TODO 总线结构 TODO 总线控制 判优控制（仲裁逻辑）和通信控制。 总线判优控制 主设备：对总线有控制选；从设备：只能响应从主设备发来的总线命令，对总线没有控制权。 若多个主设备同时要使用总线时，由总线的判优、仲裁逻辑按一定的优先等级顺序确定哪个主设备能使用总线，只有获得总线使用权的主设备才能开始传送数据。 总线判优控制可分集中式和分布式两种： 将控制逻辑集中在一处； 将控制逻辑分散在与总线连接的各个部件或设备上。 集中控制优先仲裁方式 链式查询方式 控制总线中有 3 根线用于总线控制：BS 总线忙、BR 总线请求、BG 总线同意。其中 BG 是串行从一个 I/O 接口送到下一个 I/O 接口。如果 BG 到达的接口有总线请求 BR，BG 信号就不再往下传，意味着该接口获得了总线使用权，并建立总线忙 BS 信号，表示它占用了总线。 离总线控制部件最近的设备具有最高优先级。 只需很少几根线就能按一定有限次序实现总线控制，并且很容易扩充设备，但对电路故障很敏感，且优先级别低的设备可能很难获得请求。 计数器定时查询方式 与链式查询方式相比，多了一组设备地址线，少了一根总线同意线 BG。 总线控制部件接到由 BR 送来的总线请求信号后，在 BS=0 时，总线控制部件中的计数器开始计数，并通过设备地址线向各设备发出一组地址信号。当某个请求占用总线的设备地址与计数值一致时，便获得总线使用权，此时终止计数查询。 初始值可由程序设置；终止计数后可以重头开始，也可以从上一次计数终点开始。 对电路故障敏感度小于链式查询方式，但增加了控制线数（设备地址）目，控制也较复杂。 独立请求方式 每一台设备均有一对总线请求线和总线同意线。当设备要求使用总线时，便发出改设备的请求信号。总线控制部件中有一排队电路，可根据优先次序确定响应哪一台设备的请求。 响应快，优先次序控制灵活（根据程序改变），控制线数量多，总线控制更复杂。 通信控制 通常将完成一次总线操作的时间称为总线周期，可分为以下 4 各阶段。 申请分配阶段：主模块提出申请，总线仲裁机构决定下一传数周期的总线使用权授予某一申请者； 寻址阶段：取得了使用权的主模块通过总线发出本次要访问的从模块的地址及有关命令。启动参与本次传数的从模块； 传数阶段：主模块与从模块进行数据交换，数据由源模块发出，经数据总线流入目的模块； 结束阶段：主模块的有关信息均从系统总线上撤除，让出总线使用权。 总线通信控制主要解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调如何配合。 同步通信 读命令：CPU 在 T1 上升沿发出地址信息；在 T2 的上升沿发出读命令；与地址信号相符合的输入设备按命令进行一系列内部操作，且必须在 T3 的上升沿到来之前将 CPU 所需数据送到数据总线上；CPU 在 T3 时钟周期内将数据上的信息传送到其内部寄存器；CPU 在 T4 上升沿撤销读命令，输入设备不再向数据总线上传送数据，撤销它对数据总线的驱动。 规定明确、统一，模板间配合简单一致。 缺点是主、从模块时间配合属于强制性“同步”，必须在限定时间内完成规定的要求。并且对所有从模块都用统一时限，各模块速度不同，必须以最慢速度的部件来设计公共时钟，严重影响总线工作效率，给设计带来局限性，缺乏灵活性。 异步通信 (a) 不互锁：主模块发出请求信号后，不必等待接到从模块的回答信号，而是经过一段时间，确认从模块已收到请求信号后，便撤销其请求信号；从模块接到请求信号后，在条件允许时发出回答信号，并且经过一段时间确认主模块已收到回答信号后，自动撤销回答信号。 (b) 半互锁：主模块发出请求信号，必须接待到从模块的回答信号后再撤销其请求信号，有旧互锁关系；而从模块接到请求信号后发出回答信号，但不必等待获知主模块的请求信号，而是隔一段时间后自动撤销其回答信号，无互锁关系。 © 全互锁：皆需获得回答信号后撤销。在网络通信中，通信双方采用的就是全互锁方式。 半同步通信 保留了同步通信的基本特点，同时又像异步通信那样允许不同速度的模块和谐地工作。 增设了一条等待响应信号线，采用插入等待周期的措施来协调通信双方的配合问题。 分离式通信 将一个传输周期（总线周期）分解为两个子周期，两个传输子周期都只有单方面的信息流，每个模块都变成了主模块。 存储器 概述 存储器分类 a) 按存储介质分类。 半导体存储器 由半导体器件组成，用超大规模集成电路工艺制成芯片，体积小、功耗低、存取时间短。当电源消失时，所存信息也随即像丢失，是一种易失性存储器。 非挥发性材料制成的半导体存储器，克服了信息易失的弊病。 双极型 TTL 半导体存储器：高速。 MOS 半导体存储器：高集成度，制造简单，成本低，故被广泛应用。 磁表面存储器 在金属或塑料基体的表面上涂一层磁性材料作为记录介质，工作时磁层随载磁体高速运转，用磁头在磁层上进行读、写操作。 用具有矩形磁滞回线特性的材料作次表面物质，按其剩磁状态的不同而区分 0 或 1，而且剩磁状态不会轻易丢失，故这类存储器具有非易失性的特点。 磁芯存储器 被半导体存储器取代。 光盘存储器 用激光在磁光材料上进行读、写的存储器，具有非易失性的特点。 记录密度高、耐用性好、可靠性高和可互换性强等特点。 b) 按存取方式分类 随机存储器 Random Access Memory RAM 是一种可读、写存储器，其特点是存储器的任何一个存储单元的内容都可以随机存取，且存取时间与存储单位的物理位置无关。计算机系统中的主存都采用这种随机存储器。 静态 RAM，以触发器原理寄存信息。 动态 RAM，以电容充放电原理寄存信息。 只读存储器 Read Only Memory ROM 掩模型只读存储器 Masked ROM，MROM；可编程只读存储器 Programmable ROM，PROM；可擦除可编程只读存储器 Erasable Programmable ROM，EPROM；用电可擦除可编程只读存储器 Electrically Erasable Programmable ROM， EEPROM；Flash Memory。 串行访问存储器 对存储单元进行读写操作时，需按其物理位置的先后顺序寻找地址，则这种存储器称为串行访问存储器。 由于信息所在位置不同，读写时间均不相同。 c) 按在计算机中的作用分类 主存储器 可以和 CPU 直接交换信息。 速度快、容量小、每位价格高。 辅助存储器 是主存储器的后援存储器，用来存放当时暂时不用的程序和书，不能与 CPU 直接交换信息。 速度慢、容量大、每位价格低。 缓冲存储器 用在两个速度不同的部件之中。 层次结构 由上至下，位价越来越低，速度越来越慢，容量越来越大。 寄存器中的数直接在 CPU 内部参与运算。 主存用来存放将要参与运行的程序和数据，其速度与 CPU 速度差距较大，为使它们匹配，在主存与 CPU 之间插入了一种比主存速度更快、容量跟更小的高速缓冲存储器 Cache。 寄存器、缓存、主存这三类存储器都是由速度不同、位价不等的半导体存储材料制成的，它们都设在主机内。 磁盘、磁带属于辅助存储器，其容量比主存大得多，大都用来存放暂时未用到的程序和数据文件。 CPU 不能直接访问辅存，辅存只能与主存交换信息，因此辅存的速度可以比主存慢很多。 缓存 - 主存层次主要解决 CPU 与主存速度不匹配的问题。 主存 - 辅存层次主要解决存储系统的容量问题。形成了虚拟存储系统，在这个系统中，程序员变成的地址范围与虚拟存储器的地址空间相对应。 程序员编程时，可用的地址空间远远大于主存空间，使程序员以为自己占有一个容量极大的主存（虚拟存储器）。其逻辑地址转变为物理地址的工作由计算机系统的硬件和操作系统自动完成的，对程序员是透明的。 当虚地址的内容在主存时，机器便可立即使用；若虚地址的内容不在主存，则必须先将此虚地址内容传递到主存的合适单元后再为机器所用。 主存储器 主存的基本组成。 根据 MAR 储存器地址寄存器中的地址访问某个存储单元时，还需要经过地址译码、驱动等电路，才能找到所需要访问的单元。 读出时，需经过读出放大器，才能将被选中单元的存储字送到 MDR 主存数据寄存器。 写入时，MDR 中的数据也必须经过写入电路才能真正写入到被选中的单元中。 现代计算机的主存都由半导体集成电路构成，驱动器、译码器和读写电路均制作在存储芯片中，而 MAR 和 MDR 制作在 CPU 芯片中。存储芯片和 CPU 芯片可通过总线连接。 当要从存储器读出来某一信息字时，首先由 CPU 将该字的地址送到 MAR，经地址总线送至主存，然后发出读命令，主存接到读命令后将该单元内容读至数据总线上。 若要向主存存入一个信息字时，首先 CPU 将该字所在主存单元的地址经 MAR 送到地址总线，并将信息字送入 MDR，然后向主存发出写命令，主存接到写命令后，便将数据线上的信息写入到对应地址线指出的主存单元中。 主存中存储单元地址的分配 不同的机器存储字长不同，常用 8 位二进制数代表一个字节，因此存储字长都取 8 的倍数。 通常计算机系统即可按字寻址，也可按字节寻址。 主存的技术指标 存储容量：存储单元 x 存储字长；1 字长 = 8 字节。 存储速度：由存取时间和存取周期来表示。 存取时间、访问时间 Memory Access Time，是指启动一次存储器操作（读或写）到完成该操作所需的全部时间。存取时间分为读出时间和写入时间两种。 存取周期 Memory Cycle Time 是指存储器进行连续两次独立的存储器操作所需的最小间隔时间，通常存储周期大于存储时间。 存储器带宽 与存储周期密切相关，表示单位时间内存储器存取的信息量。通过以下方式提高存储器带宽： 缩短存储周期； 增加存储字长，使每个存取周期可读 / 写更多的二进制数； 增加存储体。 半导体存储芯片简介 基本结构 半导体存储芯片采用超大规模集成电路制造工艺，在一个芯片内集成具有记忆功能的存储矩阵、译码驱动电路和读写电路等。 译码驱动能把地址总线送来的地址信号翻译成对应存储单元的选择信号，该信号在读 / 写电路的配合下完成对被选中单元的读写操作。 读写电路包括读出放大器和写入电路，用来完成读写操作。 地址线是单向输入的，数据线是双向的，位数与芯片容量有关。 地址线和数据线位数共同反映存储芯片的容量。例如，地址线为 10 根，数据线为 4 根，则芯片容量为 210x4 K 位。 控制线主要包括读写控制线和片选线两种（不同存储芯片不同，可共用一根或分用两根）。由于半导体存储器是由许多芯片组成的，为此需要用片选信号来确定哪个芯片被选中。 译码驱动方式 线选法：用一根字选择线直接选中一个存储单元的各位。 重合法： 随机存取存储器 静态 RAM、Static RAM、SRAM 存储器用于寄存 0 和 1 代码的电路成为存储器的基本单元电路。","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}]},{"title":"模型评估与选择","slug":"算法/模型评估与选择","date":"2022-07-29T10:24:58.000Z","updated":"2023-01-04T07:23:27.612Z","comments":true,"path":"2022/07/29/算法/模型评估与选择/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。","text":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。 错误率和精度 123456789101112131415161718import numpy as np# 真实的数据标签real_label = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]) \\ \\ \\# 分类器的预测标签classifier_pred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])compare_result = (real_label == classifier_pred)compare_result = compare_result.astype(np.int)# m 为样本数量 b 为预测错误样本m = len(real)b = m - np.sum(cmp)# 错误率error_rate = (b / m)*100# 精确度 accaccuracy = (1 - b / m)*100 误差 模型在训练样本上的误差称为训练误差或经验误差；模型在新样本上的误差称为泛化误差。 过拟合模型：虽然训练误差接近 0，泛化误差非常大。 欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。 偏差和方差 偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。 偏差 bias，与真实值的偏离程度； 方差 variance，该随机变量在其期望值附近的波动程度。 评估方法 评估：对学习器的泛化误差进行评估并进而做出选择。 留出法 以一定比例划分训练集和测试集。 1234567891011121314151617181920212223# 导入包import numpy as npfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): &#x27;&#x27;&#x27; return: 返回随机生成 200 个点的坐标 &#x27;&#x27;&#x27; dots = 200 # 样本数 dim = 2 # 数据维度 X = np.random.randn(dots,dim) # 建立数据集，shape(200,2) # 建立样本 X 的类别 Y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): Y[i] = 1 return X, Y# 加载数据X,Y = load_pts()# 使用train_test_split划分训练集和测试集train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0) 交叉验证 交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。 ![Untitled](/img/模型评估与选择/Untitled 1.png) 12345678910111213# 导入包from sklearn.model_selection import KFoldimport numpy as np# 生成数据集，随机生成40个点data = np.random.randn(40,2)# 交叉验证法kf = KFold(n_splits = 4, shuffle = False, random_state = None) for train, test in kf.split(data): print(train) print(test,&#x27;\\n&#x27;) 自助法 有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ： 每次随机从 D 中挑选一个样本； 将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中； 重复执行 m 次该过程； 最后得到包含 m 个样本数据集 D’。 由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。 123456789101112131415161718# 导入包import numpy as np# 任意设置一个数据集X = [1,4,3,23,4,6,7,8,9,45,67,89,34,54,76,98,43,52]# 通过产生的随机数获得抽取样本的序号 bootstrapping = []for i in range(len(X)): bootstrapping.append(np.random.randint(0,len(X),(1)))# 通过序号获得原始数据集中的数据D_1 = []for i in range(len(X)): print(int(bootstrapping[i])) D_1.append(X[int(bootstrapping[i])]) print(D_1) 总结 采样方法 与原始数据集的分布是否相同 相比原始数据集的容量 是否适用小数据集 是否适用大数据集 是否存在估计偏差 留出法 分层抽样 否 变小 否 是 是 交叉验证法 分层抽样 否 变小 否 是 是 自助法 放回抽样 否 不变 是 否 是 性能度量 性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。 性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。 回归任务常用性能度量：MSE mean square error，均方差。 分类任务常用性能度量：acc accuracy，精度；错误率 查准率、查全率、F1 对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为： 真正例 true positive 假正例 false positive 真反例 true negative 假反例 false negative ![Untitled](/img/模型评估与选择/Untitled 2.png) P( Precision )=TPTP+FPR( Recall )=TPTP+FNP(\\text { Precision })=\\frac{T P}{T P+F P} \\\\R(\\text { Recall })=\\frac{T P}{T P+F N} P( Precision )=TP+FPTP​R( Recall )=TP+FNTP​ Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。 Precision，查准率：在预测为正的样本中，实际为正的概率。 P-R 曲线，BRP，Break Even Point：平衡单 P = R。 ![Untitled](/img/模型评估与选择/Untitled 3.png) 由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。 BRP 还是过于简单，更常用的是 F1 度量。 F1=2×P×RP+R=2TPn+TP−TNF 1=\\frac{2 \\times P \\times R}{P+R}=\\frac{2 T P}{n+T P-T N} F1=P+R2×P×R​=n+TP−TN2TP​ F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。 当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。 Fβ=(1+β2)×P×R(β2×P)+RF_{\\beta}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R} Fβ​=(β2×P)+R(1+β2)×P×R​ β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。 如果使用了类似交叉验证法，我们会得到多个 confusion matrix： 宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1； 微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as np# 加载数据集def generate_data(random_state=2021): &quot;&quot;&quot; :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] &quot;&quot;&quot; noise_rate = 0.1 # 噪声比例 sample_num = 4096 # 总样本数 noise_sample_num = int(sample_num*noise_rate) # 噪声样本数 np.random.seed(random_state) Pred_Score = np.random.uniform(0,1,sample_num) GT_label = (Pred_Score&gt;0.5).astype(np.int) noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=False, p=None) for index in noise_ids: GT_label[index] = 1 if GT_label[index] == 0 else 0 return GT_label, Pred_ScoreGT_label, Pred_Score = generate_data()# 请你补全以下代码，计算查准率与查全率def get_PR(GT_label, Pred_Score, threshold, random_state=2021): &quot;&quot;&quot; 计算错误率和精度 :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] :threshold: 评估阈值 :random_state: 随机种子 :返回值: P: 查准率 R: 查全率 &quot;&quot;&quot; Pred_Label = list(map(lambda x: 1 if x &gt; threshold else 0, Pred_Score)) from sklearn.metrics import precision_score, recall_score P = precision_score(GT_label, Pred_Label) R = recall_score(GT_label, Pred_Label) &quot;&quot;&quot; TODO &quot;&quot;&quot; return P, R P, R = get_PR(GT_label, Pred_Score, 0.55, random_state=2021)print(&quot;查准率P ：&#123;:.2f&#125;&quot;.format(P))print(&quot;查全率R ：&#123;:.2f&#125;&quot;.format(R)) ROC 与 AUC 原理 ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。 TPR( Precision )=TPTP+FNFPR( Precision )=FPFP+TN\\begin{aligned}T P R(\\text { Precision }) &amp;=\\frac{T P}{T P+F N} \\\\F P R(\\text { Precision }) &amp;=\\frac{F P}{F P+T N}\\end{aligned} TPR( Precision )FPR( Precision )​=TP+FNTP​=FP+TNFP​​ TPR 真正率，真正样本与实际为正的样本的比率； FPR 假正率，加正样本与实际为负的样本的比率。 若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两 个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC Area Under ROC Curve。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): dots = 200 # 点数 X = np.random.randn(dots,2) * 15 # 建立数据集，shape(200,2)，坐标放大15倍 # 建立 X 的类别 y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): if X[i,0] &gt; -15 and X[i,0] &lt; 15 and X[i,1] &gt; -15 and X[i,1] &lt; 15: # 矩形框内的样本都是目标类（正例） y[i] = 1 if 0 == np.random.randint(i+1) % 10: # 对数据随机地插入错误，20 个左右 y[i] = 1 - y[i] # 数据集可视化 plt.scatter(X[np.argwhere(y==0).flatten(),0], X[np.argwhere(y==0).flatten(),1],s = 20, color = &#x27;blue&#x27;, edgecolor = &#x27;k&#x27;) plt.scatter(X[np.argwhere(y==1).flatten(),0], X[np.argwhere(y==1).flatten(),1],s = 20, color = &#x27;red&#x27;, edgecolor = &#x27;k&#x27;) plt.xlim(-40,40) plt.ylim(-40,40) plt.grid(False) plt.tick_params( axis=&#x27;x&#x27;, which=&#x27;both&#x27;, bottom=False, top=False) return X, yX, y = load_pts()plt.show()### 训练模型 ###from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.svm import SVC# 将数据集拆分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) # 建立模型 clf1 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=4,min_samples_split=4)clf2 = GradientBoostingClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=10)clf3 = SVC(kernel=&#x27;rbf&#x27;, gamma=0.001, probability=True)# 训练模型clf1.fit(X_train, y_train)clf2.fit(X_train, y_train)clf3.fit(X_train, y_train)### 评估模型 ###from sklearn.metrics import roc_curve# 模型预测y_score1 = clf1.predict_proba(X_test)y_score2 = clf2.predict_proba(X_test)y_score3 = clf3.predict_proba(X_test)# 获得 FPR、TPR 值fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,1])fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,1])fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,1])### 绘制 ROC 曲线 ###from sklearn.metrics import aucplt.figure()# 绘制 ROC 函数def plot_roc_curve(fpr, tpr, c, name): lw = 2 roc_auc = auc(fpr,tpr) plt.plot(fpr, tpr, color=c,lw=lw, label= name +&#x27; (area = %0.2f)&#x27; % roc_auc) plt.plot([0,1], [0,1], color=&#x27;navy&#x27;, lw=lw, linestyle=&#x27;--&#x27;) plt.xlim([0, 1.0]) plt.ylim([0, 1.05]) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.ylabel(&#x27;True Positive Rate&#x27;) #plt.title(&#x27;&#x27;) plt.legend(loc=&quot;lower right&quot;) plot_roc_curve(fpr1, tpr1, &#x27;red&#x27;,&#x27;DecisionTreeClassifier &#x27;) plot_roc_curve(fpr2, tpr2, &#x27;navy&#x27;,&#x27;GradientBoostingClassifier &#x27;) plot_roc_curve(fpr3, tpr3, &#x27;green&#x27;,&#x27;SVC &#x27;) plt.show() 比较检验（TODO） 模型性能比较的重要因素： 实验评估得到的性能不等于泛化性能； 测试集上的性能与测试集本身的选择有很大关系； 很多机器学习算法本身有一定的随机性。 统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。 假设检验 由样本推测总体的方法。 交叉验证 t 检验 McNemar 检验 Friedman 检验与 Nemenyi 后续检验 类别不平衡 在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。 如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。 解决不平衡分类问题的策略可以分为两大类： 从数据层面入手 , 通过改变训练集样本分布降低不平衡程度； 从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。 数据层面解决类别不平衡 扩大数据样本。 重采样：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。 过采样：复制稀有样本； 123456789101112131415161718192021222324252627282930313233# 导入包from sklearn.datasets import make_classificationfrom collections import Counterfrom imblearn.over_sampling import RandomOverSampler# 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)# 打印每个类别样本数print(Counter(y))# 过采样ros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)# 打印过采样后每个类别样本数print(sorted(Counter(y_resampled).items()))# 生成新的稀有样本# 导入包from imblearn.over_sampling import SMOTE# 过采样sm = SMOTE(random_state=42)X_res, y_res = sm.fit_resample(X, y)# 打印过采样后每个类别样本数print(&#x27;Resampled dataset shape %s&#x27; % Counter(y_res)) 欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。 123456789# 导入包from imblearn.under_sampling import RandomUnderSampler# 欠采样rus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)# 打印欠采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。 因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。 12345678# 导入包from imblearn.combine import SMOTEENN# 过采样与欠采样结合smote_enn = SMOTEENN(random_state=0)X_resampled, y_resampled = smote_enn.fit_resample(X, y)# 打印采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 算法层面解决类别不平衡 惩罚项方法：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，错分稀有类的样本需要付出更大的代价。 通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。 例如，调整 SVM 以惩罚稀有类别的错误分类。 1234567# LABEL 0 4000# LABEL 1 200# 导入相关包from sklearn.svm import SVC# 添加惩罚项clf = SVC(C=0.8, probability=True, class_weight=&#123;0:0.25, 1:0.75&#125;) 特征选择方法 样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。 选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"贝叶斯算法","slug":"算法/贝叶斯算法","date":"2022-07-29T10:24:58.000Z","updated":"2023-01-04T07:41:34.361Z","comments":true,"path":"2022/07/29/算法/贝叶斯算法/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/","excerpt":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。","text":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。 条件概率 P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率： P(A∣B)=P(A∩B)P(B)P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)} P(A∣B)=P(B)P(A∩B)​ P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率： P(B∣A)=P(A∩B)P(A)P(B \\mid A)=\\frac{P(A \\cap B)}{P(A)} P(B∣A)=P(A)P(A∩B)​ 那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出贝叶斯公式： P(A∣B)=P(B∣A)×P(A)P(B)P(A \\mid B)=\\frac{P(B \\mid A) \\times P(A)}{P(B)}{\\scriptsize } P(A∣B)=P(B)P(B∣A)×P(A)​ 贝叶斯 基础思想： 已知类条件概率密度参数表达式和先验概率； 利用贝叶斯公式转换成后验概率； 根据后验概率大小进行决策分类。 根据以上基本思想，可以得到贝叶斯概率计算公式表达为**：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）**。 优点： 贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断； 它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信； 如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了； 它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。 缺点： 它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出； 有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。 扩展阅读： 一文读懂概率论学习：贝叶斯理论 贝叶斯决策论&amp;朴素贝叶斯算法 朴素贝叶斯法讲解 sklearn 贝叶斯方法 贝叶斯推断：广告邮件自动识别的代码实现 若邮件包含某个关键词，求此邮件是广告的概率。 12345678910111213141516171819# 广告邮件数量ad_number = 4000# 正常邮件数量normal_number = 6000# 所有广告邮件中，出现 “红包” 关键词的邮件的数量ad_hongbao_number = 1000# 所有正常邮件中，出现 “红包” 关键词的邮件的数量normal_hongbao_number = 6# 广告的先验概率 P(A)P_ad = ad_number / (ad_number + normal_number)# 包含红包的先验概率 P(B)P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)# 广告 包含红包的似然概率 P(B|A)P_hongbao_ad = ad_hongbao_number / ad_number# 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbaoprint(P_ad_hongbao) 10.9940357852882705 极大似然估计 极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。 极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 极大似然估计与贝叶斯推断是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。 极大似然估计的简单计算 一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。 ddp(10061)p61(1−p)39=(10061)(61p60(1−p)39−39p61(1−p)38)=(10061)p60(1−p)38(61(1−p)−39p)=(10061)p60(1−p)38(61−100p)=0\\begin{array}{c} \\frac{d}{d p}\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{61}(1-p)^{39}=\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right)\\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\\right) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61-100 p) \\\\ =0 \\end{array} dpd​(10061​)p61(1−p)39=(10061​)(61p60(1−p)39−39p61(1−p)38)=(10061​)p60(1−p)38(61(1−p)−39p)=(10061​)p60(1−p)38(61−100p)=0​ 当 P=61100,0P = \\frac{61}{100}, 0P=10061​,0 时，导数为零。因为 1 &lt; P &lt; 0，所以 P=61100P = \\frac{61}{100}P=10061​。 极大似然估计的简单应用 求极大似然估计 MLE 的一般步骤： 由总体分布导出样本的联合概率函数（或联合密度）； 把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 θθθ 看作自变量，得到似然函数 l(θ)l(θ)l(θ)； 求似然函数 l(θ)l(θ)l(θ) 的最大值点，常常转化为求 lnl(θ)lnl(θ)lnl(θ) 的最大值点，即 θθθ 的 MLE； 在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。 若随机变量 xxx 服从一个数学期望为 μμμ、方差为 σ2σ^2σ2 的正态分布，记为 N(μ,σ2)N(μ,σ^2)N(μ,σ2)，假设 μ=30,σ=2μ=30, σ=2μ=30,σ=2。 1234567891011import numpy as npfrom scipy.stats import normimport matplotlib.pyplot as pltμ = 30 # 数学期望σ = 2 # 方差x = μ + σ * np.random.randn(10000) # 正态分布plt.hist(x, bins=100) # 直方图显示plt.show()print(norm.fit(x)) # 返回极大似然估计，估计出参数约为 30 和 2 朴素贝叶斯分类器 朴素贝叶斯分类器是一系列假设特征之间强（朴素）独立条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。 MNIST 手写体数字识别 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import warningswarnings.filterwarnings(&quot;ignore&quot;)# numpy 库import numpy as np# tensorflow 库中的 mnist 数据集import tensorflow as tfmnist = tf.keras.datasets.mnist# sklearn 库中的 BernoulliNBfrom sklearn.naive_bayes import BernoulliNB# 绘图工具库 pltimport matplotlib.pyplot as pltprint(&quot;读取数据中 ...&quot;)# 载入数据(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量train_images = train_images.reshape(len(train_images),784)test_images = test_images.reshape(len(test_images),784)print(&#x27;读取完毕!&#x27;)def plot_images(imgs): &quot;&quot;&quot;绘制几个样本图片 :param show: 是否显示绘图 :return: &quot;&quot;&quot; sample_num = min(9, len(imgs)) img_figure = plt.figure(1) img_figure.set_figwidth(5) img_figure.set_figheight(5) for index in range(0, sample_num): ax = plt.subplot(3, 3, index + 1) ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;) ax.grid(False) plt.margins(0, 0) plt.show()plot_images(train_images)print(&quot;初始化并训练贝叶斯模型...&quot;)# 定义 朴素贝叶斯模型classifier_BNB = BernoulliNB()# 训练模型classifier_BNB.fit(train_images,train_labels)print(&#x27;训练完成!&#x27;)print(&quot;测试训练好的贝叶斯模型...&quot;)# 分类器在测试集上的预测值test_predict_BNB = classifier_BNB.predict(test_images)print(&quot;预测完成!&quot;)# 计算准确率accuracy = classifier_BNB.score(test_images, test_labels)print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy) 对结果进行统计比较分析。 12345678910111213141516171819202122# 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 class_num = &#123;&#125;# 每个类别预测为 0-9 类别的个数，predict_num = []# 每个类别预测的准确率class_accuracy = &#123;&#125;for i in range(10): # 找到类别是 i 的下标 class_is_i_index = np.where(test_labels == i)[0] # 统计类别是 i 的个数 class_num[i] = len(class_is_i_index) # 统计类别 i 预测为 0-9 各个类别的个数 predict_num.append( [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)]) # 统计类别 i 预测的准确率 class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100 print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % ( i, class_num[i], predict_num[i][i], class_accuracy[i])) 用热力图对结果进行分析。 123456789101112import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) np.random.seed(0)uniform_data = predict_numax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)ax.set_xlabel(&#x27;真实值&#x27;)ax.set_ylabel(&#x27;预测值&#x27;)plt.show()","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"考研","slug":"考研/考研","date":"2022-04-21T06:19:23.000Z","updated":"2023-09-19T07:02:34.131Z","comments":true,"path":"2022/04/21/考研/考研/","link":"","permalink":"https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/%E8%80%83%E7%A0%94/","excerpt":"考研院校信息整理汇总。","text":"考研院校信息整理汇总。 报考专业 深大 - 人工智能与金融科技 初试科目 101 思想政治理论 201 英语一 301 数学一 408 计算机学科专业基础综合 复试科目 FSX8 机器学习 计算机考研 408 包括（150） 数据结构 45 计算机组成原理 45 操作系统 35 计算机网络 25","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[]}],"categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"},{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"日常","slug":"日常","permalink":"https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"},{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://wingowen.github.io/tags/Pandas/"},{"name":"SQL","slug":"SQL","permalink":"https://wingowen.github.io/tags/SQL/"},{"name":"高等数学","slug":"高等数学","permalink":"https://wingowen.github.io/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"},{"name":"脚本命令","slug":"脚本命令","permalink":"https://wingowen.github.io/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"name":"网络相关","slug":"网络相关","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"},{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"},{"name":"网站收集","slug":"网站收集","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"},{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"}]}