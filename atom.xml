<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WINGO&#39;S BLOG</title>
  
  
  <link href="https://wingowen.github.io/atom.xml" rel="self"/>
  
  <link href="https://wingowen.github.io/"/>
  <updated>2022-08-01T03:15:02.902Z</updated>
  <id>https://wingowen.github.io/</id>
  
  <author>
    <name>Wingo Wen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>决策树算法</title>
    <link href="https://wingowen.github.io/2022/07/31/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/"/>
    <id>https://wingowen.github.io/2022/07/31/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/</id>
    <published>2022-07-31T02:58:15.000Z</published>
    <updated>2022-08-01T03:15:02.902Z</updated>
    
    <content type="html"><![CDATA[<p>基本概念。</p><span id="more"></span><h1 id="决策树的基本概念"><a href="#决策树的基本概念" class="headerlink" title="决策树的基本概念"></a>决策树的基本概念</h1><p>决策树节点：</p><ul><li>叶节点表示一份类别或者一个值；</li><li>非叶节点表示一个属性划分。</li></ul><p>决策树的有向边代表了属性划分的不同取值：</p><ul><li>当属性是离散时，可将属性的每一个取值用一条边连接到子结点；</li><li>当属性是连续时，需要特殊处理。</li></ul><p>决策树是一种描述实例进行分类的树形结构。</p><p>对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。</p><p><img src="/IMAGE/决策树算法/决策树的学习过程.png" alt="img"></p><p>决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略。</p><p>决策树的学习过程本质上是从训练数据中寻找一组分类规则。</p><p>决策树学习也可以看做是由训练数据集估计条件概率模型。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;基本概念。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>计算机组成</title>
    <link href="https://wingowen.github.io/2022/07/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/"/>
    <id>https://wingowen.github.io/2022/07/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/</id>
    <published>2022-07-30T07:15:43.000Z</published>
    <updated>2022-08-01T03:13:29.473Z</updated>
    
    <content type="html"><![CDATA[<p>北大计算机组成课程。</p><p>计算机基本结构：冯诺依曼结构，计算机执行指令的过程。</p><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;p&gt;北大计算机组成课程。&lt;/p&gt;
&lt;p&gt;计算机基本结构：冯诺依曼结构，计算机执行指令的过程。&lt;/p&gt;</summary>
    
    
    
    <category term="考研" scheme="https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
    <category term="计算机科学" scheme="https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>模型评估与选择</title>
    <link href="https://wingowen.github.io/2022/07/29/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>https://wingowen.github.io/2022/07/29/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2022-07-29T10:24:58.000Z</published>
    <updated>2022-07-31T02:49:32.783Z</updated>
    
    <content type="html"><![CDATA[<p>错误率和精度，误差，偏差和方差。</p><p>评估方法：留出法，交叉验证，自助法。</p><p>二分类任务性能度量：查准率，查全率，F1，ROC，AUC。</p><p>数据层面解决类别不平衡：欠采样，过采样，~结合。</p><p>算法层面解决类别不平衡：惩罚项。</p><span id="more"></span><h1 id="错误率和精度"><a href="#错误率和精度" class="headerlink" title="错误率和精度"></a>错误率和精度</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实的数据标签</span></span><br><span class="line">real_label = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>])                                         \                              \   \</span><br><span class="line"><span class="comment"># 分类器的预测标签</span></span><br><span class="line">classifier_pred = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">compare_result = (real_label == classifier_pred)</span><br><span class="line">compare_result = compare_result.astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># m 为样本数量 b 为预测错误样本</span></span><br><span class="line">m = <span class="built_in">len</span>(real)</span><br><span class="line">b = m - np.<span class="built_in">sum</span>(cmp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误率</span></span><br><span class="line">error_rate = (b / m)*<span class="number">100</span></span><br><span class="line"><span class="comment"># 精确度 acc</span></span><br><span class="line">accuracy = (<span class="number">1</span> - b / m)*<span class="number">100</span></span><br></pre></td></tr></table></figure><h1 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h1><p>模型在训练样本上的误差称为<strong>训练误差</strong>或<strong>经验误差</strong>；模型在新样本上的误差称为<strong>泛化误差。</strong></p><p>过拟合模型：虽然训练误差接近 0，泛化误差非常大。</p><p>欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。</p><h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。</p><ul><li>偏差 bias，与真实值的<strong>偏离程度</strong>；</li><li>方差 variance，该随机变量在其期望值附近的<strong>波动程度</strong>。</li></ul><p><img src="/IMAGE/模型评估与选择/Untitled.png" alt="Untitled"></p><h1 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h1><p><strong>评估：对学习器的泛化误差进行评估并进而做出选择。</strong></p><h2 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h2><p>以一定比例划分训练集和测试集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 导入包</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"># 加载数据集</span><br><span class="line">def load_pts(): </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    return: 返回随机生成 200 个点的坐标</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    dots = 200  # 样本数</span><br><span class="line">    dim = 2 # 数据维度</span><br><span class="line">    X = np.random.randn(dots,dim) # 建立数据集，shape(200,2)</span><br><span class="line">    # 建立样本 X 的类别</span><br><span class="line">    Y = np.zeros(dots, dtype=&#x27;int&#x27;)      </span><br><span class="line">    for i in range(X.shape[0]):</span><br><span class="line">            Y[i] = 1           </span><br><span class="line">    return X, Y</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">X,Y = load_pts()</span><br><span class="line"></span><br><span class="line"># 使用train_test_split划分训练集和测试集</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0)</span><br></pre></td></tr></table></figure><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。</p><p><img src="/IMAGE/模型评估与选择/Untitled 1.png" alt="Untitled"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集，随机生成40个点</span></span><br><span class="line">data = np.random.randn(<span class="number">40</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证法</span></span><br><span class="line">kf = KFold(n_splits = <span class="number">4</span>, shuffle = <span class="literal">False</span>, random_state = <span class="literal">None</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(data):</span><br><span class="line">    <span class="built_in">print</span>(train)</span><br><span class="line">    <span class="built_in">print</span>(test,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h2><p>有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ：</p><ul><li>每次随机从 D 中挑选一个样本；</li><li>将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中；</li><li>重复执行 m 次该过程；</li><li>最后得到包含 m 个样本数据集 D’。</li></ul><p>由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任意设置一个数据集</span></span><br><span class="line">X = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">23</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">45</span>,<span class="number">67</span>,<span class="number">89</span>,<span class="number">34</span>,<span class="number">54</span>,<span class="number">76</span>,<span class="number">98</span>,<span class="number">43</span>,<span class="number">52</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过产生的随机数获得抽取样本的序号 </span></span><br><span class="line">bootstrapping = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">    bootstrapping.append(np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(X),(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过序号获得原始数据集中的数据</span></span><br><span class="line">D_1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">int</span>(bootstrapping[i]))</span><br><span class="line">    D_1.append(X[<span class="built_in">int</span>(bootstrapping[i])])</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(D_1)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="table-container"><table><thead><tr><th></th><th>采样方法</th><th>与原始数据集的分布是否相同</th><th>相比原始数据集的容量</th><th>是否适用小数据集</th><th>是否适用大数据集</th><th>是否存在估计偏差</th></tr></thead><tbody><tr><td>留出法</td><td>分层抽样</td><td>否</td><td>变小</td><td>否</td><td>是</td><td>是</td></tr><tr><td>交叉验证法</td><td>分层抽样</td><td>否</td><td>变小</td><td>否</td><td>是</td><td>是</td></tr><tr><td>自助法</td><td>放回抽样</td><td>否</td><td>不变</td><td>是</td><td>否</td><td>是</td></tr></tbody></table></div><h1 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h1><p>性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。</p><p>性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。</p><p>回归任务常用性能度量：MSE mean square error，均方差。</p><p>分类任务常用性能度量：acc accuracy，精度；错误率</p><h2 id="查准率、查全率、F1"><a href="#查准率、查全率、F1" class="headerlink" title="查准率、查全率、F1"></a>查准率、查全率、F1</h2><p>对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为：</p><ul><li>真正例 true positive</li><li>假正例 false positive</li><li>真反例 true negative</li><li>假反例 false negative</li></ul><p><img src="/IMAGE/模型评估与选择/Untitled 2.png" alt="Untitled"></p><script type="math/tex; mode=display">P(\text { Precision })=\frac{T P}{T P+F P} \\R(\text { Recall })=\frac{T P}{T P+F N}</script><p>Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。</p><p>Precision，查准率：在预测为正的样本中，实际为正的概率。</p><p>P-R 曲线，BRP，Break Even Point：平衡单 P = R。</p><p><img src="/IMAGE/模型评估与选择/Untitled 3.png" alt="Untitled"></p><p>由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。<br>BRP 还是过于简单，更常用的是 F1 度量。</p><script type="math/tex; mode=display">F 1=\frac{2 \times P \times R}{P+R}=\frac{2 T P}{n+T P-T N}</script><p>F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。</p><p>当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。</p><script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script><p>β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。</p><p>如果使用了类似交叉验证法，我们会得到多个 confusion matrix：</p><ul><li>宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1；</li><li>微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">random_state=<span class="number">2021</span></span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果</span></span><br><span class="line"><span class="string">            Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    noise_rate = <span class="number">0.1</span> <span class="comment"># 噪声比例</span></span><br><span class="line">    sample_num = <span class="number">4096</span>  <span class="comment"># 总样本数</span></span><br><span class="line">    noise_sample_num = <span class="built_in">int</span>(sample_num*noise_rate) <span class="comment"># 噪声样本数</span></span><br><span class="line">    np.random.seed(random_state)</span><br><span class="line">    Pred_Score = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,sample_num)</span><br><span class="line">    GT_label = (Pred_Score&gt;<span class="number">0.5</span>).astype(np.<span class="built_in">int</span>)</span><br><span class="line">    noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=<span class="literal">False</span>, p=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> noise_ids:</span><br><span class="line">        GT_label[index] = <span class="number">1</span> <span class="keyword">if</span> GT_label[index] == <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> GT_label, Pred_Score</span><br><span class="line"></span><br><span class="line">GT_label, Pred_Score = generate_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请你补全以下代码，计算查准率与查全率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_PR</span>(<span class="params">GT_label, Pred_Score, threshold, random_state=<span class="number">2021</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算错误率和精度</span></span><br><span class="line"><span class="string">    :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果</span></span><br><span class="line"><span class="string">    :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    :threshold: 评估阈值</span></span><br><span class="line"><span class="string">    :random_state: 随机种子</span></span><br><span class="line"><span class="string">    :返回值: P: 查准率</span></span><br><span class="line"><span class="string">            R: 查全率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Pred_Label = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>, Pred_Score))</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line">    P = precision_score(GT_label, Pred_Label)</span><br><span class="line">    R = recall_score(GT_label, Pred_Label)</span><br><span class="line">    <span class="string">&quot;&quot;&quot; TODO &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> P, R</span><br><span class="line">    </span><br><span class="line">P, R = get_PR(GT_label, Pred_Score, <span class="number">0.55</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查准率P ：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(P))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查全率R ：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(R))</span><br></pre></td></tr></table></figure><h2 id="ROC-与-AUC-原理"><a href="#ROC-与-AUC-原理" class="headerlink" title="ROC 与 AUC 原理"></a>ROC 与 AUC 原理</h2><p>ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。</p><script type="math/tex; mode=display">\begin{aligned}T P R(\text { Precision }) &=\frac{T P}{T P+F N} \\F P R(\text { Precision }) &=\frac{F P}{F P+T N}\end{aligned}</script><p>TPR 真正率，真正样本与实际为正的样本的比率；</p><p>FPR 假正率，加正样本与实际为负的样本的比率。</p><p>若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两  个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC  Area Under ROC Curve。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_pts</span>(): </span><br><span class="line">    dots = <span class="number">200</span>  <span class="comment"># 点数</span></span><br><span class="line">    X = np.random.randn(dots,<span class="number">2</span>) * <span class="number">15</span>  <span class="comment"># 建立数据集，shape(200,2)，坐标放大15倍</span></span><br><span class="line">    <span class="comment"># 建立 X 的类别</span></span><br><span class="line">    y = np.zeros(dots, dtype=<span class="string">&#x27;int&#x27;</span>)      </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> X[i,<span class="number">0</span>] &gt; -<span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">0</span>] &lt; <span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">1</span>] &gt; -<span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">1</span>] &lt; <span class="number">15</span>:  <span class="comment"># 矩形框内的样本都是目标类（正例）</span></span><br><span class="line">            y[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> == np.random.randint(i+<span class="number">1</span>) % <span class="number">10</span>:  <span class="comment"># 对数据随机地插入错误，20 个左右</span></span><br><span class="line">            y[i] = <span class="number">1</span> - y[i]  </span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 数据集可视化          </span></span><br><span class="line">    plt.scatter(X[np.argwhere(y==<span class="number">0</span>).flatten(),<span class="number">0</span>], X[np.argwhere(y==<span class="number">0</span>).flatten(),<span class="number">1</span>],s = <span class="number">20</span>, color = <span class="string">&#x27;blue&#x27;</span>, edgecolor = <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.scatter(X[np.argwhere(y==<span class="number">1</span>).flatten(),<span class="number">0</span>], X[np.argwhere(y==<span class="number">1</span>).flatten(),<span class="number">1</span>],s = <span class="number">20</span>, color = <span class="string">&#x27;red&#x27;</span>, edgecolor = <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.xlim(-<span class="number">40</span>,<span class="number">40</span>)</span><br><span class="line">    plt.ylim(-<span class="number">40</span>,<span class="number">40</span>)</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    plt.tick_params(</span><br><span class="line">    axis=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">    which=<span class="string">&#x27;both&#x27;</span>,</span><br><span class="line">    bottom=<span class="literal">False</span>,</span><br><span class="line">    top=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">X, y = load_pts()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">### 训练模型 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集拆分成训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型 </span></span><br><span class="line">clf1 = DecisionTreeClassifier(max_depth=<span class="number">5</span>, min_samples_leaf=<span class="number">4</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">clf2 = GradientBoostingClassifier(max_depth=<span class="number">8</span>, min_samples_leaf=<span class="number">10</span>, min_samples_split=<span class="number">10</span>)</span><br><span class="line">clf3 = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">0.001</span>, probability=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">clf1.fit(X_train, y_train)</span><br><span class="line">clf2.fit(X_train, y_train)</span><br><span class="line">clf3.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 评估模型 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_score1 = clf1.predict_proba(X_test)</span><br><span class="line">y_score2 = clf2.predict_proba(X_test)</span><br><span class="line">y_score3 = clf3.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得 FPR、TPR 值</span></span><br><span class="line">fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,<span class="number">1</span>])</span><br><span class="line">fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,<span class="number">1</span>])</span><br><span class="line">fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">### 绘制 ROC 曲线 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 ROC 函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_roc_curve</span>(<span class="params">fpr, tpr, c, name</span>):</span><br><span class="line">    lw = <span class="number">2</span></span><br><span class="line">    roc_auc = auc(fpr,tpr)</span><br><span class="line">    plt.plot(fpr, tpr, color=c,lw=lw, </span><br><span class="line">             label= name +<span class="string">&#x27; (area = %0.2f)&#x27;</span> % roc_auc)</span><br><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], color=<span class="string">&#x27;navy&#x27;</span>, lw=lw, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">1.0</span>])</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1.05</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">    <span class="comment">#plt.title(&#x27;&#x27;)</span></span><br><span class="line">    plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">    </span><br><span class="line">plot_roc_curve(fpr1, tpr1, <span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;DecisionTreeClassifier &#x27;</span>)   </span><br><span class="line">plot_roc_curve(fpr2, tpr2, <span class="string">&#x27;navy&#x27;</span>,<span class="string">&#x27;GradientBoostingClassifier &#x27;</span>)   </span><br><span class="line">plot_roc_curve(fpr3, tpr3, <span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;SVC &#x27;</span>) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="比较检验（TODO）"><a href="#比较检验（TODO）" class="headerlink" title="比较检验（TODO）"></a>比较检验（TODO）</h1><p>模型性能比较的重要因素：</p><ul><li>实验评估得到的性能不等于泛化性能；</li><li>测试集上的性能与测试集本身的选择有很大关系；</li><li>很多机器学习算法本身有一定的随机性。</li></ul><p>统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。</p><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><p>由样本推测总体的方法。</p><h3 id="交叉验证-t-检验"><a href="#交叉验证-t-检验" class="headerlink" title="交叉验证 t 检验"></a>交叉验证 t 检验</h3><h3 id="McNemar-检验"><a href="#McNemar-检验" class="headerlink" title="McNemar 检验"></a>McNemar 检验</h3><h3 id="Friedman-检验与-Nemenyi-后续检验"><a href="#Friedman-检验与-Nemenyi-后续检验" class="headerlink" title="Friedman 检验与 Nemenyi 后续检验"></a>Friedman 检验与 Nemenyi 后续检验</h3><h1 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h1><p>在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。</p><p>如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。</p><p>解决不平衡分类问题的策略可以分为两大类：</p><ul><li>从数据层面入手 , 通过改变训练集样本分布降低不平衡程度；</li><li>从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。</li></ul><h2 id="数据层面解决类别不平衡"><a href="#数据层面解决类别不平衡" class="headerlink" title="数据层面解决类别不平衡"></a>数据层面解决类别不平衡</h2><p><strong>扩大数据样本</strong>。</p><p><strong>重采样</strong>：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。</p><ul><li><p>过采样：复制稀有样本；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> RandomOverSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">5000</span>, n_features=<span class="number">2</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">                           n_redundant=<span class="number">0</span>, n_repeated=<span class="number">0</span>, n_classes=<span class="number">3</span>,</span><br><span class="line">                           n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">                           weights=[<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.94</span>],</span><br><span class="line">                           class_sep=<span class="number">0.8</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(Counter(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过采样</span></span><br><span class="line">ros = RandomOverSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = ros.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印过采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成新的稀有样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过采样</span></span><br><span class="line">sm = SMOTE(random_state=<span class="number">42</span>)</span><br><span class="line">X_res, y_res = sm.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印过采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Resampled dataset shape %s&#x27;</span> % Counter(y_res))</span><br></pre></td></tr></table></figure></li><li><p>欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> RandomUnderSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 欠采样</span></span><br><span class="line">rus = RandomUnderSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = rus.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印欠采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br></pre></td></tr></table></figure></li><li><p>过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。</p><p>因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.combine <span class="keyword">import</span> SMOTEENN</span><br><span class="line"><span class="comment"># 过采样与欠采样结合</span></span><br><span class="line">smote_enn = SMOTEENN(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = smote_enn.fit_resample(X, y)</span><br><span class="line"><span class="comment"># 打印采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h2 id="算法层面解决类别不平衡"><a href="#算法层面解决类别不平衡" class="headerlink" title="算法层面解决类别不平衡"></a>算法层面解决类别不平衡</h2><p><strong>惩罚项方法</strong>：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，<strong>错分稀有类的样本需要付出更大的代价</strong>。</p><p>通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。</p><p>例如，调整 SVM 以惩罚稀有类别的错误分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LABEL 0 4000</span></span><br><span class="line"><span class="comment"># LABEL 1 200</span></span><br><span class="line"><span class="comment"># 导入相关包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加惩罚项</span></span><br><span class="line">clf = SVC(C=<span class="number">0.8</span>, probability=<span class="literal">True</span>, class_weight=&#123;<span class="number">0</span>:<span class="number">0.25</span>, <span class="number">1</span>:<span class="number">0.75</span>&#125;)</span><br></pre></td></tr></table></figure><p><strong>特征选择方法</strong></p><p>样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。</p><p>选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;错误率和精度，误差，偏差和方差。&lt;/p&gt;
&lt;p&gt;评估方法：留出法，交叉验证，自助法。&lt;/p&gt;
&lt;p&gt;二分类任务性能度量：查准率，查全率，F1，ROC，AUC。&lt;/p&gt;
&lt;p&gt;数据层面解决类别不平衡：欠采样，过采样，~结合。&lt;/p&gt;
&lt;p&gt;算法层面解决类别不平衡：惩罚项。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯算法</title>
    <link href="https://wingowen.github.io/2022/07/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>https://wingowen.github.io/2022/07/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</id>
    <published>2022-07-29T10:24:58.000Z</published>
    <updated>2022-07-31T02:49:06.928Z</updated>
    
    <content type="html"><![CDATA[<p>条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。</p><p>朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。</p><span id="more"></span><h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><p>P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(A \cap B)}{P(B)}</script><p>P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率：</p><script type="math/tex; mode=display">P(B \mid A)=\frac{P(A \cap B)}{P(A)}</script><p>那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(B \mid A) \times P(A)}{P(B)}{\scriptsize }</script><h1 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h1><p><strong>基础思想：</strong></p><ul><li>已知类条件概率密度参数表达式和先验概率；</li><li>利用贝叶斯公式转换成后验概率；</li><li>根据后验概率大小进行决策分类。</li></ul><p>根据以上基本思想，可以得到贝叶斯概率计算公式表达为<strong>：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）</strong>。</p><p><strong>优点：</strong></p><ul><li>贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断；</li><li>它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信；</li><li>如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了；</li><li>它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。</li></ul><p><strong>缺点：</strong></p><ul><li>它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出；</li><li>有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。</li></ul><p><strong>扩展阅读：</strong></p><ul><li><a href="https://www.jiqizhixin.com/articles/2019-11-21">一文读懂概率论学习：贝叶斯理论</a></li><li><a href="https://zhuanlan.zhihu.com/p/5056240">贝叶斯决策论&amp;朴素贝叶斯算法</a></li><li><a href="https://www.bilibili.com/video/av57126177?from=search&amp;seid=1588787263892359481">朴素贝叶斯法讲解</a></li><li><a href="https://scikit-learn.org/stable/modules/naive_bayes.html">sklearn 贝叶斯方法</a></li></ul><h2 id="贝叶斯推断：广告邮件自动识别的代码实现"><a href="#贝叶斯推断：广告邮件自动识别的代码实现" class="headerlink" title="贝叶斯推断：广告邮件自动识别的代码实现"></a>贝叶斯推断：广告邮件自动识别的代码实现</h2><p>若邮件包含某个关键词，求此邮件是广告的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广告邮件数量</span></span><br><span class="line">ad_number = <span class="number">4000</span></span><br><span class="line"><span class="comment"># 正常邮件数量</span></span><br><span class="line">normal_number = <span class="number">6000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有广告邮件中，出现 “红包” 关键词的邮件的数量</span></span><br><span class="line">ad_hongbao_number = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 所有正常邮件中，出现 “红包” 关键词的邮件的数量</span></span><br><span class="line">normal_hongbao_number = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 广告的先验概率 P(A)</span></span><br><span class="line">P_ad = ad_number / (ad_number + normal_number)</span><br><span class="line"><span class="comment"># 包含红包的先验概率 P(B)</span></span><br><span class="line">P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)</span><br><span class="line"><span class="comment"># 广告 包含红包的似然概率 P(B|A)</span></span><br><span class="line">P_hongbao_ad = ad_hongbao_number / ad_number</span><br><span class="line"><span class="comment"># 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)</span></span><br><span class="line">P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbao</span><br><span class="line"><span class="built_in">print</span>(P_ad_hongbao)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9940357852882705</span><br></pre></td></tr></table></figure><h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><p>极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。</p><p>极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：<strong>“模型已定，参数未知”</strong>。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p><blockquote><p><strong>极大似然估计</strong>与<strong>贝叶斯推断</strong>是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。</p></blockquote><h2 id="极大似然估计的简单计算"><a href="#极大似然估计的简单计算" class="headerlink" title="极大似然估计的简单计算"></a>极大似然估计的简单计算</h2><p>一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。</p><script type="math/tex; mode=display">\begin{array}{c}\frac{d}{d p}\left(\begin{array}{c}100 \\61\end{array}\right) p^{61}(1-p)^{39}=\left(\begin{array}{c}100 \\61\end{array}\right)\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\right) \\=\left(\begin{array}{c}100 \\61\end{array}\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\=\left(\begin{array}{c}100 \\61\end{array}\right) p^{60}(1-p)^{38}(61-100 p) \\=0\end{array}</script><p>当 $P = \frac{61}{100}, 0$ 时，导数为零。因为 1 &lt; P &lt; 0，所以 $P = \frac{61}{100}$。</p><h2 id="极大似然估计的简单应用"><a href="#极大似然估计的简单应用" class="headerlink" title="极大似然估计的简单应用"></a>极大似然估计的简单应用</h2><p>求极大似然估计 MLE 的一般步骤：</p><ul><li>由总体分布导出样本的联合概率函数（或联合密度）；</li><li>把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 $θ$ 看作自变量，得到似然函数 $l(θ)$；</li><li>求似然函数 $l(θ)$ 的最大值点，常常转化为求 $lnl(θ)$ 的最大值点，即 $θ$ 的 MLE；</li><li>在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。</li></ul><p>若随机变量 $x$ 服从一个数学期望为 $μ$、方差为 $σ^2$ 的正态分布，记为 $N(μ,σ^2)$，假设 $μ=30, σ=2$。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.stats import norm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">μ = 30  # 数学期望</span><br><span class="line">σ = 2  # 方差</span><br><span class="line">x = μ + σ * np.random.randn(10000)  # 正态分布</span><br><span class="line"></span><br><span class="line">plt.hist(x, bins=100)  # 直方图显示</span><br><span class="line">plt.show()</span><br><span class="line">print(norm.fit(x))  # 返回极大似然估计，估计出参数约为 30 和 2</span><br></pre></td></tr></table></figure><h1 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h1><p>朴素贝叶斯分类器是一系列假设特征之间<strong>强（朴素）独立</strong>条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。<strong>所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。</strong></p><p>朴素贝叶斯的思想基础是：<strong>对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</strong></p><p>对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。</p><p><img src="/IMAGE/贝叶斯算法/贝叶斯算法流程.png" alt="img"></p><h2 id="MNIST-手写体数字识别"><a href="#MNIST-手写体数字识别" class="headerlink" title="MNIST 手写体数字识别"></a>MNIST 手写体数字识别</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line"># numpy 库</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># tensorflow 库中的 mnist 数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line"># sklearn 库中的 BernoulliNB</span><br><span class="line">from sklearn.naive_bayes import BernoulliNB</span><br><span class="line"></span><br><span class="line"># 绘图工具库 plt</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">print(&quot;读取数据中 ...&quot;)</span><br><span class="line"></span><br><span class="line"># 载入数据</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line"># 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量</span><br><span class="line">train_images = train_images.reshape(len(train_images),784)</span><br><span class="line">test_images =  test_images.reshape(len(test_images),784)</span><br><span class="line"></span><br><span class="line">print(&#x27;读取完毕!&#x27;)</span><br><span class="line"></span><br><span class="line">def plot_images(imgs):</span><br><span class="line">    &quot;&quot;&quot;绘制几个样本图片</span><br><span class="line">    :param show: 是否显示绘图</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sample_num = min(9, len(imgs))</span><br><span class="line">    img_figure = plt.figure(1)</span><br><span class="line">    img_figure.set_figwidth(5)</span><br><span class="line">    img_figure.set_figheight(5)</span><br><span class="line">    for index in range(0, sample_num):</span><br><span class="line">        ax = plt.subplot(3, 3, index + 1)</span><br><span class="line">        ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">    plt.margins(0, 0)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_images(train_images)</span><br><span class="line"></span><br><span class="line">print(&quot;初始化并训练贝叶斯模型...&quot;)</span><br><span class="line"></span><br><span class="line"># 定义 朴素贝叶斯模型</span><br><span class="line">classifier_BNB = BernoulliNB()</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">classifier_BNB.fit(train_images,train_labels)</span><br><span class="line"></span><br><span class="line">print(&#x27;训练完成!&#x27;)</span><br><span class="line"></span><br><span class="line">print(&quot;测试训练好的贝叶斯模型...&quot;)</span><br><span class="line"></span><br><span class="line"># 分类器在测试集上的预测值</span><br><span class="line">test_predict_BNB = classifier_BNB.predict(test_images)</span><br><span class="line"></span><br><span class="line">print(&quot;预测完成!&quot;)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">accuracy = classifier_BNB.score(test_images, test_labels)</span><br><span class="line"></span><br><span class="line">print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对结果进行统计比较分析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 </span><br><span class="line">class_num = &#123;&#125;</span><br><span class="line"># 每个类别预测为 0-9 类别的个数，</span><br><span class="line">predict_num = []</span><br><span class="line"># 每个类别预测的准确率</span><br><span class="line">class_accuracy = &#123;&#125;</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    # 找到类别是 i 的下标</span><br><span class="line">    class_is_i_index = np.where(test_labels == i)[0]</span><br><span class="line">    # 统计类别是 i 的个数</span><br><span class="line">    class_num[i] = len(class_is_i_index)</span><br><span class="line"></span><br><span class="line">    # 统计类别 i 预测为 0-9 各个类别的个数</span><br><span class="line">    predict_num.append(</span><br><span class="line">        [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)])</span><br><span class="line"></span><br><span class="line">    # 统计类别 i 预测的准确率</span><br><span class="line">    class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100</span><br><span class="line"></span><br><span class="line">    print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % (</span><br><span class="line">    i, class_num[i], predict_num[i][i], class_accuracy[i]))</span><br></pre></td></tr></table></figure><p>用热力图对结果进行分析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">sns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)</span><br><span class="line">sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) </span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line">uniform_data = predict_num</span><br><span class="line">ax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)</span><br><span class="line">ax.set_xlabel(&#x27;真实值&#x27;)</span><br><span class="line">ax.set_ylabel(&#x27;预测值&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。&lt;/p&gt;
&lt;p&gt;朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>考研</title>
    <link href="https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/"/>
    <id>https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/</id>
    <published>2022-04-21T06:19:23.000Z</published>
    <updated>2022-08-01T03:14:39.913Z</updated>
    
    <content type="html"><![CDATA[<p>考研院校信息整理汇总。</p><!--> more <--><p>报考专业 <a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/4/114/085410">深大 - 人工智能与金融科技</a></p><p>初试科目</p><ul><li><p><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/101">101 思想政治理论</a></p></li><li><p><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/201">201 英语一</a></p></li><li><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/301">301 数学一</a></li><li><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/408">408 计算机学科专业基础综合</a></li></ul><p>复试科目 <a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/FSX8">FSX8 机器学习</a></p><p>计算机考研 408 包括（150）</p><ul><li>数据结构 45</li><li>计算机组成原理 45</li><li>操作系统 35</li><li>计算机网络 25</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;考研院校信息整理汇总。&lt;/p&gt;
&lt;!--&gt; more &lt;--&gt;
&lt;p&gt;报考专业 &lt;a href=&quot;http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/4/114/085410&quot;&gt;深大 - 人工智能与金融科技&lt;/a</summary>
      
    
    
    
    <category term="考研" scheme="https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
    <category term="考研" scheme="https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
</feed>
