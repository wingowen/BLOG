{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"https://wingowen.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-07-29T10:17:24.000Z","updated":"2022-07-29T10:18:07.390Z","comments":true,"path":"categories/index.html","permalink":"https://wingowen.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-07-29T10:18:35.000Z","updated":"2022-07-29T10:18:54.011Z","comments":true,"path":"tags/index.html","permalink":"https://wingowen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hexo","slug":"杂项/Hexo","date":"2023-09-20T05:41:38.000Z","updated":"2023-09-20T05:45:49.754Z","comments":true,"path":"2023/09/20/杂项/Hexo/","link":"","permalink":"https://wingowen.github.io/2023/09/20/%E6%9D%82%E9%A1%B9/Hexo/","excerpt":"","text":"文章置顶 修改 \\node_modules\\hexo-generator-index-pin-top\\lib\\generator.js 123456789101112131415161718192021// ... const posts = locals.posts.sort(config.index_generator.order_by); // 添加以下根据 Top 排序代码 posts.data = posts.data.sort(function (a, b) &#123; if (a.top &amp;&amp; b.top) &#123; if (a.top == b.top) return b.date - a.date; else return b.top - a.top; &#125; else if (a.top &amp;&amp; !b.top) &#123; return -1; &#125; else if (!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); sort(posts.data, (a, b) =&gt; (b.sticky || 0) - (a.sticky || 0)); //...&#125;;","categories":[{"name":"杂项","slug":"杂项","permalink":"https://wingowen.github.io/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://wingowen.github.io/tags/Hexo/"}]},{"title":"MySQL","slug":"数据库/MySQL","date":"2023-09-20T01:53:41.000Z","updated":"2023-09-20T03:45:37.292Z","comments":true,"path":"2023/09/20/数据库/MySQL/","link":"","permalink":"https://wingowen.github.io/2023/09/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/","excerpt":"","text":"单进程多线程 SQL 解析器 &gt; SQL 查询优化器 &gt; 执行器 &gt; 存储引擎 SQL Select 语句完整的执行顺序： from 子句组装来自不同数据源的数据； where 子句基于指定的条件对记录行进行筛选； group by 子句将数据划分为多个分组； 使用聚集函数进行计算； 使用 having 子句筛选分组； 计算所有的表达式； select 的字段； 使用 order by 对结果集进行排序。 InnoDB 存储引擎 unzip 单页 16k。 支持事务，设计目标主要面向在线事务处理 OLAP 应用。其特点是行锁设计、支持外间，并支持类似于 Oracle 的非锁定读，即默认读取操作不会产生锁。 MVCC（Multi-Version Concurrency Control）是一种数据库并发控制机制，用于在多个事务并发执行时保证数据的一致性和隔离性。MVCC 基于版本控制的思想，为每个事务提供一个独立的数据版本，从而避免了读写冲突和锁竞争。 在数据库中，“undo 页”（也称为回滚段或回滚段页）是用于实现事务的回滚和并发控制的一种数据结构。它用于存储已提交事务的旧版本数据，以便在需要回滚或提供多版本并发控制时使用。 脏页（Dirty Page）是指在数据库缓冲区中已经被修改但尚未写回到磁盘的数据页。当数据库执行写操作时，相应的数据页会被标记为脏页，表示该页的内容已经被修改且与磁盘上的数据不一致。 123select * from innodb_buffer_page_lruwhere oldest_modification &gt; 0; 缓冲池 多缓冲池实例，根据 hash 将页分配到不同的实例 LRU Latest Recent Used 最近最少使用算法进行管理 123show variables like &#x27;innodb_buffer_pool_size&#x27;\\G;show variables like &#x27;innodb_buffer_pool_instance&#x27;\\G;show engine innodb status&#x27;\\G; Redo Log Buffer（重做日志缓冲区）是数据库系统中用于存储事务操作的日志记录的缓冲区。它是实现事务的持久性和恢复能力的关键组件。 参考资料 MySQL 技术内幕 InnoDB 存储引擎 - 第二版","categories":[{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"InooDB","slug":"InooDB","permalink":"https://wingowen.github.io/tags/InooDB/"}]},{"title":"大数据组件","slug":"大数据/大数据组件","date":"2023-09-19T09:11:59.000Z","updated":"2023-09-20T06:20:59.054Z","comments":true,"path":"2023/09/19/大数据/大数据组件/","link":"","permalink":"https://wingowen.github.io/2023/09/19/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6/","excerpt":"简单描述组件之间的关系文件系统：HDFS数据库存储：HBase、Kudu查询引擎：Hive、Impala","text":"简单描述组件之间的关系文件系统：HDFS数据库存储：HBase、Kudu查询引擎：Hive、Impala Hive Hive 存储的是纯逻辑表，即表的元数据 Hive 本身不存储数据，完全依赖于 HDFS 和 MapReduce，将结构化的数据文件映射为一张张数据库表 Hive 基于 MapReduce 处理数据，基于行模式处理 OLTP &amp; OLAP Hase Google‘s Big Table 的开源实现 NoSQL 分布式 KV 数据库 Hbase 存储的是物理表，适合存放非结构化数据 Hbase 基于列模式处理数据，适合海量数据的随机访问 OLTP 底层存储引擎是基于 LSM-Tree 数据结构设计的。写入数据时会先写 WAL 日志，再将数据写到写缓存 MemStore中，等写缓存达到一定规模后或满足其他触发条件才会 flush 刷写到磁盘，每一次刷写磁盘都会生成新的 HFile 文件，HBase 会定期执行 compaction 操作以合并 HFile 读取数据时会依次从 BlockCache、MemStore 以及 HFile 中 seek 数据，再加上一些其他设计比如布隆过滤器、索引等 Spark 在线数据分析 HbaseClient 直接访问 Hbase Spark 引入 Hbase Connector 后通过 Spark SQL / DDL 直接操作数据 Hive 在线数据分析 建 Hbase 外表然后进行 SQL 查询 离线处理 通过 LTS 导出到 HDFS，存储为 Parquet 格式，使用 Spark 进行分析 通过 LTS 增量订阅 Hbase 数据，写入到 Kafka，使用 Spark Streaming 对 Kafka 进行流式计算 WAL（Write-Ahead Logging）是一种数据库系统中常用的日志记录技术，用于确保数据的持久性和一致性。WAL日志是在对数据库进行修改之前，首先将修改操作记录到日志中，然后再将修改应用到数据库文件中。 Impala 使用分布式查询引擎（由 Query Planner、Query Coordinator 和 Query Exec Engine 三部分组成），可以直接从 HDFS 或 Hase 中进行实时交互式 SQL 查询 kudu 同时提供低延迟的随机读写和高效的数据分析能力，是一个融合 HDFS 和 HBase 的功能的新组件，","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://wingowen.github.io/tags/Hive/"},{"name":"Kudu","slug":"Kudu","permalink":"https://wingowen.github.io/tags/Kudu/"},{"name":"Impala","slug":"Impala","permalink":"https://wingowen.github.io/tags/Impala/"}]},{"title":"Pandas 练习","slug":"Python/Pandas-练习","date":"2023-09-19T07:11:03.000Z","updated":"2023-09-19T07:40:22.864Z","comments":true,"path":"2023/09/19/Python/Pandas-练习/","link":"","permalink":"https://wingowen.github.io/2023/09/19/Python/Pandas-%E7%BB%83%E4%B9%A0/","excerpt":"","text":"12import numpy as npimport pandas as pd 12data = &#123;&quot;grammer&quot;:[&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,&quot;R&quot;,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;], &quot;score&quot;:[1,2,np.nan,4,5,6,7,10]&#125; 1df = pd.DataFrame(data=data) 1df[df[&quot;grammer&quot;]==&#x27;Python&#x27;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } grammer score 0 Python 1.0 7 Python 10.0 1df.columns Index([&#39;grammer&#39;, &#39;score&#39;], dtype=&#39;object&#39;) 1df.rename(columns=&#123;&#x27;score&#x27;:&#x27;popularity&#x27;&#125;, inplace=True) 1df[&#x27;grammer&#x27;].value_counts() Python 2 GO 1 Java 1 C 1 R 1 PHP 1 SQL 1 Name: grammer, dtype: int64 12# 线性插值填充df[&#x27;popularity&#x27;] = df[&#x27;popularity&#x27;].fillna(df[&#x27;popularity&#x27;].interpolate()) 1df[df[&#x27;popularity&#x27;]&gt;3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } grammer popularity 3 GO 4.0 4 R 5.0 5 SQL 6.0 6 PHP 7.0 7 Python 10.0 1df.drop_duplicates(subset=[&#x27;grammer&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } grammer popularity 0 Python 1.0 1 C 2.0 2 Java 3.0 3 GO 4.0 4 R 5.0 5 SQL 6.0 6 PHP 7.0 1df[&#x27;popularity&#x27;].mean() 4.75 1df[&#x27;grammer&#x27;].to_list() [&#39;Python&#39;, &#39;C&#39;, &#39;Java&#39;, &#39;GO&#39;, &#39;R&#39;, &#39;SQL&#39;, &#39;PHP&#39;, &#39;Python&#39;] 1df.shape (8, 2) 123# 改变列的顺序col = df.columns[[1,0]]df = df[col] 1df[df[&#x27;popularity&#x27;] == df[&#x27;popularity&#x27;].max()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } popularity grammer 7 10.0 Python 1df[-5:df.shape[0]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } popularity grammer 3 4.0 GO 4 5.0 R 5 6.0 SQL 6 7.0 PHP 7 10.0 Python 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } popularity grammer 3 4.0 GO 4 5.0 R 5 6.0 SQL 6 7.0 PHP 7 10.0 Python 1df.drop(index=len(df)-1, inplace=True) 12row = &#123;&#x27;popularity&#x27;: 6.6, &#x27;grammer&#x27; : &#x27;Perl&#x27;&#125;df = df.append(row, ignore_index=True) 1df = df.sort_values(&quot;popularity&quot;) 1df[&#x27;grammer&#x27;].map(lambda x: len(x)) 0 6 1 1 2 4 3 2 4 1 5 3 7 4 6 3 Name: grammer, dtype: int64 PART 021df = pd.read_excel(&quot;pandas120.xlsx&quot;) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary 0 2020-03-16 11:30:18 本科 20k-35k 1 2020-03-16 10:58:48 本科 20k-40k 2 2020-03-16 10:46:39 不限 20k-35k 3 2020-03-16 10:45:44 本科 13k-20k 4 2020-03-16 10:20:41 本科 10k-20k 1234def cal_ave(x): min, max = x.split(&#x27;-&#x27;) return int((int(min[:-1])+int(max[:-1]))/2*1000)df[&#x27;salary&#x27;] = df[&#x27;salary&#x27;].apply(cal_ave) 1df.groupby(df[&#x27;education&#x27;]).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } salary education 不限 19600.000000 大专 10000.000000 本科 19361.344538 硕士 20642.857143 1df[&#x27;createTime&#x27;] = df[&#x27;createTime&#x27;].map(lambda x : x.strftime(&#x27;%m-%d&#x27;)) 1df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 135 entries, 0 to 134 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 createTime 135 non-null object 1 education 135 non-null object 2 salary 135 non-null int64 dtypes: int64(1), object(2) memory usage: 3.3+ KB 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } salary count 135.000000 mean 19159.259259 std 8661.686922 min 3500.000000 25% 14000.000000 50% 17500.000000 75% 25000.000000 max 45000.000000 123bins = [0,5000, 20000, 50000]group_names = [&#x27;低&#x27;, &#x27;中&#x27;, &#x27;高&#x27;]df[&#x27;categories&#x27;] = pd.cut(df[&#x27;salary&#x27;], bins, labels=group_names) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary categories 0 03-16 本科 27500 高 1 03-16 本科 30000 高 2 03-16 不限 27500 高 3 03-16 本科 16500 中 4 03-16 本科 15000 中 ... ... ... ... ... 130 03-16 本科 14000 中 131 03-16 硕士 37500 高 132 03-16 本科 30000 高 133 03-16 本科 19000 中 134 03-16 本科 30000 高 135 rows × 4 columns 1df.sort_values(&#x27;salary&#x27;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary categories 53 03-16 本科 45000 高 37 03-16 本科 40000 高 101 03-16 本科 37500 高 16 03-16 本科 37500 高 131 03-16 硕士 37500 高 ... ... ... ... ... 123 03-16 本科 4500 低 126 03-16 本科 4000 低 110 03-16 本科 4000 低 96 03-16 不限 3500 低 113 03-16 本科 3500 低 135 rows × 4 columns 1df.loc[32] createTime 03-16 education 硕士 salary 22500 categories 高 Name: 32, dtype: object 1np.median(df[&#x27;salary&#x27;]) 17500.0 1df.salary.plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; 12# KED Kernel Density Estimationdf.salary.plot(kind=&#x27;kde&#x27;) &lt;AxesSubplot:ylabel=&#39;Density&#39;&gt; 1del df[&#x27;categories&#x27;] 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary 0 03-16 本科 27500 1 03-16 本科 30000 2 03-16 不限 27500 3 03-16 本科 16500 4 03-16 本科 15000 ... ... ... ... 130 03-16 本科 14000 131 03-16 硕士 37500 132 03-16 本科 30000 133 03-16 本科 19000 134 03-16 本科 30000 135 rows × 3 columns 1df.salary.max() - df.salary.min() 41500 1pd.concat([df[:1], df[-2:-1]]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary 0 03-16 本科 27500 133 03-16 本科 19000 1df.append(df.loc[7]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary 0 03-16 本科 27500 1 03-16 本科 30000 2 03-16 不限 27500 3 03-16 本科 16500 4 03-16 本科 15000 ... ... ... ... 131 03-16 硕士 37500 132 03-16 本科 30000 133 03-16 本科 19000 134 03-16 本科 30000 7 03-16 本科 12500 136 rows × 3 columns 1df.set_index(&quot;createTime&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } education salary createTime 03-16 本科 27500 03-16 本科 30000 03-16 不限 27500 03-16 本科 16500 03-16 本科 15000 ... ... ... 03-16 本科 14000 03-16 硕士 37500 03-16 本科 30000 03-16 本科 19000 03-16 本科 30000 135 rows × 2 columns 1df_r = pd.DataFrame(np.random.randint(1, 10, 135), columns=[&#x27;random&#x27;]) 1df = pd.concat([df, df_r], axis=1) 1df[&#x27;sub&#x27;] = df.salary - df.random 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary random sub 0 03-16 本科 27500 2 27498 1 03-16 本科 30000 6 29994 2 03-16 不限 27500 5 27495 3 03-16 本科 16500 3 16497 4 03-16 本科 15000 8 14992 ... ... ... ... ... ... 130 03-16 本科 14000 1 13999 131 03-16 硕士 37500 5 37495 132 03-16 本科 30000 8 29992 133 03-16 本科 19000 7 18993 134 03-16 本科 30000 6 29994 135 rows × 5 columns 1df.isna().sum() createTime 0 education 0 salary 0 random 0 sub 0 dtype: int64 1df.isna().values.any() False 1df.salary.astype(float) 0 27500.0 1 30000.0 2 27500.0 3 16500.0 4 15000.0 ... 130 14000.0 131 37500.0 132 30000.0 133 19000.0 134 30000.0 Name: salary, Length: 135, dtype: float64 1len(df[df.salary &gt; 10000].salary) 119 12df.education.value_counts() 本科 119 硕士 7 不限 5 大专 4 Name: education, dtype: int64 1df.education.unique() array([&#39;本科&#39;, &#39;不限&#39;, &#39;硕士&#39;, &#39;大专&#39;], dtype=object) 1df.education.nunique() 4 1df[(df.salary + df[&#x27;sub&#x27;]) &gt; 60000].tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } createTime education salary random sub 92 03-16 本科 35000 6 34994 101 03-16 本科 37500 4 37496 131 03-16 硕士 37500 5 37495 金融数据处理1df = pd.read_excel(&#x27;./600000.SH.xls&#x27;) WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero 1df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 0 600000.SH 浦发银行 2016-01-04 16.1356 16.1444 16.1444 15.4997 15.7205 42240610 754425783 -0.4151 -2.5725 17.8602 0.2264 3.320318e+11 3.320318e+11 1.865347e+10 6.5614 1 600000.SH 浦发银行 2016-01-05 15.7205 15.4644 15.9501 15.3672 15.8618 58054793 1034181474 0.1413 0.8989 17.8139 0.3112 3.350163e+11 3.350163e+11 1.865347e+10 6.6204 2 600000.SH 浦发银行 2016-01-06 15.8618 15.8088 16.0208 15.6234 15.9855 46772653 838667398 0.1236 0.7795 17.9307 0.2507 3.376278e+11 3.376278e+11 1.865347e+10 6.6720 1df.isna().sum() 代码 1 简称 2 日期 2 前收盘价(元) 2 开盘价(元) 2 最高价(元) 2 最低价(元) 2 收盘价(元) 2 成交量(股) 2 成交金额(元) 2 涨跌(元) 2 涨跌幅(%) 2 均价(元) 2 换手率(%) 2 A股流通市值(元) 2 总市值(元) 2 A股流通股本(股) 2 市盈率 2 dtype: int64 1df[df[&#x27;日期&#x27;].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 327 NaN NaN NaT NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 328 数据来源：Wind资讯 NaN NaT NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 12345data = dffor columname in data.columns: if data[columname].count() != len(data): loc = data[columname][data[columname].isnull().values==True].index.tolist() print(&#x27;列名：&quot;&#123;&#125;&quot;, 第&#123;&#125;行位置有缺失值&#x27;.format(columname,loc)) 列名：&quot;代码&quot;, 第[327]行位置有缺失值 列名：&quot;简称&quot;, 第[327, 328]行位置有缺失值 列名：&quot;日期&quot;, 第[327, 328]行位置有缺失值 列名：&quot;前收盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;开盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;最高价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;最低价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;收盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;成交量(股)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;成交金额(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;涨跌(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;涨跌幅(%)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;均价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;换手率(%)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;A股流通市值(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;总市值(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;A股流通股本(股)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;市盈率&quot;, 第[327, 328]行位置有缺失值 1df.dropna(axis=0, how=&#x27;any&#x27;, inplace=True) 12# df = df.set_index(&#x27;日期&#x27;)df[&#x27;收盘价(元)&#x27;].plot(kind=&#x27;line&#x27;) &lt;AxesSubplot:&gt; 1df[[&#x27;收盘价(元)&#x27;, &#x27;开盘价(元)&#x27;]].plot(kind=&#x27;line&#x27;) &lt;AxesSubplot:&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 25910 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 25910 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0, flags=flags) 1df[&#x27;涨跌幅(%)&#x27;].plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; 1df[&#x27;涨跌幅(%)&#x27;].plot(kind=&#x27;hist&#x27;, bins=30) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; 12345temp = pd.DataFrame(columns = data.columns.to_list())for i in range(len(data)): if type(data.iloc[i,13]) != float: temp = temp.append(data.loc[i])temp.head(1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 26 600000.SH 浦发银行 2016-02-16 16.2946 16.2946 16.2946 16.2946 16.2946 -- -- 0.0 0.0 -- -- 3.441565e+11 3.441565e+11 1.865347e+10 6.801 1i = df[df[&#x27;换手率(%)&#x27;].isin([&#x27;--&#x27;])].index 1i Int64Index([26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], dtype=&#39;int64&#39;) 1df = df.drop(index=i) 1df[&#x27;换手率(%)&#x27;].plot(kind=&#x27;kde&#x27;) &lt;AxesSubplot:ylabel=&#39;Density&#39;&gt; 1df[&#x27;收盘价(元)&#x27;].diff() 0 NaN 1 0.1413 2 0.1237 3 -0.5211 4 -0.0177 ... 322 -0.0800 323 -0.1000 324 -0.0600 325 -0.0600 326 -0.1000 Name: 收盘价(元), Length: 309, dtype: float64 1df[&#x27;收盘价(元)&#x27;] - df[&#x27;收盘价(元)&#x27;].shift(1) 0 NaN 1 0.1413 2 0.1237 3 -0.5211 4 -0.0177 ... 322 -0.0800 323 -0.1000 324 -0.0600 325 -0.0600 326 -0.1000 Name: 收盘价(元), Length: 309, dtype: float64 1df[&#x27;收盘价(元)&#x27;].pct_change() 0 NaN 1 0.008988 2 0.007799 3 -0.032598 4 -0.001145 ... 322 -0.005277 323 -0.006631 324 -0.004005 325 -0.004021 326 -0.006729 Name: 收盘价(元), Length: 309, dtype: float64 1df = data.set_index(&#x27;日期&#x27;) 1df[&#x27;收盘价(元)&#x27;].rolling(5).mean() 日期 2016-01-04 NaN 2016-01-05 NaN 2016-01-06 NaN 2016-01-07 NaN 2016-01-08 15.69578 ... 2017-05-03 15.14200 2017-05-04 15.12800 2017-05-05 15.07000 2017-05-08 15.00000 2017-05-09 14.92000 Name: 收盘价(元), Length: 327, dtype: float64 1df[&#x27;收盘价(元)&#x27;].rolling(5).sum() 日期 2016-01-04 NaN 2016-01-05 NaN 2016-01-06 NaN 2016-01-07 NaN 2016-01-08 78.4789 ... 2017-05-03 75.7100 2017-05-04 75.6400 2017-05-05 75.3500 2017-05-08 75.0000 2017-05-09 74.6000 Name: 收盘价(元), Length: 327, dtype: float64 1234temp = pd.DataFrame()temp[&#x27;m5&#x27;] = df[&#x27;收盘价(元)&#x27;].rolling(5).mean()temp[&#x27;m20&#x27;] = df[&#x27;收盘价(元)&#x27;].rolling(20).mean()temp.plot() &lt;AxesSubplot:xlabel=&#39;日期&#39;&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) 1df[&#x27;收盘价(元)&#x27;].resample(rule=&#x27;W&#x27;).max() 日期 2016-01-10 15.9855 2016-01-17 15.8265 2016-01-24 15.6940 2016-01-31 15.0405 2016-02-07 16.2328 ... 2017-04-16 15.9700 2017-04-23 15.5600 2017-04-30 15.2100 2017-05-07 15.1600 2017-05-14 14.8600 Freq: W-SUN, Name: 收盘价(元), Length: 71, dtype: float64 12df[&#x27;收盘价(元)&#x27;].plot()df[&#x27;收盘价(元)&#x27;].resample(&#x27;7D&#x27;).max().plot() &lt;AxesSubplot:xlabel=&#39;日期&#39;&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) 1data[&#x27;开盘价(元)&#x27;].expanding(min_periods=1).mean() 0 16.144400 1 15.804400 2 15.805867 3 15.784525 4 15.761120 ... 322 16.055594 323 16.052552 324 16.049159 325 16.045266 326 16.041122 Name: 开盘价(元), Length: 327, dtype: float64 12df[&#x27;expanding Open mean&#x27;]=df[&#x27;开盘价(元)&#x27;].expanding(min_periods=1).mean()df[[&#x27;开盘价(元)&#x27;, &#x27;expanding Open mean&#x27;]].plot(figsize=(16, 6)) &lt;AxesSubplot:xlabel=&#39;日期&#39;&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0, flags=flags) pandas &amp; numpy123456# 随机df1 = pd.DataFrame(np.random.randint(0, 100, 20))# 等步长df2 = pd.DataFrame(np.arange(0, 100, 5))# 正态分布df3 = pd.DataFrame(np.random.normal(0, 1, 20)) 1df3.plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; 1df = pd.concat([df1,df2,df3],ignore_index=True) 1df = pd.concat([df1,df2,df3], axis=1, ignore_index=True) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 25 0 0.573071 1 86 5 -0.474405 2 34 10 -0.910378 3 18 15 0.713983 4 9 20 0.732289 5 67 25 -2.087864 6 5 30 -1.126019 7 51 35 -1.512201 8 3 40 -0.697655 9 10 45 -1.082615 10 59 50 -0.809815 11 28 55 -0.158042 12 21 60 -1.753240 13 96 65 0.687153 14 77 70 -1.534696 15 25 75 1.169113 16 26 80 -0.015986 17 52 85 0.517278 18 60 90 1.045848 19 35 95 0.856043 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 20.000000 20.000000 20.000000 mean 39.350000 47.500000 -0.293407 std 27.642978 29.580399 1.034218 min 3.000000 0.000000 -2.087864 25% 20.250000 23.750000 -1.093466 50% 31.000000 47.500000 -0.316223 75% 59.250000 71.250000 0.693860 max 96.000000 95.000000 1.169113 1df.columns = [&#x27;col1&#x27;, &#x27;col2&#x27;, &#x27;col3&#x27;] 1df[&#x27;col1&#x27;][~df[&#x27;col1&#x27;].isin(df[&#x27;col2&#x27;])] 1 86 2 34 3 18 4 9 5 67 7 51 8 3 10 59 11 28 12 21 13 96 14 77 16 26 17 52 Name: col1, dtype: int64 1df[&#x27;col1&#x27;].append(df[&#x27;col2&#x27;]).value_counts().head(3) 25 3 60 2 35 2 dtype: int64 1np.where(df[&#x27;col1&#x27;] % 5==0) (array([ 0, 6, 9, 15, 18, 19]),) 1df[df.columns[::-1]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col3 col2 col1 0 0.573071 0 25 1 -0.474405 5 86 2 -0.910378 10 34 3 0.713983 15 18 4 0.732289 20 9 5 -2.087864 25 67 6 -1.126019 30 5 7 -1.512201 35 51 8 -0.697655 40 3 9 -1.082615 45 10 10 -0.809815 50 59 11 -0.158042 55 28 12 -1.753240 60 21 13 0.687153 65 96 14 -1.534696 70 77 15 1.169113 75 25 16 -0.015986 80 26 17 0.517278 85 52 18 1.045848 90 60 19 0.856043 95 35 1df[&#x27;col1&#x27;].take([1,10,15]) 1 86 10 59 15 25 Name: col1, dtype: int64 12tem = np.diff(np.sign(np.diff(df[&#x27;col1&#x27;])))np.where(tem == -2)[0] + 1 array([ 1, 5, 7, 10, 13, 18]) 1df.mean(axis=1) 0 8.524357 1 30.175198 2 14.363207 3 11.237994 4 9.910763 5 29.970712 6 11.291327 7 28.162600 8 14.100782 9 17.972462 10 36.063395 11 27.613986 12 26.415587 13 53.895718 14 48.488435 15 33.723038 16 35.328005 17 45.839093 18 50.348616 19 43.618681 dtype: float64 1np.convolve(df[&#x27;col2&#x27;], np.ones(3)/3, mode=&#x27;valid&#x27;) array([ 5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60., 65., 70., 75., 80., 85., 90.]) 1df[&#x27;col2&#x27;].rolling(window=3).mean() 0 NaN 1 NaN 2 5.0 3 10.0 4 15.0 5 20.0 6 25.0 7 30.0 8 35.0 9 40.0 10 45.0 11 50.0 12 55.0 13 60.0 14 65.0 15 70.0 16 75.0 17 80.0 18 85.0 19 90.0 Name: col2, dtype: float64 1df.sort_values(&quot;col3&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 67 25 -2.087864 12 21 60 -1.753240 14 77 70 -1.534696 7 51 35 -1.512201 6 5 30 -1.126019 9 10 45 -1.082615 2 34 10 -0.910378 10 59 50 -0.809815 8 3 40 -0.697655 1 86 5 -0.474405 11 28 55 -0.158042 16 26 80 -0.015986 17 52 85 0.517278 0 25 0 0.573071 13 96 65 0.687153 3 18 15 0.713983 4 9 20 0.732289 19 35 95 0.856043 18 60 90 1.045848 15 25 75 1.169113 1df.col1[df[&#x27;col1&#x27;] &gt; 50]= &#x27;高&#x27; &lt;ipython-input-97-d0c96ec6289a&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://img/pandas.pydata.org/img/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df.col1[df[&#39;col1&#39;] &gt; 50]= &#39;高&#39; 1np.linalg.norm(df[&#x27;col2&#x27;]-df[&#x27;col3&#x27;]) 248.99392792632952 123456789df1= pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],&#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;],&#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;],&#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;, &#x27;B3&#x27;]&#125;)df2= pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],&#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;],&#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C1&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;],&#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D1&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;) 1pd.merge(df1, df2, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 1pd.merge(df1, df2, how=&#x27;inner&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 1pd.merge(df1, df2, how=&#x27;left&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 123df = pd.DataFrame(&#123;0: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0), 1: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0), 2: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0)&#125;) 1na = np.array(df) 1np.argwhere(na==1) array([[ 0, 0], [ 1, 1], [ 1, 2], [ 3, 0], [ 3, 1], [ 3, 2], [ 4, 2], [ 5, 0], [ 5, 1], [ 6, 0], [ 6, 2], [ 7, 0], [ 8, 0], [ 8, 2], [ 9, 2], [10, 2], [11, 2], [12, 0], [12, 1], [14, 0], [14, 1], [15, 0], [15, 2], [17, 0], [17, 2], [18, 2], [19, 0], [19, 2], [21, 1], [21, 2], [22, 0], [22, 1], [23, 2], [24, 0], [24, 1], [25, 0], [28, 1], [29, 1], [30, 0], [30, 1], [31, 2], [32, 0], [33, 0], [33, 1], [33, 2], [34, 0], [35, 0], [36, 1], [36, 2], [37, 1], [37, 2], [38, 0], [38, 1], [39, 0], [39, 1], [40, 0], [41, 1], [42, 1], [43, 0], [43, 1], [43, 2], [44, 2], [45, 2], [47, 1], [47, 2], [48, 1], [49, 0], [49, 1], [50, 0], [50, 1], [51, 2], [52, 2], [53, 1], [54, 0], [55, 0], [55, 2], [56, 0], [57, 2], [58, 0], [58, 1], [58, 2], [59, 2], [61, 1], [61, 2], [62, 1], [63, 0], [64, 1], [65, 0], [65, 1], [66, 0], [66, 1], [66, 2], [67, 2], [68, 0], [68, 1], [68, 2], [69, 0], [69, 2], [70, 2], [71, 0], [71, 1], [71, 2], [72, 1], [73, 1], [74, 2], [76, 1], [76, 2], [77, 1], [77, 2], [78, 1], [78, 2], [79, 0], [79, 2], [80, 1], [81, 0], [81, 1], [81, 2], [83, 1], [84, 0], [84, 1], [84, 2], [85, 0], [86, 0], [86, 1], [86, 2], [87, 2], [89, 0], [90, 0], [90, 2], [91, 1], [91, 2], [92, 1], [93, 0], [96, 0], [96, 2], [98, 0], [98, 1]]) 1pd.pivot_table(df,values=[0,1],index=0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 1 2 0 0 0.418182 0.490909 1 0.488889 0.444444 1df = pd.read_csv(&quot;./数据.csv&quot;, encoding=&#x27;gbk&#x27;) 1!file -i 数据.csv 数据.csv: regular file 1!yum install enca zsh:1: command not found: yum 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } positionId positionName companyId companyLogo companySize industryField financeStage companyLabelList firstType secondType ... plus pcShow appShow deliver gradeDescription promotionScoreExplain isHotHire count aggregatePositionIds famousCompany 0 6802721 数据分析 475770 i/image2/M01/B7/3E/CgoB5lwPfEaAdn8WAABWQ0Jgl5s... 50-150人 移动互联网,电商 A轮 ['绩效奖金', '带薪年假', '定期体检', '弹性工作'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 1 5204912 数据建模 50735 image1/M00/00/85/CgYXBlTUXeeAR0IjAABbroUk-dw97... 150-500人 电商 B轮 ['年终奖金', '做五休二', '六险一金', '子女福利'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] False 2 6877668 数据分析 100125 image2/M00/0C/57/CgqLKVYcOA2ADcFuAAAE8MukIKA74... 2000人以上 移动互联网,企业服务 上市公司 ['节日礼物', '年底双薪', '股票期权', '带薪年假'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 3 6496141 数据分析 26564 i/image2/M01/F7/3F/CgoB5lyGAQGAZeI-AAAdOqXecnw... 500-2000人 电商 D轮及以上 ['生日趴', '每月腐败基金', '每月补贴', '年度旅游'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] True 4 6467417 数据分析 29211 i/image2/M01/77/B8/CgoB5l1WDyGATNP5AAAlY3h88SY... 2000人以上 物流丨运输 上市公司 ['技能培训', '免费班车', '专项奖金', '岗位晋升'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 100 6884346 数据分析师 21236 i/image/M00/43/F6/CgqKkVeEh76AUVPoAAA2Bj747wU6... 500-2000人 移动互联网,医疗丨健康 C轮 ['技能培训', '年底双薪', '节日礼物', '绩效奖金'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 101 6849100 商业数据分析 72076 i/image2/M01/92/A4/CgotOV2LPUmAR_8dAAB_DlDMiXA... 500-2000人 移动互联网,电商 C轮 ['节日礼物', '股票期权', '带薪年假', '年度旅游'] 市场|商务类 市场|营销 ... NaN 0 0 0 NaN NaN 0 0 [] False 102 6803432 奔驰·耀出行-BI数据分析专家 751158 i/image3/M01/64/93/Cgq2xl48z2mAeYRoAAD6Qf_Jeq8... 150-500人 移动互联网 不需要融资 [] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] False 103 6704835 BI数据分析师 52840 i/image2/M00/26/CA/CgoB5lofsguAfk9ZAACoL3r4p24... 2000人以上 电商 上市公司 ['技能培训', '年底双薪', '节日礼物', '绩效奖金'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] True 104 6728058 数据分析专家-LQ(J181203029) 2474 i/image2/M01/14/4D/CgoB5lyq5fqAAHHzAAAa148hbk8... 2000人以上 汽车丨出行 不需要融资 ['弹性工作', '节日礼物', '岗位晋升', '技能培训'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] True 105 rows × 53 columns 1pd.pivot_table(df,values=[&quot;companyId&quot;,&quot;salary&quot;,&quot;score&quot;],index=&quot;positionId&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } companyId salary score positionId 5203054 329 30000 4.0 5204912 50735 15000 176.0 5269002 50576 37500 1.0 5453691 166666 30000 4.0 5519962 50735 37500 14.0 ... ... ... ... 6882983 7461 27500 15.0 6884346 21236 25000 0.0 6886661 321001 37500 5.0 6888169 751158 42500 1.0 6896403 285786 30000 3.0 95 rows × 3 columns 1pip install nbconvert pandoc Requirement already satisfied: nbconvert in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (6.0.7) Collecting pandoc Downloading pandoc-2.3.tar.gz (33 kB) Requirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.5.1) Requirement already satisfied: traitlets&gt;=4.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (5.0.5) Requirement already satisfied: nbformat&gt;=4.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (5.0.8) Requirement already satisfied: pygments&gt;=2.4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (2.7.2) Requirement already satisfied: defusedxml in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.6.0) Requirement already satisfied: jupyter-core in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (4.6.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (1.4.3) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.8.4) Requirement already satisfied: jupyterlab-pygments in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.1.2) Requirement already satisfied: testpath in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.4.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.3) Requirement already satisfied: bleach in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (3.2.1) Requirement already satisfied: jinja2&gt;=2.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (2.11.2) Collecting plumbum Downloading plumbum-1.8.2-py3-none-any.whl (127 kB) \u001b[K |████████████████████████████████| 127 kB 14 kB/s eta 0:00:01 \u001b[?25hRequirement already satisfied: ply in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from pandoc) (3.11) Requirement already satisfied: async-generator in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (1.10) Requirement already satisfied: jupyter-client&gt;=6.1.5 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (6.1.7) Requirement already satisfied: nest-asyncio in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (1.4.2) Requirement already satisfied: ipython-genutils in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from traitlets&gt;=4.2-&gt;nbconvert) (0.2.0) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.4-&gt;nbconvert) (3.2.0) Requirement already satisfied: webencodings in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (0.5.1) Requirement already satisfied: six&gt;=1.9.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (1.15.0) Requirement already satisfied: packaging in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (20.4) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jinja2&gt;=2.4-&gt;nbconvert) (1.1.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (2.8.1) Requirement already satisfied: pyzmq&gt;=13 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (19.0.2) Requirement already satisfied: tornado&gt;=4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (6.0.4) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (0.17.3) Requirement already satisfied: setuptools in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (50.3.1.post20201107) Requirement already satisfied: attrs&gt;=17.4.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (20.3.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from packaging-&gt;bleach-&gt;nbconvert) (2.4.7) Building wheels for collected packages: pandoc Building wheel for pandoc (setup.py) ... \u001b[?25ldone \u001b[?25h Created wheel for pandoc: filename=pandoc-2.3-py3-none-any.whl size=33271 sha256=22e3e077eb44a4ba73332321d2656d5d67e118abef8947040a2719d096f363f4 Stored in directory: /Users/wingo.wen/Library/Caches/pip/wheels/90/3a/a8/3237a93e3a6261bd24edabf3277ca59f64c1710b3d8c7c72a0 Successfully built pandoc Installing collected packages: plumbum, pandoc Successfully installed pandoc-2.3 plumbum-1.8.2 Note: you may need to restart the kernel to use updated packages. 1!jupyter nbconvert --to markdown pandas.ipynb","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://wingowen.github.io/tags/Pandas/"}]},{"title":"SQL 练习","slug":"数据库/SQL-练习","date":"2023-09-19T07:03:55.000Z","updated":"2023-09-19T07:25:20.863Z","comments":true,"path":"2023/09/19/数据库/SQL-练习/","link":"","permalink":"https://wingowen.github.io/2023/09/19/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL-%E7%BB%83%E4%B9%A0/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381CREATE TABLE `course` ( `c_id` text, `c_name` text, `t_id` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `score` ( `s_id` text, `c_id` text, `s_score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `student` ( `s_id` text, `s_name` text, `s_birth` text, `s_sex` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `teacher` ( `t_id` text, `t_name` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `course` (`c_id`, `c_name`, `t_id`) VALUES(&#x27;02&#x27;, &#x27;数学&#x27;, &#x27;01&#x27;),(&#x27;03&#x27;, &#x27;英语&#x27;, &#x27;03&#x27;),(&#x27;01&#x27;, &#x27;语文&#x27;, &#x27;02&#x27;);INSERT INTO `score` (`s_id`, `c_id`, `s_score`) VALUES(&#x27;01&#x27;, &#x27;02&#x27;, &#x27;90&#x27;),(&#x27;01&#x27;, &#x27;03&#x27;, &#x27;99&#x27;),(&#x27;02&#x27;, &#x27;01&#x27;, &#x27;70&#x27;),(&#x27;02&#x27;, &#x27;02&#x27;, &#x27;60&#x27;),(&#x27;02&#x27;, &#x27;03&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;01&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;02&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;03&#x27;, &#x27;80&#x27;),(&#x27;04&#x27;, &#x27;01&#x27;, &#x27;50&#x27;),(&#x27;04&#x27;, &#x27;02&#x27;, &#x27;30&#x27;),(&#x27;04&#x27;, &#x27;03&#x27;, &#x27;20&#x27;),(&#x27;05&#x27;, &#x27;01&#x27;, &#x27;76&#x27;),(&#x27;05&#x27;, &#x27;02&#x27;, &#x27;87&#x27;),(&#x27;06&#x27;, &#x27;01&#x27;, &#x27;31&#x27;),(&#x27;06&#x27;, &#x27;03&#x27;, &#x27;34&#x27;),(&#x27;07&#x27;, &#x27;02&#x27;, &#x27;89&#x27;),(&#x27;07&#x27;, &#x27;03&#x27;, &#x27;98&#x27;),(&#x27;01&#x27;, &#x27;01&#x27;, &#x27;80&#x27;);INSERT INTO `student` (`s_id`, `s_name`, `s_birth`, `s_sex`) VALUES(&#x27;02&#x27;, &#x27;钱电&#x27;, &#x27;1990-12-21&#x27;, &#x27;男&#x27;),(&#x27;03&#x27;, &#x27;孙风&#x27;, &#x27;1990-05-20&#x27;, &#x27;男&#x27;),(&#x27;04&#x27;, &#x27;李云&#x27;, &#x27;1990-08-06&#x27;, &#x27;男&#x27;),(&#x27;05&#x27;, &#x27;周梅&#x27;, &#x27;1991-12-01&#x27;, &#x27;女&#x27;),(&#x27;06&#x27;, &#x27;吴兰&#x27;, &#x27;1992-03-01&#x27;, &#x27;女&#x27;),(&#x27;07&#x27;, &#x27;郑竹&#x27;, &#x27;1989-07-01&#x27;, &#x27;女&#x27;),(&#x27;08&#x27;, &#x27;王菊&#x27;, &#x27;1990-01-20&#x27;, &#x27;女&#x27;),(&#x27;01&#x27;, &#x27;赵雷&#x27;, &#x27;1990-01-01&#x27;, &#x27;男&#x27;);INSERT INTO `teacher` (`t_id`, `t_name`) VALUES(&#x27;02&#x27;, &#x27;李四&#x27;),(&#x27;03&#x27;, &#x27;王五&#x27;),(&#x27;01&#x27;, &#x27;张三&#x27;);# 查询&quot;01&quot;课程比&quot;02&quot;课程成绩高的学生的信息及课程分数select *from student st left join score sc01 on st.s_id = sc01.s_id and sc01.c_id = 01 left join score sc02 on st.s_id = sc02.s_id and sc02.c_id = 02where sc01.s_score &gt; sc02.s_score;# 查询平均成绩大于等于60分的同学的学生编号和学生姓名和平均成绩select *from student st left join ( select s_id,avg(s_score) as ms from score group by s_id ) st1 on st.s_id=st1.s_id WHERE st1.ms &gt; 60;# 查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩select *from student st left join ( SELECT s_id, count(1) as class_num, sum(s_score) score_sum FROM score group by s_id ) c on st.s_id = c.s_id;# 查询&quot;李&quot;姓老师的数量select count(1) from teacher where t_name like &quot;李%&quot;;# 查询学过&quot;张三&quot;老师授课的同学的信息select * FROM student stleft JOIN score sc on st.s_id = sc.s_idleft JOIN course c on c.c_id = sc.c_idleft JOIN teacher t on c.t_id = t.t_idwhere t.t_name=&quot;张三&quot;;# 查询学过编号为&quot;01&quot;并且也学过编号为&quot;02&quot;的课程的同学的信息select * FROM student stleft JOIN score sc on st.s_id = sc.s_idleft JOIN course c on c.c_id = sc.c_idwhere c.c_id=&#x27;01&#x27; or c.c_id=&#x27;02&#x27;;# 查询没有学全所有课程的同学的信息SELECT * FROM student st WHERE st.s_id in (select st.s_id as c_numFROM student st left JOIN score sc on st.s_id = sc.s_idGROUP BY st.s_idhaving count(sc.c_id) &lt; 3);select student.* from studentjoin (select count(c_id)num1 from course)tmp1left join( select s_id,count(c_id)num2 from score group by s_id)tmp2on student.s_id=tmp2.s_id and tmp1.num1=tmp2.num2where tmp2.s_id is null;# 查询至少有一门课与学号为&quot;01&quot;的同学所学相同的同学的信息select st.* from student stleft join score sc on sc.s_id = st.s_idwhere sc.c_id in (select c_id from score where s_id = 01)and st.s_id!=01group by st.s_id,s_name,s_birth,s_sex;select student.* from studentjoin (select c_id from score where score.s_id=01)tmp1join (select s_id,c_id from score)tmp2 on tmp1.c_id =tmp2.c_id and student.s_id =tmp2.s_idwhere student.s_id not in(&#x27;01&#x27;)group by student.s_id,s_name,s_birth,s_sex;select * from studentjoin (select c_id from score where score.s_id=01)tmp1join (select s_id,c_id from score)tmp2 on tmp1.c_id =tmp2.c_id and student.s_id =tmp2.s_id;# 查询和&quot;01&quot;号的同学学习的课程完全相同的其他同学的信息:# hive不支持group_concat方法,可用 concat_ws(’|’, collect_set(str)) 实现select * FROM(select st.*, GROUP_CONCAT(tmp.c_id SEPARATOR &#x27;;&#x27;) as all_c_id from student stjoin (select s_id,c_id from score)tmpon st.s_id =tmp.s_idWHERE st.s_id != 01group by st.s_id,s_name,s_birth,s_sex) aleft join (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01 group by score.s_id) tmp2on a.s_id = a.s_idwhere a.all_c_id = tmp2.all_c_id; select student.*,tmp1.course_id from studentjoin (select s_id ,GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id from score group by s_id having s_id not in (1))tmp1 on student.s_id = tmp1.s_idjoin (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id2 from score where s_id=1)tmp2 on tmp1.course_id = tmp2.course_id2;select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id2 from score where s_id=1;select * FROM(select st.*, GROUP_CONCAT(tmp.c_id SEPARATOR &#x27;;&#x27;) as all_c_id from student stjoin (select s_id,c_id from score)tmpon st.s_id =tmp.s_idWHERE st.s_id != 01group by st.s_id,s_name,s_birth,s_sex) aleft join (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01 group by score.s_id) tmp2on a.s_id = a.s_idwhere a.all_c_id = tmp2.all_c_id;select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01;# 查询没学过&quot;张三&quot;老师讲授的任一门课程的学生姓名SELECT st.* FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idLEFT JOIN course c ON c.c_id = sc.c_idLEFT JOIN teacher t ON t.t_id = c.t_idWHERE t.t_name=&#x27;张三&#x27;;# 查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩SELECT st.s_id FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idWHERE sc.s_score &lt; 60;SELECT st.s_id, st.s_name, round(avg(sc.s_score)) FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idWHERE sc.s_score &lt; 60GROUP BY s_id, s_nameHAVING count(sc.s_score) &gt;= 2;select student.s_id,student.s_name,tmp.avg_score from studentinner join (select s_id from score where s_score&lt;60 group by score.s_id having count(s_id)&gt;1)tmp2on student.s_id = tmp2.s_idleft join ( select s_id,round(AVG (score.s_score)) avg_score from score group by s_id)tmp on tmp.s_id=student.s_id;# 检索&quot;01&quot;课程分数小于60，按分数降序排列的学生信息:SELECT st.*, sc.s_score from student stLEFT JOIN score sc on sc.s_id = st.s_idwhere sc.c_id = 01 and sc.s_score &lt; 60order by sc.s_score DESC;# 按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩select a.s_id,tmp1.s_score as chinese,tmp2.s_score as math,tmp3.s_score as english, round(avg (a.s_score),2) as avgScorefrom score aleft join (select s_id,s_score from score s1 where c_id=&#x27;01&#x27;)tmp1 on tmp1.s_id=a.s_idleft join (select s_id,s_score from score s2 where c_id=&#x27;02&#x27;)tmp2 on tmp2.s_id=a.s_idleft join (select s_id,s_score from score s3 where c_id=&#x27;03&#x27;)tmp3 on tmp3.s_id=a.s_idgroup by a.s_id,tmp1.s_score,tmp2.s_score,tmp3.s_score order by avgScore desc;select * from course;SELECT sc.s_id, a.s_score as chinese, b.s_score as math, c.s_score as english, round(avg(a.s_score)) as avg_score FROM score scLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;01&#x27;) a on a.s_id=sc.s_idLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;02&#x27;) b on b.s_id=sc.s_idLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;03&#x27;) c on c.s_id=sc.s_idgroup by sc.s_id, a.s_score, b.s_score, c.s_scoreorder by avg_score desc;# 查询各科成绩最高分、最低分和平均分：以如下形式显示：课程ID，课程name，最高分，最低分，平均分，及格率，中等率，优良率，优秀率:# 及格为&gt;=60，中等为：70-80，优良为：80-90，优秀为：&gt;=90select c.c_id, c.c_name, max(s_score), min(s_score),round(avg(s_score)),round(sum(case when sc.s_score&gt;=60 and sc.s_score&lt;70 then 1 else 0 end)/count(1),2) &quot;及格率&quot;,round(sum(case when sc.s_score&gt;=70 and sc.s_score&lt;80 then 1 else 0 end)/count(1),2) &quot;中等率&quot;,round(sum(case when sc.s_score&gt;=80 and sc.s_score&lt;90 then 1 else 0 end)/count(1),2) &quot;优良率&quot;,round(sum(case when sc.s_score&gt;=90 then 1 else 0 end)/count(1)) &quot;优秀率&quot;from score scLEFT JOIN course c on c.c_id=sc.c_idgroup by c_id, c.c_name;# 按各科成绩进行排序，并显示排名select s1.*,row_number()over(order by s1.s_score desc) Ranking from score s1 where s1.c_id=&#x27;01&#x27;union all select s2.*,row_number()over(order by s2.s_score desc) Ranking from score s2 where s2.c_id=&#x27;02&#x27;union all select s3.*,row_number()over(order by s3.s_score desc) Ranking from score s3 where s3.c_id=&#x27;03&#x27;order by noRanking asc;SELECT sc.s_score, sc.s_id FROM score sc where sc.c_id = 1 GROUP BY sc.s_score, sc.s_id order by sc.s_score;# 查询学生的总成绩并进行排名select sum(s_score) as sum from scoregroup by s_idorder by sum;# 查询不同老师所教不同课程平均分从高到低显示(SELECT t_name, c.c_name, round(avg(sc.s_score)) from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_id where t.t_id=01 group by t_name, c.c_name )Union(SELECT t_name, c.c_name, round(avg(sc.s_score)) from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_id where t.t_id=02group by t_name, c.c_name;SELECT t_name, c.c_name, round(avg(sc.s_score)) avg from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_idgroup by t_name, c.c_nameorder by avg;# 查询所有课程的成绩第2名到第3名的学生信息及该课程成绩(SELECT * from (select * from score where c_id=&#x27;01&#x27; order by s_score desc limit 3)tmp1order by tmp1.s_score asc limit 2)UNION ALL(SELECT * from (select * from score where c_id=&#x27;02&#x27; order by s_score desc limit 3)tmp1order by tmp1.s_score asc limit 2);# 查询学生平均成绩及其名次SELECT @rownum := @rownum + 1 as rank, s.avgFROM (SELECT round(avg(s_score)) as avg FROM score GROUP by s_id order by avg desc) s,(SELECT @rownum := 0) r;# 查询各科成绩前三名的记录select score.c_id,course.c_name,student.s_name,s_score from scorejoin student on student.s_id=score.s_idjoin course on course.c_id=score.c_idWHERE score.c_id=&#x27;01&#x27;order by s_score desc limit 3;# 查询每门课程被选修的学生数SELECT c.c_name, count(1) from student stleft join score sc on sc.s_id = st.s_idleft join course c on c.c_id = sc.c_idgroup by c.c_name;SELECT * from student stleft join score sc on sc.s_id = st.s_idleft join course c on c.c_id = sc.c_id;# 查询所有学生的课程及分数情况select a.s_name, SUM(case c.c_name when &#x27;语文&#x27; then b.s_score else 0 end ) as chainese, SUM(case c.c_name when &#x27;数学&#x27; then b.s_score else 0 end ) as math, SUM(case c.c_name when &#x27;英语&#x27; then b.s_score else 0 end ) as english, SUM(b.s_score) as sumScore from student a join score b on a.s_id=b.s_id join course c on b.c_id=c.c_id group by s_name,a.s_id;# 查询每门课程成绩最好的前三名SELECT s_name, s_score FROM student stLEFT JOIN score sc on sc.s_id = st.s_id and sc.c_id=01order by s_score desc limit 3;# 统计每门课程的学生选修人数（超过5人的课程才统计）# 要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列select c.c_id, c_name, count(1) as num from course cleft join score sc on sc.c_id = c.c_idleft join student st on st.s_id = sc.s_idgroup by c_name, c.c_idhaving num&gt;5order by num,c.c_id;select distinct course.c_id,tmp.num from course join (select c_id,count(1) as num from score group by c_id)tmp where tmp.num&gt;=5 order by tmp.num desc ,course.c_id asc;# 查询各学生的年龄(周岁):# 按照出生日期来算，当前月日 &lt; 出生年月的月日则，年龄减一SELECT *,year(CURRENT_DATE())-year(s_birth) + (case when MONTH(CURRENT_DATE())&gt;MONTH(s_birth) then 1 when MONTH(CURRENT_DATE())=MONTH(s_birth) and DAY(CURRENT_DATE())&gt;DAY(s_birth) then 0 else 0end) as age FROM student;# 查询本周过生日的学生SELECT WEEKOFYEAR(STR_TO_DATE(year(CURRENT_DATE())+date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;),&#x27;%m-%d&#x27;));SELECT date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;);SELECT *, WEEKOFYEAR(concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(s_birth,&#x27;%m-%d&#x27;))) as a, WEEKOFYEAR(CURRENT_DATE()) as b,concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(s_birth,&#x27;%m-%d&#x27;)) a1,WEEKOFYEAR(s_birth) as cfrom student;# 细微差别SELECT WEEKOFYEAR(&#x27;2023-01-02&#x27;);SELECT WEEK(&#x27;2023-01-02&#x27;);select WEEKOFYEAR(concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;)));select s_name,s_sex,s_birth from student where MONTH(s_birth)=&#x27;12&#x27;;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://wingowen.github.io/tags/SQL/"}]},{"title":"高等数学","slug":"基础科学/高等数学","date":"2023-01-04T09:13:09.000Z","updated":"2023-09-20T06:47:27.218Z","comments":true,"path":"2023/01/04/基础科学/高等数学/","link":"","permalink":"https://wingowen.github.io/2023/01/04/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/","excerpt":"函数与极限","text":"函数与极限 函数与极限 函数 x,yx,yx,y 为连个变量 (x∈D)(x \\in D)(x∈D)，∀x∈D\\forall x \\in D∀x∈D 若存在唯一确定的 yyy 与 xxx 对应，称 yyy 为 xxx 的函数，记 y=f(x)y=f(x)y=f(x)。 R={y∣y=f(x),x∈D}R = \\{ y | y=f(x), x \\in D \\} R={y∣y=f(x),x∈D} DDD - 定义域；RRR - 值域。 特殊的函数 符号函数。 y=sng⁡x={−1,x&lt;00,x=01,x&gt;0y=\\operatorname{sng} x=\\left\\{ \\begin{array}{rr} -1, &amp; x&lt;0 \\\\ 0, &amp; x=0 \\\\ 1, &amp; x&gt;0 \\end{array}\\right. y=sngx=⎩⎪⎨⎪⎧​−1,0,1,​x&lt;0x=0x&gt;0​ 狄利克雷函数，QQQ 表示有理数。 f(x)={1x∈Q0x∉Qf(x)=\\left\\{\\begin{array}{ll} 1 &amp; x \\in Q \\\\ 0 &amp; x \\notin Q \\end{array}\\right.f(x)={10​x∈Qx∈/Q​ 左取整函数 [X][X][X]。 反函数 y=f(x)y=f(x)y=f(x) 严格单调，⇒x=arcf(y)\\Rightarrow x=arcf(y)⇒x=arcf(y)。 例题：求 y=ln(x+x2+1)y=ln(x+ \\sqrt{x^2 + 1})y=ln(x+x2+1​) 的反函数。 ⇒ey=x+x2+1\\Rightarrow e^y = x+ \\sqrt{x^2 + 1}⇒ey=x+x2+1​ ① ∵(x+x2+1)(−x+x2+1)=1\\because (x+ \\sqrt{x^2 + 1})(-x+ \\sqrt{x^2 + 1}) = 1∵(x+x2+1​)(−x+x2+1​)=1 ∴−x+x2+1=1ey=e−y\\therefore -x+ \\sqrt{x^2 + 1} = \\frac{1}{e^y} = e^{-y}∴−x+x2+1​=ey1​=e−y ② ① - ② ⇒2x=ey−e−y⇒x=ey−e−y2\\Rightarrow 2x=e^y-e^{-y} \\Rightarrow x = \\frac{e^y-e^{-y}}{2}⇒2x=ey−e−y⇒x=2ey−e−y​ 基本初等函数 初等函数 基本初等函数的四则运算及复合运算而形成的式子。 初等性质 奇偶性 DDD 关于原点对称。 奇函数：∀x∈D,f(−x)=−f(x)\\forall x \\in D, f(-x) = -f(x)∀x∈D,f(−x)=−f(x)。 偶函数：∀x∈D,f(−x)=f(x)\\forall x \\in D, f(-x) = f(x)∀x∈D,f(−x)=f(x)。 单调性 ∀x1,x2∈D,x1&lt;x2⇒f(x1)&lt;f(x2)\\forall x_1, x_2 \\in D, x1 &lt; x2 \\Rightarrow f(x1)&lt;f(x2)∀x1​,x2​∈D,x1&lt;x2⇒f(x1)&lt;f(x2)，单调递增。 ∀x1,x2∈D,x1&lt;x2⇒f(x1)&lt;f(x2)\\forall x_1, x_2 \\in D, x1&lt;x2 \\Rightarrow f(x1)&lt;f(x2)∀x1​,x2​∈D,x1&lt;x2⇒f(x1)&lt;f(x2)，单调递减。 有界性 ∃M&gt;0,∀x∈D⇒∣f(x)∣&lt;M\\exists M&gt;0, \\forall x \\in D \\Rightarrow |f(x)|&lt;M∃M&gt;0,∀x∈D⇒∣f(x)∣&lt;M，则称 f(x)f(x)f(x) 在 DDD 上有界，即有下界和上界。 周期性 ∃T&gt;0,∀x∈D(x+T∈D)⇒f(x+T)=f(x)\\exists T&gt;0, \\forall x \\in D (x+T \\in D) \\Rightarrow f(x+T)=f(x)∃T&gt;0,∀x∈D(x+T∈D)⇒f(x+T)=f(x)，f(x)f(x)f(x) 为周期为 T 的周期函数。 TminT_{min}Tmin​ 为最小正周期。 数列极限 (ε−N)(\\varepsilon - N)(ε−N) an{a_n}an​ 为数列，AAA 为常数。∀ε&gt;0,∃N&gt;0,n&gt;N，∣an−A∣&lt;ε\\forall \\varepsilon &gt;0, \\exists N&gt;0, n&gt;N ，|a_n -A|&lt;\\varepsilon∀ε&gt;0,∃N&gt;0,n&gt;N，∣an​−A∣&lt;ε。 lim⁡n→∞an=A/an→A(n→∞)\\lim_{n \\to \\infty} a_n= A \\quad / \\quad a_n → A (n → {\\infty}) n→∞lim​an​=A/an​→A(n→∞) 例题：证明 lim⁡n→∞n−1n+1=1\\lim_{n \\to \\infty} \\frac{n-1}{n+1}=1limn→∞​n+1n−1​=1。 ∣n−1n+1−1∣=2n+1&lt;ε|\\frac{n-1}{n+1} -1| = \\frac{2}{n+1} &lt; \\varepsilon∣n+1n−1​−1∣=n+12​&lt;ε n&gt;2ε−1n &gt; \\frac{2}{\\varepsilon} - 1n&gt;ε2​−1 取 N=[2ε−1]N=[\\frac{2}{\\varepsilon} - 1]N=[ε2​−1]，当 n&gt;N→2n+1&lt;εn&gt;N \\to \\frac{2}{n+1} &lt; \\varepsilonn&gt;N→n+12​&lt;ε 成立。 数列计算性质 唯一性，使用反证法证明 假设 lim⁡n→∞an=A\\lim_{n \\to \\infty} a_n = Alimn→∞​an​=A 且 lim⁡n→∞an=B\\lim_{n \\to \\infty} a_n = Blimn→∞​an​=B 成立。 设 ε&lt;A−B2&gt;0\\varepsilon &lt; \\frac{A-B}{2} &gt; 0ε&lt;2A−B​&gt;0 ∃Na&gt;0\\exists N_a &gt; 0∃Na​&gt;0 使之成立，∣an−A∣&lt;A−B2⇒A+B2&lt;an&lt;3A−B2|a_n - A| &lt; \\frac{A-B}{2} \\Rightarrow \\frac{A+B}{2}&lt;a_{n}&lt;\\frac{3 A-B}{2}∣an​−A∣&lt;2A−B​⇒2A+B​&lt;an​&lt;23A−B​ ① ∃Nb&gt;0\\exists N_b &gt; 0∃Nb​&gt;0 使之成立，∣an−B∣&lt;A−B2⇒3B−A2&lt;an&lt;A+B2|a_n - B| &lt; \\frac{A-B}{2} \\Rightarrow \\frac{3 B-A}{2}&lt;a_{n}&lt;\\frac{A+B}{2}∣an​−B∣&lt;2A−B​⇒23B−A​&lt;an​&lt;2A+B​ ② ① 与 ② 冲突，故假设不成立。 有界性 iflim⁡n→∞an=Aif \\lim_{n \\to \\infty} a_n = Aiflimn→∞​an​=A，则 ∃M\\exists M∃M 使得 ∣an∣≤M|a_n|≤M∣an​∣≤M 成立。反之不成立。 证：有极限则一定有界。 取 ε=1\\varepsilon = 1ε=1，则 ∃N&gt;0\\exists N&gt;0∃N&gt;0，当 n&gt;Nn&gt;Nn&gt;N 时，∣an−A∣&lt;1|a_n - A| &lt; 1∣an​−A∣&lt;1 ∵∣∣an∣−∣A∣∣≤∣an−A∣\\because ||a_n|-|A|| ≤ |a_n-A|∵∣∣an​∣−∣A∣∣≤∣an​−A∣ ∴∣∣an∣−∣A∣∣&lt;1→∣an∣&lt;1+A\\therefore ||a_n|-|A|| &lt; 1 \\to |a_n| &lt; 1 + A∴∣∣an​∣−∣A∣∣&lt;1→∣an​∣&lt;1+A，注意，这是在 n&gt;Nn&gt;Nn&gt;N 的条件下成立的。 取 M=MAX{∣a1∣,∣a2∣,∣a3∣,...,∣an∣,1+A}M = MAX\\{|a_1|,|a_2|,|a_3|,...,|a_n|,1+A\\}M=MAX{∣a1​∣,∣a2​∣,∣a3​∣,...,∣an​∣,1+A} 则 ∀n,∣an∣≤M\\forall n, |a_n| ≤ M∀n,∣an​∣≤M 成立 保号性 iflim⁡n→∞an=A&gt;0if \\lim_{n \\to \\infty} a_n = A &gt; 0iflimn→∞​an​=A&gt;0，则 ∃N&gt;0\\exists N &gt; 0∃N&gt;0 当 n&gt;Nn&gt;Nn&gt;N 时，an&gt;0a_n&gt;0an​&gt;0 iflim⁡n→∞an=A&gt;0if \\lim_{n \\to \\infty} a_n = A &gt; 0iflimn→∞​an​=A&gt;0，则 ∃N&gt;0\\exists N &gt; 0∃N&gt;0 当 n&gt;Nn&gt;Nn&gt;N 时，an&lt;0a_n&lt;0an​&lt;0 证：取 ε=A2&gt;0\\varepsilon = \\frac{A}{2} &gt; 0ε=2A​&gt;0 函数极限 ∀ε&gt;0,∃δ&gt;0\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0∀ε&gt;0,∃δ&gt;0，当 0&lt;∣x−a∣&lt;δ0&lt;|x-a|&lt;\\delta0&lt;∣x−a∣&lt;δ 时，∣f(x)−A∣&lt;ε|f(x)-A|&lt;\\varepsilon∣f(x)−A∣&lt;ε。称 f(x)f(x)f(x) 当 x→ax \\to ax→a 时，以 AAA 为极限。 lim⁡n→af(x)=A/f(x)→A(x→a)\\lim_{n \\to a} f(x)= A \\quad / \\quad f(x) → A (x → a) n→alim​f(x)=A/f(x)→A(x→a) Case ∀ε&gt;0,∃X&gt;0,x&gt;X,∣f(x)−A∣&lt;ε→limx→∞+\\forall \\varepsilon&gt;0, \\exists X&gt;0, x&gt;X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty^+}∀ε&gt;0,∃X&gt;0,x&gt;X,∣f(x)−A∣&lt;ε→limx→∞+​ ∀ε&gt;0,∃X&lt;0,x&gt;−X,∣f(x)−A∣&lt;ε→limx→∞−\\forall \\varepsilon&gt;0, \\exists X&lt;0, x&gt;-X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty^-}∀ε&gt;0,∃X&lt;0,x&gt;−X,∣f(x)−A∣&lt;ε→limx→∞−​ ∀ε&gt;0,∃X&gt;0,∣x∣&gt;X,∣f(x)−A∣&lt;ε→limx→∞\\forall \\varepsilon&gt;0, \\exists X&gt;0, |x|&gt;X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty}∀ε&gt;0,∃X&gt;0,∣x∣&gt;X,∣f(x)−A∣&lt;ε→limx→∞​ Note x→ax \\to ax→a 包含 x→a−x \\to a^-x→a− 表示左邻域；x→a+x \\to a^+x→a+ 表示右邻域。 u˚(a,δ)\\mathring{u}(a, \\delta)u˚(a,δ) 表示 aaa 的去心 δ\\deltaδ 邻域。 limn→af(x)lim_{n \\to a} f(x)limn→a​f(x) 与 f(a)f(a)f(a) 无关。 函数存在分段时，需要分开讨论，即分为左极限 x→a−x \\to a^-x→a− 与右极限 x→a+x \\to a^+x→a+。 唯一性 局部有界性 无穷小与无穷大 无穷小 0 为无穷小，但无穷小不一定为 0。 无穷小余自变量趋向有关。（去心邻域） 运算法则 α→0,β→0⇒α±β→0\\alpha \\rightarrow 0, \\beta \\rightarrow 0 \\Rightarrow \\alpha \\pm \\beta \\rightarrow 0α→0,β→0⇒α±β→0 α→0,k×α→0\\alpha \\rightarrow 0, k \\times \\alpha \\rightarrow 0α→0,k×α→0 α→0,β→0⇒α×β→0\\alpha \\rightarrow 0, \\beta \\rightarrow 0 \\Rightarrow \\alpha \\times \\beta \\rightarrow 0α→0,β→0⇒α×β→0 lim⁡x→x0f(x)=A⇔f(x)=A+α,α→0(x→x0)\\lim_{x \\to x_0}f(x)=A \\Leftrightarrow f(x)=A+\\alpha, \\alpha \\rightarrow 0 (x \\rightarrow x_0)limx→x0​​f(x)=A⇔f(x)=A+α,α→0(x→x0​) 无穷大 ∀M&gt;0,∃δ&gt;0,0&lt;∣x−x0∣&lt;δ,∣f(x)∣&gt;M⇒limx→x0f(x)=∞\\forall M&gt;0, \\exists \\delta&gt;0, 0&lt;|x-x_0|&lt;\\delta , |f(x)|&gt;M \\Rightarrow lim_{x \\to x_0}f(x)=\\infty∀M&gt;0,∃δ&gt;0,0&lt;∣x−x0​∣&lt;δ,∣f(x)∣&gt;M⇒limx→x0​​f(x)=∞ 无穷小与无穷大互为倒数。 极限地运算法则 四则运算 limx→x0f(x)=A,limx→x0g(x)=Blim_{x \\to x_0}f(x) = A, lim_{x \\to x_0}g(x) = Blimx→x0​​f(x)=A,limx→x0​​g(x)=B limx→x0[f(x)±g(x)]=limx→x0f(x)±limx→x0g(x)=A±Blim_{x \\to x_0}[f(x) \\pm g(x)] = lim_{x \\to x_0}f(x) \\pm lim_{x \\to x_0}g(x) = A \\pm Blimx→x0​​[f(x)±g(x)]=limx→x0​​f(x)±limx→x0​​g(x)=A±B limx→x0kf(x)=klimx→x0f(x)lim_{x \\to x_0}kf(x) = klim_{x \\to x_0}f(x)limx→x0​​kf(x)=klimx→x0​​f(x) limx→x0f(x)g(x)=limx→x0f(x)limx→x0g(x)=ABlim_{x \\to x_0}f(x)g(x) = lim_{x \\to x_0}f(x)lim_{x \\to x_0}g(x) = ABlimx→x0​​f(x)g(x)=limx→x0​​f(x)limx→x0​​g(x)=AB limx→x0f(x)g(x)=limx→x0f(x)limx→x0g(x)=AB,limx→x0g(x)≠0lim_{x \\to x_0}\\frac{f(x)}{g(x)} = \\frac{lim_{x \\to x_0}f(x)}{lim_{x \\to x_0}g(x)} = \\frac{A}{B}, lim_{x \\to x_0}g(x) \\ne 0limx→x0​​g(x)f(x)​=limx→x0​​g(x)limx→x0​​f(x)​=BA​,limx→x0​​g(x)=0 Note P(x)P(x)P(x) 为最大次数为 nnn 的队列，Q(x)Q(x)Q(x) 为最大次数为 mmm 的队列 若 n&gt;m,limx→x0P(x)Q(x)=∞n&gt;m, lim_{x \\to x_0}\\frac{P(x)}{Q(x)} = \\inftyn&gt;m,limx→x0​​Q(x)P(x)​=∞ 若 n&lt;m,limx→x0P(x)Q(x)=0n&lt;m, lim_{x \\to x_0}\\frac{P(x)}{Q(x)} = 0n&lt;m,limx→x0​​Q(x)P(x)​=0 复合函数极限（套娃）。 极限存在准则 准则一：收敛定理 / 夹逼定理 f(x)≤g(x)≤h(x),limf(x)=limg(x)=A⇒limg(x)=Af(x) \\le g(x) \\le h(x), limf(x)=limg(x)=A \\Rightarrow limg(x)=Af(x)≤g(x)≤h(x),limf(x)=limg(x)=A⇒limg(x)=A 准则二：单调有界函数必有极限 重要极限 limx→0sinxx=1lim_{x \\to 0}\\frac{sinx}{x}=1 limx→0​xsinx​=1 limx→0(1+1n)n=elim_{x \\to 0}(1+\\frac{1}{n})^n=e limx→0​(1+n1​)n=e 无穷小的比较","categories":[{"name":"基础科学","slug":"基础科学","permalink":"https://wingowen.github.io/categories/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"常用命令","slug":"运维/常用命令","date":"2023-01-04T09:03:22.000Z","updated":"2023-01-04T09:04:26.430Z","comments":true,"path":"2023/01/04/运维/常用命令/","link":"","permalink":"https://wingowen.github.io/2023/01/04/%E8%BF%90%E7%BB%B4/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"BASH 脚本 12345678910111213141516# 条件判断if [ $1 = &quot;test&quot; ]; then UNION=else UNION=fi# 判断文件是否存在if [[ ! -f [file_path] ]]; then echo &#x27;文件不存在&#x27;fi# 判断文件夹是否存在if [[ ! -d [dir_path] ]]; then echo &#x27;文件夹不存在&#x27;fi find 123find . | grep -v volumns | grep -v &quot;\\.tar&quot; | cpio -pdm ../wefe-v4/ 用户与用户组 12345678910111213141516171819# 查看系统中所有用户# 用户名:用户密码:用户 ID:群组 ID:用户信息:家目录:shell 类型cat /etc/passwd# 查看用户组信息# 用户组名:用户组密码:用户组 ID:用户列表cat /etc/group# 新建用户useradd username# 设置密码passwd username# 将普通用户放入管理员组提升为管理员usermod -G wheel username# 限制 su - 命令只有 wheel 组成员可以运行vi /etc/pam.d/su Swap 12345678910111213141516171819# 查看系统 swap 大小free -g# 查看系统的挂载盘df -h# 创建 swap 文件夹fallocate -l 48G /data/swap# 创建 swap 区域mkswap -L swap /data/swapchmod 600 /data/swap# 挂载 swap 分区swapon /data/swap# 卸载 swap 分区swapoff /data/swap 环境变量 12# 登陆系统时 shell 读取的顺序/etc/profile -&gt;/etc/enviroment --&gt;$HOME/.profile --&gt;$HOME/.env 开机自启 12345# 开机自启脚本# /etc/rc.d/rc.local 文件会在 Linux 系统各项服务都启动完毕之后再被运行chmod +x /etc/rc.d/rc.localchmod +x auto_run_script.shvi /etc/rc.d/rc.local SSH 免密登陆过程 场景：A 机器通过 SSH 免密登陆到机器 B。 条件： A 生成了私钥、公钥； B 拥有 A 的公钥（在 authorized_keys 中）。 过程： A → B：发送连接请求，信息包括用户名以及 IP 等 B 在 authorized_keys 中查找相应用户名和 IP 对应的 公钥；若有符合的公钥，则生成一个随机字符串 R，并使用 A 的公钥对 R 进行加密生成 F®； B → A：发送 F® A 对 F® 进行解密得到 R； A → B：发送 R B 检查 R=R，则允许此次登陆。 1234ssh-keygen -t rsa -P &quot;&quot;# /etc/ssh/sshd_configPermitRootLogin yes","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"脚本命令","slug":"脚本命令","permalink":"https://wingowen.github.io/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/"}]},{"title":"网络相关","slug":"运维/网络相关","date":"2022-09-01T02:45:43.000Z","updated":"2023-01-04T08:53:33.149Z","comments":true,"path":"2022/09/01/运维/网络相关/","link":"","permalink":"https://wingowen.github.io/2022/09/01/%E8%BF%90%E7%BB%B4/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/","excerpt":"iptables / netfilter","text":"iptables / netfilter iptables iptables 是 Linux 防火墙的管理工具而已，位于 /sbin/iptables；真正实现防火墙功能的是 netfilter，它是 Linux 内核中实现包过滤的内部结构。 iptables 传输数据包的过程： 当一个数据包进入网卡时，它首先进入 PREROUTING 链，内核根据数据包目的 IP 判断是否需要转送出去。 如果数据包就是进入本机的，它就会沿着图向下移动，到达 INPUT 链。数据包到了 INPUT 链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过 OUTPUT 链，然后到达 POSTROUTING 链输出。 如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出。 iptables的规则表和链： 表（tables）提供特定的功能，iptables 内置了 4 个表 filter 表，包过滤； nat 表，网络地址转换； mangle 表，包重构、修改； raw 表，数据跟踪处理。 链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一 条或数条规则。当一个数据包到达一个链时，iptables 就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则 iptables 将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。 五个链为： PREROUTING：路由选择前； INPUT：路由选择后，进入到主机中； FORWARD：路由选择后，转发； OUTPUT：路由选择后（判断用哪张网卡发出包）,流出； POSTROUTING：最后的数据流出。 常用命令： 123456789101112131415161718192021222324252627282930313233343536373839# 查看规则iptables -t 表名 -Liptables -t nat --line -nvL PREROUTING # --line 显示规则的行号# -n 不解析IP# -v 显示详细内容# 添加规则iptables -t filter -A INPUT -s 192.168.1.146 -j DROPiptables -t filter -I INPUT -s 192.168.1.146 -j ACCEPT# 指定位置iptables -t filter -I INPUT 5 -s 192.168.1.146 -j REJECT # 设置指定表的指定链的默认策略（默认动作），并非添加规则。iptables -t filter -P FORWARD ACCEPT# -I 插入到第一行# -A 插入到最后# 删除规则iptables -t filter -D INPUT 3iptables -t filter -D INPUT -s 192.168.1.146 -j DROPiptables -t filter -F INPUTiptables -t filter -F# -F 清空# 删除自定义链iptables -X WEB# 修改规则iptables -t filter -R INPUT 3 -s 192.168.1.146 -j ACCEPT# 清除包的计数iptabls -t nat -Z PREROUTING# 清除nat表所有链的计数iptabls -t nat -Z# 保存service iptables saveiptables-save &gt; /etc/sysconfig/iptables","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"网络相关","slug":"网络相关","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"}]},{"title":"服务安装","slug":"运维/服务安装","date":"2022-08-31T02:41:25.000Z","updated":"2023-09-20T06:27:26.756Z","comments":true,"path":"2022/08/31/运维/服务安装/","link":"","permalink":"https://wingowen.github.io/2022/08/31/%E8%BF%90%E7%BB%B4/%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/","excerpt":"","text":"JDK12345678wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27;yum install jdk-8u202-linux-x64.rpm -ycat &gt;&gt; ~/.bash_profile &lt;&lt; EOFJAVA_HOME=/usr/java/jdk1.8.0_202-amd64/EOFsource ~/.bash_profile MySQL123456789101112131415161718192021222324252627282930313233343536# mysql 服务下载启动# 方式一：若速度过慢采用离线下载方式yum install -y &lt;http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm&gt;yum install mysql-community-server.x86_64 -y --nogpgcheck# 方式二：离线下载mkdir /opt/mysqlcd /opt/mysqlwget &lt;http://mirrors.ustc.edu.cn/mysql-ftp/Downloads/MySQL-5.7/mysql-5.7.38-1.el7.x86_64.rpm-bundle.tar&gt;tar -xvf mysql-5.7.38-1.el7.x86_64.rpm-bundle.tarrm -rf mysql-5.7.38-1.el7.x86_64.rpm-bundle.taryum install createrepo -ycreaterepo ./cat &gt;&gt; /etc/yum.repos.d/mysql.repo &lt;&lt; EOF [mysql]name=mysqlbaseurl=file:///opt/mysql/gpgcheck=0enabled=1 EOFyum install mysql-server -y# 方式二结束systemctl start mysqldsystemctl enable mysqld# mysql 服务配置# 获取初始密码进行登陆并修改密码cat /var/log/mysqld.log | grep password# set global validate_password_policy=0;SET PASSWORD = PASSWORD(&#x27;12341234&#x27;);grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;12341234&quot;;flush privileges; 123456789# 获取初始密码cat /var/log/mysqld.log | grep password# 登陆控制台mysql -p# 设置密码及权限&gt; set global validate_password_policy=0;&gt; SET PASSWORD = PASSWORD(&#x27;wefe2022&#x27;);&gt; grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;wefe2022&quot;;&gt; flush privileges; YUMYUM 源替换。 123456yum install wget -ycd /etc/yum.repos.dwget -O /etc/yum.repos.d/CentOS-Base.repo &lt;http://mirrors.aliyun.com/repo/Centos-7.repo&gt;wget -O /etc/yum.repos.d/epel.repo &lt;http://mirrors.aliyun.com/repo/epel-7.repo&gt;yum clean allyum makecache Nacos12345678910111213141516mkdir /data/nacos &amp;&amp; cd /data/nacos# 检查依赖yum install -y whichjava -version# nacos 官网下载wget &lt;https://github.com/alibaba/nacos/releases/download/2.0.3/nacos-server-2.0.3.zip&gt;# 备用下载链接wget &lt;https://welab-wefe-release.oss-cn-shenzhen.aliyuncs.com/IAM/RES/nacos-server-2.0.3.zip&gt;yum install unzip -yunzip nacos-server-2.0.3.zipcd nacos/bin# 启动 nacos 单机版sh startup.sh -m standalone# 服务访问地址：ip:8848/nacos# 默认账号密码：nacos / nacos Nginx12rpm -Uvh &lt;http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm&gt;yum install nginx -y Redis123456yum install redis -ysystemctl start redis# 测试 redis 服务redis-cli ping# PONG K8S 部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 关闭防火墙、Swap、SELinuxsystemctl stop firewalldsystemctl disable firewalldswapoff -asetenforce 0cat /etc/selinux/config# SELINUX=disabled# 安装系统依赖以及 dockeryum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum list docker-ce --showduplicates | sort -ryum install docker-ce.x86_64 3:18.09.9 -ysystemctl start dockersystemctl enable docker# 需保证 docker 和 kubelet 的的 cgroupdriver 是相同的cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;graph&quot;: &quot;/data/docker-compose&quot;, &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOFsystemctl restart docker# 安装 kubelet、kubeadm、kubectlcat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetes Repositorybaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOFyum install -y kubelet-1.23.0-0.x86_64 kubeadm-1.23.0-0.x86_64 kubectl-1.23.0-0.x86_64 --disableexcludes=kubernetes # Kubernetes 集群网络有很多种实现，有很大一部分都用到了 Linux 网桥# 由于网桥是虚拟的二层设备，同节点的 Pod 之间通信直接走二层转发，跨节点通信才会经过宿主机 eth0# 创建/etc/sysctl.d/k8s.conf文件，添加如下内容。表示 bridge 设备在二层转发时也去调用 iptables 配置的三层规则 (包含 conntrack)cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF# 执行如下命令使修改生效modprobe br_netfiltersysctl -p /etc/sysctl.d/k8s.confsystemctl start kubeletsystemctl enable kubelet# Master 机器操作# 修改本机 IP、镜像拉取地址 registry.aliyuncs.com/google_containerskubeadm config print init-defaults &gt; init-config.yamlkubeadm config images pull --config=init-config.yamlkubeadm init --config=init-config.yamlcd ~mkdir .kubecp /etc/kubernetes/admin.conf config# Worker 按提示操作kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers[kubeadm join ...]# 若集群出问题则重置kubeadm reset# 安装网络插件 flannelkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlcat &gt; /run/flannel/subnet.env &lt;&lt; EOFFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOF# 注意！！！这里踩坑了，卡了好几天# flannel 启动后，会生成 cni.0 与 flannel.1，这两网卡需要在同一网段cni0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 10.244.0.1 netmask 255.255.255.0 broadcast 10.244.0.255 inet6 fe80::34b6:76ff:fe2c:7f58 prefixlen 64 scopeid 0x20&lt;link&gt; ether 36:b6:76:2c:7f:58 txqueuelen 1000 (Ethernet) RX packets 107164 bytes 8642460 (8.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 105234 bytes 9770864 (9.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.244.0.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::203e:21ff:fe5f:575b prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:3e:21:5f:57:5b txqueuelen 0 (Ethernet) RX packets 196206 bytes 60155180 (57.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 233768 bytes 30816014 (29.3 MiB) TX errors 0 dropped 8 overruns 0 carrier 0 collisions 0 Spark on Yarn 集群部署假设三台机器的 hostname 以及 ip 如下： 1234# hostname ipnode0 10.11.0.2node1 10.11.0.3node2 10.11.0.4 以下操作如无特殊说明默认在 node0 机器上操作。 SSH 免密登陆配置 12345678910111213yum install ssh -y# 在三台机器分别生成钥匙，提示输入回车即可ssh-keygen -t rsa -P &quot;&quot;# 在三台机器分别添加 hosts 解析echo &quot;10.11.0.2 node010.11.0.3 node110.11.0.4 node2&quot; &gt;&gt; /etc/hosts;# 在 node0 上执行以下代码ssh-copy-id -i ~/.ssh/id_rsa.pub root@node0ssh-copy-id -i ~/.ssh/id_rsa.pub root@node1ssh-copy-id -i ~/.ssh/id_rsa.pub root@node2 JDK 安装 12345678910# 在 node0 节点上操作wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27; yum install jdk-8u202-linux-x64.rpm -y echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_202-amd64/PATH=\\$PATH:\\$JAVA_HOME/bin&quot; &gt; /etc/profile.d/jdk.shsource /etc/profile.d/jdk.shscp /etc/profile.d/jdk.sh node1:/etc/profile.d/jdk.shscp /etc/profile.d/jdk.sh node2:/etc/profile.d/jdk.sh 资源下载 12345678910mkdir -p /data/big-data-app/rescd /data/big-data-app/reswget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgzwget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gztar -xvf spark-3.1.3-bin-hadoop2.7.tgzmv spark-3.1.3-bin-hadoop2.7 ../spark3tar -xvf hadoop-2.7.7.tar.gzmv hadoop-2.7.7.tar.gz ../hadoop2 HDFS 1234567891011121314151617181920212223242526# 增量同步工具yum install -y ssh rsync# hadoop envecho &quot;export HADOOP_HOME=/data/big-data-app/hadoop2export HADOOP_INSTALL=\\$HADOOP_HOMEexport HADOOP_MAPRED_HOME=\\$HADOOP_HOMEexport HADOOP_HDFS_HOME=\\$HADOOP_HOMEexport HADOOP_COMMON_HOME=\\$HADOOP_HOMEexport HADOOP_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport YARN_HOME=\\$HADOOP_HOMEexport YARN_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport PATH=\\$PATH:\\$HADOOP_HOME/sbin:\\$HADOOP_HOME/bin&quot; &gt; /etc/profile.d/hadoop.shsource /etc/profile.d/hadoop.shscp /etc/profile.d/hadoop.sh node1:/etc/profile.d/hadoop.shscp /etc/profile.d/hadoop.sh node2:/etc/profile.d/hadoop.sh# spark envecho &quot;SPARK_HOME=/data/big-data-app/spark3PATH=\\$SPARK_HOME/bin:\\$PATH&quot; &gt; /etc/profile.d/spark.shsource /etc/profile.d/spark.shscp /etc/profile.d/spark.sh node1:/etc/profile.d/spark.sh************scp************ /etc/profile.d/spark.sh node2:/etc/profile.d/spark.sh 修改配置，配置文件在 $HADOOP_CONF_DIR 目录下。 capacity-scheduler.xml 12345&lt;!-- DefaultResourceCalculator only uses Memory --&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;&lt;/property&gt; core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node0:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node0:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;node0:50091&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 启动动态资源需依赖 jar 包资源，操作如下 12# 在每个节点上进行操作cp $SPARK_HOME/yarn/spark-3.1.3-yarn-shuffle.jar $HADOOP_HOME/share/hadoop/yarn/lib/ 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 根据节点内存设置节点分配给 YARN 的内存，为 1024 的倍数--&gt; &lt;!-- 32G 服务器，则可分配 32*0.75*1024=24576，一般分配比例为 0.75~0.85 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;32768&lt;/value&gt; &lt;/property&gt; &lt;!-- 每个 Container 能申请的资源最大值 --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;16384&lt;/value&gt; &lt;/property&gt; &lt;!-- 动态资源相关配置 START --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt; &lt;/property&gt; &lt;!-- 动态资源相关配置 END --&gt;&lt;/configuration&gt; slaves 12node1node2 Spark 配置 1234567891011121314151617181920echo &quot;node1node2&quot; &gt; $SPARK_HOME/conf/workersecho &quot;spark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs:///logsspark.yarn.historyServer.address node0:18080&quot; &gt;&gt; $SPARK_HOME/conf/spark-default.conf# 创建日志存储目录./bin/hdfs dfs -mkdir /logs# 声明 spark 日志服务启动参数export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 \\-Dspark.history.retainedApplications=3 \\-Dspark.history.fs.logDirectory=hdfs://node0:9000/logs&quot;# 启动日志服务cd $SPARK_HOME/sbinsh start-history-server.sh 分发大数据资源 12rsync -arv /data/big-data-app node1:/data/big-data-apprsync -arv /data/big-data-app node2:/data/big-data-app 格式化 NameNode 12cd $HADOOP_HOME/bin./hdfs namenode -format 启动服务并检查 1234567891011cd $HADOOP_HOME/binstart-all.sh# 到各节点检查进程jps# node0 NameNode SecondaryNode ResourceManager# node1 DataNode NodeManager# ndoe2 DataNode NodeManager# HDFS URL http://node0:50070# YARN URL http://node0:8088 提交集群任务测试 12345678cd $SPARK_HOME./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\\--master yarn \\\\--deploy-mode cluster \\\\--driver-memory 4g \\\\--executor-memory 2g \\\\--executor-cores 1 \\\\examples/jars/spark-examples*.jar \\\\ Hive 部署基于 Docker 部署 HiveHive Docker 部署资源 基于 Dockerfile 生成相应镜像1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354FROM ubuntu:14.04ADD sources.list /etc/apt/sources.listADD apache-hive-2.3.9-bin.tar.gz /ADD mysql-connector-java-5.1.38.jar /ADD hadoop-2.7.2.tar.gz /COPY config/* /tmp/WORKDIR /root# install openssh-server, openjdk and wget,install hadoop 2.7.2RUN apt-get update &amp;&amp; \\ apt-get install -y --reinstall software-properties-common &amp;&amp; \\ add-apt-repository -y ppa:openjdk-r/ppa &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y openssh-server openjdk-8-jdk &amp;&amp; \\ apt-get clean all &amp;&amp; \\ mv /hadoop-2.7.2 /usr/local/hadoop &amp;&amp; \\ mv /apache-hive-2.3.9-bin /usr/local/hive &amp;&amp; \\ cp /mysql-connector-java-5.1.38.jar /usr/local/hive/lib/ &amp;&amp; \\ apt-get -y --purge remove software-properties-common# set environment variableENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64ENV HADOOP_HOME=/usr/local/hadoopENV HIVE_HOME=/usr/local/hiveENV PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin # ssh without key and hadoop configRUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &#x27;&#x27; &amp;&amp; \\ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; \\ mkdir -p ~/hdfs/namenode &amp;&amp; \\ mkdir -p ~/hdfs/datanode &amp;&amp; \\ mkdir $HADOOP_HOME/logs &amp;&amp; \\ mv /tmp/ssh_config ~/.ssh/config &amp;&amp; \\ mv /tmp/hadoop-env.sh /usr/local/hadoop/etc/hadoop/hadoop-env.sh &amp;&amp; \\ mv /tmp/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml &amp;&amp; \\ mv /tmp/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml &amp;&amp; \\ mv /tmp/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml &amp;&amp; \\ mv /tmp/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml &amp;&amp; \\ mv /tmp/slaves $HADOOP_HOME/etc/hadoop/slaves &amp;&amp; \\ mv /tmp/start-hadoop.sh ~/start-hadoop.sh &amp;&amp; \\ mv /tmp/run-wordcount.sh ~/run-wordcount.sh &amp;&amp; \\ mv /tmp/hive-site.xml /usr/local/hive/conf/ &amp;&amp; \\ chmod +x ~/start-hadoop.sh &amp;&amp; \\ chmod +x ~/run-wordcount.sh &amp;&amp; \\ chmod +x $HADOOP_HOME/sbin/start-dfs.sh &amp;&amp; \\ chmod +x $HADOOP_HOME/sbin/start-yarn.sh &amp;&amp; \\ /usr/local/hadoop/bin/hdfs namenode -format# format namenode#RUN /usr/local/hadoop/bin/hdfs namenode -formatCMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;service ssh start; bash&quot;]","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"网站收集","slug":"网站收集","date":"2022-08-29T08:57:30.000Z","updated":"2023-09-20T05:34:00.167Z","comments":true,"path":"2022/08/29/网站收集/","link":"","permalink":"https://wingowen.github.io/2022/08/29/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/","excerpt":"收集一些有用的网站。","text":"收集一些有用的网站。 杂项 gitee update Hexo 博客主题 pure 使用说明 | Cofess - Web Developer &amp; Designer JAVA Spring All 刷题 HiveSQL 50 Pandas 120","categories":[{"name":"日常","slug":"日常","permalink":"https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"}],"tags":[{"name":"网站收集","slug":"网站收集","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"}]},{"title":"计算机科学","slug":"计算机科学/计算机科学","date":"2022-08-15T09:16:17.000Z","updated":"2023-09-20T06:44:20.191Z","comments":true,"path":"2022/08/15/计算机科学/计算机科学/","link":"","permalink":"https://wingowen.github.io/2022/08/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/","excerpt":"协程。","text":"协程。 协程操作系统在线程等待 IO 的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待 IO 的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。 协程刚好可以解决上述 2 个问题。协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。 协程只有在等待 IO 的过程中才能重复利用线程。 假设协程运行在线程之上，并且协程调用了一个阻塞 IO 操作，这时候会发生什么？实际上操作系统并不知道协程的存在，它只知道线程，因此在协程调用阻塞 IO 操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥其作用。","categories":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[]},{"title":"降维","slug":"算法/降维","date":"2022-08-14T08:48:21.000Z","updated":"2022-08-29T09:04:45.843Z","comments":true,"path":"2022/08/14/算法/降维/","link":"","permalink":"https://wingowen.github.io/2022/08/14/%E7%AE%97%E6%B3%95/%E9%99%8D%E7%BB%B4/","excerpt":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效 信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。","text":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效 信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。 低维嵌入介绍 在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个低维嵌入 embedding。 缓解维数灾难的一个重要途径是降维 dimension reduction。它是通过某种数学变换将原始高纬度属性空间转变为一个低维子空间，在这个子空间中样本密度大幅提高，计算距离也变得更为容易。低维嵌入的目的是解决 k 邻近学习方法可操作性弱的问题。 将一个三维问题垂直投影，变成一个二维问题。 这种方法叫做多维缩放 Multiple Dimensional Scaling，简称 MDS，这是一种经典的降维方法。 MDS 12345678910111213141516# 导入包import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasets,manifoldfrom collections import Counterdef load_data(): # 使用 scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() return iris.data,iris.target# 产生用于降维的数据集X, y=load_data()print(X.shape)print(Counter(y))","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"特征选择","slug":"算法/特征选择","date":"2022-08-13T14:41:12.000Z","updated":"2022-08-29T09:05:47.711Z","comments":true,"path":"2022/08/13/算法/特征选择/","link":"","permalink":"https://wingowen.github.io/2022/08/13/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。","text":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。 过滤式选择 先对数据集进行特征选择，然后再训练分类器，特征选择过程与后续训练无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。 Relief 选择法 Relief Relevant Features，该方法设计了一个相关统计量来度量特征的重要性，并且其是一个向量，其每个分量分别对应一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。最终只需要确定一个阈值 r，然后选择比 r 大的相关统计量分量所对应的特征即可。也可以指定选取相关统计量分量最大的前 k 个特征。 FInsher 选择法 计算数据集中每个类别样本的类内特征方差与类间特征方差。 类内特征方差越小，类间特征方差越大，越有利于后续的分类训练，即该特征需要保留，反之该特征需要滤除。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#导入包import pandas as pdimport numpy as np#创建示例样本sample = [[1,1,5],[10,0,1],[2,5,6]]label = [1, 0, 1]print(&quot;sample:&quot;,sample)print(&quot;label:&quot;,label)#判断样本长度与类标长度是否匹配if len(sample) != len(label): print(&#x27;Sample does not match label&#x27;) exit()#创建并保存计算过程中的变量df1 = pd.DataFrame(sample)df2 = pd.DataFrame(label, columns=[&#x27;label&#x27;])data = pd.concat([df1, df2], axis=1) # 合并成为一个dataframeprint(&quot;data:\\n&quot;,data,&#x27;\\n&#x27;)data0 = data[data.label == 0]#对标签分类，分成包含0和1的两个dataframedata1 = data[data.label == 1]n = len(label)#标签长度n1 = sum(label)#1类标签的个数n0 = n - n1#0类标签的个数lst = []#用于返回的列表features_list = list(data.columns)[:-1]print(&quot;特征维数:&quot;)print(features_list)#fisher score计算for feature in features_list: print(&#x27;\\nfeature&#x27;,feature,&#x27;:&#x27;) # 算关于类标0 m0_feature_mean = data0[feature].mean() # 0 类标签在第 m 维上的均值 print(&quot;m0_feature_mean&quot;,m0_feature_mean) m0_SW=sum((data0[feature] -m0_feature_mean )**2) # 0类在第 m 维上的类内方差 print(&quot;m0_SW&quot;,m0_SW) # 算关于类标1 m1_feature_mean = data1[feature].mean() # 1 类标签在第 m 维上的均值 print(&quot;m1_feature_mean&quot;,m1_feature_mean) m1_SW=sum((data1[feature] -m1_feature_mean )**2)# 1 类标签在第 m 维上的类内方差 print(&quot;m1_SW&quot;,m1_SW) # 算关于 data m_all_feature_mean = data[feature].mean() # 所有类标签在第 m 维上的均值 print(&quot;m_all_feature_mean&quot;,m_all_feature_mean) m0_SB = n0 / n * (m0_feature_mean - m_all_feature_mean) ** 2 # 0 类标签在第 m 维上的类间方差 print(&quot;m0_SB&quot;,m0_SB) m1_SB = n1 / n * (m1_feature_mean - m_all_feature_mean) ** 2 # 1 类标签在第 m 维上的类间方差 print(&quot;m1_SB&quot;,m1_SB) m_SB = m1_SB + m0_SB # 计算SB print(&quot;m_SB&quot;,m_SB) m_SW = (m0_SW + m1_SW) / n # 计算 SW print(&quot;m_SW&quot;,m_SW) if m_SW == 0: # 0/0类型也是返回nan m_fisher_score = np.nan else: # 计算Fisher score m_fisher_score = m_SB / m_SW #计算第m维特征的Fisher score #Fisher score值添加进列表 print(&quot;m_fisher_score&quot;,m_fisher_score) lst.append(m_fisher_score) 包裹式选择 包裹式特征选择直接将最终要使用的学习器的性能作为特征子集的评价准则，包裹式特征选择的目的就是为给定的学习器选择最有利于其性能而量身定做的特征子集。 一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，比过滤式特征选择更好，但由于在特征选择过程中要多次训练学习器，因此包裹式特征选择的计算开销比过滤式特征选择大得多。 LVW 是一个经典的包裹式特征选择方法，它在拉斯维加斯方法框架下使用随机策略进行子集搜索，以最终分类器误差作为特征子集评价标准。 除了 LVW 包裹式特征选择之外，RFE(递归特征消除)也是一种常见的包裹式特征选择方法，RFE特征选择使用模型准确率来判断哪些特征（或特征组合）对预测结果贡献较大，并且递归地去除贡献小的特征。 除了 RFE 之外，还有一种选择算法称为 RFECV，其是以 RFE 为基础进行改进得到的。 RFECV 通过交叉验证的方式执行 RFE，以此来选择最佳数量的特征，即不手动设置保留的特征数量。对于一个数量为 d 的 feature 的集合，他的所有的子集的个数是 2 的 d 次方减 1 (包含空集)。指定一个外部的学习算法，比如 SVM 之类。通过该算法计算所有子集的validation error。选择 error 最小的那个子集作为所挑选的特征。 123456789101112131415161718192021222324252627282930313233343536373839404142#导入包from sklearn.feature_selection import RFE,RFECVfrom sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn import model_selection&#x27;&#x27;&#x27;class RFE(BaseEstimator, MetyuanaEstimatorMixin, SelectorMixin): 参数： BaseEstimator: 基模型 n_features_to_select：目标特征数量 return：经过选择后的特征 比较经过特征选择和未经特征选择的数据集，对 LinearSVC 的预测性能的区别&#x27;&#x27;&#x27;### 加载数据iris = load_iris()X, y = iris.data, iris.target### 特征提取estimator = LinearSVC()selector = RFE(estimator=estimator, n_features_to_select=2)X_t = selector.fit_transform(X, y) #对样本进行特征选择，最终保留n_features_to_select个特征。print(&quot;\\n特征选择结果显示:&quot;)print(&quot;原数据样本X：&quot;,X[1])print(&quot;特征选择后样本X_t：&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0, stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))clf_t.fit(X_train_t, y_train_t)print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t))) 嵌入式选择 嵌入式特征选择是将特征选择过程与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。 正则化嵌入式选择 L1L1L1 范数与 L2L2L2 范数都有利于降低过拟合，但前者还会带来一个额外的好处，即 L1L1L1 范数比 L2L2L2 范数更容易获得稀疏解，即它求得的 www 会有更少的非零分量。 其中，基于 L1L1L1 正则化的学习方法是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。 123456789101112131415161718192021222324252627282930313233#导入包from sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectFromModelfrom sklearn import model_selection#导入数据集并打印示例iris = load_iris()X, y = iris.data, iris.targetprint(&quot;原始数据特征维数：&quot;,len(X[1])) # (150, 4)print(&quot;原始数据样本：&quot;,X[1])#特征选择lsvc = LinearSVC(C=0.01, penalty=&quot;l1&quot;, dual=False).fit(X, y) #设置分类器model = SelectFromModel(lsvc, prefit=True) #设置模型为特征选择X_t = model.transform(X) #获取经过筛选的数据print(&quot;特征选择数据特征维数&quot;,len(X_t[1])) #(150, 3)print(&quot;特征选择后数据样本&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0,stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)clf_t.fit(X_train_t, y_train_t)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t)))","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性回归与逻辑回归","slug":"算法/线性回归与逻辑回归","date":"2022-08-12T02:35:16.000Z","updated":"2022-09-09T11:25:52.979Z","comments":true,"path":"2022/08/12/算法/线性回归与逻辑回归/","link":"","permalink":"https://wingowen.github.io/2022/08/12/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等","text":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等 线性回归 线性回归分析 Linear Regression Analysis 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。 所以线性的定义为：自变量之间只存在线性关系，即自变量只能通过相加、或者相减进行组合。 监督学习 如果现在有一个房子 H1，面积是 S，监督学习如何估算它的价格？ 监督学习从训练集中找到面积最接近 S 的房子 H2，预测 H1 的价格等于 H2 的价格。 监督学习根据训练集，找到一个数学表达式，对任意的面积的房子都可以估算出其价格。 h 代表假设函数：Training Set → Learning Algorithm → h；Size of House + h → Estimated Price。 线性回归的假设模型 hθ(x)=θ0+θ1xh_{\\theta}(x)=\\theta_{0}+\\theta_{1} x hθ​(x)=θ0​+θ1​x 如何求解模型，有以下两种思路。 尝试一些 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 的组合，选择能使得画出的直线正好穿过训练集的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 。 尝试一些 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 的组合，然后在训练集上进行预测，选能使得预测值与真实的房子价格最接近的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 。 选择最佳的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​，使得 hθ(x)h_{\\theta}(x)hθ​(x) 对所有的训练样本 (x,y)(x, y)(x,y) 来说，尽可能的接近 yyy。 损失函数 Train Set: {(x(1),y(1)),(x(2),y(2)),⋯ ,(x(m),y(m))}\\left\\{\\left(x^{(1)}, y^{(1)}\\right),\\left(x^{(2)}, y^{(2)}\\right), \\cdots,\\left(x^{(m)}, y^{(m)}\\right)\\right\\}{(x(1),y(1)),(x(2),y(2)),⋯,(x(m),y(m))} 12m∑i=1m(hθ(x(i))−y(i))2\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2} 2m1​i=1∑m​(hθ​(x(i))−y(i))2 最小化损失函数，得到的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 是最佳的。 12345678910# 房屋的价格和面积数据import numpy as npdata = np.array([[2104, 460], [1416, 232], [1534, 315], [852,178]])# 使用线性回归模型计算预测值def get_predict(x, theta0, theta1): h = theta0 + theta1 * x #todo return h 梯度下降算法 梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合 (θ0,θ1,......,θn)(\\theta_{0},\\theta_{1},......,\\theta_{n})(θ0​,θ1​,......,θn​) 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合， 可能会找到不同的局部最小值。 实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，需要同时更新 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​，实现方法是：你应该计算公式右边的部分，通过那一部分计算出 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​的值，然后同时更新 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​。 temp0 :=θ0−α∂∂θ0J(θ0,θ1) temp1 :=θ1−α∂∂θ1J(θ0,θ1)θ0:= temp0 θ1:= temp1 \\text { temp0 }:=\\theta_{0}-\\alpha \\frac{\\partial}{\\partial \\theta_{0}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\text { temp1 }:=\\theta_{1}-\\alpha \\frac{\\partial}{\\partial \\theta_{1}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\theta_{0}:=\\text { temp0 } \\\\ \\theta_{1}:=\\text { temp1 } temp0 :=θ0​−α∂θ0​∂​J(θ0​,θ1​) temp1 :=θ1​−α∂θ1​∂​J(θ0​,θ1​)θ0​:= temp0 θ1​:= temp1 逻辑回归 二分类问题下，采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。它适用于标签 y 取值离散的情况。 逻辑函数 Logistic Function，一个最常用的逻辑函数是 Sigmoid Function，以 Z=0 为决策界限，公式如下： g(z)=11+e−zg(z)=\\frac{1}{1+e^{-z}} g(z)=1+e−z1​ 123import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 逻辑回归模型假设 hθ(x)=g(θTX)h_\\theta(x)=g(\\theta^TX) hθ​(x)=g(θTX) hθ(x)h_\\theta(x)hθ​(x) 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性 （estimated probablity），即 $ h_\\theta(x) = P(y=1|x;\\theta)$。 例如，如果对于给定的 x，通过已经确定的参数计算得出 hθ(x)h_\\theta(x)hθ​(x)=0.7，则表示有 70% 的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7 = 0.3。 损失函数 线性回归模型的代价函数是所有模型误差的平方和，若逻辑回归的假设模型沿用这个定义，得到的函数将是一个非凸函数 non-convex function。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 J(θ)=1m∑i=1mCost(hθ(x(i)),y(i))J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)}) J(θ)=m1​i=1∑m​Cost(hθ​(x(i)),y(i)) Cost⁡(hθ(x),y)={−log⁡(hθ(x)) if y=1−log⁡(1−hθ(x)) if y=0\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{aligned} -\\log \\left(h_{\\theta}(x)\\right) &amp; \\text { if } y=1 \\\\ -\\log \\left(1-h_{\\theta}(x)\\right) &amp; \\text { if } y=0 \\end{aligned}\\right. Cost(hθ​(x),y)={−log(hθ​(x))−log(1−hθ​(x))​ if y=1 if y=0​ Cost(hθ(x),y)=−y×log(hθ(x))−(1−y)×log(1−hθ(x))Cost(h_\\theta(x), y)=-y\\times{log(h_\\theta(x))}-(1-y)\\times{log(1-h_\\theta(x))} Cost(hθ​(x),y)=−y×log(hθ​(x))−(1−y)×log(1−hθ​(x)) J(θ)=−1m∑i=1m[y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i)))]J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))] J(θ)=−m1​i=1∑m​[y(i)log(hθ​(x(i)))+(1−y(i))log(1−hθ​(x(i)))] 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) return np.sum(first - second) / (len(X)) 当实际的 y=1 且 hθ(x)h_\\theta(x)hθ​(x) 也为1 时误差为 0，当 y=1 但 hθ(x)h_\\theta(x)hθ​(x) 不为 1 时误差随着 hθ(x)h_\\theta(x)hθ​(x) 的变小而变大； 当实际的 y=0 且 hθ(x)h_\\theta(x)hθ​(x) 也为 0 时代价为 0，当 y=0 但 hθ(x)h_\\theta(x)hθ​(x) 不为 0 时误差随着 hθ(x)h_\\theta(x)hθ​(x) 的变大而变大。 同样使用梯度下降法对参数进行更新： θj:=θj−α1m∑i=1m(hθ(x(i))−y(i))xj(i)\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j θj​:=θj​−αm1​i=1∑m​(hθ​(x(i))−y(i))xj(i)​ 123456789import numpy as np# 返回某一轮训练中的梯度def _gradient(X, Y_label, theta): # _f用来计算 y 的值 y_pred = _f(X, theta, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) return w_grad 多元分类 我们将多个类中的一个类标记为正向类 y=1，然后将其他所有类都标记为负向类，这个模型记作 h(1)θ(x)h^{(1)_\\theta(x)}h(1)θ​(x)。接着，类似地第我们选择另一个类标记为正向类 y=2，再将其它类都标记为负向类，将这个模型记作 h(2)θ(x)h^{(2)_\\theta(x)}h(2)θ​(x)，依此类推。 最后我们得到一系列的模型简记为： h(i)θ(x)=p(y=i∣x;θ)h^{(i)_\\theta(x)} = p(y=i|x;\\theta) h(i)θ​(x)=p(y=i∣x;θ) 最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：h(i)θ(x)h^{(i)_\\theta(x)}h(i)θ​(x)， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 h(i)θ(x)h^{(i)_\\theta(x)}h(i)θ​(x) 最大的 i，即 max⁡ih(i)θ(x)\\max_ih^{(i)_\\theta(x)}maxi​h(i)θ​(x)。 过拟合化和正则化 过拟合在训练数据上的表现非常好；对于非训练的数据点，过拟合的模型表现与我们的期望有较大的偏。 减少拟合化的方法： 减少选取变量的数量：选取最重要的参数； 正则化：一种减小参数大小的办法。 正则化 回归：岭回归。 分类：L1 正则化，L2 正则化。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Python 编程","slug":"Python/Python编程","date":"2022-08-01T03:17:02.000Z","updated":"2023-01-04T09:00:47.662Z","comments":true,"path":"2022/08/01/Python/Python编程/","link":"","permalink":"https://wingowen.github.io/2022/08/01/Python/Python%E7%BC%96%E7%A8%8B/","excerpt":"Python 编程开发查漏补缺。 gRPC Redis","text":"Python 编程开发查漏补缺。 gRPC Redis GRPCGoogle 开发的基于 HTTP/2 和 Protocol Buffer 3 的 RPC 框架。 Protocol Buffers, protobuf：结构数据序列化机制。 gRPC 默认使用 Brotocol Buffers，用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。 定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 GRPC 服务器来处理客户端调用。在客户端拥有一个存根 Stub，存根负责接收本地方法调用，并将它们委派给各自的具体实现对象（在远程服务器上）。 简单实现实现一个简单的 gRPC HelloWorld。 proto file定义 Protocol Buffers 规则文件。 123456789101112131415161718syntax = &quot;proto3&quot;;package helloworld;service Greeter &#123; // 定义方法参数和返回类型 rpc SayHello (HelloRequest) returns (HelloResponse) &#123;&#125;&#125;// 请求结构声明message HelloRequest &#123; string name = 1;&#125;// 响应结构声明message HelloResponse &#123; string message = 1;&#125; 运行 grpc_tools 工具。 1python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto 生成 python 代码。 helloworld_pb2.py 为 Protocol Buffers 的 Python 实现。 1234567891011121314151617181920# helloworld_pb2_grpc.py 用于 gRPC 实现的 Python 方法实现# 客户端存根class GreeterStub(object): def __init__(self, channel): self.SayHello = channel.unary_unary( &#x27;/helloworld.Greeter/SayHello&#x27;, request_serializer=helloworld__pb2.HelloRequest.SerializeToString, response_deserializer=helloworld__pb2.HelloResponse.FromString, )# 服务端服务class GreeterServicer(object): def SayHello(self, request, context): context.set_code(grpc.StatusCode.UNIMPLEMENTED) context.set_details(&#x27;Method not implemented!&#x27;) raise NotImplementedError(&#x27;Method not implemented!&#x27;) def add_GreeterServicer_to_server(servicer, server): # ...... server自定义 gRPC 服务端。 1234567891011121314151617181920212223import grpcimport randomfrom concurrent import futuresimport helloworld_pb2import helloworld_pb2_grpc# 实现定义的方法，继承并实现方法class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloResponse(message=&#x27;Hello &#123;msg&#125;&#x27;.format(msg=request.name))def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) # 绑定处理器 helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) # 未使用 SSL，所以是不安全的 server.add_insecure_port(&#x27;[::]:50054&#x27;) server.start() print(&#x27;gRPC 服务端已开启，端口为 50054...&#x27;) server.wait_for_termination()if __name__ == &#x27;__main__&#x27;: serve() client自定义客户端。 1234567891011121314import grpcimport helloworld_pb2, helloworld_pb2_grpcdef run(): # 本次不使用 SSL，所以 channel 是不安全的 channel = grpc.insecure_channel(&#x27;localhost:50054&#x27;) # 客户端实例 stub = helloworld_pb2_grpc.GreeterStub(channel) # 调用服务端方法 response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#x27;World&#x27;)) print(&quot;Greeter client received: &quot; + response.message)if __name__ == &#x27;__main__&#x27;: run() RedisREmote DIctionary Server, Redis 是一个 key-value 存储系统，是跨平台的非关系型数据库。 123456789101112131415pip install redisimport redis # 导入redis 模块# 获取连接r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True) # Redis 实例会维护一个自己的连接池，建立连接池，从连接池获取连接pool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)r = redis.Redis(connection_pool=pool)r.set(&#x27;name&#x27;, &#x27;runoob&#x27;, nx, xx) # 设置 name 对应的值, 当 nx = Ture 则只有 Key 不存在才执行插入; xx 相反print(r[&#x27;name&#x27;])print(r.get(&#x27;name&#x27;), px, ex) # 取出键 name 对应的值, px 毫秒 ex 秒 为过期时间print(type(r.get(&#x27;name&#x27;))) # 查看类型 在使用中，Redis 存储可分为两大类： set 即 k v，这里的 v 通常是一个字符串。 hset 即 k Hash-v，这里的 v 是一个 Redis Hash，是一个 string 类型的 field（字段）和 value（值）的映射表。 缓存技术缓存就是利用编程技术将数据存储在临时位置，而不是每次都从源数据去检索。 Flask基于 Flask 实现自定义控制器规则。 TODO","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"}]},{"title":"决策树算法","slug":"算法/决策树算法","date":"2022-07-31T02:58:15.000Z","updated":"2023-01-04T07:19:42.822Z","comments":true,"path":"2022/07/31/算法/决策树算法/","link":"","permalink":"https://wingowen.github.io/2022/07/31/%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/","excerpt":"基本概念。","text":"基本概念。 决策树的基本概念 决策树节点： 叶节点表示一份类别或者一个值； 非叶节点表示一个属性划分。 决策树的有向边代表了属性划分的不同取值： 当属性是离散时，可将属性的每一个取值用一条边连接到子结点； 当属性是连续时，需要特殊处理。 决策树是一种描述实例进行分类的树形结构。 对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。 决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略；决策树的学习过程本质上是从训练数据中寻找一组分类规则；决策树学习也可以看做是由训练数据集估计条件概率模型。 由上述描述可以得知，决策树学习是一个递归过程，有三种情况会导致递归返回： 当前结点包含的所有样本属于同一类别。 当前属性结合为空，或所有样本在所有属性上的取值都相同：将当前结点标记为叶结点，其类别为该节点包含的样本最多的类别。 当前结点包含的样本基本为空：将当前结点标记为叶结点，其类别为父节点包含样本最多的类别 决策树的学习结果为：树结构 + 叶节点的取值（类别） 信息增益 熵，又称信息熵，是信息论的重要概念。熵是度量样本集合纯度的指标，熵越大，样本的纯度越低。假设当前样本集合 DDD 中第 iii 类样本所占比例的为 pi(i=1,2,...,C)p_i(i = 1,2,...,C)pi​(i=1,2,...,C)，则 DDD 的熵定义为： H(D)=−∑i=1Cpilog⁡2piH(D)=-\\sum_{i=1}^{C} p_{i} \\log _{2} p_{i} H(D)=−i=1∑C​pi​log2​pi​ 信息增益表示特征对于当前样本集纯度提升的程度。某属性的信息增益越大，说明使用改属性进行划分获得的纯度提升越大。因此使用信息增益进行决策树属性选择时，选择属性信息增益最大的作为当前节点。 G(D,a)=H(D)−∑v=1V∣Dv∣∣D∣H(Dv)G(D, a)=H(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} H\\left(D^{v}\\right) G(D,a)=H(D)−v=1∑V​∣D∣∣Dv∣​H(Dv) 123456789101112131415161718192021222324252627282930# 信息增益计算def get_G(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data 中所有样本属于同一类，由于 xlnx 在 x = 1和 x-&gt;0 是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE return H - Eprint(&#x27;各个属性的信息增益为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_G(data,i,len(data.columns)-1)) 信息增益比 以信息增益作为划分数据集的特征，会导致对可取值数目较多的属性有所偏好。为了缓解这种不良影响，采用信息增益比作为选择属性的准则。 123456789101112131415161718192021222324252627282930def get_GR(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益比 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益比, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 IV = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data中所有样本属于同一类，由于xlnx 在x = 1和x-&gt;0是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE IV = IV - new_data.shape[0] / data.shape[0] * (np.log2(new_data.shape[0] / data.shape[0])) G = H - E return G / IVprint(&#x27;各个属性的信息增益比为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_GR(data,i,len(data.columns)-1)) 基尼系数 纯度使用基尼指数来度量，在使用基尼系数作为指标时，应该选择基尼指数最小的属性。 1234567891011121314151617181920212223def get_Gini(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的基尼系数 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的基尼系数, type:float, e.g.:0.32 &#x27;&#x27;&#x27; Gini = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] Gini_v = 1 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] Gini_v = Gini_v - p * p Gini = Gini + new_data.shape[0] / data.shape[0] * Gini_v return Giniprint(&#x27;各个属性的基尼指数为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_Gini(data,i,len(data.columns)-1)) ID3 ID3 算法的核心是在决策树各结点上使用信息增益准则选择特征：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，根据特征的不同取值建立子结点。递归地调用以上方法，构建决策树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def build_tree_id3(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用 ID3 算法在 data 数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_G(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_id3(new_data, nu, new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)root_id3 = build_tree_id3(data, None, ori_ppt, len(data.columns)-1)# 可视化createPlot(root_id3) C4.5 C4.5 算法对 ID3 算法进行了改进，即使用信息增益比来选择特征，其余和 ID3 算法基本相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def build_tree_c45(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用C4.5算法在data数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_GR(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_c45(new_data, nu,new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)# print(data)root_c45 = build_tree_c45(data, None ,ori_ppt, len(data.columns)-1)createPlot(root_c45) 损失函数与剪枝 决策树的剪枝往往通过最小化决策树的损失函数实现。 123456789101112131415161718192021def cal_loss(root, alpha): &#x27;&#x27;&#x27; 计算以root为根结点的决策树的损失值 :param root: 根结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 以root为根结点的决策树的损失值, type:float, e.g.:0.24 &#x27;&#x27;&#x27; loss = 0 for leaf in root.leaf_ch: data = leaf.data for v in np.unique(data.iloc[:,len(data.columns)-1]): ntk = data[data.iloc[:,len(data.columns)-1] == v].shape[0] loss = loss - ntk * np.log2(ntk / data.shape[0]) loss = loss + alpha * len(root.leaf_ch) return lossori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)cal_loss(root_c45, 0.3) 决策树生成算法递归地产生决策树，直到无法继续。这种做法会带来过拟合问题。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，构建过于复杂的决策树。因此，一种解决方法是考虑决策树的复杂程度，从而对决策树进行简化。对决策树进行简化的过程称为剪枝。即从决策树中裁掉一些子树或叶结点，将其根节点或父节点作为新的叶结点。 剪枝算法的实现 计算每个节点的经验熵。 递归地从树的叶结点向上回缩，若回缩后的损失值 &gt; 回缩前的损失值，则进行剪枝，父节点变为叶节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def tree_pruning(root, leaf, alpha): &#x27;&#x27;&#x27; 决策树剪枝 :param root: 根结点, type:Node :param leaf: 叶结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 剪枝后的决策树根结点, type:Node &#x27;&#x27;&#x27; new_root = copy.deepcopy(root) pre_loss = cal_loss(root, alpha) flag = 1 for nl in leaf.fa.leaf_ch.copy(): nl.can_delete = 1 new_set = set() for leaf_ch in root.leaf_ch: if leaf_ch.can_delete != 1: new_set.add(leaf_ch) root.leaf_ch = new_set leaf.fa.set_leaf(leaf.fa.data.iloc[:,len(root.data.columns)-1].value_counts().keys()[0]) root.add_leaf_ch(leaf.fa) after_loss = cal_loss(root, alpha) if after_loss &gt;= pre_loss: #不剪枝 root = new_root flag = 0 return root, flag ori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)# createPlot(root_c45)#设置超参数alpha = 0.3update = 1while update == 1: update = 0 for leaf in root_c45.leaf_ch.copy(): root_c45,flag = tree_pruning(root_c45, leaf, alpha) if flag: update = 1# root_c45createPlot(root_c45) 连续值处理 缺失值处理","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算机组成","slug":"计算机科学/计算机组成","date":"2022-07-30T07:15:43.000Z","updated":"2023-09-20T06:47:40.247Z","comments":true,"path":"2022/07/30/计算机科学/计算机组成/","link":"","permalink":"https://wingowen.github.io/2022/07/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/","excerpt":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线","text":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线 计算机系统概论 计算机系统层次结构 微程序机器 M0 微指令系统： 由硬件直接执行微命令； 实际机器 M1 机器语言机器：用微程序解释机器指令； 虚拟机器 M2 操作系统机器：用机器语言解释操作系统； 虚拟机器 M3 汇编语言机器：用汇编程序翻译成机器语言程序； 虚拟机器 M4 高级语言机器：用编译程序翻译成汇编语言程序或其它中间语言程序。 计算机的基本组成 冯·诺依曼提出存储程序的概念，以此概念为基础的计算机通称为冯·诺依曼计算机，其具有如下特点： 计算机由运算器、存储器、控制器、输入设备和输出设备五大部件组成； 指令和数据以同地位存放于存储器内，可按地址寻访； 指令和数据均用二进制数表示； 指令由操作码和地址码组成，操作码用来表示操作的性质，地址码用来表示操作数在存储器中的位置； 指令在存储器内按顺序存放。通常，指令是顺序执行的，在特定条件下，可根据运算结果或根据设定的条件改变执行顺序。 机器以运算器为中心，输入输出设备与存储器间的数据传送通过运算器完成。 各部件的功能如下： 运算器用来完成算术运算和逻辑运算，并将运算的中间结果暂存在运算器内； 存储器用来存放数据和程序； 控制器用来控制、指挥程序的运行、程序的输入输出以及处理运算结果； 运算器和控制器在逻辑关系和电路结构上联系十分紧密，两大部件往往集成在同一芯片上，因此通常将它们合起来统称为中央处理器 CPU, Central Processing Unit。 现代计算机组成：CPU, I/O 以及 主存储器 Main Memory, MM。 CPU + MM 称为主机；I/O 有称为外部设备。 Arithmetic Logic Unit, ALU 算术逻辑部件，用来完成算术逻辑运算；Control Unit, CU 控制单元，用来解释存储器中的指令，并发出各种操作命令来执行指令。 ALU 和 CU 是 CPU 的核心部件； I/O 设备也受 CU控制，用来完成相应的输入、输出操作。 计算机的工作步骤 TODO 系统总线 计算机系统的五大部件之间的互连方式有两种： 各部件之间使用单独的连线，成为分散连接； 另一种是各部件连到一组公共信息传输线上，成为总线连接。 总线是连接多个部件的信息传输线，是各部件共享的传输介质。当多个部件与总线相连时，如果出现两个或两个以上部件同时向总线发送信息，势必导致信号冲突，传输无效。因此，在某一时刻，只允许有一个部件向总线发送信息，而多个部件可以同时从总线上接收相同信息。 以运算器为中心的结构。I/O 设备与主存交换信息时仍然要占用 CPU，因此还会影响 CPU 的工作效率。 单总线（系统总线）。I/O 设备与主存交换信息时，原则上不影响 CPU 的工作，CPU 仍可继续处理不访问主存或 I/O 设备的操作，提高了 CPU 的工作效率。 但当某一时刻各部件都要占用总线时，就会发生冲突，必须设置总线判优逻辑，让各部件按优先级高低来占用总线，这也会影响整机的工作速度。 以存储器为中心的双总线结构框图。存储中线只供主存与 CPU 之间传输信息，即提高了传输效率，又减轻了系统总线的负担，还保留了 I/O 设备与存储器交换信息不经过 CPU 的特点。 总线分类 片内总线：芯片内部总线； 系统总线：各大部件之间的信息数据信息；按系统总线传输信息的不同，又可分为三类**：数据总线、地址总线和控制总线**。 通信总线：计算机系统之间或与其他系统之间的通信。 总线特性及性能指标 TODO 总线结构 TODO 总线控制 判优控制（仲裁逻辑）和通信控制。 总线判优控制 主设备：对总线有控制选；从设备：只能响应从主设备发来的总线命令，对总线没有控制权。 若多个主设备同时要使用总线时，由总线的判优、仲裁逻辑按一定的优先等级顺序确定哪个主设备能使用总线，只有获得总线使用权的主设备才能开始传送数据。 总线判优控制可分集中式和分布式两种： 将控制逻辑集中在一处； 将控制逻辑分散在与总线连接的各个部件或设备上。 集中控制优先仲裁方式 链式查询方式 控制总线中有 3 根线用于总线控制：BS 总线忙、BR 总线请求、BG 总线同意。其中 BG 是串行从一个 I/O 接口送到下一个 I/O 接口。如果 BG 到达的接口有总线请求 BR，BG 信号就不再往下传，意味着该接口获得了总线使用权，并建立总线忙 BS 信号，表示它占用了总线。 离总线控制部件最近的设备具有最高优先级。 只需很少几根线就能按一定有限次序实现总线控制，并且很容易扩充设备，但对电路故障很敏感，且优先级别低的设备可能很难获得请求。 计数器定时查询方式 与链式查询方式相比，多了一组设备地址线，少了一根总线同意线 BG。 总线控制部件接到由 BR 送来的总线请求信号后，在 BS=0 时，总线控制部件中的计数器开始计数，并通过设备地址线向各设备发出一组地址信号。当某个请求占用总线的设备地址与计数值一致时，便获得总线使用权，此时终止计数查询。 初始值可由程序设置；终止计数后可以重头开始，也可以从上一次计数终点开始。 对电路故障敏感度小于链式查询方式，但增加了控制线数（设备地址）目，控制也较复杂。 独立请求方式 每一台设备均有一对总线请求线和总线同意线。当设备要求使用总线时，便发出改设备的请求信号。总线控制部件中有一排队电路，可根据优先次序确定响应哪一台设备的请求。 响应快，优先次序控制灵活（根据程序改变），控制线数量多，总线控制更复杂。 通信控制 通常将完成一次总线操作的时间称为总线周期，可分为以下 4 各阶段。 申请分配阶段：主模块提出申请，总线仲裁机构决定下一传数周期的总线使用权授予某一申请者； 寻址阶段：取得了使用权的主模块通过总线发出本次要访问的从模块的地址及有关命令。启动参与本次传数的从模块； 传数阶段：主模块与从模块进行数据交换，数据由源模块发出，经数据总线流入目的模块； 结束阶段：主模块的有关信息均从系统总线上撤除，让出总线使用权。 总线通信控制主要解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调如何配合。 同步通信 读命令：CPU 在 T1 上升沿发出地址信息；在 T2 的上升沿发出读命令；与地址信号相符合的输入设备按命令进行一系列内部操作，且必须在 T3 的上升沿到来之前将 CPU 所需数据送到数据总线上；CPU 在 T3 时钟周期内将数据上的信息传送到其内部寄存器；CPU 在 T4 上升沿撤销读命令，输入设备不再向数据总线上传送数据，撤销它对数据总线的驱动。 规定明确、统一，模板间配合简单一致。 缺点是主、从模块时间配合属于强制性“同步”，必须在限定时间内完成规定的要求。并且对所有从模块都用统一时限，各模块速度不同，必须以最慢速度的部件来设计公共时钟，严重影响总线工作效率，给设计带来局限性，缺乏灵活性。 异步通信 (a) 不互锁：主模块发出请求信号后，不必等待接到从模块的回答信号，而是经过一段时间，确认从模块已收到请求信号后，便撤销其请求信号；从模块接到请求信号后，在条件允许时发出回答信号，并且经过一段时间确认主模块已收到回答信号后，自动撤销回答信号。 (b) 半互锁：主模块发出请求信号，必须接待到从模块的回答信号后再撤销其请求信号，有旧互锁关系；而从模块接到请求信号后发出回答信号，但不必等待获知主模块的请求信号，而是隔一段时间后自动撤销其回答信号，无互锁关系。 © 全互锁：皆需获得回答信号后撤销。在网络通信中，通信双方采用的就是全互锁方式。 半同步通信 保留了同步通信的基本特点，同时又像异步通信那样允许不同速度的模块和谐地工作。 增设了一条等待响应信号线，采用插入等待周期的措施来协调通信双方的配合问题。 分离式通信 将一个传输周期（总线周期）分解为两个子周期，两个传输子周期都只有单方面的信息流，每个模块都变成了主模块。 存储器 概述 存储器分类 a) 按存储介质分类。 半导体存储器 由半导体器件组成，用超大规模集成电路工艺制成芯片，体积小、功耗低、存取时间短。当电源消失时，所存信息也随即像丢失，是一种易失性存储器。 非挥发性材料制成的半导体存储器，克服了信息易失的弊病。 双极型 TTL 半导体存储器：高速。 MOS 半导体存储器：高集成度，制造简单，成本低，故被广泛应用。 磁表面存储器 在金属或塑料基体的表面上涂一层磁性材料作为记录介质，工作时磁层随载磁体高速运转，用磁头在磁层上进行读、写操作。 用具有矩形磁滞回线特性的材料作次表面物质，按其剩磁状态的不同而区分 0 或 1，而且剩磁状态不会轻易丢失，故这类存储器具有非易失性的特点。 磁芯存储器 被半导体存储器取代。 光盘存储器 用激光在磁光材料上进行读、写的存储器，具有非易失性的特点。 记录密度高、耐用性好、可靠性高和可互换性强等特点。 b) 按存取方式分类 随机存储器 Random Access Memory RAM 是一种可读、写存储器，其特点是存储器的任何一个存储单元的内容都可以随机存取，且存取时间与存储单位的物理位置无关。计算机系统中的主存都采用这种随机存储器。 静态 RAM，以触发器原理寄存信息。 动态 RAM，以电容充放电原理寄存信息。 只读存储器 Read Only Memory ROM 掩模型只读存储器 Masked ROM，MROM；可编程只读存储器 Programmable ROM，PROM；可擦除可编程只读存储器 Erasable Programmable ROM，EPROM；用电可擦除可编程只读存储器 Electrically Erasable Programmable ROM， EEPROM；Flash Memory。 串行访问存储器 对存储单元进行读写操作时，需按其物理位置的先后顺序寻找地址，则这种存储器称为串行访问存储器。 由于信息所在位置不同，读写时间均不相同。 c) 按在计算机中的作用分类 主存储器 可以和 CPU 直接交换信息。 速度快、容量小、每位价格高。 辅助存储器 是主存储器的后援存储器，用来存放当时暂时不用的程序和书，不能与 CPU 直接交换信息。 速度慢、容量大、每位价格低。 缓冲存储器 用在两个速度不同的部件之中。 层次结构 由上至下，位价越来越低，速度越来越慢，容量越来越大。 寄存器中的数直接在 CPU 内部参与运算。 主存用来存放将要参与运行的程序和数据，其速度与 CPU 速度差距较大，为使它们匹配，在主存与 CPU 之间插入了一种比主存速度更快、容量跟更小的高速缓冲存储器 Cache。 寄存器、缓存、主存这三类存储器都是由速度不同、位价不等的半导体存储材料制成的，它们都设在主机内。 磁盘、磁带属于辅助存储器，其容量比主存大得多，大都用来存放暂时未用到的程序和数据文件。 CPU 不能直接访问辅存，辅存只能与主存交换信息，因此辅存的速度可以比主存慢很多。 缓存 - 主存层次主要解决 CPU 与主存速度不匹配的问题。 主存 - 辅存层次主要解决存储系统的容量问题。形成了虚拟存储系统，在这个系统中，程序员变成的地址范围与虚拟存储器的地址空间相对应。 程序员编程时，可用的地址空间远远大于主存空间，使程序员以为自己占有一个容量极大的主存（虚拟存储器）。其逻辑地址转变为物理地址的工作由计算机系统的硬件和操作系统自动完成的，对程序员是透明的。 当虚地址的内容在主存时，机器便可立即使用；若虚地址的内容不在主存，则必须先将此虚地址内容传递到主存的合适单元后再为机器所用。 主存储器 主存的基本组成。 根据 MAR 储存器地址寄存器中的地址访问某个存储单元时，还需要经过地址译码、驱动等电路，才能找到所需要访问的单元。 读出时，需经过读出放大器，才能将被选中单元的存储字送到 MDR 主存数据寄存器。 写入时，MDR 中的数据也必须经过写入电路才能真正写入到被选中的单元中。 现代计算机的主存都由半导体集成电路构成，驱动器、译码器和读写电路均制作在存储芯片中，而 MAR 和 MDR 制作在 CPU 芯片中。存储芯片和 CPU 芯片可通过总线连接。 当要从存储器读出来某一信息字时，首先由 CPU 将该字的地址送到 MAR，经地址总线送至主存，然后发出读命令，主存接到读命令后将该单元内容读至数据总线上。 若要向主存存入一个信息字时，首先 CPU 将该字所在主存单元的地址经 MAR 送到地址总线，并将信息字送入 MDR，然后向主存发出写命令，主存接到写命令后，便将数据线上的信息写入到对应地址线指出的主存单元中。 主存中存储单元地址的分配 不同的机器存储字长不同，常用 8 位二进制数代表一个字节，因此存储字长都取 8 的倍数。 通常计算机系统即可按字寻址，也可按字节寻址。 主存的技术指标 存储容量：存储单元 x 存储字长；1 字长 = 8 字节。 存储速度：由存取时间和存取周期来表示。 存取时间、访问时间 Memory Access Time，是指启动一次存储器操作（读或写）到完成该操作所需的全部时间。存取时间分为读出时间和写入时间两种。 存取周期 Memory Cycle Time 是指存储器进行连续两次独立的存储器操作所需的最小间隔时间，通常存储周期大于存储时间。 存储器带宽 与存储周期密切相关，表示单位时间内存储器存取的信息量。通过以下方式提高存储器带宽： 缩短存储周期； 增加存储字长，使每个存取周期可读 / 写更多的二进制数； 增加存储体。 半导体存储芯片简介 基本结构 半导体存储芯片采用超大规模集成电路制造工艺，在一个芯片内集成具有记忆功能的存储矩阵、译码驱动电路和读写电路等。 译码驱动能把地址总线送来的地址信号翻译成对应存储单元的选择信号，该信号在读 / 写电路的配合下完成对被选中单元的读写操作。 读写电路包括读出放大器和写入电路，用来完成读写操作。 地址线是单向输入的，数据线是双向的，位数与芯片容量有关。 地址线和数据线位数共同反映存储芯片的容量。例如，地址线为 10 根，数据线为 4 根，则芯片容量为 210x4 K 位。 控制线主要包括读写控制线和片选线两种（不同存储芯片不同，可共用一根或分用两根）。由于半导体存储器是由许多芯片组成的，为此需要用片选信号来确定哪个芯片被选中。 译码驱动方式 线选法：用一根字选择线直接选中一个存储单元的各位。 重合法： 随机存取存储器 静态 RAM、Static RAM、SRAM 存储器用于寄存 0 和 1 代码的电路成为存储器的基本单元电路。","categories":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"模型评估与选择","slug":"算法/模型评估与选择","date":"2022-07-29T10:24:58.000Z","updated":"2023-01-04T07:23:27.612Z","comments":true,"path":"2022/07/29/算法/模型评估与选择/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。","text":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。 错误率和精度 123456789101112131415161718import numpy as np# 真实的数据标签real_label = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]) \\ \\ \\# 分类器的预测标签classifier_pred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])compare_result = (real_label == classifier_pred)compare_result = compare_result.astype(np.int)# m 为样本数量 b 为预测错误样本m = len(real)b = m - np.sum(cmp)# 错误率error_rate = (b / m)*100# 精确度 accaccuracy = (1 - b / m)*100 误差 模型在训练样本上的误差称为训练误差或经验误差；模型在新样本上的误差称为泛化误差。 过拟合模型：虽然训练误差接近 0，泛化误差非常大。 欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。 偏差和方差 偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。 偏差 bias，与真实值的偏离程度； 方差 variance，该随机变量在其期望值附近的波动程度。 评估方法 评估：对学习器的泛化误差进行评估并进而做出选择。 留出法 以一定比例划分训练集和测试集。 1234567891011121314151617181920212223# 导入包import numpy as npfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): &#x27;&#x27;&#x27; return: 返回随机生成 200 个点的坐标 &#x27;&#x27;&#x27; dots = 200 # 样本数 dim = 2 # 数据维度 X = np.random.randn(dots,dim) # 建立数据集，shape(200,2) # 建立样本 X 的类别 Y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): Y[i] = 1 return X, Y# 加载数据X,Y = load_pts()# 使用train_test_split划分训练集和测试集train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0) 交叉验证 交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。 ![Untitled](/img/模型评估与选择/Untitled 1.png) 12345678910111213# 导入包from sklearn.model_selection import KFoldimport numpy as np# 生成数据集，随机生成40个点data = np.random.randn(40,2)# 交叉验证法kf = KFold(n_splits = 4, shuffle = False, random_state = None) for train, test in kf.split(data): print(train) print(test,&#x27;\\n&#x27;) 自助法 有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ： 每次随机从 D 中挑选一个样本； 将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中； 重复执行 m 次该过程； 最后得到包含 m 个样本数据集 D’。 由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。 123456789101112131415161718# 导入包import numpy as np# 任意设置一个数据集X = [1,4,3,23,4,6,7,8,9,45,67,89,34,54,76,98,43,52]# 通过产生的随机数获得抽取样本的序号 bootstrapping = []for i in range(len(X)): bootstrapping.append(np.random.randint(0,len(X),(1)))# 通过序号获得原始数据集中的数据D_1 = []for i in range(len(X)): print(int(bootstrapping[i])) D_1.append(X[int(bootstrapping[i])]) print(D_1) 总结 采样方法 与原始数据集的分布是否相同 相比原始数据集的容量 是否适用小数据集 是否适用大数据集 是否存在估计偏差 留出法 分层抽样 否 变小 否 是 是 交叉验证法 分层抽样 否 变小 否 是 是 自助法 放回抽样 否 不变 是 否 是 性能度量 性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。 性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。 回归任务常用性能度量：MSE mean square error，均方差。 分类任务常用性能度量：acc accuracy，精度；错误率 查准率、查全率、F1 对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为： 真正例 true positive 假正例 false positive 真反例 true negative 假反例 false negative ![Untitled](/img/模型评估与选择/Untitled 2.png) P( Precision )=TPTP+FPR( Recall )=TPTP+FNP(\\text { Precision })=\\frac{T P}{T P+F P} \\\\R(\\text { Recall })=\\frac{T P}{T P+F N} P( Precision )=TP+FPTP​R( Recall )=TP+FNTP​ Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。 Precision，查准率：在预测为正的样本中，实际为正的概率。 P-R 曲线，BRP，Break Even Point：平衡单 P = R。 ![Untitled](/img/模型评估与选择/Untitled 3.png) 由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。 BRP 还是过于简单，更常用的是 F1 度量。 F1=2×P×RP+R=2TPn+TP−TNF 1=\\frac{2 \\times P \\times R}{P+R}=\\frac{2 T P}{n+T P-T N} F1=P+R2×P×R​=n+TP−TN2TP​ F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。 当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。 Fβ=(1+β2)×P×R(β2×P)+RF_{\\beta}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R} Fβ​=(β2×P)+R(1+β2)×P×R​ β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。 如果使用了类似交叉验证法，我们会得到多个 confusion matrix： 宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1； 微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as np# 加载数据集def generate_data(random_state=2021): &quot;&quot;&quot; :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] &quot;&quot;&quot; noise_rate = 0.1 # 噪声比例 sample_num = 4096 # 总样本数 noise_sample_num = int(sample_num*noise_rate) # 噪声样本数 np.random.seed(random_state) Pred_Score = np.random.uniform(0,1,sample_num) GT_label = (Pred_Score&gt;0.5).astype(np.int) noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=False, p=None) for index in noise_ids: GT_label[index] = 1 if GT_label[index] == 0 else 0 return GT_label, Pred_ScoreGT_label, Pred_Score = generate_data()# 请你补全以下代码，计算查准率与查全率def get_PR(GT_label, Pred_Score, threshold, random_state=2021): &quot;&quot;&quot; 计算错误率和精度 :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] :threshold: 评估阈值 :random_state: 随机种子 :返回值: P: 查准率 R: 查全率 &quot;&quot;&quot; Pred_Label = list(map(lambda x: 1 if x &gt; threshold else 0, Pred_Score)) from sklearn.metrics import precision_score, recall_score P = precision_score(GT_label, Pred_Label) R = recall_score(GT_label, Pred_Label) &quot;&quot;&quot; TODO &quot;&quot;&quot; return P, R P, R = get_PR(GT_label, Pred_Score, 0.55, random_state=2021)print(&quot;查准率P ：&#123;:.2f&#125;&quot;.format(P))print(&quot;查全率R ：&#123;:.2f&#125;&quot;.format(R)) ROC 与 AUC 原理 ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。 TPR( Precision )=TPTP+FNFPR( Precision )=FPFP+TN\\begin{aligned}T P R(\\text { Precision }) &amp;=\\frac{T P}{T P+F N} \\\\F P R(\\text { Precision }) &amp;=\\frac{F P}{F P+T N}\\end{aligned} TPR( Precision )FPR( Precision )​=TP+FNTP​=FP+TNFP​​ TPR 真正率，真正样本与实际为正的样本的比率； FPR 假正率，加正样本与实际为负的样本的比率。 若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两 个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC Area Under ROC Curve。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): dots = 200 # 点数 X = np.random.randn(dots,2) * 15 # 建立数据集，shape(200,2)，坐标放大15倍 # 建立 X 的类别 y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): if X[i,0] &gt; -15 and X[i,0] &lt; 15 and X[i,1] &gt; -15 and X[i,1] &lt; 15: # 矩形框内的样本都是目标类（正例） y[i] = 1 if 0 == np.random.randint(i+1) % 10: # 对数据随机地插入错误，20 个左右 y[i] = 1 - y[i] # 数据集可视化 plt.scatter(X[np.argwhere(y==0).flatten(),0], X[np.argwhere(y==0).flatten(),1],s = 20, color = &#x27;blue&#x27;, edgecolor = &#x27;k&#x27;) plt.scatter(X[np.argwhere(y==1).flatten(),0], X[np.argwhere(y==1).flatten(),1],s = 20, color = &#x27;red&#x27;, edgecolor = &#x27;k&#x27;) plt.xlim(-40,40) plt.ylim(-40,40) plt.grid(False) plt.tick_params( axis=&#x27;x&#x27;, which=&#x27;both&#x27;, bottom=False, top=False) return X, yX, y = load_pts()plt.show()### 训练模型 ###from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.svm import SVC# 将数据集拆分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) # 建立模型 clf1 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=4,min_samples_split=4)clf2 = GradientBoostingClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=10)clf3 = SVC(kernel=&#x27;rbf&#x27;, gamma=0.001, probability=True)# 训练模型clf1.fit(X_train, y_train)clf2.fit(X_train, y_train)clf3.fit(X_train, y_train)### 评估模型 ###from sklearn.metrics import roc_curve# 模型预测y_score1 = clf1.predict_proba(X_test)y_score2 = clf2.predict_proba(X_test)y_score3 = clf3.predict_proba(X_test)# 获得 FPR、TPR 值fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,1])fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,1])fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,1])### 绘制 ROC 曲线 ###from sklearn.metrics import aucplt.figure()# 绘制 ROC 函数def plot_roc_curve(fpr, tpr, c, name): lw = 2 roc_auc = auc(fpr,tpr) plt.plot(fpr, tpr, color=c,lw=lw, label= name +&#x27; (area = %0.2f)&#x27; % roc_auc) plt.plot([0,1], [0,1], color=&#x27;navy&#x27;, lw=lw, linestyle=&#x27;--&#x27;) plt.xlim([0, 1.0]) plt.ylim([0, 1.05]) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.ylabel(&#x27;True Positive Rate&#x27;) #plt.title(&#x27;&#x27;) plt.legend(loc=&quot;lower right&quot;) plot_roc_curve(fpr1, tpr1, &#x27;red&#x27;,&#x27;DecisionTreeClassifier &#x27;) plot_roc_curve(fpr2, tpr2, &#x27;navy&#x27;,&#x27;GradientBoostingClassifier &#x27;) plot_roc_curve(fpr3, tpr3, &#x27;green&#x27;,&#x27;SVC &#x27;) plt.show() 比较检验（TODO） 模型性能比较的重要因素： 实验评估得到的性能不等于泛化性能； 测试集上的性能与测试集本身的选择有很大关系； 很多机器学习算法本身有一定的随机性。 统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。 假设检验 由样本推测总体的方法。 交叉验证 t 检验 McNemar 检验 Friedman 检验与 Nemenyi 后续检验 类别不平衡 在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。 如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。 解决不平衡分类问题的策略可以分为两大类： 从数据层面入手 , 通过改变训练集样本分布降低不平衡程度； 从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。 数据层面解决类别不平衡 扩大数据样本。 重采样：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。 过采样：复制稀有样本； 123456789101112131415161718192021222324252627282930313233# 导入包from sklearn.datasets import make_classificationfrom collections import Counterfrom imblearn.over_sampling import RandomOverSampler# 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)# 打印每个类别样本数print(Counter(y))# 过采样ros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)# 打印过采样后每个类别样本数print(sorted(Counter(y_resampled).items()))# 生成新的稀有样本# 导入包from imblearn.over_sampling import SMOTE# 过采样sm = SMOTE(random_state=42)X_res, y_res = sm.fit_resample(X, y)# 打印过采样后每个类别样本数print(&#x27;Resampled dataset shape %s&#x27; % Counter(y_res)) 欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。 123456789# 导入包from imblearn.under_sampling import RandomUnderSampler# 欠采样rus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)# 打印欠采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。 因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。 12345678# 导入包from imblearn.combine import SMOTEENN# 过采样与欠采样结合smote_enn = SMOTEENN(random_state=0)X_resampled, y_resampled = smote_enn.fit_resample(X, y)# 打印采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 算法层面解决类别不平衡 惩罚项方法：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，错分稀有类的样本需要付出更大的代价。 通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。 例如，调整 SVM 以惩罚稀有类别的错误分类。 1234567# LABEL 0 4000# LABEL 1 200# 导入相关包from sklearn.svm import SVC# 添加惩罚项clf = SVC(C=0.8, probability=True, class_weight=&#123;0:0.25, 1:0.75&#125;) 特征选择方法 样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。 选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"贝叶斯算法","slug":"算法/贝叶斯算法","date":"2022-07-29T10:24:58.000Z","updated":"2023-01-04T07:41:34.361Z","comments":true,"path":"2022/07/29/算法/贝叶斯算法/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/","excerpt":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。","text":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。 条件概率 P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率： P(A∣B)=P(A∩B)P(B)P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)} P(A∣B)=P(B)P(A∩B)​ P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率： P(B∣A)=P(A∩B)P(A)P(B \\mid A)=\\frac{P(A \\cap B)}{P(A)} P(B∣A)=P(A)P(A∩B)​ 那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出贝叶斯公式： P(A∣B)=P(B∣A)×P(A)P(B)P(A \\mid B)=\\frac{P(B \\mid A) \\times P(A)}{P(B)}{\\scriptsize } P(A∣B)=P(B)P(B∣A)×P(A)​ 贝叶斯 基础思想： 已知类条件概率密度参数表达式和先验概率； 利用贝叶斯公式转换成后验概率； 根据后验概率大小进行决策分类。 根据以上基本思想，可以得到贝叶斯概率计算公式表达为**：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）**。 优点： 贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断； 它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信； 如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了； 它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。 缺点： 它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出； 有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。 扩展阅读： 一文读懂概率论学习：贝叶斯理论 贝叶斯决策论&amp;朴素贝叶斯算法 朴素贝叶斯法讲解 sklearn 贝叶斯方法 贝叶斯推断：广告邮件自动识别的代码实现 若邮件包含某个关键词，求此邮件是广告的概率。 12345678910111213141516171819# 广告邮件数量ad_number = 4000# 正常邮件数量normal_number = 6000# 所有广告邮件中，出现 “红包” 关键词的邮件的数量ad_hongbao_number = 1000# 所有正常邮件中，出现 “红包” 关键词的邮件的数量normal_hongbao_number = 6# 广告的先验概率 P(A)P_ad = ad_number / (ad_number + normal_number)# 包含红包的先验概率 P(B)P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)# 广告 包含红包的似然概率 P(B|A)P_hongbao_ad = ad_hongbao_number / ad_number# 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbaoprint(P_ad_hongbao) 10.9940357852882705 极大似然估计 极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。 极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 极大似然估计与贝叶斯推断是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。 极大似然估计的简单计算 一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。 ddp(10061)p61(1−p)39=(10061)(61p60(1−p)39−39p61(1−p)38)=(10061)p60(1−p)38(61(1−p)−39p)=(10061)p60(1−p)38(61−100p)=0\\begin{array}{c} \\frac{d}{d p}\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{61}(1-p)^{39}=\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right)\\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\\right) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61-100 p) \\\\ =0 \\end{array} dpd​(10061​)p61(1−p)39=(10061​)(61p60(1−p)39−39p61(1−p)38)=(10061​)p60(1−p)38(61(1−p)−39p)=(10061​)p60(1−p)38(61−100p)=0​ 当 P=61100,0P = \\frac{61}{100}, 0P=10061​,0 时，导数为零。因为 1 &lt; P &lt; 0，所以 P=61100P = \\frac{61}{100}P=10061​。 极大似然估计的简单应用 求极大似然估计 MLE 的一般步骤： 由总体分布导出样本的联合概率函数（或联合密度）； 把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 θθθ 看作自变量，得到似然函数 l(θ)l(θ)l(θ)； 求似然函数 l(θ)l(θ)l(θ) 的最大值点，常常转化为求 lnl(θ)lnl(θ)lnl(θ) 的最大值点，即 θθθ 的 MLE； 在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。 若随机变量 xxx 服从一个数学期望为 μμμ、方差为 σ2σ^2σ2 的正态分布，记为 N(μ,σ2)N(μ,σ^2)N(μ,σ2)，假设 μ=30,σ=2μ=30, σ=2μ=30,σ=2。 1234567891011import numpy as npfrom scipy.stats import normimport matplotlib.pyplot as pltμ = 30 # 数学期望σ = 2 # 方差x = μ + σ * np.random.randn(10000) # 正态分布plt.hist(x, bins=100) # 直方图显示plt.show()print(norm.fit(x)) # 返回极大似然估计，估计出参数约为 30 和 2 朴素贝叶斯分类器 朴素贝叶斯分类器是一系列假设特征之间强（朴素）独立条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。 MNIST 手写体数字识别 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import warningswarnings.filterwarnings(&quot;ignore&quot;)# numpy 库import numpy as np# tensorflow 库中的 mnist 数据集import tensorflow as tfmnist = tf.keras.datasets.mnist# sklearn 库中的 BernoulliNBfrom sklearn.naive_bayes import BernoulliNB# 绘图工具库 pltimport matplotlib.pyplot as pltprint(&quot;读取数据中 ...&quot;)# 载入数据(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量train_images = train_images.reshape(len(train_images),784)test_images = test_images.reshape(len(test_images),784)print(&#x27;读取完毕!&#x27;)def plot_images(imgs): &quot;&quot;&quot;绘制几个样本图片 :param show: 是否显示绘图 :return: &quot;&quot;&quot; sample_num = min(9, len(imgs)) img_figure = plt.figure(1) img_figure.set_figwidth(5) img_figure.set_figheight(5) for index in range(0, sample_num): ax = plt.subplot(3, 3, index + 1) ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;) ax.grid(False) plt.margins(0, 0) plt.show()plot_images(train_images)print(&quot;初始化并训练贝叶斯模型...&quot;)# 定义 朴素贝叶斯模型classifier_BNB = BernoulliNB()# 训练模型classifier_BNB.fit(train_images,train_labels)print(&#x27;训练完成!&#x27;)print(&quot;测试训练好的贝叶斯模型...&quot;)# 分类器在测试集上的预测值test_predict_BNB = classifier_BNB.predict(test_images)print(&quot;预测完成!&quot;)# 计算准确率accuracy = classifier_BNB.score(test_images, test_labels)print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy) 对结果进行统计比较分析。 12345678910111213141516171819202122# 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 class_num = &#123;&#125;# 每个类别预测为 0-9 类别的个数，predict_num = []# 每个类别预测的准确率class_accuracy = &#123;&#125;for i in range(10): # 找到类别是 i 的下标 class_is_i_index = np.where(test_labels == i)[0] # 统计类别是 i 的个数 class_num[i] = len(class_is_i_index) # 统计类别 i 预测为 0-9 各个类别的个数 predict_num.append( [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)]) # 统计类别 i 预测的准确率 class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100 print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % ( i, class_num[i], predict_num[i][i], class_accuracy[i])) 用热力图对结果进行分析。 123456789101112import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) np.random.seed(0)uniform_data = predict_numax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)ax.set_xlabel(&#x27;真实值&#x27;)ax.set_ylabel(&#x27;预测值&#x27;)plt.show()","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"考研","slug":"杂项/考研","date":"2022-04-21T06:19:23.000Z","updated":"2023-09-20T06:48:11.440Z","comments":true,"path":"2022/04/21/杂项/考研/","link":"","permalink":"https://wingowen.github.io/2022/04/21/%E6%9D%82%E9%A1%B9/%E8%80%83%E7%A0%94/","excerpt":"考研院校信息整理汇总。","text":"考研院校信息整理汇总。 报考专业 深大 - 人工智能与金融科技 初试科目 101 思想政治理论 201 英语一 301 数学一 408 计算机学科专业基础综合 复试科目 FSX8 机器学习 计算机考研 408 包括（150） 数据结构 45 计算机组成原理 45 操作系统 35 计算机网络 25","categories":[],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"Spring 实战学习笔记","slug":"后台技术/Spring 实战学习笔记","date":"2020-05-20T07:12:26.000Z","updated":"2023-09-20T07:00:03.774Z","comments":true,"path":"2020/05/20/后台技术/Spring 实战学习笔记/","link":"","permalink":"https://wingowen.github.io/2020/05/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20%E5%AE%9E%E6%88%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"《Spring 实战》阅读笔记。","text":"《Spring 实战》阅读笔记。 bean 的装配 在 XML 中进行显式配置； 在 Java 中进行显式配置； 隐式的 bean 发现机制和自动装配。 自动化装配 bean 组件扫描 component scanningSpring 会自动发现应用上下文中所创建的 bean； 自动装配 autowiringSpring 自动满足 bean 之间的依赖。 @Component @Named 注解表明该类会作为组件类并告知 Spring 要为这个类创建 bean。 @ComponentSacn(basePackages=&#123;...&#125;) &lt;context:component-scan base-package=&quot;...&quot; /&gt; 注解启动了组件扫描。 @Configuration 注解表明这个类是一个配置类，该类应该包含如何在 Spring 应用上下文中创建 bean 的细节。 1234@Configuration@ComponentScan // 默认扫描同包类，可指定 public class MyConfig &#123;&#125; @Autowired(required=&quot;T/F&quot;) @Inject 注解实现自动装配。（在 Spring 应用上下文中寻找匹配某个 bean 需求的其他 bean） 可以用在类的任何方法上，不管是构造器、Setter 方法还是其他的方法 Spring 都会尝试满足方法参数上所声明的依赖。假如有且只有一个 bean 匹配依赖需求的话那么这个 bean 将会被装配进来。 如果没有匹配的 bean 那么在应用上下文创建的时候 Spring 会抛出一个异常。为了避免异常的出现你可以将@Autowired的required属性设置为false。 如果有多个bean都能满足依赖关系的话Spring将会抛出一个异常表明没有明确指定要选择哪个bean进行自动装配。 @Bean 注解会告诉 Spring 这个方法将会返回一个对象该对象要注册为 Spring 应用上下文中的 bean。方法体中包含了最终产生 bean 实例的逻辑。 XML 装配 bean 对强依赖使用构造器注入而对可选性的依赖使用属性注入。 12345678910111213141516171819202122232425262728293031&lt;bean id=&quot;&quot; class=&quot;&quot;&gt; &lt;!-- 构造器注入 --&gt; &lt;constructor-arg ref=&quot;...&quot; /&gt; &lt;!-- 构造器参数名 --&gt; &lt;c:[name]-ref=&quot;...&quot; /&gt; &lt;!-- 参数索引 --&gt; &lt;c:_0-ref=&quot;...&quot; /&gt; &lt;!-- 只有一个索引的省略形式 --&gt; &lt;c:_-ref=&quot;...&quot; /&gt; &lt;!-- 字面量装配 --&gt; &lt;constructor-arg value=&quot;...&quot; /&gt; &lt;c:_[name]=&quot;...&quot; /&gt; &lt;c:_0=&quot;...&quot; /&gt; &lt;c:_=&quot;...&quot; /&gt; &lt;!-- list / set --&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;ref bean=&quot;&quot; /&gt; &lt;value&gt;&quot;...&quot;&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;!-- 属性注入（用法同上） --&gt; &lt;property name=&quot;...&quot; ref=&quot;...&quot; /&gt; &lt;!-- 属性名 --&gt; &lt;p:[name]-ref=&quot;...&quot; /&gt;&lt;/bean&gt;&lt;!-- util 命名空间 --&gt;&lt;util:list id=&quot;...&quot;&gt; &lt;value&gt;&quot;...&quot;&lt;/value&gt;&lt;/util:list&gt; 配置的引用 @ComponentScan &lt;context:component-scan&gt; @Import(&#123;MyConfig.class, ...&#125;) &lt;bean class=&quot;MyConfig.class&quot; /&gt; @ImportResource(“classpath:my-config.xml”) &lt;import resource=&quot;my-config.xml&quot; /&gt; 处理歧义性 将可选 bean 中的某一个设为首选 primary 的 bean 或者使用限定符 qualifier 来帮助 Spring 将可选的 bean 的范围缩小到只有一个 bean。 @Primary &lt;bean id=&quot;...&quot; class=&quot;...&quot; primary=&quot;true&quot; /&gt; 注解声明为首选的 bean。 @Qualifier(&quot;[Name]&quot;) 注解与@Autowired和@Inject协同使用在注入的时候指定想要注入进去的是哪个 bean。 注解与@Component和@Bean协同使用创建自定义限定符。 bean 的 id 发生改变则装配失败？创建自定义限定符可以解决此问题。 可以创建自定义的限定符注解，解决 Java 不允许在同一个条目上重复出现相同类型的多个注解的规则。 1234@Target(&#123;ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifierpublic @interface Wingo &#123;&#125; bean 的作用域 在默认情况下 Spring 应用上下文中所有 bean 都是作为以单例 singleton 的形式创建的。也就是说不管给定的一个 bean 被注入到其他 bean 多少次每次所注入的都是同一个实例。 单例 Singleton 在整个应用中只创建 bean 的一个实例； 原型 Prototype 每次注入或者通过 Spring 应用上下文获取的时候都会创建一个新的 bean 实例； 会话 Session 在 Web 应用中为每个会话创建一个 bean 实例； 请求 Rquest 在 Web 应用中为每个请求创建一个 bean 实例。 @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) &lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;prototype&quot; /&gt; 在典型的电子商务应用中可能会有一个 bean 代表用户的购物车。如果购物车是单例的话那么将会导致所有的用户都会向同一个购物车中添加商品。另一方面如果购物车是原型作用域的那么在应用中某一个地方往购物车中添加商品在应用的另外一个地方可能就不可用了因为在这里注入的是另外一个原型作用域的购物车。 就购物车 bean 来说会话作用域是最为合适的因为它与给定的用户关联性最大。要指定会话作用域我们可以使用@Scope注解它的使用方式与指定原型作用域是相同的。 12345@Component@Scope&#123;value=WebApplication.SCOPE_SESSION, proxyMode=ScopedProxyMode.INTERFACES&#125;public ShoppingCart cart()&#123; // ??? 这里有点不理解，咋就把一个方法申明成一个 Component 了，然后代理还是用的 INTERFACE&#125; 12345&lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;prototype&quot; &gt; &lt;!-- 默认情况下它会使用 CGLib 创建目标类的代理 --&gt; &lt;!-- 设置为 false 则生成基于接口的代理 --&gt; &lt;aop:scope-proxy proxy-target-class=&quot;false &quot;/&gt;&lt;/bean&gt; 要注意的是@Scope同时还有一个proxyMode属性它被设置成了ScopedProxyMode.INTERFACES。这个属性解决了将会话或请求作用域的bean注入到单例bean中所遇到的问题。 因为StoreService是一个单例的bean会在Spring应用上下文加载的时候创建。当它创建的时候Spring会试图将ShoppingCart bean注入到setShoppingCart()方法中。但是ShoppingCartbean是会话作用域的此时并不存在。直到某个用户进入系统创建了会话之后才会出现ShoppingCart实例。 另外系统中将会有多个ShoppingCart实例每个用户一个。我们并不想让 Spring 注入某个固定的ShoppingCart实例到StoreService中。我们希望的是当StoreService处理购物车功能时它所使用的ShoppingCart实例恰好是当前会话所对应的那一个。 请求作用域的 bean 会面临相同的装配问题。因此请求作用域的 bean 应该也以作用域代理的方式进行注入。 Spring 并不会将实际的ShoppingCart bean 注入到 StoreService 中，Spring 会注入一个到ShoppingCart bean 的代理。这个代理会暴露与ShoppingCart相同的方法所以StoreService会认为它就是一个购物车。但是当StoreService调用ShoppingCart的方法时代理会对其进行懒解析并将调用委托给会话作用域内真正的ShoppingCart bean。 如果ShoppingCart是接口而不是类的话这是可以的也是最为理想的代理模式。但如果ShoppingCart是一个具体的类的话 Spring 就没有办法创建基于接口的代理了。此时它必须使用 CGLib 来生成基于类的代理。所以如果 bean 类型是具体类的话我们必须要将proxyMode属性设置为ScopedProxyMode.TARGET_CLASS以此来表明要以生成目标类扩展的方式创建代理。 运行时值注入 属性占位符 Property placeholder； Spring 表达式语言 SpEL。 @PropertySource(&quot;classpath:app.properties&quot;) 注解声明属性源并通过 Spring 的Environment来检索属性。 @Value(&quot;$&#123;xxx.xxx&#125;&quot;) 注解配合解析属性占位符使用。 为了使用占位符我们必须要配置一个PropertySourcesPlaceholderConfigurer bean。 1234@Beanpublic static PropertysourcesPlaceholderConfigurer placeholderConfigurer() &#123; return new PropertySourcesPlaceholderConfigurer();&#125; 123&lt;beans&gt; &lt;context:property-placeholder /&gt;&lt;/beans&gt; 解析外部属性能够将值的处理推迟到运行时但是它的关注点在于根据名称解析来自于 Spring Environment和属性源的属性。而 Spring 表达式语言提供了一种更通用的方式在运行时计算所要注入的值。 Spring 3 引入了 Spring 表达式语言 Spring Expression Language，SpEL 它能够以一种强大和简洁的方式将值装配到 bean 属性和构造器参数中，在这个过程中所使用的表达式会在运行时计算得到值。 在 XML 配置中你可以将 SpEL 表达式传入&lt;property&gt;或```的 value 属性中或者将其作为 p- 命名空间或 c- 命名空间条目的值。 #&#123;&#125; @Value(&quot;#&#123;ststemProperties['xxx.xxx']&#125;&quot;) 使用 bean 的 ID 来引用 bean； 调用方法和访问对象的属性； 对值进行算术、关系和逻辑运算； 正则表达式匹配； 集合操作。 环境与 Profile 配置 @profile(&quot;dev&quot;) 12345678&lt;beans&gt; &lt;beans profile=&quot;dev&quot;&gt; &lt;/beans&gt; &lt;beans profile=&quot;prod&quot;&gt; &lt;/beans&gt;&lt;/beans&gt; 激活 @ActiveProfiles 123456789101112131415161718&lt;!-- web.xml --&gt;&lt;!-- 作为 Web 应用的上下文参数 --&gt;&lt;context-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 作为 DispatcherServlet 的初始化参数 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 条件化 Spring 4 引入了一个新的@Conditional注解它可以用到带有@Bean注解的方法上。如果给定的条件计算结果为true就会创建这个 bean 否则的话这个 bean 会被忽略。 123456789101112131415public class MyConfig &#123; @Bean @Conditional(MyCondition.class) // 条件化地创建 bean public ConditionBean conditionBean() &#123; return new ConditonBean(); &#125; &#125;public class Mycondition implements Condition &#123; public boolean matches (ConditionContext ctxt, AnnotatedTypeMetadata metadata)&#123; // 具体判断逻辑 &#125;&#125; 面向切面 面向切面编程的基本原理 通过 POJO 创建切面 使用@AspectJ注解 为 AspectJ 切面注入依赖 Spring 只支持方法级别的连接点，如果需要方法拦截之外的连接点拦截功能那么我们可以利用 Aspect 来补充 Spring AOP 的功能。 描述切面的常用术语有通知 advice、切点 pointcut 和连接点 join point。 引入 Introdution 允许向现有的类添加新方法或属性。 织入 Weaving 织入是把切面应用到目标对象并创建新的代理对象的过程。 编译期切面在目标类编译时被织入。这种方式需要特殊的编译器。AspectJ 的织入编译器就是以这种方式织入切面的。 类加载期切面在目标类加载到 JVM 时被织入。这种方式需要特殊的类加载器ClassLoader它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ 5 的加载时织入 load-time weavingLTW 就支持以这种方式织入切面。 运行期切面在应用运行的某个时刻被织入。一般情况下在织入切面时 AOP 容器会为目标对象动态地创建一个代理对象。Spring AOP 就是以这种方式织入切面的。 Spring 切面可以应用 5 种类型的通知： 前置通知 Before 在目标方法被调用之前调用通知功能； 后置通知 After 在目标方法完成之后调用通知此时不会关心方法的输出是什么； 返回通知 After-returning 在目标方法成功执行之后调用通知； 异常通知 After-throwing 在目标方法抛出异常后调用通知； 环绕通知Around通知包裹了被通知的方法在被通知的方法调用之前和调用之后执行自定义的行为。 代理类封装了目标类并拦截被通知方法的调用再把调用转发给真正的目标 bean。当代理拦截到方法调用时在调用目标 bean 方法之前会执行切面逻辑。 定义切面 Spring 借助 AspectJ 的切点表达式语言来定义 Spring 切面。 AspectJ 指示器 描 述 arg() 限制连接点匹配参数为指定类型的执行方法 @args() 限制连接点匹配参数由指定注解标注的执行方法 execution() 用于匹配是连接点的执行方法 this() 限制连接点匹配 AOP 代理的 bean 引用为指定类型的类 target 限制连接点匹配目标对象为指定类型的类 @target() 限制连接点匹配特定的执行对象这些对象对应的类要具有指定类型的注解 within() 限制连接点匹配指定的类型 @within() 限制连接点匹配指定注解所标注的类型当使用 Spring AOP 时方法定义在由指定的注解所标注的类里 @annotation 限定匹配带有指定注解的连接点 XML 中的切面声明。 AOP配置元素 用 途 &lt;aop:advisor&gt; 定义 AOP 通知器 &lt;aop:after&gt; 定义 AOP 后置通知不管被通知的方法是否执行成功 &lt;aop:after-returning&gt; 定义 AOP 返回通知 &lt;aop:after-throwing&gt; 定义 AOP 异常通知 &lt;aop:around&gt; 定义 AOP 环绕通知 &lt;aop:aspect&gt; 定义一个切面 &lt;aop:aspectj-autoproxy&gt; 启用 @AspectJ注解驱动的切面 &lt;aop:before&gt; 定义一个 AOP 前置通知 &lt;aop:config&gt; 顶层的 AOP 配置元素。大多数的 &lt;aop:*&gt;元素必须包含在&lt;aop:config&gt;元素内 &lt;aop:declare-parents&gt; 以透明的方式为被通知的对象引入额外的接口 &lt;aop:pointcut&gt; 定义一个切点 为了阐述 Spring 中的切面我们需要有个主题来定义切面的切点。为此我们定义一个Performance接口。 1234package concert;public interface Performance &#123; public void perform();&#125; 通知 Spring 使用 AspectJ 注解来声明通知方法。 注 解 通 知 @After 通知方法会在目标方法返回或抛出异常后调用 @AfterReturning 通知方法会在目标方法返回后调用 @AfterThrowing 通知方法会在目标方法抛出异常后调用 @Around 通知方法会将目标方法封装起来 @Before 通知方法会在目标方法调用之前执行 1234567891011@Aspectpublic class Audience &#123; @Pointcut(&quot;execution(* concert.Performance.perform(..))&quot;) public void performance()&#123;&#125; @Before(&quot;performance()&quot;) public void silenceCellPhones() &#123; System.out.println(&quot;Silencing cell phones&quot;) &#125;&#125; 12345678910@Configuration@EnableAspectJAutoProxy // 启动 AspectJ 自动代理@ComponentScanpublic class ConcertConfig &#123; @Bean public Audience audience() &#123; // 声明 Audience bean return new Audience(); &#125;&#125; 123456789101112&lt;beans&gt; &lt;context:component-scan base-package=&quot;concert&quot; /&gt; &lt;aop:aspectj-autoproxy /&gt; &lt;bean id=&quot;audience&quot; class=&quot;concert.Audiance&quot; /&gt; &lt;aop:config&gt; &lt;aop:aspect ref=&quot;audience&quot;&gt; &lt;aop:pointcut id=&quot;performance&quot; expression=&quot;execution(* concert.Performance.perform(..))&quot; /&gt; &lt;aop:before pointcut-ref=&quot;performance&quot; method=&quot;silenceCellPhones&quot; /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 环绕通知 环绕通知是最为强大的通知类型。它能够让你所编写的逻辑将被通知的目标方法完全包装起来。实际上就像在一个通知方法中同时编写前置通知和后置通知。 通知方法中可以做任何的事情，当要将控制权交给被通知的方法时它需要调用ProceedingJoinPoint的proceed()方法。 1234567891011@Around(&quot;performance()&quot;)public void watchPerformance(ProceedingJoinPoint jp) try &#123; System.out.println(&quot;Silencing cell phones&quot;); System.out.println(&quot;Taking seats&quot;); jp.proceed(); // 不调这个方法的话那么你的通知实际上会阻塞对被通知方法的调用 System.out.println(&quot;CLAP CLAP CLAP!!!&quot;); &#125; catch (Throwable e) &#123; System.out.println(&quot;Demanding a refund&quot;); &#125;&#125; 123456&lt;aop:config&gt; &lt;aop:aspect ref=&quot;audience&quot;&gt; &lt;aop:pointcut id=&quot;performance&quot; expression=&quot;execution(* concert.Performance.perform(..))&quot; /&gt; &lt;aop:around pointcut-ref=&quot;performance&quot; method=&quot;watchPerformance&quot; /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 参数化通知，使用参数化的通知来记录磁道播放的次数。 引入 如果切面能够为现有的方法增加额外的功能，为什么不能为一个对象增加新的方法呢？实际上利用被称为引入的 AOP 概念切面可以为 Spring bean 添加新方法。 Web 中的 Spring Spring 通常用来开发 Web 应用。 构建 Spring Web 映射请求到 Spring 控制器 透明地绑定表单参数 校验表单提交 使用 Spring MVC 所经历的所有站点。 请求旅程的第一站是 Spring 的DispatcherServlet。与大多数基于Java 的 Web 框架一样 Spring MVC 所有的请求都会通过一个前端控制器 front controllerServlet。前端控制器是常用的 Web 应用程序模式在这里一个单实例的 Servlet 将请求委托给应用程序的其他组件来执行实际的处理。在 Spring MVC 中DispatcherServlet就是前端控制器。 DispatcherServlet的任务是将请求发送给 Spring MVC 控制器 controller。控制器是一个用于处理请求的 Spring 组件。在典型的应用程序中可能会有多个控制器DispatcherServlet需要知道应该将请求发送给哪个控制器。所以DispatcherServlet会查询一个或多个处理器映射 handler mapping 来确定请求的下一站在哪里。处理器映射会根据请求所携带的 URL 信息来进行决策。 一旦选择了合适的控制器DispatcherServlet会将请求发送给选中的控制器。到了控制器请求会卸下其负载用户提交的信息并耐心等待控制器处理这些信息。实际上设计良好的控制器本身只处理很少甚至不处理工作而是将业务逻辑委托给一个或多个服务对象进行处理。 控制器在完成逻辑处理后通常会产生一些信息这些信息需要返回给用户并在浏览器上显示。这些信息被称为模型 model。不过仅仅给用户返回原始的信息是不够的——这些信息需要以用户友好的方式进行格式化一般会是 HTML。所以信息需要发送给一个视图 view 通常会是 JSP。 控制器所做的最后一件事就是将模型数据打包并且标示出用于渲染输出的视图名。它接下来会将请求连同模型和视图名发送回DispatcherServlet。 这样控制器就不会与特定的视图相耦合传递给DispatcherServlet的视图名并不直接表示某个特定的 JSP。实际上它甚至并不能确定视图就是JSP。相反它仅仅传递了一个逻辑名称，这个名字将会用来查找产生结果的真正视图。DispatcherServlet将会使用视图解析器 view resolver 来将逻辑视图名匹配为一个特定的视图实现，它可能是也可能不是 JSP。 既然DispatcherServlet已经知道由哪个视图渲染结果，那请求的任务基本上也就完成了。它的最后一站是视图的实现，可能是 JSP，在这里它交付模型数据。请求的任务就完成了。视图将使用模型数据渲染输出，这个输出会通过响应对象传递给客户端。 DispatcherServlet 此 Servlet 为 Spring MVC 的核心类。 按照传统的方式像DispatcherServlet这样的 Servlet 会配置在 web.xml 文件中这个文件会放到应用的 WAR 包里面。当然这是配置DispatcherServlet的方法之一。但是借助于 Servlet 3 规范和 Spring 3.1 的功能增强可以使用 Java 将DispatcherServlet配置在 Servlet 容器中而不会再使用 web.xml 文件。 12345678910111213141516171819// 配置 DispatcherServletpackage spittr.config;import org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;public class SpittrWebAppInitializer extends AbstractAnnotationConfigDispactcherServletInitializer &#123; @Override protected String[] getServletMappings() &#123; // 将 DispatcherServlet 映射到 “/” return new String[] &#123;&quot;/&quot;&#125;; &#125; // 定义拦截器 ContextLoaderListener 应用上下文的 beans @Override protected class&lt;?&gt;[] getRootConfigClasses() &#123; return new class&lt;?&gt;[] &#123;RootConfig.class&#125;; &#125; // 定义 DispatcherServlet 应用上下文的 beans @Override protected class&lt;?&gt;[] getServletConfigClasses() &#123; // 指定配置类 return new class&lt;?&gt;[] &#123;WebConfig.class&#125;; &#125;&#125; 扩展AbstractAnnotation-ConfigDispatcherServletInitializer的任意类都会自动地配置Dispatcher-Servlet和 Spring 应用上下文，Spring 的应用上下文会位于应用程序的 Servlet 上下文之中。 最小但可用的 Spring MVC 配置。 12345678910111213141516171819@Configuration@EnableWebMvc // 启用 Spring MVC@ComponentScan(&quot;spittr.web&quot;) // 开启组件扫描 @Controllerpublic class WebConfig extends WebMvcConfigurerAdapter &#123; @Bean public ViewResolver viewResolver() &#123; // 配置 JSP 视图解析器 InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(&quot;/WEB-INF/views/&quot;); resolver.setSuffix(&quot;.jsp&quot;); resolver.setExposeContextBeansAsAttributes(true); return resolver; &#125; @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); // 配置静态资源的处理 &#125;&#125; 12345678@Configuration@ComponentScan(basePackages=&#123;&quot;spitter&quot;&#125;, excludeFilter=&#123; @Filter&#123;type=FilterType.ANNOTATION, value=EnbaleWebMVC.class&#125; &#125;)public class RootConfig &#123;&#125; 添加其它组件 除了DispatcherServlet以外，项目可能还需要额外的 Servlet 和 Filter ，并且可能还需要对DispatcherServlet本身做一些额外的配置。 在AbstractAnnotation-ConfigDispatcherServletInitializer将DispatcherServlet注册到Servlet容器中之后，就会调用customizeRegistration()并将Servlet注册后得到的Registration.Dynamic传递进来。通过重载customizeRegistration()方法我们可以对DispatcherServlet进行额外的配置。 如果我们想往 Web 容器中注册其他组件的话，只需创建一个新的初始化器就可以了。最简单的方式就是实现 Spring 的WebApplicationInitializer接口。 12345678910111213public class MyServletInitializer extends WebApplicationInitializer &#123; @Override public void onStartup(ServletContext servletContext) throws ServletException &#123; // 添加自定义 servlet Dynamic myServlet = servlectContext.addServlet(&quot;myServlet&quot;, MyServlet.class); myServlect.addMapping(&quot;/custom/**&quot;); // 添加自定义 Filter javax.servlet.FilterRegistration.Dynamic filter = servletContext.addFilter(&quot;myFilter&quot;, MyFilter.class); filter.addMappingForUrlPatterns(null, false, &quot;/custom/**&quot;); &#125;&#125; 源码片段： 1234// 寻找任何继承了 WebApplicationInitializer 接口的类并用其来配置 servlet 容器for (WebApplicationInitializer initializer : initializers) &#123; initializer.onStartup(servletContext);&#125; 编写基本控制器 @Controller 注解辅助实现组件扫描，与@Component注解所实现的效果是一样的，但是在表意性上可能会差一些。 @RequestMapping(value=&quot;/&quot;, method=GET) 注解绑定请求：value属性指定了这个方法所要处理的请求路径，method属性细化了它所处理的 HTTP 方法。 控制器方法的 Model 参数实际上就是一个 Map，也就是 key-value 对的集合，它会传递给视图，这样数据就能渲染到客户端了。 12(Model model)(Map mode) 传参 Spring MVC 允许以多种方式将客户端中的数据传送到控制器的处理器方法中。 查询参数 Query Parameter； 表单参数 Form Parameter； 路径变量 Path Variable。 @RequrearParam(&quot;ParamName&quot;) [URL]?ParamName=xxx 注解用于参数。 @RequestMapping(value=&quot;/&#123;ParamName&#125;&quot;, method=GET) @PathVariable&#123;&quot;ParamName&quot;&#125; [URL]/xxx 表单 POST 请求的控制器方法参数可直接使用对象，Spring MVC 将会使用请求中同名的参数进行填充。 校验 从 Spring 3.0 开始在 Spring MVC 中提供了对 Java 校验 API 的支持。 Java 校验 API 所提供的校验注解。 注 解 描 述 @AssertFalse 所注解的元素必须是Boolean类型并且值为 false @AssertTrue 所注解的元素必须是Boolean类型并且值为 true @DecimalMax 所注解的元素必须是数字并且它的值要小于或等于给定的 BigDecimalString值 @DecimalMin 所注解的元素必须是数字并且它的值要大于或等于给定的 BigDecimalString值 @Digits 所注解的元素必须是数字并且它的值必须有指定的位数 @Future 所注解的元素的值必须是一个将来的日期 @Max 所注解的元素必须是数字并且它的值要小于或等于给定的值 @Min 所注解的元素必须是数字并且它的值要大于或等于给定的值 @NotNull 所注解元素的值必须不能为 null @Null 所注解元素的值必须为 null @Past 所注解的元素的值必须是一个已过去的日期 @Pattern 所注解的元素的值必须匹配给定的正则表达式 @Size 所注解的元素的值必须是 String、集合或数组并且它的长度要符合给定的范围 @valid 注解用于参数，添加了@Valid注解这会告知Spring需要确保这个对象满足校验限制。 属性上添加校验限制并不能阻止表单提交，如果有校验出现错误的话那么这些错误可以通过Errors对象进行访问，Errors对象可作为控制器方法的一个参数。首次调用Errors.hasErrors()来检查是否有错误，之后再进行业务逻辑处理。 渲染 Web 视图 将模型数据渲染为 HTML 使用 JSP 视图 通过 tiles 定义视图布局 使用 Thymeleaf 视图 Spring 自带了 13 个视图解析器能够将逻辑视图名转换为物理实现。 视图解析器 描 述 BeanNameViewResolver 将视图解析为 Spring 应用上下文中的 bean 其中 bean 的 ID 与视图的名字相同 ContentNegotiatingViewResolver 通过考虑客户端需要的内容类型来解析视图委托给另外一个能够产生对应内容类 型的视图解析器 FreeMarkerViewResolver 将视图解析为 FreeMarker 模板 InternalResourceViewResolver 将视图解析为 Web 应用的内部资源一般为 JSP JasperReportsViewResolver 将视图解析为 JasperReports 定义 ResourceBundleViewResolver 将视图解析为资源 bundle 一般为属性文件 TilesViewResolver 将视图解析为 Apache Tile 定义其中 tile ID 与视图名称相同。注意有两个不同的 TilesViewResolver实现分别对应于 Tiles 2.0 和 Tiles 3.0 UrlBasedViewResolver 直接根据视图的名称解析视图视图的名称会匹配一个物理视图的定义 VelocityLayoutViewResolver 将视图解析为 Velocity 布局从不同的 Velocity 模板中组合页面 VelocityViewResolver 将视图解析为 Velocity 模板 XmlViewResolver 将视图解析为特定 XML 文件中的 bean 定义。类似于BeanName-ViewResolver XsltViewResolver 将视图解析为 XSLT 转换后的结果 配置适用于 JSP 的视图解析器InternalResourceViewResolver。它遵循一种约定会在视图名上添加前缀和后缀进而确定一个 Web 应用中视图资源的物理路径。 12345678@Beanpublic ViewResolver viewResolver() &#123; // 配置 JSP 视图解析器 InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(&quot;/WEB-INF/views/&quot;); resolver.setSuffix(&quot;.jsp&quot;); resolver.serViewClass(InternalResourceViewResolver) // 解析 JSTL 视图 return resolver;&#125; 1234&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot; p:prefix=&quot;/WEB-INF/views&quot; p:suffix=&quot;.jsp&quot; p:viewClass=&quot;InternalResourceViewResolver&quot; /&gt; 解析 JSTL 视图，使用 Spring 的 JSP 库。 借助 Spring 表单绑定标签库中所包含的标签我们能够将模型对象绑定到渲染后的 HTML 表单中。 12&lt;!-- 为了使用表单绑定库需要在JSP页面中对其进行声明 --&gt;&lt;%@taglib prefix=&quot;sf&quot; uri=&quot;http://www.springframework.org/tags/form&quot; %&gt; JSP标签 描 述 &lt;sf:checkbox&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为checkbox &lt;sf:checkboxes&gt; 渲染成多个 HTML &lt;input&gt;标签其中type属性设置为checkbox &lt;sf:errors&gt; 在一个 HTML &lt;span&gt;中渲染输入域的错误 &lt;sf:form&gt; 渲染成一个 HTML &lt;form&gt;标签并为其内部标签暴露绑定路径用于数据绑定 &lt;sf:hidden&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为hidden &lt;sf:input&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为text &lt;sf:label&gt; 渲染成一个 HTML &lt;label&gt;标签 &lt;sf:option&gt; 渲染成一个 HTML &lt;option&gt;标签其selected属性根据所绑定的值进行设置 &lt;sf:options&gt; 按照绑定的集合、数组或 Map 渲染成一个HTML &lt;option&gt;标签的列表 &lt;sf:password&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为password &lt;sf:radiobutton&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为radio &lt;sf:radiobuttons&gt; 渲染成多个 HTML &lt;input&gt;标签其中type属性设置为radio &lt;sf:select&gt; 渲染为一个 HTML &lt;select&gt;标签 &lt;sf:textarea&gt; 渲染为一个 HTML &lt;textarea&gt;标签 ThymeLeaf Thymeleaf 模板是原生的,不依赖于标签库。它能在接受原始 HTML 的地方进行编辑和渲染。因为它没有与 Servlet 规范耦合，因此 Thymeleaf 模板能够进入 JSP 所无法涉足的领域。 ThymeleafViewResolver将逻辑视图名称解析为 Thymeleaf 模板视图； SpringTemplateEngine处理模板并渲染结果； TemplateResolver加载 Thymeleaf 模板。 12345678910111213141516171819202122232425// thymeleaf 视图解析器@Beanpublic ViewResolver viewResolver(TemplateEngine templateEngine)&#123; ThymeleafViewResolver viewResolver = new ThymeleafViewResolver(); viewResolver.setTemplateEngine(templateEngine); return viewResolver;&#125;// 模板引擎@Beanpublic TemplateEngine templateEngine(TemplateResolver templateResolver)&#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); return templateEngine;&#125;// 模板解析器@Beanpublic ITemplateResolver templateResolver()&#123; SpringResourceTemplateResolver templateResolver = new SpringResourceTemplateResolver(); templateResolver.setPrefix(&quot;/WEB-INF/templates/&quot;); templateResolver.setSuffix(&quot;.html&quot;); templateResolver.setTemplateMode(&quot;HTML5&quot;) return templateResolver;&#125; Thymeleaf 教程 文件上传 一般表单提交所形成的请求结果是很简单的，就是以&amp;符分割的多个 name-value 对。 尽管这种编码形式很简单，并且对于典型的基于文本的表单提交也足够满足要求，但是对于传送二进制数据，如上传图片，就显得力不从心了。与之不同的是，multipart 格式的数据会将一个表单拆分为多个部分（part），每个部分对应一个输入域。在一般的表单输入域中，它所对应的部分中会放置文本型数据，但是如果上传文件的话，它所对应的部分可以是二进制。 在编写控制器方法处理文件上传之前我们必须要配置一个 multipart 解析器通过它来告诉DispatcherServlet该如何读取 multipart 请求。 1234@Beanpublic MultipartResolver multipartResolver() throws IOException &#123; return new StandardServletMultipartResolver();&#125; 在 Servlet registration 上调用setMultipartConfig()方法传入一个MultipartConfig-Element实例。 12345DispatcherServlet ds = new DispatchServlet();Dynamic registration = context.addServlet(&quot;appServlet&quot;, ds);registration.addMapping(&quot;/&quot;);// 将临时路径设置为 /tmp/spittr/uploadsregistration.setMultipartConfig(new MultipartConfigElement(&quot;/tmp/spittr/uploads&quot;)); 如果我们配置DispatcherServlet的Servlet初始化类继承了Abstract AnnotationConfigDispatcherServletInitializer或AbstractDispatcher-ServletInitializer的话那么我们不会直接创建DispatcherServlet实例并将其注册到 Servlet 上下文中。这样的话将不会有对Dynamic Servlet registration 的引用供我们使用了。但是我们可以通过重载customizeRegistration()方法它会得到一个Dynamic作为参数来配置 multipart 的具体细节。 123456@Overrideprotected void customizeRegistration(Dynamic registration) &#123; registration.setMultipartConfig( new MultipartConfigElement(&quot;/tmp/spittr/uploads&quot;, 2097152, 4194304, 0); );&#125; 123456789101112&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatchServlet &lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;multipart-config&gt; &lt;location&gt;/tmp/spittr/upload&lt;/location&gt; &lt;max-file-size&gt;2097152&lt;/max-file-size&gt; &lt;max-request-size&gt;4194304&lt;/max-request-size&gt; &lt;/multipart-config&gt;&lt;/servlet&gt; 对于非 Servlet 3.0 环境，Spring 内置了 CommonsMultipartResolver，可以作为 StandardServletMultipartResolver 的替代方案。 @RequestPart 注解指定用于接收请求中对应 part 的数组。 1234&lt;form method=&quot;POST&quot; th:object=&quot;$&#123;spitter&#125;&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;label&gt;Profile Picture&lt;/label&gt;: &lt;input type=&quot;file&quot; name=&quot;profilePicture&quot; accept=&quot;image/jpeg,image/png,image/gif&quot; /&gt;&lt;br/&gt;&lt;/form&gt; 1234567@RequestMapping(value=&quot;/register&quot;, method=POST)public String processRegistration( @RequestPart(&quot;profilePicture&quot;) byte[] profilePicture, @Valid Spittr spittr, Errors errors) &#123; // 将文件保存到某个位置&#125; 使用上传文件的原始byte比较简单但是功能有限。因此 Spring 还提供了MultipartFile接口它为处理 multipart 数据提供了内容更为丰富的对象。 12345678910111213141516package org.springframework.web.multipart;import java.io.File;import java.io.IOException;import java.io.InputStream;public interface MultipartFile &#123; String getName(); String getOriginalFilename(); String getContentType(); boolean isEmpty(); long getSize(); byte[] getBytes() throws IOException; InputStream getInputStream() throws IOException; void transferTo(File dest) throws IOException;&#125; 12// 将上传的文件写入到文件系统中的便捷方法profilePicture.tranferTo(new File(&quot;/data/spittr/&quot; + profilePicture.getOriginalFilename())); 如将应用部署到 Servlet 3.0 的容器中，那么会有MultipartFile的一个替代方案。Spring MVC 也能接受javax.servlet.http.Part作为控制器方法的参数。 Part 接口与MultipartFile并没有太大的差别，Part 接口中的一些方法其实是与MultipartFile相对应的。 异常处理 特定的 Spring 异常将会自动映射为指定的 HTTP 状态码； 异常上可以添加@ResponseStatus注解从而将其映射为某一个 HTTP 状态码； 在方法上可以添加@ExceptionHandler注解使其用来处理异常。 Spring 的一些异常会默认映射为 HTTP 状态码。 Spring异常 HTTP状态码 BindException 400 - Bad Request ConversionNotSupportedException 500 - Internal Server Error HttpMediaTypeNotAcceptableException 406 - Not Acceptable HttpMediaTypeNotSupportedException 415 - Unsupported Media Type HttpMessageNotReadableException 400 - Bad Request HttpMessageNotWritableException 500 - Internal Server Error HttpRequestMethodNotSupportedException 405 - Method Not Allowed MethodArgumentNotValidException 400 - Bad Request MissingServletRequestParameterException 400 - Bad Request MissingServletRequestPartException 400 - Bad Request NoSuchRequestHandlingMethodException 404 - Not Found TypeMismatchException 400 - Bad Request 1234567891011@RequestMapping(value=&quot;/&#123;spittleId&#125;&quot;, method=RequestMethod.GET)public String spittle( @PathVariable(&quot;spittleId&quot;) long spittleId, Model model) &#123; Spittle spittle = spittleRepository.findOne(spittleId); if (spittle == null) &#123; // 若未找到此对象则抛出异常 throw new SpittleNotFoundException(); &#125; model.addAttribute(spittle); return &quot;spittle&quot;;&#125; 12345678package spittr.web;import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.ResponseStatus;@ResponseStatus(value=HttpStatus.NOT_FOUND, reason=&quot;Spittle Not Found&quot;)public class SpittleNotFoundException extends RuntimeException &#123;&#125; 在很多的场景下，将异常映射为状态码是很简单的方案，并且就功能来说也足够了。但是如果想在响应中不仅要包括状态码，还要包含所产生的错误，那该怎么办呢？此时的话，我们就不能将异常视为 HTTP 错误了，而是要按照处理请求的方式来处理异常了。 123456789101112// 在处理请求的方法中直接处理异常@RequestMapping(method=RequestMethod.POST)public String saveSpittle(SpittleForm form, Model model) &#123; try &#123; spittleRepository.save( new Spittle(null, form.getMessage(), new Date(), form.getLongitude(), form.getLatitude()) ); return &quot;redirect:/spittles&quot;; &#125; catch (DuplicateSpittleException e) &#123; return &quot;error/duplicate&quot;; &#125;&#125; 它运行起来没什么问题，但是这个方法有些复杂。该方法可以有两个路径，每个路径会有不同的输出。如果能让saveSpittle()方法只关注正确的路径，而让其他方法处理异常的话，那么它就能简单一些。 123456789101112@RequestMapping(method=RequestMethod.POST)public String saveSpittle(SpittleForm form, Model model) &#123; spittleRepository.save( new Spittle(null, form.getMessage(), new Date(), form.getLongitude(), form.getLatitude()) ); return &quot;redirect:/spittles&quot;;&#125;@ExceptionHandler(DuplicateSpittleException.class)public String handleDuplicateSpittle() &#123; return &quot;error/duplicate&quot;;&#125; 如果要在多个控制器中处理异常那@ExceptionHandler注解所标注的方法是很有用的。不过如果多个控制器类中都会抛出某个特定的异常那么你可能会发现要在所有的控制器方法中重复相同的@ExceptionHandler方法。或者为了避免重复我们会创建一个基础的控制器类所有控制器类要扩展这个类从而继承通用的@ExceptionHandler方法。 Spring 3.2 为这类问题引入了一个新的解决方案控制器通知。控制器通知 controller advice 是任意带有@ControllerAdvice注解的类这个类会包含一个或多个如下类型的方法 @ExceptionHandler注解标注的方法； @InitBinder注解标注的方法； @ModelAttribute注解标注的方法。 1234567@ControllerAdvicepublic class AppWideExceptionHandler &#123; @ExceptionHandler(DuplicateSpittleException.class) public String handleNotFound() &#123; return &quot;error/duplicate&quot;; &#125;&#125; 重定向 在处理完 POST 请求后通常来讲一个最佳实践就是执行一下重定向。除了其他的一些因素外，这样做能够防止用户点击浏览器的刷新按钮或后退箭头时客户端重新执行危险的 POST 请求。 当控制器方法返回的String值以“redirect:”开头的话那么这个String不是用来查找视图的而是用来指导浏览器进行重定向的路径。 1return &quot;redirect:/spitter/&quot; + spitter.getUsername(); 模型的属性是以请求属性的形式存放在请求中的在重定向后无法存活。 使用 URL 模板以路径变量和 / 或查询参数的形式传递数据； 通过 flash 属性发送数据。 123456@RequestMapping(value=&quot;/register&quot;, method=POST);public String processRegistration(Spitter spitter, Model model) &#123; spitterRepository.save(spitter); model.addAttribute(&quot;username&quot;, spitter.getUsername()); return &quot;redirect:/spitter/&#123;username&#125;&quot;;&#125; 除此之外，模型中所有其他的原始类型值都可以添加到 URL 中作为查询参数。 1234567@RequestMapping(value=&quot;/register&quot;, method=POST)public String processRegistration(Spitter spitter, Model model) &#123; spitterRepository.save(spitter); model.addAttribute(&quot;username&quot;, spitter.getUsername()); model.addAttribute(&quot;spitterId&quot;, spitter.getId()); // 会以查询参数的形式进行重定向 return &quot;redirect:/spitter/&#123;username&#125;&quot;;&#125; 如果 username 属性的值是 habuma 并且 spitterId 属性的值是 42，那么结果得到的重定向 URL 路径将会是 /spitter/habuma?spitterId=42。 在 URL 中，并没有办法发送更为复杂的值，但这正是flash属性能够提供帮助的领域。 Spring 提供了通过RedirectAttributes设置 flash 属性的方法，这是 Spring 3.1 引入的Model的一个子接口。RedirectAttributes提供了Model的所有功能除此之外还有几个方法是用来设置 flash 属性的。 1234567@RequestMapping(value=&quot;/register&quot;, method=POST)public String processRegistration(Spitter spitter, RedirectAttribute model) &#123; spitterRespository.save(spitter); model.addAttribute(&quot;username&quot;, spitter.getUsername()); model.addFlashAttribute(&quot;spitter&quot;, spitter); // flash 属性 return &quot;redirect:/spitter/&#123;username&#125;&quot;;&#125; 安全 Spring Security 介绍； 使用 Servlet 规范中的 Filter 保护Web应用； 基于数据库和 LDAP 进行认证。 Spring Security 是为基于 Spring 的应用程序提供声明式安全保护的安全性框架。Spring Security 提供了完整的安全性解决方案，能够在 Web 请求级别和方法调用级别处理身份认证和授权。因为基于 Spring 框架所以 Spring Security 充分利用了依赖注入 dependency injectionDI 和面向切面的技术。 Spring Security 从两个角度来解决安全性问题。它使用 Servlet 规范中的 Filter 保护 Web 请求并限制 URL 级别的访问。Spring Security 还能够使用 Spring AOP 保护方法调用——借助于对象代理和使用通知能够确保只有具备适当权限的用户才能访问安全保护的方法。 Spring Security 被分成了11个模块。 模 块 描 述 ACL 支持通过访问控制列表 access control list 为域对象提供安全性 Aspects 当使用 Spring Security 注解时，会使用基于 AspectJ 的切面，而不是使用标准的 Spring AOP CAS Client 提供与 Jasig 的中心认证服务 Central Authentication Service 进行集成的功能 Configuration 包含通过 XML 和 Java 配置 Spring Security 的功能支持 Core 提供 Spring Security 基本库 Cryptography 提供了加密和密码编码的功能 LDAP 支持基于 LDAP 进行认证 OpenID 支持使用 OpenID 进行集中式认证 Remoting 提供了对 Spring Remoting 的支持 Tag Library Spring Security 的 JSP 标签库 Web 提供了 Spring Security 基于 Filter 的 Web 安全性支持 Spring Security 借助一系列 Servlet Filter 来提供各种安全性功能。这是否意味着我们需要在 web.xml 或WebApplicationInitializer中配置多个 Filter 呢？实际上借助于 Spring 的小技巧我们只需配置一个 Filter 就可以了。 DelegatingFilterProxy是一个特殊的 Servlet Filter 它本身所做的工作并不多。只是将工作委托给一个javax.servlet.Filter实现类这个实现类作为一个&lt;bean&gt;注册在 Spring 应用的上下文。 不管我们通过 web.xml 还是通过AbstractSecurityWebApplicationInitializer的子类来配置DelegatingFilterProxy它都会拦截发往应用中的请求并将请求委托给ID为springSecurityFilterChain bean。 springSecurityFilterChain本身是另一个特殊的 Filter，它也被称为FilterChainProxy。它可以链接任意一个或多个其他的 Filter。Spring Security 依赖一系列 Servlet Filter 来提供不同的安全特性。不需要知道这些细节因为开发中不需要显式声明springSecurityFilterChain以及它所链接在一起的其他 Filter。当我们启用 Web 安全性的时候会自动创建这些Filter。 简单的安全性配置 方 法 描 述 configure(WebSecurity) 通过重载配置 Spring Security 的 Filter 链 configure(HttpSecurity) 通过重载配置如何通过拦截器保护请求 configure(AuthenticationManagerBuilder) 通过重载配置 user-detail 服务 @EnableWebSecurity注解将会启用 Web 安全功能。但它本身并没有什么用处 Spring Security 必须配置在一个实现了WebSecurityConfigurer的 bean 中或者简单起见扩展WebSecurityConfigurerAdapter。 @EnableWebSecurity可以启用任意 Web 应用的安全性功能，不过如果是使用 Spring MVC 开发的那么就应该考虑使用@EnableWebMvcSecurity`替代它。 12345678910111213@Configuration@EnableWebMvcSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(&quot;user&quot;).password(&quot;password&quot;).roles(&quot;USER&quot;) .and .withUser(&quot;user&quot;).password(&quot;password&quot;).authorities(&quot;ROLE_USER&quot;); // 等价 &#125;&#125; AuthenticationManagerBuilder有多个方法用来配置 Spring Security 对认证的支持。通过inMemoryAuthentication()方法我们可以启用、配置并任意填充基于内存的用户存储。 withUser()方法返回的是UserDetailsManagerConfigurer.UserDetailsBuilder，这个对象提供了多个进一步配置用户的方法包括设置用户密码的password()方法以及为给定用户授予一个或多个角色权限的roles()方法。 配置用户详细信息的方法。 方 法 描 述 accountExpired(boolean) 定义账号是否已经过期 accountLocked(boolean) 定义账号是否已经锁定 and() 用来连接配置 authorities(GrantedAuthority...) 授予某个用户一项或多项权限 authorities(List) 授予某个用户一项或多项权限 authorities(String...) 授予某个用户一项或多项权限 credentialsExpired(boolean) 定义凭证是否已经过期 disabled(boolean) 定义账号是否已被禁用 password(String) 定义用户的密码 roles(String...) 授予某个用户一项或多项角色 基于数据库表认证 用户数据通常会存储在关系型数据库中并通过 JDBC 进行访问。为了配置 Spring Security 使用以 JDBC 为支撑的用户存储，可以使用jdbcAuthentication()方法。 123456789101112@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .jdbcAuthentication() .dataSource(dataSource) .usersByUsernameQuery( &quot;select username, password, true &quot; + &quot;from Spitter where username=?&quot;) .authoritiesByUsernameQuery( &quot;select username, &#x27;ROLE_USER&#x27; from Spitter where username=?&quot;) .passwordEncoder(new StandardPasswordEnconder(&quot;123456&quot;));&#125; passwordEncoder()方法可以接受 Spring Security 中PasswordEncoder接口的任意实现。Spring Security 的加密模块包括了三个这样的实现BCryptPasswordEncoder、NoOpPasswordEncoder和StandardPasswordEncoder。 基于 LDAP认证 LDAP（Light Directory Access Portocol），它是基于 X.500 标准的轻量级目录访问协议。 拦截请求 适量地应用安全性。 在任何应用中并不是所有的请求都需要同等程度地保护。有些请求需要认证而另一些可能并不需要。有些请求可能只有具备特定权限的用户才能访问没有这些权限的用户会无法访问。 对每个请求进行细粒度安全性控制的关键在于重载configure(HttpSecurity)方法。 123456789@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(&quot;/spitters/me&quot;).hasAuthority(&quot;ROLE_SPITTER&quot;) // 路径可使用 Ant 风格的通配符 .antMatchers(HttpMethoed.POST, &quot;/spitters&quot;).hasAuthority(&quot;ROLE_SPITTER&quot;) // .antMatchers(HttpMethoed.POST, &quot;/spitters&quot;).hasRole(&quot;SPITTER&quot;) 同上 .anyRequest().permitAll();&#125; 用来定义如何保护路径的配置方法。 方 法 能够做什么 access(String) 如果给定的SpEL表达式计算结果为true就允许访问 anonymous() 允许匿名用户访问 authenticated() 允许认证过的用户访问 denyAll() 无条件拒绝所有访问 fullyAuthenticated() 如果用户是完整认证的话不是通过 Remember-me 功能认证的就允许访问 hasAnyAuthority(String...) 如果用户具备给定权限中的某一个的话就允许访问 hasAnyRole(String...) 如果用户具备给定角色中的某一个的话就允许访问 hasAuthority(String) 如果用户具备给定权限的话就允许访问 hasIpAddress(String) 如果请求来自给定 IP 地址的话就允许访问 hasRole(String) 如果用户具备给定角色的话就允许访问 not() 对其他访问方法的结果求反 permitAll() 无条件允许访问 rememberMe() 如果用户是通过Remember-me功能认证的就允许访问 上表中的大多数方法都是一维的，例如使用hasRole()限制某个特定的角色的同时不能在相同的路径上同时通过hasIpAddress()限制特定的 IP 地址。 借助access()方法我们也可以将 SpEL 作为声明访问限制的一种方式。 安全表达式 计 算 结 果 authentication 用户的认证对象 denyAll 结果始终为 false hasAnyRole(list of roles) 如果用户被授予了列表中任意的指定角色结果为 true hasRole(role) 如果用户被授予了指定的角色结果为 true hasIpAddress(IP Address) 如果请求来自指定IP的话结果为 true isAnonymous() 如果当前用户为匿名用户结果为 true isAuthenticated() 如果当前用户进行了认证的话结果为 true isFullyAuthenticated() 如果当前用户进行了完整认证的话，即非 Remember-me 功能进行的认证结果为 true isRememberMe() 如果当前用户是通过 Remember-me 自动认证的结果为 true permitAll 结果始终为 true principal 用户的 principal 对象 12.antMatchers(&quot;/spitter/me&quot;).access(&quot;hasRole(&#x27;ROLE_SPITTER&#x27;) and hasIpAddress(&#x27;192.168.1.2&#x27;)&quot;); // 二维认证 强制通道的安全性 对于敏感的信息，为了保证注册表单的数据通过 HTTPS 传送，可以在配置中添加requiresChannel()方法。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(&quot;/spitter/me&quot;).hasRole(&quot;SPITTER&quot;) .antMatchers(HttpMethod.POST, &quot;/spittles&quot;).hasRole(&quot;SPITTER&quot;) .anyRequest().permitAll() .and() .requeresChannel() .antMatchers(&quot;/spitter/form&quot;).requiresSecure(); // 需要 HTTPS&#125; 不论何时只要是对 “/spitter/form” 的请求，Spring Security 都视为需要安全通道通过调用requiresChannel()确定的并自动将请求重定向到 HTTPS 上。 与之相反有些页面并不需要通过 HTTPS 传送，可以使用requiresInsecure()代替requiresSecure()方法将首页声明为始终通过 HTTP 传送。 防止跨站请求伪造 如果一个站点欺骗用户提交请求到其他服务器的话就会发生 cross-site request forgery CSRF 攻击。 Spring Security 通过一个同步 token 的方式来实现 CSRF 防护的功能。它将会拦截状态变化的请求，例如非GET、HEAD、OPTIONS和TRACE的请求并检查 CSRF token。如果请求中不包含 CSRF token 的话或者 token 不能与服务器端的 token 相匹配请求将会失败并抛出CsrfException异常。 这意味着在你的应用中所有的表单必须在一个 “_csrf” 域中提交 token 而且这个 token 必须要与服务器端计算并存储的 token 一致这样的话当表单提交的时候才能进行匹配。 12345&lt;!-- Thymeleaf 只要 &lt;form&gt; 标签的 action 属性添加了 Thymeleaf 命名空间前缀则会自动生成 “_csrf” 隐藏域 --&gt;&lt;form method=&quot;POST&quot; th:action=&quot;@&#123;/spittles&#125;&quot;&gt;&lt;/form&gt;&lt;!-- JSP --&gt;&lt;input type=&quot;hidden&quot; name=&quot;$&#123;_csrf.parameterName&#125;&quot; value=&quot;$&#123;_csrf.token&#125;&quot; /&gt; 12345678// 手动禁用，不推荐@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http // ... .csrf() .disable();&#125; 认证用户页面 实际上在重写configure(HttpSecurity)之前我们都能使用一个简单却功能完备的登录页。但是一旦重写了configure(HttpSecurity)方法就失去了这个简单的登录页面。 12345678910111213@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() // 指定自定义的登录页面的访问路径 .and() .authorizeRequests() .antMatchers(&quot;/spitter/me&quot;).hasRole(&quot;SPITTER&quot;) .antMatchers(HttpMethod.POST, &quot;/spittles&quot;).hasRole(&quot;SPITTER&quot;) .anyRequest().permitAll() .and() .requeresChannel() .antMatchers(&quot;/spitter/form&quot;).requiresSecure();&#125; HTTP Basic 认证（HTTP Basic Authentication）会直接通过 HTTP 请求本身，对要访问应用程序的用户进行认证。你可能在以前见过 HTTP Basic 认证。当在 Web 浏览器中使用时，它将向用户弹出一个简单的模态对话框。 但这只是 Web 浏览器的显示方式。本质上，这是一个 HTTP 401 响应， 表明必须要在请求中包含一个用户名和密码。在 REST 客户端向它使用的服务进行认证的场景中，这种方式比较适合。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(&quot;/login&quot;) .and() .httpBasic() .realmName(&quot;Spittr&quot;) .and() // ...&#125; Spring Security 使得为应用添加 Remember-me 功能变得非常容易。为了启用这项功能只需在configure()方法所传入的HttpSecurity对象上调用rememberMe()即可。这个功能是通过在 cookie 中存储一个 token 完成的。 存储在 cookie 中的 token 包含用户名、密码、过期时间和一个私钥 —— 在写入 cookie 前都进行了 MD5 哈希。默认情况下，私钥的名为 SpringSecured，但在这里我们将其设置为 spitterKey，使它专门用于 Spittr应用。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(&quot;/login&quot;) .and() .rememberMe() .tokenValiditySeconds(2419200) // token 四周内有效 .key(&quot;spittrKey&quot;) // ...&#125; 退出功能是通过 Servlet 容器中的 Filter 实现的（默认情况下），这个 Filter 会拦截针对 “/logout” 的请求。 1&lt;a th:href=&quot;@&#123;/logout&#125;&quot;&gt;Logout&lt;/a&gt; 当用户点击这个链接的时候，会发起对 “/logout” 的请求，这个请求会被 Spring Security 的 LogoutFilter 所处理。用户会退出应用，所有的 Remember-me token 都会被清除掉。在退出完成后，用户浏览器将会 重定向到 “/login?logout”，从而允许用户进行再次登录。 如果你希望用户被重定向到其他的页面，如应用的首页，那么可以在 configure() 中进行配置。 1234567891011@overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(&quot;/login&quot;) .and() .logout() .logoutSuccessUrl(&quot;/&quot;) // 指定退出成功后的跳转页面 .logout(&quot;signout&quot;) // 重写的默认的 LogoutFilter 拦截路径 // ...&#125; 保护视图 使用 Spring Security 的 JSP 标签库。 JSP 标签 作 用 &lt;security:accesscontrollist&gt; 如果用户通过访问控制列表授予了指定的权限那么渲染该标签体中的内容 &lt;security:authentication&gt; 渲染当前用户认证对象的详细信息 &lt;security:authorize&gt; 如果用户被授予了特定的权限或者 SpEL 表达式的计算结果为 true 那么渲染该标签 体中的内容 使用&lt;security:authentication&gt; JSP 标签来访问用户的认证详情。 认 证 属 性 描 述 authorities 一组用于表示用户所授予权限的 GrantedAuthority 对象 Credentials 用于核实用户的凭证通常这会是用户的密码 details 认证的附加信息IP地址、证件序列号、会话 ID 等 principal 用户的基本信息对象 1234567891011121314&lt;sec:authorize access=&quot;hasRole(&#x27;ROLE_SPITTER&#x27;)&quot;&gt; &lt;s:url value=&quot;/spittles&quot; var=&quot;spittle_url&quot; /&gt; &lt;sf:form modelAttribute=&quot;spittle&quot; action=&quot;$&#123;spittle_url&#125;&quot;&gt; &lt;sf:label path=&quot;text&quot;&gt; &lt;s:message code=&quot;label.spittle&quot; text=&quot;Enter spittle:&quot; /&gt; &lt;/sf:label&gt; &lt;sf:textarea path=&quot;text&quot; row=&quot;2&quot; cols=&quot;40&quot; /&gt; &lt;sf:errors path=&quot;text&quot; /&gt; &lt;br/&gt; &lt;div class=&quot;spitItSubmit&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Spit it !&quot; class=&quot;status-btn round-btn disabled&quot; /&gt; &lt;/div&gt; &lt;/sf:form&gt;&lt;/sec:authorize&gt; Thymeleaf 的安全方言提供了条件化渲染和显示认证细节的能力。 属 性 作 用 sec:authentication 渲染认证对象的属性 sec:authorize 基于表达式的计算结果条件性的渲染内容 sec:authorize-acl 基于表达式的计算结果条件性的渲染内容 sec:authorize-expr sec:authorize属性的别名 sec:authorize-url 基于给定URL路径相关的安全规则条件性的渲染内容 1234567@Beanpublic SpringTemplateEngine templateEngine(TemplateResolver templateResolver) &#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); templateEngine.addDialect(new SpringSecurityDialect()); // 注册安全方言 return templateEngine;&#125; 123&lt;div sec:authorize=&quot;isAuthenticated&quot;&gt; Hello &lt;span sec:authentication=&quot;name&quot;&gt;someone&lt;/span&gt;&lt;/div&gt; 后端中的 Spring 关注 Spring 如何帮助我们在后端处理数据。 Spring 与 JDBC 定义 Spring 对数据访问的支持； 配置数据库资源； 使用 Spring 的 JDBC 模版。 为了避免持久化的逻辑分散到应用的各个组件中最好将数据访问的功能放到一个或多个专注于此项任务的组件中。这样的组件通常称为数据访问对象 data access object DAO 或 Repository。 为了避免应用与特定的数据访问策略耦合在一起编写良好的 Repository 应该以接口的方式暴露功能。 服务对象本身并不会处理数据访问而是将数据访问委托给 Repository，Repository 接口确保其与服务对象的松耦合。 Spring 将数据访问过程中固定的和可变的部分明确划分为两个不同的类模板 template 和回调 callback。模板管理过程中固定的部分而回调处理自定义的数据访问代码。 配置数据源 通过 JDBC 驱动程序定义的数据源； 通过 JNDI 查找的数据源； 连接池的数据源。 JNDI（Java Naming and Directory Interface ），类似于在一个中心注册一个东西，以后要用的时候，只需要根据名字去注册中心查找，注册中心返回你要的东西。在 web 程序中可以将一些东西（比如数据库相关的）交给服务器软件去配置和管理（有全局配置和单个 web 程序的配置），在程序代码中只要通过名称查找就能得到注册的东西，而且如果注册的东西有变，比如更换了数据库，我们只需要修改注册信息，名称不改，因此代码也不需要修改。 12345678@Beanpublic JndiObjectFactoryBean dataSource() &#123; JndiObjectFactoryBean jndiObjectFB = new JndiObjectFactoryBean(); jndiObjectFB.setJndiName(&quot;jdbc/SpittrDS&quot;); jndiObjectFB.setResourceRef(true); jndiObjectFB.setProxyInterface(javax.sql.DataSource.class); return jndiObjectFB;&#125; 使用数据源连接池。（推荐） 1234567891011@Beanpublic BasicDataSource dataSource() &#123; BasicDataSource ds = new BasicDataSource(); ds.setDriverClassName(&quot;org.h2.Driver&quot;); ds.setUrl(&quot;jdbc:h2:tcp://localhost/~/spitter&quot;); ds.setUsername(&quot;sa&quot;); ds.setPassword(&quot;&quot;); ds.setInitialSize(5); ds.setMaxActive(10); return ds;&#125; 在Spring中通过 JDBC 驱动定义数据源是最简单的配置方式。Spring 提供了三个这样的数据源类均位于org.springframework.jdbc.datasource包中供选择。 DriverManagerDataSource在每个连接请求时都会返回一个新建的连接。与 DBCP 的BasicDataSource不同由DriverManagerDataSource提供的连接并没有进行池化管理 SimpleDriverDataSource与DriverManagerDataSource的工作方式类似但是它直接使用 JDBC 驱动来解决在特定环境下的类加载问题这样的环境包括OSGi容器 SingleConnectionDataSource在每个连接请求时都会返回同一个的连接。尽管SingleConnectionDataSource不是严格意义上的连接池数据源但是你可以将其视为只有一个连接的池。 123456789@Beanpublic DataSource dataSource() &#123; DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(&quot;org.h2.Driver&quot;); ds.setUrl(&quot;jdbc:h2:tcp://localhost/~/spitter&quot;); ds.setUsername(&quot;sa&quot;); ds.setPassword(&quot;&quot;); return ds;&#125; 与具备池功能的数据源相比，唯一的区别在于这些数据源 bean 都没有提供连接池功能，所以没有可配置的池相关的属性。 Spring 的 JDBC 命名空间能够简化嵌入式数据库的配置，可以使用EmbeddedDatabaseBuilder来构建DataSource。 12345678@Beanpublic DataSource dataSource() &#123; return new EmbeddedDatabaseBuilder() .setType(EmbeddedDatabaseType.H2) .setScript(&quot;classpath:schema.sql&quot;) .setScript(&quot;classpath:text-data.sql&quot;) .build();&#125; 使用 @profile 选择数据源。 JDBC 模板 JdbcTemplate最基本的 Spring JDBC 模板，这个模板支持简单的 JDBC 数据库访问功能以及基于索引参数的查询； NamedParameterJdbcTemplate使用该模板类执行查询时可以将值以命名参数的形式绑定到 SQL 中而不是使用简单的索引参数； SimpleJdbcTemplate该模板类利用 Java 5 的一些特性如自动装箱、泛型以及可变参数列表来简化 JDBC 模板的使用。 为了让JdbcTemplate正常工作，只需要为其设置DataSource就可以了。之后将jdbcTemplate装配到 Repository 中并使用它来访问数据库就可以了。 1234@Beanpublic JdbcTemplate jdbcTemplate(DataSource dataSource) &#123; return new JdbcTemplate(dataSource);&#125; 1234567891011@Respositorypublic class JdbcSpitterRepository implements SpitterRepository &#123; // 通过注入非具体的 JdbcTemplate 达到松耦合 private JdbcOperations jdbcOperations; @Inject public JdbcSpitterRepository(JdbcOperations jdbcOperations) &#123; this.jdbcOperations= jdbcOperations; &#125; // ...&#125; ORM 持久化数据 使用 Spring 和 Hibernate； 借助上下文 Session 编写不依赖于 Spring 的 Repository； 通过 Spring 使用 JPA； 借助 Spring Data 实现自动化的 JPA Repository。 HIbernate 使用 Hibernate 所需的主要接口是org.hibernate.Session。Session接口提供了基本的数据访问功能如保存、更新、删除以及从数据库加载对象的功能。通过 Hibernate 的Session接口应用程序的 Repository 能够满足所有的持久化需求。 获取 Hibernate Session 对象的标准方式是借助于 Hibernate SessionFactory接口的实现类。除了一些其他的任务SessionFactory主要负责 Hibernate Session的打开、关闭以及管理。 JPA 在 Spring 中使用 JPA 的第一步是要在 Spring 应用上下文中将实体管理器工厂（entity manager factory）按照 bean 的形式来进行配置。 基于 JPA 的应用程序需要使用 EntityManagerFactory 的实现类来获取 EntityManager 实例。 容器管理的 JPA 采取了一个不同的方式。当运行在容器中时，可以使用容器（在我们的场景下是 Spring）提供的信息来生成 EntityManagerFactory。 12345678910111213141516171819@Beanpublic LocalContainerEntityManagerFactoryBean entityManagerFactory( DataSource dataSource, JpaVendorAdapter jpaVendorAdapter) &#123; LocalContainerEntityManagerFactoryBean emfb = new LocalContainerEntityManagerFactoryBean(); emfb.setDataSource(dataSource); emfb.setJpaVendorAdapter(jpaVendorAdapter); return emfb;&#125;@Beanpublic JpaVendorAdapter jpaVendorAdapter() &#123; // Hibernate 厂商提供的适配器 HibernateJpaVendorAdapter adapter = new HibernateJpaVendorAdapter(); adapter.setDatabase(&quot;HSQL&quot;); adapter.setShowSql(true); adapter.setGenerateDdl(false); adapter.setDatabasePlatform(&quot;org.hibernet.dialect.HSQLDialect&quot;); return adapter;&#125; 1234567public interface SpitterRepository extends JpaRepository&lt;Spitter, Long&gt; &#123;&#125;@Configuration@EnableJpaRepositories(basePackages=&quot;com.habuma.spittr.db&quot;)public class JpaConfiguration &#123;&#125; Repository 方法的命名遵循一种模式，有助于 Spring Data 生成针对数据库的查询。（主题可以省略） 12// 方法签名List&lt;Spitter&gt; readByFirstnameIgnoringCaseOrLastnameIgnoringCase(String first, String last); 声明自定义查询。 12@Query(&quot;select s from Spitter s where s.email like &#x27;%gmail.com&#x27;&quot;)List&lt;Spitter&gt; findAllGmailSpitters(); 混合自定义查询，即嵌套查询。 NoSQL 为 MongoDB 和 Neo4j 编写 Repository； 为多种数据存储形式持久化数据； 组合使用 Spring 和 Redis。 MongoDB MongoDB 是最为流行的开源文档数据库之一。Spring Data MongoDB 提供了三种方式在 Spring 应用中使用 MongoDB。 通过注解实现对象-文档映射； 使用MongoTemplate实现基于模板的数据库访问； 自动化的运行时 Repository 生成功能。 杂项 基于LDAP进行认证使用 Apache Common Lang 包来实现equals()和hashCode()方法。 12345678@Overridepublic boolean equals(Object that) &#123; return EqualsBuilder.reflectionEquals(this, that, &quot;id&quot;, &quot;time&quot;);&#125;@Overridepublic int hashCode() &#123; return HashCodeBuilder.reflectionHashCode(this, &quot;id&quot;, &quot;time&quot;);&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"}]},{"title":"RabbitMQ 初步使用","slug":"后台技术/RabbitMQ 初步使用","date":"2020-04-20T06:02:25.000Z","updated":"2023-09-20T07:00:03.774Z","comments":true,"path":"2020/04/20/后台技术/RabbitMQ 初步使用/","link":"","permalink":"https://wingowen.github.io/2020/04/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/RabbitMQ%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"RabbitMQ 的简单介绍及项目使用。","text":"RabbitMQ 的简单介绍及项目使用。 在虚拟机中基于 Docker 安装 RabbitMQ 镜像，启动镜像并暴露其端口。 1docker run -d -p 5672:5672 -p 15672:15672 --hostname my_rabbitmq --name rabbitmq rabbitmq:management 访问 [虚拟机 IP]:15672 进入 RabbitMQ 的控制台页面，默认账号密码皆为 guest。 基本概念 高级消息队列协议，即 Advanced Message Queuing Protocol（AMQP）是面向消息中间件提供的开放的应用层协议，其设计目标是对于消息的排序、路由（包括点对点和订阅-发布）、保持可靠性、保证安全性。 RabbitMQ 实现了 AMQP。 RabbitMQ 的基本使用流程： 客户端连接到消息队列服务器，打开一个 channel； 客户端声明一个 exchange，并设置相关属性； 客户端声明一个 queue，并设置相关属性； 客户端使用 routing key，在 exchange 和 queue 之间建立好绑定关系； 客户端投递消息到 exchange。 Broker：简单来说就是消息队列服务器实体。 Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。 Queue：消息队列载体，每个消息都会被投入到一个或多个队列。 Binding：绑定，它的作用就是把 exchange 和 queue 按照路由规则绑定起来。 Routing Key：路由关键字，exchange 根据这个关键字进行消息投递。 vhost：虚拟主机，一个 broker 里可以开设多个 vhost，用作不同用户的权限分离。 producer：消息生产者，就是投递消息的程序。 consumer：消息消费者，就是接受消息的程序。 channel：消息通道，在客户端的每个连接里，可建立多个 channel，每个 channel 代表一个会话任务。 简单示例 编写一个基本的 consumer 以及 provider 进行消息的接受与投递。 simple-consumer 编写一个简单的消费者连接到 RabbitMQ，并且初始化所需的 Exchange、Queue、Routing Key、Binding 等。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 123456789spring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000spring.rabbitmq.listener.simple.acknowledge-mode=manualspring.rabbitmq.listener.simple.concurrency=5spring.rabbitmq.listener.simple.max-concurrency=10 Receiver 通过 @RabbitListener进行初始化并绑定：可在配置文件中编写后通过$&#123;xxx.xxx...&#125;进行取值。 ！！！注意！！！此处的 Order 类依赖于 provider 模块的 Order 类。 1234567891011121314151617181920212223@Componentpublic class RabbitReceiver &#123; @RabbitListener( bindings = @QueueBinding( value = @Queue(value = &quot;queue-1&quot;, durable=&quot;true&quot;), exchange = @Exchange(value = &quot;exchange-1&quot;, durable = &quot;true&quot;, type = &quot;topic&quot;, ignoreDeclarationExceptions = &quot;true&quot;), key = &quot;springboot.*&quot; ) ) @RabbitHandler public void onOrderMessage(@Payload Order order, Channel channel, @Headers Map&lt;String, Object&gt; headers) throws Exception &#123; System.err.println(&quot;--------------------------------------&quot;); System.err.println(&quot;消费端 Order: &quot; + order.getId()); Long deliveryTag = (Long)headers.get(AmqpHeaders.DELIVERY_TAG); //手工ACK channel.basicAck(deliveryTag, false); &#125;&#125; 运行项目。 提示：连接成功。打开 RabbitMQ 控制台。 可以看到有一条来自本地端口 9371 的连接。 查看 Exchange 可以看到客户端所绑定的规则已经被自动初始化（当然也可以选择手工绑定） simple-provider 编写一个简单的提供者连接到 RabbitMQ，并发送消息到对应的队列中。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910spring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000# 启用回调spring.rabbitmq.publisher-confirm-type=correlatedspring.rabbitmq.publisher-returns=truespring.rabbitmq.template.mandatory=true 新建一个实体对象用于发送。 12345678910public class Order implements Serializable &#123; private static final long serialVersionUID = -3603050220195298978L; private String id; private String name; // ...&#125; Sender 编写发送方法，并自定义回调形式。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class RabbitSender &#123; @Autowired private RabbitTemplate rabbitTemplate; // 回调函数: confirm 确认 final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.err.println(&quot;correlationData: &quot; + correlationData); System.err.println(&quot;ack: &quot; + ack); if(!ack)&#123; System.err.println(&quot;异常处理....&quot;); &#125; &#125; &#125;; // 回调函数: return 返回 final RabbitTemplate.ReturnCallback returnCallback = new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(org.springframework.amqp.core.Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.err.println(&quot;return exchange: &quot; + exchange + &quot;, routingKey: &quot; + routingKey + &quot;, replyCode: &quot; + replyCode + &quot;, replyText: &quot; + replyText); &#125; &#125;; // 发送消息方法调用: 构建自定义对象消息 public void sendOrder(Order order) throws Exception &#123; rabbitTemplate.setConfirmCallback(confirmCallback); rabbitTemplate.setReturnCallback(returnCallback); // id + 时间戳 全局唯一 CorrelationData correlationData = new CorrelationData(&quot;987654321&quot;); rabbitTemplate.convertAndSend(&quot;exchange-1&quot;, &quot;springboot.order&quot;, order, correlationData); &#125;&#125; 编写测试方法进行测试。 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestclass SimpleProviderApplicationTests &#123; @Autowired private RabbitSender rabbitSender; @Test public void testSender() throws Exception &#123; Order order = new Order(&quot;001&quot;, &quot;第一个订单&quot;); rabbitSender.sendOrder(order); &#125;&#125; 控制台打印了了自定义回调的确认信息。 RabbitMQ 控制台可以查看到发送的消息的详细信息。 启动消费者后，消费者监听到队列中有消息并了进行消费。 可靠交付 可靠交付即将 RabbitMQ 处理订单的状态持久化到数据库中，并根据此状态表对不同状态的交付任务进行处理： 1）在达到规定的最大重试次数之前，定时任务在规定的时间间隔自动进行任务的重新交付； 2）任务达到了最大的重试次数后，为避免浪费资源将其状态更新为失败，之后人工处理。 12345678910111213141516171819-- 表 my_order 订单结构CREATE TABLE IF NOT EXISTS `my_order` ( `id` varchar(128) NOT NULL, -- 订单 ID `name` varchar(128), -- 订单名称 其他业务熟悉忽略 `message_id` varchar(128) NOT NULL, -- 消息唯一 ID PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- 表 log 消息记录结构CREATE TABLE IF NOT EXISTS `log` ( `message_id` varchar(128) NOT NULL, -- 消息唯一 ID `message` varchar(4000) DEFAULT NULL, -- 消息内容 `try_count` int(4) DEFAULT &#x27;0&#x27;, -- 重试次数 `status` varchar(10) DEFAULT &#x27;&#x27;, -- 消息投递状态：0 投递中 1 投递成功 2 投递失败 `next_retry` timestamp NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27;, -- 下一次重试时间 `create_time` timestamp NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27;, -- 创建时间 `update_time` timestamp NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27;, -- 更新时间 PRIMARY KEY (`message_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 项目依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- RabbitMQ --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mybatis-plus --&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- 常用工具类 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.12&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819## 配置数据源spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/rabbitmq_test?useSSL=falsespring.datasource.driver-class-name=com.mysql.jdbc.Driver# 打印 SQL 语句#mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImplmybatis-plus.mapper-locations=classpath:/mapper/*.xml## RabbitMQspring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000# 回调配置spring.rabbitmq.publisher-confirm-type=correlatedspring.rabbitmq.publisher-returns=truespring.rabbitmq.template.mandatory=true 新建一个 rabbitmq-entity 模块，编写传输实体类。 12345678910public class Order implements Serializable &#123; private static final long serialVersionUID = 2584584237793643231L; private String id; private String name; private String messageId; //...&#125; 新建一个 stable-provider 模块，编写可靠的交付者。 编写状态的实体类，以及常量类 12345678910111213@Datapublic class Log implements Serializable &#123; private static final long serialVersionUID = -8716210355448974538L; private String messageId; private String message; private Integer tryCount = 0; private String status; private Date nextRetry; private Date createTime; private Date updateTime;&#125; 1234567891011121314151617public class Constant &#123; private Constant() &#123; throw new IllegalStateException(&quot;Utility class&quot;); &#125; /** 订单状态 0 表示待发送 */ public static final String ORDER_SENDING = &quot;0&quot;; /** 订单状态 1 表示发送成功 */ public static final String ORDER_SEND_SUCCESS = &quot;1&quot;; /** 订单状态 2 表示发送失败 */ public static final String ORDER_SEND_FAILURE = &quot;2&quot;; /** 订单超时时间，即一分钟后订单进行交付重试 */ public static final Integer ORDER_TIMEOUT = 1; /** 订单最大重试次数 */ public static final Integer ORDER_RETRY_MAX_TIMES = 3;&#125; 编写 Mapper。 1234567891011121314151617181920212223242526@Componentpublic interface LogMapper extends BaseMapper&lt;Log&gt; &#123; /** * 更新最终消息发送结果 成功 or 失败 * @param messageId 消息的唯一 ID * @param status 订单状态 * @param updateTime 订单状态更新时间 */ void changeBrokerMessageLogStatus(@Param(&quot;messageId&quot;) String messageId , @Param(&quot;status&quot;) String status , @Param(&quot;updateTime&quot;) Date updateTime ); /** * 查询消息状态为 0 且已经超时的消息集合 * @return 符合条件的订单集合 */ List&lt;Log&gt; query4StatusAndTimeoutMessage(); /** * @param messageId 消息的唯一 ID * @param date 时间 */ void update4ReSend(@Param(&quot;messageId&quot;) String messageId, @Param(&quot;updateTime&quot;) Date date);&#125; 12345678910// Order 为 SQL 关键字，并且只有一个插入方法，故完全使用自定义方法不继承@Componentpublic interface OrderMapper&#123; /** * @param order 订单 */ void insert(Order order);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.wingo.stableprovider.mapper.LogMapper&quot;&gt; &lt;!-- 通用查询映射结果 --&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.wingo.stableprovider.entity.Log&quot;&gt; &lt;id column=&quot;message_id&quot; property=&quot;messageId&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;message&quot; property=&quot;message&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;try_count&quot; property=&quot;tryCount&quot; jdbcType=&quot;INTEGER&quot; /&gt; &lt;result column=&quot;status&quot; property=&quot;status&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;next_retry&quot; property=&quot;nextRetry&quot; jdbcType=&quot;TIMESTAMP&quot; /&gt; &lt;result column=&quot;create_time&quot; property=&quot;createTime&quot; jdbcType=&quot;TIMESTAMP&quot; /&gt; &lt;result column=&quot;update_time&quot; property=&quot;updateTime&quot; jdbcType=&quot;TIMESTAMP&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; message_id, message, try_count, status, next_retry, create_time, update_time &lt;/sql&gt; &lt;update id=&quot;changeBrokerMessageLogStatus&quot; &gt; UPDATE log SET status = #&#123;status&#125;, update_time = #&#123;updateTime&#125; WHERE message_id = #&#123;messageId&#125; &lt;/update&gt; &lt;select id=&quot;query4StatusAndTimeoutMessage&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT &lt;include refid=&quot;Base_Column_List&quot; /&gt; FROM log WHERE status = &#x27;0&#x27; AND next_retry &lt;![CDATA[ &lt;= ]]&gt; sysdate() &lt;/select&gt; &lt;update id=&quot;update4ReSend&quot; &gt; UPDATE log SET try_count = try_count + 1, update_time = #&#123;updateTime&#125; WHERE message_id = #&#123;messageId&#125; &lt;/update&gt;&lt;/mapper&gt; 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.wingo.stableprovider.mapper.OrderMapper&quot;&gt; &lt;!-- 通用查询映射结果 --&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.wingo.entity.Order&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;message_id&quot; property=&quot;messageId&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; id, name, message_id &lt;/sql&gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;com.wingo.entity.Order&quot; &gt; INSERT INTO my_order (id, name, message_id) VALUES (#&#123;id&#125;, #&#123;name&#125;, #&#123;messageId&#125;) &lt;/insert&gt;&lt;/mapper&gt; 编写一个服务用于向数据库初始化订单数据以及消息状态信息。 123456789101112131415161718192021222324252627282930313233@Servicepublic class OrderService &#123; @Autowired private OrderMapper orderMapper; @Autowired private LogMapper logMapper; @Autowired private OrderSender orderSender; public void createOrder(Order order) throws Exception &#123; // 数据库插入订单信息 orderMapper.insert(order); // 初始化日志信息 Log log = new Log(); Date orderTime = new Date(); log.setMessageId(order.getMessageId()); // 将订单转为 JSON 字符串在重新发送时使用 String jOrder = JSON.toJSONString(order); log.setMessage(jOrder); log.setStatus(&quot;0&quot;); log.setNextRetry(DateUtils.addMinutes(orderTime, Constant.ORDER_TIMEOUT)); log.setCreateTime(new Date()); log.setUpdateTime(new Date()); // 数据库插入日志信息 logMapper.insert(log); // order message sender orderSender.sendOrder(order); &#125;&#125; Sender 编写 RabbitMQ 的交付类。 12345678910111213141516171819202122232425262728293031323334@Componentpublic class OrderSender &#123; @Autowired private RabbitTemplate rabbitTemplate; @Autowired private LogMapper logMapper; // 回调函数: confirm 确认 final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.err.println(&quot;correlationData: &quot; + correlationData); String messageId = correlationData.getId(); if(ack)&#123; // 如果 confirm 返回成功，则对 Order 表中对应的订单状态进行更新 logMapper.changeBrokerMessageLogStatus(messageId, Constant.ORDER_SEND_SUCCESS, new Date()); &#125; else &#123; // 失败则进行具体的后续操作：重试或者补偿等手段 System.err.println(&quot;异常处理...&quot;); &#125; &#125; &#125;; // 发送消息方法调用: 构建自定义对象消息 public void sendOrder(Order order) throws Exception &#123; // 设置自定义回调方法 rabbitTemplate.setConfirmCallback(confirmCallback); // 消息唯一 ID CorrelationData correlationData = new CorrelationData(order.getMessageId()); rabbitTemplate.convertAndSend(&quot;order-exchange&quot;, &quot;order.stable&quot;, order, correlationData); &#125;&#125; Task 编写定时任务，定期对数据库中的任务状态进行读取，对重试满足条件的交付任务进行重新发送。 1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class RetryMessage &#123; @Autowired private OrderSender orderSender; @Autowired private LogMapper logMapper; // 3 秒后执行，之后每 10 秒执行一次 @Scheduled(initialDelay = 3000, fixedDelay = 10000) public void reSend()&#123; System.err.println(&quot;---------------定时任务开始---------------&quot;); // 取数据库中所有的状态为 0 并且已经超时的订单 List&lt;Log&gt; list = logMapper.query4StatusAndTimeoutMessage(); if(list.isEmpty())&#123; System.out.println(&quot;无交付失败订单&quot;); &#125; list.forEach(messageLog -&gt; &#123; if(messageLog.getTryCount() &gt;= Constant.ORDER_RETRY_MAX_TIMES)&#123; // 如果重试次数超过规定的最大次数则表示交付失败 logMapper.changeBrokerMessageLogStatus(messageLog.getMessageId(), Constant.ORDER_SEND_FAILURE, new Date()); System.out.println(&quot;有新的交付失败订单&quot;); &#125; else &#123; // 增加重试次数 logMapper.update4ReSend(messageLog.getMessageId(), new Date()); Order reSendOrder = JSON.parseObject(messageLog.getMessage(), Order.class); try &#123; orderSender.sendOrder(reSendOrder); System.out.println(&quot;重试订单交付&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); System.err.println(&quot;-----------异常处理-----------&quot;); &#125; &#125; &#125;); // forEach 结束 &#125;&#125; 主类上还需要加上几个注解。 123@MapperScan(&quot;com.wingo.stableprovider.mapper&quot;)@EnableScheduling@SpringBootApplication 交付失败时的控制台打印信息。 序列化错误 场景： consumer 和 provider 模块各自有一个可序列化的 entity.Order 类； 序列化与反序列化时，两个 Order 虽然代码相同，但底层序列化后的标记不同，虚拟机不会认为它们是同一个对象，出现反序列化错误。 1Caused by: org.springframework.messaging.converter.MessageConversionException: Cannot convert from 解决方案： 1）consumer 和 provider 引用同一个 entity.Order 类； 2）利用 jackson 工具进行转化，传递 Json 串。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://wingowen.github.io/tags/MQ/"}]},{"title":"Dubbo","slug":"后台技术/Dubbo 初步使用","date":"2020-04-17T01:50:34.000Z","updated":"2023-09-20T07:00:03.774Z","comments":true,"path":"2020/04/17/后台技术/Dubbo 初步使用/","link":"","permalink":"https://wingowen.github.io/2020/04/17/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Dubbo%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"Dubbo + Zookeeper 的简单介绍，以及整合使用。","text":"Dubbo + Zookeeper 的简单介绍，以及整合使用。 Dubbo 初步使用 基础理论 应用的发展与演变： 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架（ORM）是关键； 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的 Web 框架（MVC）是关键； 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架（RPC）是关键； Dubbo 是一款高性能、轻量级的开源 Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心（SOA）即 Service Oriented Architecture 是关键。 环境搭建 Java yum search java-1.8 ！！！注意，需要下载两个 java-1.8.0-openjdk.x86_64 / java-1.8.0-openjdk-devel.x86_64 yum install ... whereis java查看 java 路径（/usr/lib/jvm/） 修改环境变量：vim /etc/profile 123export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-...export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar javac环境变量是否配置正确 Zookeeper 下载 zookeeper ：清华大学开源软件镜像站 wget [下载连接]下载压缩包 tar -zxvf zookeeper-x.x.x.tar.gz 在 /usr/local/ 下新建 zookeeper 目录进行解压 修改 zookeeper 配置文件名：zoo_sample.cfg 改为 zoo.cfg 配置环境变量：vi ~/.bash_profile 123export PATHexport ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.11/export PATH=$ZOOKEEPER_HOME/bin:$PATH 常用操作 123zkServer.sh start # 开启 zookeeperjps # 显示当前所有 java 进程 pidzkServer.sh stop # 停止 zookeeper dubbo-admin netstat -aon|findstr “port-num” 查看占用端口的程序的 PID Dubbo 官方项目下载 修改 dubbo-admin 项目的配置（！！！zookeeper 的地址修改为自己配置的地址） 在 pom.xml 目录下：cmd 输入命令mvn clean package对项目进行打包 打包成功后生成 target 文件夹，target 目录下有一个可执行 jar 包 java -jar [jara_name]运行项目，运行成功后根据配置的路径进行访问 dubbo-monitor 生成的 target 文件中有一个压缩包，解压后的 assembly.bin 目录中运行 start.bat 脚本进行项目运行 项目架构 先来看一张官网给的架构图： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 各节点间的调用关系说明： ​ 服务容器负责启动，加载，运行服务提供者。 ​ 服务提供者在启动时，向注册中心注册自己提供的服务。 ​ 服务消费者在启动时，向注册中心订阅自己所需的服务。 ​ 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 ​ 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 ​ 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 项目创建 基于对官网给出的架构分析得知我们需要两个角色：服务的提供者；服务的消费者。 消费者在自身的项目中可以如同调用本地方法一般调用提供者的方法；那么消费者和提供者必须都受到同一套接口的规范，在两个项目中都编写接口显得有些繁琐，所以可以新建一个 Maven 项目专门用于定义接口。 最外层套一层壳，添加三个 module，其中外壳项目和提供接口的项目为 Maven 项目即可。 dubbo-api 此项目用于提供接口规范，不需要添加任何依赖。编写一个实体类和一个服务类接口即可。 1234567891011public class User implements Serializable &#123; private static final long serialVersionUID = 5433406871746033298L; private Integer userId; private String username; private String password; // ...&#125; 定义接口： 123456789public interface UserService &#123; /** * 获取用户列表的方法 * * @return 用户的信息列表 */ List&lt;User&gt; getUserList();&#125; 要让 dubbo-consumer 以及 dubbo-provider 获取此接口，需要给这两个项目添加 dubbo-api 项目的依赖。 123456&lt;!-- 添加自定义接口依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.wingo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; dubbo-provider 服务提供者编写了接口的具体实现方法，并且要把实现的方法通过 Dubbo 暴露出去。 123456&lt;!-- 添加 Dubbo 依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; Spring 以及 Dubbo 都要管理这个类。 12345678910@com.alibaba.dubbo.config.annotation.Service@Servicepublic class UserServiceImpl implements UserService &#123; @Override public List&lt;User&gt; getUserList() &#123; User user1 = new User(1,&quot;wingo&quot;,&quot;123456&quot;); User user2 = new User(2,&quot;admin&quot;,&quot;654321&quot;); return Arrays.asList(user1, user2); &#125;&#125; 在主类上标注 @EnableDubbo(scanBasePackages = “com.wingo.dubboprovider.service”)，并添加配置。 12345678910# 此服务的名称dubbo.application.name=user-service-provider# 注册中心的协议及地址dubbo.registry.address=127.0.0.1:2181dubbo.registry.protocol=zookeeper# 暴露的协议及端口dubbo.protocol.name=dubbodubbo.protocol.port=20881## 注册中心获取监控中心的信息dubbo.monitor.protocol=registry 启动项目，来到 dubbo-admin 的页面查看服务。 在 dubbo-admin 多了一个我们刚刚暴露出来的服务，由名称可以得知其暴露的是一个接口。 点击查看其对应的 IP 地址以及域名，正是本机的 IP 以及自定义的接口 20881。 dubbo-monitor 中可以看到这个应用的名称正是配置文件中的 user-service-provider。 dubbo-consumer 消费者向 dubbo 请求所需要的服务。 1234567891011@RestControllerpublic class UserController &#123; @Reference UserService userService; @GetMapping(&quot;/users&quot;) public List&lt;User&gt; userList ()&#123; return userService.getUserList(); &#125;&#125; 12345server.port=8082dubbo.application.name=dubbo-user-consumerdubbo.registry.address=zookeeper://127.0.0.1:2181dubbo.monitor.protocol=registry 启动消费者。 可以看出，提供者与消费者确实是通过接口通讯的。 成功请求数据。 Dubbo 特性 介绍一些 Dubbo 的使用特性。 高可用 Zookeeper 宕机与 Dubbo 直连：注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯。 集群负载均衡 Random LoadBalance： 随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance： 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance： 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance： 一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 服务降级 当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 集群容错 Failfast Cluster：快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录； Failsafe Cluster：失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作； Failback Cluster：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作； Forking Cluster：并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=“2” 来设置最大并行数； Broadcast Cluster：广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://wingowen.github.io/tags/Dubbo/"}]},{"title":"Spring Boot 任务","slug":"后台技术/Spring Boot/Spring Boot 任务","date":"2020-04-01T02:50:45.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/04/01/后台技术/Spring Boot/Spring Boot 任务/","link":"","permalink":"https://wingowen.github.io/2020/04/01/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E4%BB%BB%E5%8A%A1/","excerpt":"Spring Boot 中的异步任务，定时任务，邮件任务的使用。","text":"Spring Boot 中的异步任务，定时任务，邮件任务的使用。 异步任务 在 Java 应用中，绝大多数情况下都是通过同步的方式来实现交互处理的。但是在处理与第三方系统交互的时候，容易造成响应迟缓的情况，之前大部分都是使用多线程来完成此类任务。其实，在Spring 3.x 之后，就已经内置了@Async、@EnableAysnc来完美解决这个问题。 在 Spring 中运用 Async 注解需要注意几点： 方法名必须是 public 进行修饰的，且不能是 static 方法； 不能与调用的方法在同一个类中； 需要把该方法注入到 Spring 容器中，就是在一个类中添加异步方法，并在此类上使用@Component之类的注解。 测试实例 定义 Task 类，创建三个处理函数分别模拟三个执行任务的操作，操作消耗时间随机取（10 秒内）。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class Task &#123; public static Random random =new Random(); @Async // 注释为异步任务， public Future&lt;String&gt; doTaskOne() throws Exception &#123; System.out.println(&quot;Doing Task One&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(&quot;Task One Finished：Spend &quot; + (end - start) + &quot; Millisecond&quot;); // 若不用 Future 进行回调，无法确定任务已经完成 return new AsyncResult&lt;&gt;(&quot;Task One Finished&quot;); // 返回异步调用结果 &#125; @Async public Future&lt;String&gt; doTaskTwo() throws Exception &#123; System.out.println(&quot;Doing Task Two&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(&quot;Task Two Finished：Spend &quot; + (end - start) + &quot; Millisecond&quot;); return new AsyncResult&lt;&gt;(&quot;Task Two Finished&quot;); &#125; @Async public Future&lt;String&gt; doTaskThree() throws Exception &#123; System.out.println(&quot;Doing Task Three&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(&quot;Task Three Finished：Spend &quot; + (end - start) + &quot; Millisecond&quot;); return new AsyncResult&lt;&gt;(&quot;Task Three Finished&quot;); &#125;&#125; 12345678@SpringBootApplication@EnableAsync // 异步任务生效public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 123456789101112131415161718192021@Testpublic void test() throws Exception &#123; long start = System.currentTimeMillis(); Future&lt;String&gt; taskOne = task.doTaskOne(); Future&lt;String&gt; taskTwo = task.doTaskTwo(); Future&lt;String&gt; taskThree = task.doTaskThree(); while(true) &#123; if(taskOne.isDone() &amp;&amp; taskTwo.isDone() &amp;&amp; taskThree.isDone()) &#123; // 三个任务都调用完成，退出循环等待 break; &#125; Thread.sleep(1000); &#125; long end = System.currentTimeMillis(); System.out.println(&quot;All Finished，总耗时：&quot; + (end - start) + &quot;毫秒&quot;);&#125; 定时任务 项目开发中经常需要执行一些定时任务，比如需要在每天凌晨时候，分析一次前一天的日志信息。Spring 为我们提供了异步执行任务调度的方式，提供 TaskExecutor 、TaskScheduler 接口。 注解：@EnableScheduling、@Scheduled corn 表达式 *******从左到右分别代表：秒 分 时 日 月 星期 年份，这里 * 指所有可能的值。 在线 Cron 表达式生成器 测试 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 1234567891011121314151617@Servicepublic class TaskService &#123; /** * second（秒）, minute（分）, hour（时）, day of month（日）, month（月）, day of week（周几）. * 例子： * 0 0/5 14,18 * * ? 每天14点整，和18点整，每隔5分钟执行一次 * 0 15 10 ? * 1-6 每个月的周一至周六10:15分执行一次 * 0 0 2 ? * 6L 每个月的最后一个周六凌晨2点执行一次 * 0 0 2 LW * ? 每个月的最后一个工作日凌晨2点执行一次 * 0 0 2-4 ? * 1#1 每个月的第一个周一凌晨2点到4点期间，每个整点都执行一次； */ @Scheduled(cron = &quot;0,1,2,3,4 * * * * MON-SAT&quot;) public void runTask()&#123; System.out.println(new Date()+&quot;Doing Schedule Task&quot;); &#125;&#125; 邮件任务 最早期的时候使用 JavaMail 相关 API 来开发，需要自己去封装消息体，代码量比较庞大；后来 Spring 推出了 JavaMailSender 简化了邮件发送过程，JavaMailSender 提供了强大的邮件发送功能，可支持各种类型的邮件发送。现在 Spring Boot 在 JavaMailSender 的基础上又进行了封装，就有了现在的 spring-boot-starter-mail，让邮件发送流程更加简洁和完善。 测试 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 123456spring.mail.username=邮箱账号# 在邮件站点的个人中心中获取spring.mail.password=xxxxxspring.mail.host=smtp.163.com // 邮件协议对应的服务器spring.mail.protocol=smtp // 邮件协议spring.mail.properties.mail.smtp.ssl.enable=true 12345678910111213141516@AutowiredJavaMailSenderImpl mailSender;@Testpublic void testSimpleMessage()&#123; SimpleMailMessage message = new SimpleMailMessage(); message.setSubject(&quot;第一封测试邮件&quot;); message.setText(&quot;邮件测试...&quot;); message.setFrom(&quot;你的邮箱账号&quot;); message.setTo(&quot;发送对象的邮箱账号&quot;); mailSender.send(message);&#125; 123456789101112131415161718@Testpublic void testMimeMessage() throws Exception&#123; // 创建复杂消息 MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); helper.setSubject(&quot;第二封邮件&quot;); helper.setText(&quot;&lt;b style=&#x27;color:red&#x27;&gt;邮件测试....&lt;/b&gt;&quot;,true); helper.setFrom(&quot;你的邮箱账号&quot;); helper.setTo(&quot;发送对象的邮箱账号&quot;); // 发送邮件 helper.addAttachment(&quot;1.jpg&quot;,new File(&quot;C:\\\\Users\\\\12746\\\\Pictures\\\\22\\\\1.jpg&quot;)); helper.addAttachment(&quot;2.jpg&quot;,new File(&quot;C:\\\\Users\\\\12746\\\\Pictures\\\\22\\\\2.jpg&quot;)); mailSender.send(mimeMessage);&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Aysnc","slug":"Aysnc","permalink":"https://wingowen.github.io/tags/Aysnc/"},{"name":"Scheduled","slug":"Scheduled","permalink":"https://wingowen.github.io/tags/Scheduled/"},{"name":"Email","slug":"Email","permalink":"https://wingowen.github.io/tags/Email/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"}]},{"title":"Spring Boot 安全","slug":"后台技术/Spring Boot/Spring Boot 安全","date":"2020-04-01T02:50:45.000Z","updated":"2023-09-20T06:44:49.033Z","comments":true,"path":"2020/04/01/后台技术/Spring Boot/Spring Boot 安全/","link":"","permalink":"https://wingowen.github.io/2020/04/01/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E5%AE%89%E5%85%A8/","excerpt":"Shiro 轻量级安全框架的介绍及使用；Sping 安全框架 Spring Security 的介绍及使用。","text":"Shiro 轻量级安全框架的介绍及使用；Sping 安全框架 Spring Security 的介绍及使用。 Spring Security简介 Spring Security 是针对 Spring 项目的安全框架，也是 Spring Boot 底层安全模块默认的技术选型。可以实现强大的 web 安全控制。对于安全控制，我们仅需引入 spring-boot-starter-security 模块，进行少量的配置，即可实现强大的安全管理。 WebSecurityConfigurerAdapter：自定义 Security 策略； AuthenticationManagerBuilder：自定义认证策略； @EnableWebSecurity：开启 WebSecurity。 应用程序的两个主要区域是“认证”和“授权”（或者访问控制）。这两个主要区域是 Spring Security 的两个目标。 “认证”（Authentication），是建立一个经过声明的主体的过程，即你是谁； “授权”（Authorization）指确定一个主体是否允许在你的应用程序执行一个动作的过程，即你能干什么。 入门实例引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 建立一个 Web 层的请求接口： 12345678@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @GetMapping public String getUsers() &#123; return &quot;Hello Spring Security&quot;; &#125;&#125; 自定义用户认证逻辑 系统一般都有自定义的用户体系，Spring Security 提供了接口可以自定义认证逻辑以及登录界面。 1234567// 密码加密接口public interface PasswordEncoder &#123; // 对密码进行加密 String encode(CharSequence var1); // 对密码进行判断匹配 boolean matches(CharSequence var1, String var2);&#125; 12345// 实现 Spring Security 中的 PasswordEncoder 接，这里用提供的默认实现@Beanpublic PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; 配置用户认证逻辑： UserDetails 就是封装了用户信息的对象，返回 UserDetails 的实现类 User 的时候，可以通过 User 的构造方法，设置对应的参数。 12345678910111213141516171819@Componentpublic class MyUserDetailsService implements UserDetailsService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; logger.info(&quot;用户的用户名: &#123;&#125;&quot;, username); // TODO 根据用户名，查找到对应的密码，与权限 // 封装用户信息，并返回。参数分别是：用户名，密码，用户权限 String password = passwordEncoder.encode(&quot;123456&quot;); logger.info(&quot;password: &#123;&#125;&quot;, password); // 每次打印的密码都不一样，因为经过了加密 // 参数分别是：用户名，密码，用户权限 User user = new User(username, password, AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); return user; &#125;&#125; 自定义登录页面： 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;登录页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;自定义登录页面&lt;/h2&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;&lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 配置模块12345678910111213141516@Configurationpublic class BrowerSecurityConfig extends WebSecurityConfigurerAdapter &#123; protected void configure(HttpSecurity http) throws Exception &#123; http.formLogin() // 定义当需要用户登录时候，转到的登录页面。 .loginPage(&quot;/login.html&quot;) // 设置登录页面 .loginProcessingUrl(&quot;/user/login&quot;) // 自定义的登录接口 .and() .authorizeRequests() // 定义哪些 URL 需要被保护，哪些不需要被保护 .antMatchers(&quot;/login.html&quot;).permitAll() // 设置所有人都可以访问登录页面 .anyRequest() // 任何请求，登录后可以访问 .authenticated() .and() .csrf().disable(); // 关闭 csrf 防护 &#125;&#125; Shiro Apache Shiro是一个功能强大且易于使用的 Java安 全框架，它为开发人员提供了一种直观，全面的身份验证，授权，加密和会话管理解决方案。 核心 API Subject： 用户主体（把操作交给SecurityManager）；SecurityManager：安全管理器（关联Realm）；Realm：Shiro 连接数据的桥梁。 入门实例SQL 建表语句： 1234567CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varbinary(64) DEFAULT NULL, `password` varchar(64) DEFAULT NULL, `perms` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 导入依赖： 12345678910111213141516&lt;!-- shiro --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 自定义 Realm 类12345678910111213141516171819202122232425262728293031323334353637383940@Slf4jpublic class UserRealm extends AuthorizingRealm &#123; @Autowired private UserRepository userRepository; @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection arg0) &#123; log.info(&quot;执行授权逻辑&quot;); Subject subject = SecurityUtils.getSubject(); User user = userRepository.findUserByUsername((String) subject.getSession().getAttribute(&quot;username&quot;)); // 给用户进行授权 SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addStringPermission(user.getPerms()); return info; &#125; @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken arg0) throws AuthenticationException &#123; log.info(&quot;执行认证逻辑&quot;); Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); UsernamePasswordToken token = (UsernamePasswordToken)arg0; User user = userRepository.findUserByUsername(token.getUsername()); session.setAttribute(&quot;username&quot;, token.getUsername()); if(user==null)&#123; // shiro 底层会抛出UnKnowAccountException return null; &#125; // 判断密码 return new SimpleAuthenticationInfo(user,user.getPassword(),getName()); &#125;&#125; Shiro 配置类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Slf4j@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(&quot;shiroCacheManager&quot;) private ShiroCacheManager shiroCacheManager; @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Autowired DefaultWebSecurityManager securityManager)&#123; ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 设置安全管理器 shiroFilterFactoryBean.setSecurityManager(securityManager); // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // 设置过滤器链 Map&lt;String,String&gt; filterMap = new LinkedHashMap&lt;&gt;(16); filterMap.put(&quot;/index&quot;, &quot;anon&quot;); filterMap.put(&quot;/favicon.ico&quot;, &quot;anon&quot;); // 添加访问权限 filterMap.put(&quot;/toAdd&quot;, &quot;perms[user:add]&quot;); filterMap.put(&quot;/toUpdate&quot;, &quot;perms[user:update]&quot;); // 注意：要放在允许访问的页面后面，否则允许访问无效 filterMap.put(&quot;/**&quot;, &quot;authc&quot;); // 如果没有认证通过 跳转到的 url 地址 shiroFilterFactoryBean.setLoginUrl(&quot;/login&quot;); shiroFilterFactoryBean.setSuccessUrl(&quot;/index&quot;); // 未授权跳转页面 shiroFilterFactoryBean.setUnauthorizedUrl(&quot;/error&quot;); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterMap); return shiroFilterFactoryBean; &#125; // 安全管理器 @Bean public DefaultWebSecurityManager getDefaultWebSecurityManager(@Autowired UserRealm userRealm)&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 关联 realm securityManager.setRealm(userRealm); return securityManager; &#125; @Bean public UserRealm getRealm()&#123; return new UserRealm(); &#125; /** 配置 ShiroDialect，用于 Thymeleaf 和 Shiro 标签配合使用 */ // 添加 xmlns:shiro=&quot;http://www.pollix.at/thymeleaf/shiro&quot; 标签校验规范 @Bean public ShiroDialect getShiroDialect()&#123; return new ShiroDialect(); &#125;&#125; 控制器类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Controllerpublic class UserController &#123; @GetMapping(&quot;/login&quot;) public String login()&#123; return &quot;login&quot;; &#125; @GetMapping(&quot;/index&quot;) public String hello(Model model)&#123; Subject subject = SecurityUtils.getSubject(); model.addAttribute(&quot;username&quot;, subject.getSession().getAttribute(&quot;username&quot;)); return &quot;index&quot;; &#125; @GetMapping(&quot;/toAdd&quot;) public String add()&#123; return &quot;/user/add&quot;; &#125; @GetMapping(&quot;/toUpdate&quot;) public String update()&#123; return &quot;/user/update&quot;; &#125; @GetMapping(&quot;/toExit&quot;) public String exit()&#123; Subject subject = SecurityUtils.getSubject(); subject.logout(); return &quot;redirect:/login&quot;; &#125; @PostMapping(&quot;/login&quot;) public String login(String username, String password, Model model)&#123; Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(username,password); try &#123; Session session = subject.getSession(); session.setAttribute(&quot;username&quot;, username); subject.login(token); return &quot;redirect:/index&quot;; &#125; catch (UnknownAccountException e) &#123; model.addAttribute(&quot;msg&quot;, &quot;用户名不存在&quot;); return &quot;login&quot;; &#125;catch (IncorrectCredentialsException e) &#123; model.addAttribute(&quot;msg&quot;, &quot;密码错误&quot;); return &quot;login&quot;; &#125; &#125;&#125; 高级自定义缓存管理器：Spring 接管 Shiro 的缓存管理。 实现 Shiro 提供的 Cache 和 CacheManager 接口 👉 实现 Cache 缓存接口底层使用 SpringCache 实现。 ShiroCache12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*** 自定义 Shiro 缓存，实现 ShiroCache 接口** @param &lt;K&gt;* @param &lt;V&gt;*/public class ShiroCache&lt;K,V&gt; implements org.apache.shiro.cache.Cache&lt;K,V&gt; &#123; // Spring 的缓存管理器 private CacheManager springCacheManager; // Spring 的缓存对象 private Cache springCache; // 构造器 public ShiroCache(org.springframework.cache.CacheManager springCacheManager, String cacheName) &#123; this.springCacheManager = springCacheManager; this.springCache = springCacheManager.getCache(cacheName); &#125; // 自定义实现，都由 Spring 的缓存对象进行操作 @Override public V get(K key) throws CacheException &#123; Cache.ValueWrapper valueWrapper = springCache.get(key); if (valueWrapper == null) &#123; return null; &#125; return (V) valueWrapper.get(); &#125; @Override public V put(K key, V value) throws CacheException &#123; springCache.put(key, value); return value; &#125; @Override public V remove(K key) throws CacheException &#123; V value = this.get(key); springCache.evict(key); return value; &#125; /** * 清空缓存 * @throws CacheException */ @Override public void clear() throws CacheException &#123; springCache.clear(); &#125; /** * 获取所有缓存key的集合 * * @return */ @Override public Set&lt;K&gt; keys() &#123; return (Set&lt;K&gt;) springCacheManager.getCacheNames(); &#125; /** * 获取所有缓存value值的集合 * * @return */ @Override public Collection&lt;V&gt; values() &#123; List&lt;V&gt; list = new ArrayList&lt;&gt;(); Set&lt;K&gt; keys = keys(); for (K k : keys) &#123; list.add(this.get(k)); &#125; return list; &#125; /** * 获取缓存对象的数量 * * @return */ @Override public int size() &#123; int size = keys().size(); return size; &#125;&#125; ShiroCacheManager12345678910111213public class ShiroCacheManager&lt;K, V&gt; implements CacheManager &#123; // Spring 的缓存管理器 private org.springframework.cache.CacheManager springCacheManager; public ShiroCacheManager(org.springframework.cache.CacheManager springCacheManager) &#123; this.springCacheManager = springCacheManager; &#125; @Override public &lt;K, V&gt; Cache&lt;K, V&gt; getCache(String cacheName) throws CacheException &#123; // 通过缓存名在缓存管理器中获取对应的缓存 return new ShiroCache&lt;&gt;(springCacheManager, cacheName); &#125;&#125; 1234567891011121314151617/*** shiro 的缓存管理器* @return*/@Bean(name = &quot;shiroCacheManager&quot;)public ShiroCacheManager shiroCacheManager()&#123; // 创建一个 Redis 缓存的默认配置 RedisCacheConfiguration conf = RedisCacheConfiguration.defaultCacheConfig(); // 设置会话的有效期 conf = conf.entryTtl(Duration.ofSeconds(30)); RedisCacheManager cacheManager = RedisCacheManager .builder(redisConnectionFactory) .cacheDefaults(conf) .build(); // 不用 ShiroCacheManager 返回的其实是一个 RedisCacheManager return new ShiroCacheManager(cacheManager);&#125; 注入自定义的缓存管理器 12345678910111213141516@Slf4j@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(&quot;shiroCacheManager&quot;) private ShiroCacheManager shiroCacheManager; @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Autowired DefaultWebSecurityManager securityManager)&#123; // ... // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // ... &#125;&#125; ShiroSessionDAO1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Repositorypublic class ShiroSessionDAO extends AbstractSessionDAO &#123; @Autowired private RedisTemplate&lt;Serializable, Session&gt; redisTemplate; private String name = &quot;shiro-session:&quot;; private long timeout = 30; @Override protected Serializable doCreate(Session session) &#123; // 生成会话的唯一id Serializable sessionId = generateSessionId(session); super.create(session); redisTemplate.opsForValue().set(name + sessionId, session); return sessionId; &#125; @Override protected Session doReadSession(Serializable sessionId) &#123; super.readSession(sessionId); // 更新缓存 redisTemplate.expire(sessionId, timeout, TimeUnit.MINUTES); return redisTemplate.opsForValue().get(sessionId); &#125; @Override public void update(Session session) throws UnknownSessionException &#123; redisTemplate.opsForValue().set(name + session.getId(), session); &#125; @Override public void delete(Session session) &#123; redisTemplate.delete(name + session.getId()); &#125; @Override public Collection&lt;Session&gt; getActiveSessions() &#123; Set&lt;Serializable&gt; set = redisTemplate.keys(name + &quot;*&quot;); List&lt;Session&gt; sessionList = new ArrayList&lt;&gt;(); for (Serializable sessionId : set) &#123; sessionList.add(redisTemplate.opsForValue().get(name + sessionId)); &#125; return sessionList; &#125;&#125; 配置类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(&quot;shiroCacheManager&quot;) private ShiroCacheManager shiroCacheManager; /** * 配置自定义SessionDAO * @return */ @Bean public SessionDAO sessionDAO() &#123; SessionDAO sessionDAO = new ShiroSessionDAO(shiroCacheManager); return sessionDAO; &#125; /** * 配置会话管理器 * @return */ @Bean public SessionManager sessionManager()&#123; // WEB 环境非 HttpSession 会话管理器 DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); // 注入自定义的 SessionDao sessionManager.setSessionDAO(sessionDAO()); return sessionManager; &#125; /** * 配置安全管理器 * * @return */ @Bean public SecurityManager securityManager() &#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 注入自定义Realm securityManager.setRealm(shiroRealm()); // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // 注入会话管理器 securityManager.setSessionManager(sessionManager()); return securityManager; &#125; /** * 装配 自定义Realm * * @return */ @Bean public ShiroRealm shiroRealm() &#123; return new ShiroRealm(); &#125; /** * 配置权限权限过滤器 * @return */ @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean() &#123; ShiroFilterFactoryBean filter = new ShiroFilterFactoryBean(); // 注入安全管理器 filter.setSecurityManager(securityManager()); // 未认证的跳转地址 filter.setLoginUrl(&quot;/login&quot;); Map&lt;String, String&gt; chain = new LinkedHashMap&lt;&gt;(); chain.put(&quot;/login&quot;, &quot;anon&quot;); // 登录链接不拦截 chain.put(&quot;/css/**&quot;, &quot;anon&quot;); chain.put(&quot;/img/**&quot;, &quot;anon&quot;); chain.put(&quot;/js/**&quot;, &quot;anon&quot;); chain.put(&quot;/lib/**&quot;, &quot;anon&quot;); chain.put(&quot;/**&quot;, &quot;user&quot;); filter.setFilterChainDefinitionMap(chain); return filter; &#125; /** * 启用 Shiro 注解 * @return */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor() &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); // 注入安全管理器 advisor.setSecurityManager(securityManager()); return advisor; &#125; /** * 启用 Shiro Thymeleaf 标签支持 * @return */ @Bean public ShiroDialect shiroDialect() &#123; return new ShiroDialect(); &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Security","slug":"Security","permalink":"https://wingowen.github.io/tags/Security/"},{"name":"Shiro","slug":"Shiro","permalink":"https://wingowen.github.io/tags/Shiro/"}]},{"title":"Spring Boot 检索","slug":"后台技术/Spring Boot/Spring Boot 检索","date":"2020-03-31T06:50:45.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 检索/","link":"","permalink":"https://wingowen.github.io/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%A3%80%E7%B4%A2/","excerpt":"ElasticSearch 基本介绍以及在 Spring Boot 中的整合使用。","text":"ElasticSearch 基本介绍以及在 Spring Boot 中的整合使用。 ElasticSearch 如今的应用经常需要添加检索功能，开源的 ElasticSearch 是目前全文搜索引擎的首选。他可以快速的存储、搜索和分析海量数据。Spring Boot 通过整合 Spring Data ElasticSearch 为我们提供了非常便捷的检索功能支持。 Elasticsearch 是一个分布式搜索服务，提供 Restful API，底层基于 Lucene，采用多 shard（分片）的方式保证数据安全，并且提供自动 resharding 的功能，github 等大型的站点也是采用了 ElasticSearch 作为其搜索服务。 Elasticsearch 本身就是分布式的，即便只有一个节点，Elasticsearch 默认也会对的数据进行分片和副本操作，向集群添加新数据时，数据也会在新加入的节点中进行平衡 相关概念 一个 ElasticSearch 集群可以包含多个索引 ，相应的每个索引可以包含多个类型 。这些不同的类型存储着多个文档 ，每个文档又有多个属性 。 Elasticsearch 也是基于 Lucene 的全文检索库，本质也是存储数据，很多概念与关系型数据库是一致的，如下对照： 索引库 关系型数据库 类型（type） Table 数据表 文档（Document） Row 行 字段（Field） Columns 列 另外，在 Elasticsearch 有一些集群相关的概念： 索引集（Indices，index 的复数）：逻辑上的完整索引； 分片（shard）：数据拆分后的各个部分； 副本（replica）：每个分片的复制。 整合 Spring Boot 提供了两种方式操作 Elasticsearch，Jest 和 SpringData。 Docker 安装部署 ElasticSearch 12345# 下载镜像docker pull elasticsearch# Elasticsearch 启动是会默认分配 2G 的内存 ，我们启动是设置小一点，防止我们内存不够启动失败# 9200 是 Elasticsearch 默认的 web 通信接口，9300 是分布式情况下，Elasticsearch 各个节点的通信端口docker run -e ES_JAVA_OPTS=&quot;-Xms256m -Xmx256m&quot; -d -p 9200:9200 -p 9300:9300 --name es01 5c1e1ecfe33a Jest ElasticSearch already has a Java API which is also used by ElasticSearch internally, but Jest fills a gap, it is the missing client for ElasticSearch Http Rest interface. 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt;&lt;/dependency&gt; 配置服务器： 1spring.elasticsearch.jest.uris=http://127.0.0.1:9200 ## Elasticsearch 服务器 编写实体类： 12345678910111213141516171819202122232425public class Article &#123; // If @JestId value is null, it will be set the value of ElasticSearch generated &quot;_id&quot;. @JestId // @JestId annotation can be used to mark a property of a bean as id private Integer id; private String author; private String title; private String content; // Getter / Setter @Override public String toString() &#123; final StringBuilder sb = new StringBuilder( &quot;&#123;\\&quot;Article\\&quot;:&#123;&quot; ); sb.append( &quot;\\&quot;id\\&quot;:&quot; ) .append( id ); sb.append( &quot;,\\&quot;author\\&quot;:\\&quot;&quot; ) .append( author ).append( &#x27;\\&quot;&#x27; ); sb.append( &quot;,\\&quot;title\\&quot;:\\&quot;&quot; ) .append( title ).append( &#x27;\\&quot;&#x27; ); sb.append( &quot;,\\&quot;content\\&quot;:\\&quot;&quot; ) .append( content ).append( &#x27;\\&quot;&#x27; ); sb.append( &quot;&#125;&#125;&quot; ); return sb.toString(); &#125;&#125; 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootElasticsearchApplicationTests &#123; @Autowired JestClient jestClient; @Test public void createIndex() &#123; // 初始化一个文档 Article article = new Article(); article.setId( 1 ); article.setTitle( &quot;好消息&quot; ); article.setAuthor( &quot;张三&quot; ); article.setContent( &quot;Hello World&quot; ); // 构建一个索引 Index index = new Index.Builder(article).index(&quot;shekou&quot;).type(&quot;news&quot;).build(); try &#123; // 执行 jestClient.execute(index); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Test public void search() &#123; // 查询表达式 String query = &quot;&#123;\\n&quot; + &quot; \\&quot;query\\&quot; : &#123;\\n&quot; + &quot; \\&quot;match\\&quot; : &#123;\\n&quot; + &quot; \\&quot;content\\&quot; : \\&quot;hello\\&quot;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;\\n&quot; + &quot;&#125;&quot;; // 构建搜索功能 Search search = new Search.Builder(query).addIndex(&quot;shekou&quot;).addType(&quot;news&quot;).build(); try &#123; // 执行 SearchResult result = jestClient.execute(search); System.out.println(result.getJsonString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 搜索表达式查询： Searching Document Spring Data spring-data-elasticsearch 使用之前必须先确定 Elasticsearch 版本：点击查看官方文档 Spring Data 通过注解来声明字段的映射属性，注解的详细用法： @Document 作用在类，标记实体类为文档对象，一般有两个属性： indexName：对应索引库名称； type：对应在索引库中的类型； shards：分片数量，默认5； replicas：副本数量，默认1。 @Id作用在成员变量，标记一个字段作为 id 主键。 @Field作用在成员变量，标记为文档的字段，并指定字段映射属性： type：字段类型，是枚举：FieldType，可以是 text、long、short、date、integer、object 等： Text：存储数据时候，会自动分词，并生成索引； Keyword：存储数据时候，不会分词建立索引； Numerical：数值类型，分两类： 基本数据类型：long、interger、short、byte、double、float、half_float； 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如 10 或 100。Elasticsearch 会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 Elasticsearch 可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为 long，节省空间。 index：是否索引，布尔类型，默认是true； store：是否存储，布尔类型，默认是false； analyzer：分词器名称。 CRUD Spring Data 的强大之处，就在于你不用写任何 DAO 处理，自动根据方法名或类的信息进行 CRUD 操作。只要你定义一个接口，然后继承 Repository 提供的一些子接口，就能具备各种基本的 CRUD 功能。 123public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123;&#125; 测试 12345678910111213141516171819202122// 创建一个实体类并标注为文档@Document(indexName = &quot;item&quot;, type = &quot;docs&quot;, shards = 1, replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text) private String title; @Field(type = FieldType.Keyword) private String category; @Field(type = FieldType.Keyword) private String brand; @Field(type = FieldType.Double) private Double price; @Field(index = false, type = FieldType.Keyword) private String images;&#125; 调用 elasticsearchTemplate 创建索引并映射： 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTest(classes = EsDemoApplication.class)public class EsDemoApplicationTests &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Test public void testCreateIndex() &#123; // 根据 Item 类的 @Document 注解信息来创建 elasticsearchTemplate.createIndex(Item.class); &#125; @Test public void insert() &#123; Item item = new Item(1L, &quot;Redmi&quot;, &quot; 手机&quot;, &quot;小米&quot;, 1499.00, &quot;RedmiImage.jpg&quot;); itemRepository.save(item); &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://wingowen.github.io/tags/ElasticSearch/"}]},{"title":"Spring Boot 消息中间件","slug":"后台技术/Spring Boot/Spring Boot 消息队列","date":"2020-03-31T03:50:45.000Z","updated":"2023-09-20T07:00:03.775Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 消息队列/","link":"","permalink":"https://wingowen.github.io/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"MQ 基本介绍以及与 Spring Boot 的整合使用。","text":"MQ 基本介绍以及与 Spring Boot 的整合使用。 概述 在大多数应用程序中，可通过消息服务中间件来提升系统异步通信、扩展解耦能力。 消息代理（message broker）； 目的地（destination）。 当消息发送者发送消息以后，将由消息代理接管，消息代理保证消息传递到指定目的地。消息队列主要有两种形式的目的地。 队列（queue）：点对点消息通信（point-to-point）； 主题（topic）：发布（publish）/ 订阅（subscribe）消息通信。 应用场景：异步处理（邮件）、应用解耦、流量削峰（秒杀）。 JMS：Java 消息服务（Java Message Service）应用程序接口是一个 Java 平台中关于面向消息中间件（MOM）的 API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java 消息服务是一个与具体平台无关的API，绝大多数 MOM 提供商都对 JMS 提供支持。 AMQP：高级消息队列协议（Advanced Message Queuing Protocol）, 对于面向消息中间件的应用层协议。 常见消息中间件 ActiveMQ Apache ActiveMQ 是 Apache 软件基金会所研发的开放源代码消息中间件。 RabbitMQ RabbitMQ 是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ 服务器是用 Erlang 语言编写的， Kafka 发布订阅消息系统、分布式日志服务。本身是做日志储存的，所以对消息的顺序有严格的要求。开源流处理平台，由 Scala 和 java 编写，目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个按照分布式事务日志架构的大规模发布 / 订阅消息队列。 编码接口 JMS 编码接口： ConnectionFactory：创建连接到消息中间件的连接工厂； Connection：通信链路； Destination：消息发布和接收的地点，包括队列和主题； Session：会话，表示一个单线程的上下文； MessageConsumer：会话创建，用于接收消息； MessageProducer：会话创建，用于发送消息； Message：消息对象，包括消息头，消息属性，消息体。 一个 Connection 可以创建多个会话，即一个连接可以供多个线程使用。 Spring 支持 spring-jms 提供了对 JMS 的支持；spring-rabbit 提供了对 AMQP 的支持。 需要 ConnectionFactory 的实现来连接消息代理； 提供 JmsTemplate、RabbitTemplate 来发送消息； @JmsListener（JMS）、@RabbitListener（AMQP）注解在方法上监听消息代理发布的消息； @EnableJms、@EnableRabbit 开启支持。 Spring Boot 自动配置 JmsAutoConfiguration、RabbitAutoConfiguration RabbitMQ 核心概念 Message 消息：消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者：是一个向交换器发布消息的客户端应用程序。 Exchange 交换器：用来接收生产者发送的消息并将这些消息路由给服务器中的队列。Exchange 有 4 种类型：direct（默认：routing key = binding key）、fanout（广播）、 topic,、和 headers（匹配消息的 Header 而不是路由键），不同类型的 Exchange 转发消息的策略有所区别。 topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符： # 匹配 0 个或多个单词；*匹配一个单词。 Queue 消息队列：用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Binding 绑定：用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Exchange 和 Queue 的绑定可以是多对多的关系。 Connection 网络连接：比如一个TCP连接。 Channel 信道：多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内的虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer 消息的消费者：表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker 表示消息队列服务器实体。 RabbitMQ 整合 导入模块依赖的 starter： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot 配置： 123456## 配置 rabbitMq 服务器rabbitmq: host: 127.0.0.1 port: 5672 username: root password: root 配置一个直连型的交换机： 123456789101112131415161718192021@Configurationpublic class DirectRabbitConfig &#123; // 初始化队列名：TestDirectQueue @Bean public Queue TestDirectQueue() &#123; return new Queue(&quot;TestDirectQueue&quot;,true); // true 持久化 &#125; // 初始化 Direct 交换机：TestDirectExchange @Bean DirectExchange TestDirectExchange() &#123; return new DirectExchange(&quot;TestDirectExchange&quot;); &#125; // 定义队列和交换机绑定, 并设置用于匹配键：TestDirectRouting @Bean Binding bindingDirect() &#123; return BindingBuilder.bind(TestDirectQueue()).to(TestDirectExchange()).with(&quot;TestDirectRouting&quot;); &#125;&#125; 写个简单的接口进行消息推送（根据需求也可以改为定时任务等等，具体看需求）： 1234567891011121314151617181920@RestControllerpublic class SendMessageController &#123; @Autowired RabbitTemplate rabbitTemplate; // 使用 RabbitTemplate：提供了接收 / 发送等方法 @GetMapping(&quot;/sendDirectMessage&quot;) public String sendDirectMessage() &#123; String messageId = String.valueOf(UUID.randomUUID()); String messageData = &quot;test message, hello!&quot;; String createTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;)); Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;messageId&quot;,messageId); map.put(&quot;messageData&quot;,messageData); map.put(&quot;createTime&quot;,createTime); // 将消息携带绑定键值 TestDirectRouting 发送到交换机 TestDirectExchange rabbitTemplate.convertAndSend(&quot;TestDirectExchange&quot;, &quot;TestDirectRouting&quot;, map); return &quot;ok&quot;; &#125;&#125; consumer 创建一个消费者项目，消费者进行消息监听，需要手动创建消息接收的监听类。 123456789@Component@RabbitListener(queues = &quot;TestDirectQueue&quot;) // 监听的队列名称 TestDirectQueuepublic class DirectReceiver &#123; @RabbitHandler public void process(Map testMessage) &#123; System.out.println(&quot;DirectReceiver 消费者收到消息: &quot; + testMessage.toString()); &#125;&#125; 回调函数 12345678910111213141516171819202122232425262728293031323334@Configurationpublic class RabbitConfig &#123; @Bean public RabbitTemplate createRabbitTemplate(ConnectionFactory connectionFactory)&#123; RabbitTemplate rabbitTemplate = new RabbitTemplate(); rabbitTemplate.setConnectionFactory(connectionFactory); // 设置开启 Mandatory 才能触发回调函数，无论消息推送结果怎么样都强制调用回调函数 rabbitTemplate.setMandatory(true); // 交换机相关信息 rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.out.println(&quot;ConfirmCallback: &quot;+&quot;相关数据：&quot;+correlationData); System.out.println(&quot;ConfirmCallback: &quot;+&quot;确认情况：&quot;+ack); System.out.println(&quot;ConfirmCallback: &quot;+&quot;原因：&quot;+cause); &#125; &#125;); // 队列相关信息 rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.out.println(&quot;ReturnCallback: &quot;+&quot;消息：&quot;+message); System.out.println(&quot;ReturnCallback: &quot;+&quot;回应码：&quot;+replyCode); System.out.println(&quot;ReturnCallback: &quot;+&quot;回应信息：&quot;+replyText); System.out.println(&quot;ReturnCallback: &quot;+&quot;交换机：&quot;+exchange); System.out.println(&quot;ReturnCallback: &quot;+&quot;路由键：&quot;+routingKey); &#125; &#125;); return rabbitTemplate; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"MQ","slug":"MQ","permalink":"https://wingowen.github.io/tags/MQ/"}]},{"title":"Spring Boot 缓存","slug":"后台技术/Spring Boot/Spring Boot 缓存","date":"2020-03-31T01:59:45.000Z","updated":"2023-09-20T07:00:03.774Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 缓存/","link":"","permalink":"https://wingowen.github.io/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E7%BC%93%E5%AD%98/","excerpt":"JSR107、Spring 缓存抽象。","text":"JSR107、Spring 缓存抽象。 JRS107 Java Caching 定义了 5 个核心接口，分别是 CachingProvider, CacheManager, Cache, Entry 和 Expiry。 • CachingProvider：定义了创建、配置、获取、管理和控制多个 CacheManager。一个应用可以在运行期访问多个 CachingProvider； • CacheManager：定义了创建、配置、获取、管理和控制多个唯一命名的 Cache，这些 Cache存在于 CacheManager 的上下文中。一个 CacheManager 仅被一个 CachingProvider 所拥有； • Cache：是一个类似 Map 的数据结构并临时存储以 Key 为索引的值。一个 Cache 仅被一个 CacheManager 所拥有； • Entry：是一个存储在 Cache 中的 key-value 对； • Expiry：每一个存储在 Cache 中的条目有一个定义的有效期。一旦超过这个时间，条目为过期的状态。一旦过期，条目将不可访问、更新和删除。缓存有效期可以通过ExpiryPolicy设置。 Spring 缓存抽象 Spring 3.1 以上版本定义了 org.springframework.cache.Cache 和 org.springframework.cache.CacheManager 接口来统一不同的缓存技术；并支持使用 JCache（JSR-107）注解简化我们开发。 Cache 接口为缓存的组件规范定义，包含缓存的各种操作集合； Cache 接口下 Spring 提供了各种 xxxCache 的实现；如 RedisCache、EhCacheCache 、ConcurrentMapCache等。 每次调用需要缓存功能的方法时，Spring 会检查检查指定参数的指定的目标方法是否已经被调用过：如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户。下次调用直接从缓存中获取。 确定方法需要被缓存以及他们的缓存策略； 从缓存中读取之前缓存存储的数据。 注解 基本流程 添加缓存模块的 stater： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt; 引入 Redis 的 starter，容器中默认保存的是 RedisCacheManager。RedisCacheManager 负责创建RedisCache，RedisCache 负责操作 Redis 缓存数据。默认保存数据 Key / Value 都是 Object，默认使用的是 JDK 序列化器。 在 Spring Boot 主类中添加 EnableCaching 注解开启缓存功能： 1234567@SpringBootApplication@EnableCachingpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 在类上添加CacheConfig进行缓存配置；在方法上添加@Cacheable，标注此方法返回值需要被缓存。 12345@CacheConfig(cacheNames = &quot;users&quot;)public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Cacheable User findByName(String name);&#125; 注解详解 @CacheConfig：主要用于配置该类中会用到的一些共用的缓存配置。在这里@CacheConfig(cacheNames = &quot;users&quot;)：配置了该数据访问对象中返回的内容将存储于名为 users 的缓存对象中，我们也可以不使用该注解，直接通过@Cacheable自己配置缓存集的名字来定义。 @Cacheable：配置了 findByName 函数的返回值将被加入缓存。同时在查询时，会先从缓存中获取，若不存在才再发起对数据库的访问。该注解主要有下面几个参数： value、cacheNames：两个等同的参数（cacheNames为 Spring 4 新增，作为value的别名），用于指定缓存存储的集合名。由于 Spring 4 中新增了@CacheConfig，因此在 Spring 3 中原本必须有的value属性，也成为非必需项了； key：缓存对象存储在 Map 集合中的 key 值，非必需，缺省按照函数的所有参数组合作为 key 值，若自己配置需使用 SpEL 表达式，比如：@Cacheable(key = &quot;#p0&quot;)：使用函数第一个参数作为缓存的 key 值，更多关于 SpEL 表达式的详细内容可参考官方文档； condition：缓存对象的条件，非必需，也需使用 SpEL 表达式，只有满足表达式条件的内容才会被缓存，比如：@Cacheable(key = &quot;#p0&quot;, condition = &quot;#p0.length() &lt; 3&quot;)，表示只有当第一个参数的长度小于3的时候才会被缓存； unless：另外一个缓存条件参数，非必需，需使用 SpEL 表达式。它不同于condition参数的地方在于它的判断时机，该条件是在函数被调用之后才做判断的，所以它可以通过对 result 进行判断； keyGenerator：用于指定 key 生成器，非必需。若需要指定一个自定义的 key 生成器，需要开发者实现org.springframework.cache.interceptor.KeyGenerator接口，并使用该参数来指定。需要注意的是：该参数与key是互斥的； cacheManager：用于指定使用哪个缓存管理器，非必需。只有当有多个时才需要使用； cacheResolver：用于指定使用那个缓存解析器，非必需。需通过org.springframework.cache.interceptor.CacheResolver接口来实现自己的缓存解析器，并用该参数指定。 @CachePut：配置于函数上，能够根据参数定义条件来进行缓存，它与@Cacheable不同的是，它每次都会真实调用函数，所以主要用于数据新增和修改操作上。它的参数与@Cacheable类似，具体功能可参考上面对@Cacheable参数的解析。 @CacheEvict：配置于函数上，通常用在删除方法上，用来从缓存中移除相应数据。除了同@Cacheable一样的参数之外，它还有下面两个参数： allEntries：非必需，默认为 false。当为 true 时，会移除所有数据； beforeInvocation：非必需，默认为 false，会在调用方法之后移除数据。当为 true 时，会在调用方法之前移除数据。 Template Spring Boot 提供了帮助操作 Redis 的 Helper 类 RedisTemplate：RedisTemplate的 K:V 均为 Object； StringRedisTemplate 继承自 RedisTemplate，是专为 String:String 类型的 K:V 提供服务。 在 RedisAutoConfiguration 中 Spring Boot 自动注册了 RedisTemplate 和 StringRedisTemplate。 数据操作 Redis 中常见的五大类数据类型：String，List，Set，Hash 和 ZSet。 RedisTemplate 封装了对五大类数据进行操作的方法，每一个方法都会返回一个 Operations 对象。 123456789RedisTemplate.opsForValue(); // StringRedisTemplate.opsForList();RedisTemplate.opsForSet();RedisTemplate.opsForHash();RedisTemplate.opsForZSet(); Redis 实现缓存 入门基础 String 最基础的数据类型； List 元素不具有唯一性；有序；是一个双向链表；既可以是栈，也可以是队列； Set 元素具有唯一性；无序； Hash 存储的是key-value结构，key必须是string；类似于 MySQL 中的一条记录。 Redis 命令接口 Redis 提供了许多命令供我们使用，同样在 Spring Boot 中也封装了对应类型的命令接口供开发者使用。 123456789101112131415161718192021/** * Interface for the commands supported by Redis. * * @author Costin Leau * @author Christoph Strobl */public interface RedisCommands extends RedisKeyCommands, RedisStringCommands, RedisListCommands, RedisSetCommands, RedisZSetCommands, RedisHashCommands, RedisTxCommands, RedisPubSubCommands, RedisConnectionCommands, RedisServerCommands, RedisScriptingCommands, RedisGeoCommands, HyperLogLogCommands &#123; /** * &#x27;Native&#x27; or &#x27;raw&#x27; execution of the given command along-side the given arguments. The command is executed as is, * with as little &#x27;interpretation&#x27; as possible - it is up to the caller to take care of any processing of arguments or * the result. * * @param command Command to execute * @param args Possible command arguments (may be null) * @return execution result. */ Object execute(String command, byte[]... args);&#125; 在 Spring Boot 配置文件中添加 Redis 配置： 1234567891011121314151617## 默认密码为空redis: host: 127.0.0.1 # Redis服务器连接端口 port: 6379 jedis: pool: # 连接池最大连接数（使用负值表示没有限制） max-active: 100 # 连接池中的最小空闲连接 max-idle: 10 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: 100000 # 连接超时时间（毫秒） timeout: 5000 # 默认是索引为 0 的数据库 database: 0 代码示例 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringBootCacheApplicationTests &#123; @Autowired EmployeeMapper employeeMapper; @Autowired StringRedisTemplate stringRedisTemplate; @Autowired RedisTemplate redisTemplate; // 向 Redis 中保存一个对象并取出打印 @Test public void test() &#123; Employee employee = employeeMapper.getEmpById(1); redisTemplate.opsForValue().set(&quot;emp01&quot;,employee); Object emp01 = redisTemplate.opsForValue().get(&quot;emp01&quot;); System.out.println(emp01); &#125;&#125; 注意，Redis 中对象的保存是需要进行序列化的，默认使用 JdkSerializationRedisSerializer 序列化器，所以在 Redis 中查看保存的对象是序列化后的二进制编码，正常使用是没有问题的，查询出来的时候会自动反序列化。 如果习惯于使用 String，那么可以将其 JSON 化，两种方式：使用第三方 JSON 或者向容器中添加自定义 RedisTemplate 改变其默认序列化器。 添加自定义 RedisTemplate： 123456789101112@Configurationpublic class MyRedisConfig &#123; @Bean public RedisTemplate&lt;Object,Employee&gt; empRedisTemplate(RedisConnectionFactory connectionFactory)&#123; RedisTemplate&lt;Object,Employee&gt; template = new RedisTemplate&lt;Object, Employee&gt;(); template.setConnectionFactory(connectionFactory); Jackson2JsonRedisSerializer&lt;Employee&gt; serializer = new Jackson2JsonRedisSerializer&lt;Employee&gt;(Employee.class); template.setDefaultSerializer(serializer); return template; &#125;&#125; Lettuce Jedis 在实现上是直接连接 Redis 服务器，在多个线程间共享一个 Jedis 实例时是线程不安全的，如果想要在多线程场景下使用 Jedis ，需要使用连接池，每个线程都使用自己的 Jedis 实例，当连接数量增多时，会消耗较多的物理资源。 与 Jedis 相比， Lettuce 则完全克服了其线程不安全的缺点： Lettuce 是一个可伸缩的线程安全的 Redis 客户端，支持同步、异步和响应式模式。多个线程可以共享一个连接实例，而 不必担心多线程并发问题。它基于优秀 Netty NIO 框架构建，支持 Redis 的更多高级功能。 12345678910&lt;!-- redis 访问启动器 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- redis 客户端 Lettuce 数据库连接池依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 123456789101112131415161718# Redis数据库索引（默认为0）spring.redis.database=0# Redis服务器地址 （默认localhost）spring.redis.host=localhost# Redis服务器连接端口 （默认6379）spring.redis.port=6379# Redis服务器连接密码（默认为空）spring.redis.password=123456# 连接超时时间spring.redis.timeout=10000ms# 最大连接数（使用负值表示没有限制） 默认 8spring.redis.lettuce.pool.max-active=8# 最小空闲连接 默认 0spring.redis.lettuce.pool.min-idle=0# 最大空闲连接 默认 8spring.redis.lettuce.pool.max-idle=8# 最大阻塞等待时间（使用负值表示没有限制） 默认 -1msspring.redis.lettuce.pool.max-wait=-1ms 缓存原理 要使用 Spring Boot 的缓存功能，还需要提供一个缓存的具体实现。 Spring Boot 一定的顺序去侦测缓存实现。 Spring 提供了一个统一访问缓存的接口：CacheManager ctrl + alt + b 可查看这个接口的实现 在 RedisCacheManager 中查看 Redis 缓存的默认配置类：RedisCacheConfiguration： 从源码了解到 Sping Boot 的 Reids 缓存对 Key 和 Value 默认的序列化器分别是 String 类型和 JDK 序列器。 在 RedisCacheManager 中可以看到如何实例化这个类： 实例化这个类必须传入一个 connectionFactory，用 redis 缓存所以传入一个 redisConnectionFactory。 123456789101112131415@Configurationpublic class CacheConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; /** * SpringCacheManager 缓存管理器：使用的缓存产品是Redis * @return */ @Bean(name = &quot;springCacheManager&quot;) public RedisCacheManager springCacheManager()&#123; RedisCacheManager cacheManager = RedisCacheManager.create(redisConnectionFactory); return cacheManager; &#125;&#125; Redis 配置 123456789101112131415161718192021222324252627282930313233@Configurationpublic class RedisConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public StringRedisSerializer stringRedisSerializer() &#123; StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); return stringRedisSerializer; &#125; @Bean public StringRedisTemplate stringRedisTemplate() &#123; StringRedisTemplate stringRedisTemplate =new StringRedisTemplate(); stringRedisTemplate.setConnectionFactory(redisConnectionFactory); // 开启事务支持 stringRedisTemplate.setEnableTransactionSupport(true); return stringRedisTemplate; &#125; @Bean public RedisTemplate redisTemplate()&#123; RedisTemplate&lt;Object,Object&gt; redisTemplate= new RedisTemplate&lt;&gt;(); // 序列化器 redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setKeySerializer(stringRedisSerializer()); //redisTemplate.setHashKeySerializer(stringRedisSerializer()); // 开启事务支持 redisTemplate.setEnableTransactionSupport(true); return redisTemplate; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Cache","slug":"Cache","permalink":"https://wingowen.github.io/tags/Cache/"}]},{"title":"开发常用工具类","slug":"后台技术/开发常用工具类","date":"2020-03-12T07:31:13.000Z","updated":"2023-09-20T06:50:00.031Z","comments":true,"path":"2020/03/12/后台技术/开发常用工具类/","link":"","permalink":"https://wingowen.github.io/2020/03/12/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/","excerpt":"记录一下开发中用到的各种小工具类。","text":"记录一下开发中用到的各种小工具类。 加密类 Md5 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.security.MessageDigest;import org.apache.commons.lang3.StringUtils;public class Md5Util &#123; private Md5Util() &#123; throw new IllegalStateException(&quot;Utility class&quot;); &#125; /** * md5 加密通用工具类 * * @param encode 需加密的字符串 * @return md5 加密后的字符串（32位字符串） */ public static String md5(String encode) &#123; // 检验参数是否是 null 或者 &quot;&quot; 或者 &quot; &quot; 等 if (StringUtils.isBlank(encode)) &#123; return &quot;&quot;; &#125; char[] hexDigits = &#123;&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;&#125;; try &#123; // 设置字符集 byte[] strTemp = encode.getBytes(StandardCharsets.UTF_8); // 生成一个 md5 加密计算摘要 MessageDigest mdTemp = MessageDigest.getInstance(&quot;MD5&quot;); // 计算 md5 函数 mdTemp.update(strTemp); // digest() 最后确定返回 md5 hash 值，返回值为 8 位字符串。因为 md5 hash 值是 16 位的 hex 值，实际上就是 8 位的字符 byte[] md = mdTemp.digest(); int j = md.length; char[] str = new char[j * 2]; int k = 0; for (byte byte0 : md) &#123; // 不带符号右移四位(不管 byte0 的类型 位移处补 0)，&amp; 十六进制的 f 即 高四位清空 取低四位的值，&gt;&gt;&gt; 优先级高于 &amp; str[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; str[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(str); &#125; catch (Exception e) &#123; return &quot;&quot;; &#125; &#125;&#125; Json FastJson 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class FastJsonConvertUtil &#123; private static final SerializerFeature[] featuresWithNullValue = &#123; SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullBooleanAsFalse, SerializerFeature.WriteNullListAsEmpty, SerializerFeature.WriteNullNumberAsZero, SerializerFeature.WriteNullStringAsEmpty &#125;; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSON字符串转换为实体对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSON字符串转换为实体对象&lt;BR&gt; * @param data JSON字符串 * @param clzss 转换对象 * @return T */ public static &lt;T&gt; T convertJSONToObject(String data, Class&lt;T&gt; clzss) &#123; try &#123; T t = JSON.parseObject(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSONObject对象转换为实体对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSONObject对象转换为实体对象&lt;BR&gt; * @param data JSONObject对象 * @param clzss 转换对象 * @return T */ public static &lt;T&gt; T convertJSONToObject(JSONObject data, Class&lt;T&gt; clzss) &#123; try &#123; T t = JSONObject.toJavaObject(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSON字符串数组转为List集合对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSON字符串数组转为List集合对象&lt;BR&gt; * @param data JSON字符串数组 * @param clzss 转换对象 * @return List&lt;T&gt;集合对象 */ public static &lt;T&gt; List&lt;T&gt; convertJSONToArray(String data, Class&lt;T&gt; clzss) &#123; try &#123; List&lt;T&gt; t = JSON.parseArray(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将List&lt;JSONObject&gt;转为List集合对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将List&lt;JSONObject&gt;转为List集合对象&lt;BR&gt; * @param data List&lt;JSONObject&gt; * @param clzss 转换对象 * @return List&lt;T&gt;集合对象 */ public static &lt;T&gt; List&lt;T&gt; convertJSONToArray(List&lt;JSONObject&gt; data, Class&lt;T&gt; clzss) &#123; try &#123; List&lt;T&gt; t = new ArrayList&lt;T&gt;(); for (JSONObject jsonObject : data) &#123; t.add(convertJSONToObject(jsonObject, clzss)); &#125; return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将对象转为JSON字符串&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将对象转为JSON字符串&lt;BR&gt; * @param obj 任意对象 * @return JSON字符串 */ public static String convertObjectToJSON(Object obj) &#123; try &#123; String text = JSON.toJSONString(obj); return text; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将对象转为JSONObject对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将对象转为JSONObject对象&lt;BR&gt; * @param obj 任意对象 * @return JSONObject对象 */ public static JSONObject convertObjectToJSONObject(Object obj)&#123; try &#123; JSONObject jsonObject = (JSONObject) JSONObject.toJSON(obj); return jsonObject; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;&lt;BR&gt; * @param obj * @return */ public static String convertObjectToJSONWithNullValue(Object obj) &#123; try &#123; String text = JSON.toJSONString(obj, featuresWithNullValue); return text; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wingowen.github.io/tags/Java/"}]},{"title":"Spring Boot 数据访问","slug":"后台技术/Spring Boot/Spring Boot 数据访问","date":"2020-03-05T07:42:42.000Z","updated":"2023-09-20T06:44:49.035Z","comments":true,"path":"2020/03/05/后台技术/Spring Boot/Spring Boot 数据访问/","link":"","permalink":"https://wingowen.github.io/2020/03/05/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/","excerpt":"Spring Boot 数据访问的相关知识。","text":"Spring Boot 数据访问的相关知识。 JDBC123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 1234567spring: datasource: username: root password: 123456 url: jdbc:mysql://localhosthost:3306/jdbc driver-class-name: com.mysql.jdbc.Driver # Spring Boot 2.1.x 默认使用了 MySQL 8.0：com.mysql.cj.jdbc.Driver Sping Boot 默认使用 org.apache.tomcat.jdbc.pool.DataSource 作为数据源，相关配置都在 DataSourceProperties 里面。 默认建表配置规则 1schema-*.sql 指定建表文件 sql 文件的文件名 1234spring: datasource: schema: - classpath:user.sql Sping Boot 自动配置了 JdbcTemplate 操作数据库 JdbcTempale创建一个 User 表： 1234CREATE TABLE `User` ( `name` varchar(100) COLLATE utf8mb4_general_ci NOT NULL, `age` int NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci 编写实体对象： 12345678@Data // Getter / Setter@NoArgsConstructor // 无参构造器public class User &#123; private String name; private Integer age;&#125; 数据访问对象接口： 123456789101112131415161718public interface UserService &#123; // 新增一个用户 int create(String name, Integer age); // 根据 name 查询用户 List&lt;User&gt; getByName(String name); // 根据 name 删除用户 int deleteByName(String name); // 获取用户总量 int getAllUsers(); // 删除所有用户 int deleteAllUsers();&#125; 数据访问操作的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private JdbcTemplate jdbcTemplate; UserServiceImpl(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; &#125; @Override public int create(String name, Integer age) &#123; return jdbcTemplate.update(&quot;insert into USER(NAME, AGE) values(?, ?)&quot;, name, age); &#125; @Override public List&lt;User&gt; getByName(String name) &#123; List&lt;User&gt; users = jdbcTemplate.query (&quot;select NAME, AGE from USER where NAME = ?&quot;, (resultSet, i) -&gt; &#123; User user = new User(); user.setName(resultSet.getString(&quot;NAME&quot;)); user.setAge(resultSet.getInt(&quot;AGE&quot;)); return user; &#125;, name); return users; &#125; @Override public int deleteByName(String name) &#123; return jdbcTemplate.update(&quot;delete from USER where NAME = ?&quot;, name); &#125; @Override public int getAllUsers() &#123; return jdbcTemplate.queryForObject(&quot;select count(1) from USER&quot;, Integer.class); &#125; @Override public int deleteAllUsers() &#123; return jdbcTemplate.update(&quot;delete from USER&quot;); &#125;&#125; 整合 Druid 数据源123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--数据库连接池--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/i-auth?useSSL=false# 可以不配置 Druid 可以根据 url 自动识别数据库驱动# spring.datasource.driver-class-name=com.mysql.jdbc.Driver# 初始化连接数spring.datasource.druid.initial-size=5# 最小连接数spring.datasource.druid.min-idle=10# 最大连接数spring.datasource.druid.max-active=10# 获取连接最长等待时间 单位毫秒spring.datasource.druid.max-wait=10000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.druid.timeBetweenEvictionRunsMillis=30000# 指定获取连接时连接校验的sql查询语句spring.datasource.druid.validationQuery=select &#x27;x&#x27;# 获取连接后，确实是否要进行连接空闲时间的检查spring.datasource.druid.testWhileIdle=true# 获取连接检测spring.datasource.druid.testOnBorrow=false# 归还连接检测spring.datasource.druid.testOnReturn=false# 指定连接校验查询的超时时间spring.datasource.druid.validationQueryTimeout=600000# 配置一个连接在池总最小生存的时间spring.datasource.druid.minEvictableIdleTimeMillis=300000# 打开 PSCache，并且指定每个连接上 Cache 的大小spring.datasource.druid.poolPreparedStatements=truespring.datasource.druid.maxPoolPreparedStatementPerConnectionSize=20# 配置监控统计拦截的filter，去掉后监控界面sql无法统计，&#x27;wall&#x27;用于防火墙spring.datasource.druid.filters=stat,wall,slf4j# 通过connectProperties属性来打开mergeSql功能，慢sql记录等spring.datasource.druid.connectionProperties=druid.stat.mergeSql=true;druid.stat.logSlowSql=true;druid.stat.slowSqlMillis=5000## druid连接池监控# 需要账号密码才能访问控制台，默认为rootspring.datasource.druid.stat-view-servlet.login-username=adminspring.datasource.druid.stat-view-servlet.login-password=123# 访问路径为/druid时，跳转到StatViewServletspring.datasource.druid.stat-view-servlet.url-pattern=/druid/*# 是否能够重置数据spring.datasource.druid.stat-view-servlet.reset-enable=false# 排除一些静态资源，以提高效率spring.datasource.druid.web-stat-filter.exclusions=*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/* 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 导入 Druid 数据源@Configurationpublic class DruidConfig &#123; // 一个自定义的 DataSource 组件 @ConfigurationProperties(prefix = &quot;spring.datasource&quot;) @Bean public DataSource druid()&#123; return new DruidDataSource(); &#125; // 配置 Druid 的监控 // 配置一个管理后台的 Servlet @Bean public ServletRegistrationBean statViewServlet()&#123; ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), &quot;/druid/*&quot;); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(&quot;loginUsername&quot;,&quot;admin&quot;); initParams.put(&quot;loginPassword&quot;,&quot;123456&quot;); initParams.put(&quot;allow&quot;,&quot;&quot;); // 默认就是允许所有访问 initParams.put(&quot;deny&quot;,&quot;192.168.15.21&quot;); bean.setInitParameters(initParams); return bean; &#125; // 配置一个 Web 监控的 Filter @Bean public FilterRegistrationBean webStatFilter()&#123; FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(&quot;exclusions&quot;,&quot;*.js,*.css,/druid/*&quot;); bean.setInitParameters(initParams); bean.setUrlPatterns(Arrays.asList(&quot;/*&quot;)); return bean; &#125;&#125; 整合 MyBatis可直接使用插件：Mybatis-Plus 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 注解版 1234567891011121314151617// 指定这是一个操作数据库的 Mapper@Mapperpublic interface DepartmentMapper &#123; @Select(&quot;select * from department where id=#&#123;id&#125;&quot;) public Department getDeptById(Integer id); @Delete(&quot;delete from department where id=#&#123;id&#125;&quot;) public int deleteDeptById(Integer id); @Options(useGeneratedKeys = true,keyProperty = &quot;id&quot;) @Insert(&quot;insert into department(departmentName) values(#&#123;departmentName&#125;)&quot;) public int insertDept(Department department); @Update(&quot;update department set departmentName=#&#123;departmentName&#125; where id=#&#123;id&#125;&quot;) public int updateDept(Department department);&#125; 123456789// 使用 MapperScan 批量扫描所有的 Mapper 接口，或在 Mapper 类使用 Mapper@MapperScan(value = &quot;com.wingo.springboot.mapper&quot;)@SpringBootApplicationpublic class SpringBootDataMybatisApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootDataMybatisApplication.class, args); &#125;&#125; 自定义 MyBatis 配置规则：驼峰命名法。 1234567891011121314@Configuration public class MyBatisConfig &#123; @Bean public ConfigurationCustomizer configurationCustomizer()&#123; return new ConfigurationCustomizer()&#123; @Override public void customize(Configuration configuration) &#123; configuration.setMapUnderscoreToCamelCase(true); &#125; &#125;; &#125; &#125; 配置文件版 12345mybatis: # 指定全局配置文件的位置 config-location: classpath:mybatis/mybatis-config.xml # 指定 Sql 映射文件的位置 mapper-locations: classpath:mybatis/mapper/*.xml 整合 SpringData JPA12345678910111213&lt;!-- JPA --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 编写一个实体类 Bean 和数据表进行映射，并且配置好映射关系： 1234567891011121314// 使用 JPA 注解配置映射关系@Entity // 告诉 JPA 这是一个实体类（和数据表映射的类）@Table(name = &quot;tbl_user&quot;) // @Table 来指定和哪个数据表对应，如果省略默认表名就是 userpublic class User &#123; @Id // 这是一个主键 @GeneratedValue(strategy = GenerationType.IDENTITY) // 自增主键 private Integer id; @Column(name = &quot;last_name&quot;,length = 50) // 这是和数据表对应的一个列 private String lastName; @Column // 省略默认列名就是属性名 private String email;&#125; 编写一个 Dao 接口来操作实体类对应的数据表： 123// 继承 JpaRepository 来完成对数据库的操作，传参：实体类型 主键类型public interface UserRepository extends JpaRepository&lt;User,Integer&gt; &#123;&#125; JpaProperties 基本配置 1234567spring: jpa: hibernate: # 更新或者创建数据表结构 ddl-auto: update # 控制台显示SQL show-sql: true","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wingowen.github.io/tags/MyBatis/"},{"name":"JPA","slug":"JPA","permalink":"https://wingowen.github.io/tags/JPA/"},{"name":"Druid","slug":"Druid","permalink":"https://wingowen.github.io/tags/Druid/"},{"name":"JDBC","slug":"JDBC","permalink":"https://wingowen.github.io/tags/JDBC/"}]},{"title":"Spring Boot Web 开发","slug":"后台技术/Spring Boot/Spring Boot Web 开发","date":"2020-03-03T06:08:47.000Z","updated":"2023-09-20T07:00:03.799Z","comments":true,"path":"2020/03/03/后台技术/Spring Boot/Spring Boot Web 开发/","link":"","permalink":"https://wingowen.github.io/2020/03/03/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20Web%20%E5%BC%80%E5%8F%91/","excerpt":"Spring Boot Web 方面的开发介绍。","text":"Spring Boot Web 方面的开发介绍。 自动配置原理： xxxAutoConfiguration：帮我们给容器中自动配置组件； xxxProperties：配置类封装配置文件的内容。 静态资源映射 以 JAR 包的方引入静态资源： 123456&lt;!-- 引入 jquery-webjar 在访问的时候只需要写 webjars 下面资源的名称即可 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt; 访问 jquery.js：localhost:8080/webjars/jquery/3.3.1/jquery.js 访问当前项目的任何资源，都去静态资源文件夹找映射。 12345&quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;,&quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &quot;/&quot;：当前项目的根路径 Thymeleaf 模板引擎 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 默认是 2.1.6 版本 --&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 切换 Thymeleaf 版本 --&gt;&lt;properties&gt; &lt;thymeleaf.version&gt;3.0.9.RELEASE&lt;/thymeleaf.version&gt; &lt;!-- 布局功能的支持程序 Thymeleaf3 主程序 Layout2 以上版本 --&gt; &lt;!-- Thymeleaf2 layout1--&gt; &lt;thymeleaf-layout-dialect.version&gt;2.2.2&lt;/thymeleaf-layout-dialect.version&gt;&lt;/properties&gt; 自动配置 Spriing Boot 对于 Thymeleaf 的自动配置 12345678910# Enable template cachingspring thymeleaf cache=true# Template encodingspring thymeleaf encoding=UTF-8# Template mode to be applied to templates. See also StandardTemplateModeHandlersspring thymeleaf mode=HTML5# Prefix that gets prepended to view names when building a URLspring thymeleaf prefix=classpath: /templates/# Suffix that gets appended to view names when building a URLspring thymeleaf suffix=.html 12345678910111213@ConfigurationProperties(prefix = &quot;spring.thymeleaf&quot;)public class ThymeleafProperties &#123; private static final Charset DEFAULT_ENCODING = Charset.forName(&quot;UTF-8&quot;); private static final MimeType DEFAULT_CONTENT_TYPE = MimeType.valueOf(&quot;text/html&quot;); public static final String DEFAULT_PREFIX = &quot;classpath:/templates/&quot;; public static final String DEFAULT_SUFFIX = &quot;.html&quot;; // ...&#125; 从自动配置类中可以看出只要我们把 HTML 页面放在classpath:/templates/目录下 Thymeleaf 就能自动渲染。 Thymeleaf 的使用 123456789101112&lt;!DOCTYPE html&gt;&lt;!-- 导入 Thymeleaf 的名称空间 --&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- th:text 设置 div 里面的文本内容 --&gt; &lt;div th:text=&quot;$&#123;hello&#125;&quot;&gt;这是显示欢迎信息&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 常用标签 常用表达式 123456789101112131415161718192021行内写法：[[]] -&gt; th:text 会转义特殊字符[()] -&gt; th:utext 不会转义特殊字符内置工具类：#execInfo : information about the template being processed.#messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #&#123;…&#125; syntax.#uris : methods for escaping parts of URLs/URIs#conversions : methods for executing the configured conversion service (if any).#dates : methods for java.util.Date objects: formatting, component extraction, etc.#calendars : analogous to #dates , but for java.util.Calendar objects.#numbers : methods for formatting numeric objects.#strings : methods for String objects: contains, startsWith, prepending/appending, etc.#objects : methods for objects in general.#bools : methods for boolean evaluation.#arrays : methods for arrays.#lists : methods for lists.#sets : methods for sets.#maps : methods for maps.#aggregates : methods for creating aggregates on arrays or collections.#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). Spring MVC 自动配置 Sping Boot 对 Spring MVC 的默认配置：WebMvcAutoConfiguration 12345678910//使用 WebMvcConfigurerAdapter 可以来扩展 SpringMVC 的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // 浏览器发送 /wingo 请求来到 success registry.addViewController(&quot;/wigno&quot;).setViewName(&quot;success&quot;); &#125;&#125; 不能标注@EnableWebMvc，标注后 Spring MVC 的默认自动装配将不会进行，即开发者全面接管 Spring MVC Restful CRUD 默认访问首页： 1234567891011121314151617// 使用 WebMvcConfigurerAdapter 可以来扩展 Spring MVC 的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; // 所有的 WebMvcConfigurerAdapter 组件都会一起起作用 @Bean // 将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/&quot;).setViewName(&quot;login&quot;); registry.addViewController(&quot;/index.html&quot;).setViewName(&quot;login&quot;); &#125; &#125;; return adapter; &#125;&#125; 国际化 IDEA 会自动识别国际化目录。 IDEA 进行优化后的便于编辑的视图：（点击左下角的 Resource Bundle） Spring Boot 中的消息自动配置： 12345678public class MessageSourceAutoConfiguration &#123; // ... // 默认配置为 spring.messages.basename=messages 即国际化资源默认放在类路径下的 messages.properties String basename = context.getEnvironment().getProperty(&quot;spring.messages.basename&quot;, &quot;messages&quot;); //... &#125; 12# 自定义国际化文件的位置spring.message.basename=i18n.login 页面获取国际化的信息： 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, shrink-to-fit=no&quot;&gt; &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; &lt;meta name=&quot;author&quot; content=&quot;&quot;&gt; &lt;title&gt;Signin Template for Bootstrap&lt;/title&gt; &lt;!-- webjars 形式的静态资源映射 --&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href=&quot;asserts/css/bootstrap.min.css&quot; th:href=&quot;@&#123;/webjars/bootstrap/4.0.0/css/bootstrap.css&#125;&quot; rel=&quot;stylesheet&quot;&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href=&quot;asserts/css/signin.css&quot; th:href=&quot;@&#123;/asserts/css/signin.css&#125;&quot; rel=&quot;stylesheet&quot;&gt; &lt;/head&gt; &lt;body class=&quot;text-center&quot;&gt; &lt;form class=&quot;form-signin&quot; action=&quot;dashboard.html&quot;&gt; &lt;img class=&quot;mb-4&quot; th:src=&quot;@&#123;/asserts/img/bootstrap-solid.svg&#125;&quot; src=&quot;asserts/img/bootstrap-solid.svg&quot; alt=&quot;&quot; width=&quot;72&quot; height=&quot;72&quot;&gt; &lt;!-- Thymeleaf 用 #&#123;&#125; 表达式取出国际化信息 --&gt; &lt;h1 class=&quot;h3 mb-3 font-weight-normal&quot; th:text=&quot;#&#123;login.tip&#125;&quot;&gt;Please sign in&lt;/h1&gt; &lt;label class=&quot;sr-only&quot; th:text=&quot;#&#123;login.username&#125;&quot;&gt;Username&lt;/label&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;Username&quot; th:placeholder=&quot;#&#123;login.username&#125;&quot; required=&quot;&quot; autofocus=&quot;&quot;&gt; &lt;label class=&quot;sr-only&quot; th:text=&quot;#&#123;login.password&#125;&quot;&gt;Password&lt;/label&gt; &lt;input type=&quot;password&quot; class=&quot;form-control&quot; placeholder=&quot;Password&quot; th:placeholder=&quot;#&#123;login.password&#125;&quot; required=&quot;&quot;&gt; &lt;div class=&quot;checkbox mb-3&quot;&gt; &lt;label&gt; &lt;input type=&quot;checkbox&quot; value=&quot;remember-me&quot;/&gt; [[#&#123;login.remember&#125;]] &lt;/label&gt; &lt;/div&gt; &lt;button class=&quot;btn btn-lg btn-primary btn-block&quot; type=&quot;submit&quot; th:text=&quot;#&#123;login.btn&#125;&quot;&gt;Sign in&lt;/button&gt; &lt;p class=&quot;mt-5 mb-3 text-muted&quot;&gt;© 2017-2018&lt;/p&gt; &lt;a class=&quot;btn btn-sm&quot;&gt;中文&lt;/a&gt; &lt;a class=&quot;btn btn-sm&quot;&gt;English&lt;/a&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 根据浏览器请求中所携带的国际化 Locale 信息进行语言的转换。 123456789101112131415@Bean@ConditionalOnMissingBean@ConditionalOnProperty( prefix = &quot;spring.mvc&quot;, name = &#123;&quot;locale&quot;&#125;)public LocaleResolver localeResolver() &#123; if (this.mvcProperties.getLocaleResolver() == org.springframework.boot.autoconfigure.web.servlet.WebMvcProperties.LocaleResolver.FIXED) &#123; return new FixedLocaleResolver(this.mvcProperties.getLocale()); &#125; else &#123; AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver(); localeResolver.setDefaultLocale(this.mvcProperties.getLocale()); return localeResolver; &#125;&#125; 可以在切换语言的的链接上携带区域信息，进行语言切换。 1234567891011121314151617181920212223public class MyLocaleResolver implements LocaleResolver &#123; @Override public Locale resolveLocale(HttpServletRequest request) &#123; String myLocale = request.getParameter(&quot;locale&quot;); Locale locale = Locale.getDefault(); if(!StringUtils.isEmpty(myLocale))&#123; String[] split = myLocate.split(&quot;_&quot;); //国家 语言 locale = new Locale(split[0],split[1]); &#125; return locale; &#125; @Override public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) &#123;&#125; @Bean public LocaleResolver localeResolver()&#123; return new MyLocaleResolver(); &#125;&#125; 用户登录 12# 禁用缓存spring.thymeleaf.cache=false 修改了页面后 Ctrl + F9 进行重新编译 123&lt;!-- 登录错误消息显示 --&gt;&lt;!-- 使用 Thymeleaf 内置工具类 #Srting, 取传递变量的值 $&#123;&#125; 表达式--&gt;&lt;p style=&quot;color: red&quot; th:text=&quot;$&#123;msg&#125;&quot; th:if=&quot;$&#123;not #strings.isEmpty(msg)&#125;&quot;&gt;&lt;/p&gt; 拦截器 拦截器的编写： 12345678910111213141516171819public class LoginHandlerInterceptor implements HandlerInterceptor &#123; // 目标方法执行之前 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 用户登录后在 Session 中保存用户标志 Object user = request.getSession().getAttribute(&quot;loginUser&quot;); if(user == null)&#123; // 未登陆，返回登陆页面 request.setAttribute(&quot;msg&quot;,&quot;没有权限请先登陆&quot;); request.getRequestDispatcher(&quot;/index.html&quot;).forward(request,response); return false; &#125;else&#123; // 已登陆，放行请求 return true; &#125; &#125; // 省略其它不需要编写的抽象方法&#125; 注册拦截器 123456789101112131415161718192021// 所有的 WebMvcConfigurerAdapter 组件都会一起起作用@Bean // 将组件注册在容器public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/&quot;).setViewName(&quot;login&quot;); registry.addViewController(&quot;/index.html&quot;).setViewName(&quot;login&quot;); &#125; // 注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; // super.addInterceptors(registry); // 静态资源 *.css , *.js SpringBoot 已经做好了映射 // 除了访问登录页面的 URL 其余的 URL 全部进行拦截认证 registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns(&quot;/**&quot;) .excludePathPatterns(&quot;/index.html&quot;,&quot;/&quot;,&quot;/user/login&quot;); &#125; &#125;; return adapter;&#125; 员工列表 普通CRUD（URI 来区分操作） RestfulCRUD 查询 getEmp emp—GET 添加 addEmp?xxx emp—POST 修改 updateEmp?id=xxx&amp;xxx=xx emp/{id}—PUT 删除 deleteEmp?id=1 emp/{id}—DELETE 本次实验的请求架构： 实验功能 请求URI 请求方式 查询所有员工 emps GET 查询某个员工（来到修改页面） emp/{id} GET 来到添加页面 emp GET 添加员工 emp POST 来到修改页面（查出员工进行信息回显） emp/{id} GET 修改员工 emp PUT 删除员工 emp/{id} DELETE 员工列表，Thymeleaf 公共页面元素抽取。 123456789101112131415161718192021222324252627282930&lt;!-- common/footer.html 抽取公共片段 --&gt;&lt;footer th:fragment=&quot;copy&quot;&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;!-- 引入公共片段 --&gt;&lt;div th:insert=&quot;footer::copy&quot;&gt;&lt;/div&gt;&lt;div th:replace=&quot;footer::copy&quot;&gt;&lt;/div&gt;&lt;div th:include=&quot;footer::copy&quot;&gt;&lt;/div&gt;&lt;!-- ~&#123;templatename::selector&#125;：模板名::选择器~&#123;templatename::fragmentname&#125;:模板名::片段名insert 的公共片段在 div 标签中，如果使用 th:insert 等属性进行引入，可以不用写 ~&#123;&#125;；；行内写法可以加上：[[~&#123;&#125;]] [(~&#123;&#125;)]--&gt;&lt;!-- 效果 --&gt;&lt;div&gt; &lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery &lt;/footer&gt;&lt;/div&gt;&lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;div&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt; 引入片段时传递参数： 12345678&lt;!--引入侧边栏并传入参数；此处用的 id 选择器来引入片段 --&gt;&lt;div th:replace=&quot;commons/bar::#sidebar(activeUri=&#x27;emps&#x27;)&quot;&gt;&lt;/div&gt;&lt;!-- 应用场景：点击侧边栏的选项，被点击的选项样式发生改变，变为高亮 --&gt;&lt;!-- 解决方式：页面引用时所传递的参数进行样式的修改，生成的侧边栏样式根据传入的参数改变样式 --&gt;&lt;a class=&quot;nav-link active&quot; th:class=&quot;$&#123;activeUri==&#x27;main.html&#x27;?&#x27;nav-link active&#x27;:&#x27;nav-link&#x27;&#125;&quot; href=&quot;#&quot; th:href=&quot;@&#123;/main.html&#125;&quot;&gt; 添加及修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!-- 需要区分是员工修改还是添加 --&gt;&lt;form th:action=&quot;@&#123;/emp&#125;&quot; method=&quot;post&quot;&gt;&lt;!-- 发送 put 请求修改员工数据--&gt; &lt;!-- Spring MVC 中配置 HiddenHttpMethodFilter；（SpringBoot自动配置好的） 页面创建一个post表单； 创建一个 input 项，name=&quot;_method&quot;；value 就是我们指定的请求方式。 --&gt; &lt;!-- 若 emp 携带信息，则此表单提交的是 put 请求 --&gt; &lt;input type=&quot;hidden&quot; name=&quot;_method&quot; value=&quot;put&quot; th:if=&quot;$&#123;emp!=null&#125;&quot;/&gt; &lt;!-- 修改需要携带员工 id 用于保存修改 --&gt; &lt;input type=&quot;hidden&quot; name=&quot;id&quot; th:if=&quot;$&#123;emp!=null&#125;&quot; th:value=&quot;$&#123;emp.id&#125;&quot;&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;LastName&lt;/label&gt; &lt;input name=&quot;lastName&quot; type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;zhangsan&quot; th:value=&quot;$&#123;emp!=null&#125;?$&#123;emp.lastName&#125;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input name=&quot;email&quot; type=&quot;email&quot; class=&quot;form-control&quot; placeholder=&quot;zhangsan@atguigu.com&quot; th:value=&quot;$&#123;emp!=null&#125;?$&#123;emp.email&#125;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Gender&lt;/label&gt;&lt;br/&gt; &lt;div class=&quot;form-check form-check-inline&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;1&quot; th:checked=&quot;$&#123;emp!=null&#125;?$&#123;emp.gender==1&#125;&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt;男&lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;form-check form-check-inline&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;0&quot; th:checked=&quot;$&#123;emp!=null&#125;?$&#123;emp.gender==0&#125;&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt;女&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;department&lt;/label&gt; &lt;!--提交的是部门的id--&gt; &lt;select class=&quot;form-control&quot; name=&quot;department.id&quot;&gt; &lt;option th:selected=&quot;$&#123;emp!=null&#125;?$&#123;dept.id == emp.department.id&#125;&quot; th:value=&quot;$&#123;dept.id&#125;&quot; th:each=&quot;dept:$&#123;depts&#125;&quot; th:text=&quot;$&#123;dept.departmentName&#125;&quot;&gt;1&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Birth&lt;/label&gt; &lt;!-- #dates 工具类进行时间格式化 --&gt; &lt;input name=&quot;birth&quot; type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;zhangsan&quot; th:value=&quot;$&#123;emp!=null&#125;?$&#123;#dates.format(emp.birth, &#x27;yyyy-MM-dd HH:mm&#x27;)&#125;&quot;&gt; &lt;/div&gt; &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot; th:text=&quot;$&#123;emp!=null&#125;?&#x27;修改&#x27;:&#x27;添加&#x27;&quot;&gt;添加&lt;/button&gt;&lt;/form&gt; 删除 12345678910111213141516171819202122&lt;tr th:each=&quot;emp:$&#123;emps&#125;&quot;&gt; &lt;td th:text=&quot;$&#123;emp.id&#125;&quot;&gt;&lt;/td&gt; &lt;td&gt;[[$&#123;emp.lastName&#125;]]&lt;/td&gt; &lt;td th:text=&quot;$&#123;emp.email&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;emp.gender&#125;==0?&#x27;女&#x27;:&#x27;男&#x27;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;emp.department.departmentName&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;#dates.format(emp.birth, &#x27;yyyy-MM-dd HH:mm&#x27;)&#125;&quot;&gt;&lt;/td&gt; &lt;td&gt; &lt;a class=&quot;btn btn-sm btn-primary&quot; th:href=&quot;@&#123;/emp/&#125;+$&#123;emp.id&#125;&quot;&gt;编辑&lt;/a&gt; &lt;!-- th:attr 标签自定义属性及值 --&gt; &lt;button th:attr=&quot;del_uri=@&#123;/emp/&#125;+$&#123;emp.id&#125;&quot; class=&quot;btn btn-sm btn-danger deleteBtn&quot;&gt;删除&lt;/button&gt; &lt;/td&gt;&lt;/tr&gt;&lt;script&gt; $(&quot;.deleteBtn&quot;).click(function()&#123; // 删除当前员工 $(&quot;#deleteEmpForm&quot;).attr(&quot;action&quot;,$(this).attr(&quot;del_uri&quot;)).submit(); return false; &#125;);&lt;/script&gt; 错误处理机制 Spring Boot 默认的错误处理机制。 默认效果：对于浏览器，Spring Boot 返回一个默认的错误页面；对于其它客户端，默认响应一个 Json 数据。 1234567891011// 帮我们在页面共享信息@Overridepublic Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;String, Object&gt;(); errorAttributes.put(&quot;timestamp&quot;, new Date()); addStatus(errorAttributes, requestAttributes); addErrorDetails(errorAttributes, requestAttributes, includeStackTrace); addPath(errorAttributes, requestAttributes); return errorAttributes;&#125; 1234567891011121314151617181920212223242526// 处理默认 /error 请求@Controller@RequestMapping(&quot;$&#123;server.error.path:$&#123;error.path:/error&#125;&#125;&quot;) // 默认为错误页面路径 /errorpublic class BasicErrorController extends AbstractErrorController &#123; @RequestMapping(produces = &quot;text/html&quot;) // 产生 HTML 类型的数据；浏览器发送的请求来到这个方法处理 public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) &#123; HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); // 去哪个页面作为错误页面，包含页面地址和页面内容 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView == null ? new ModelAndView(&quot;error&quot;, model) : modelAndView); &#125; @RequestMapping @ResponseBody // 产生 Json 数据，其他客户端来到这个方法处理 public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(body, status); &#125; 1234567891011121314151617181920212223@Overridepublic ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) &#123; ModelAndView modelAndView = resolve(String.valueOf(status), model); if (modelAndView == null &amp;&amp; SERIES_VIEWS.containsKey(status.series())) &#123; modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); &#125; return modelAndView;&#125;private ModelAndView resolve(String viewName, Map&lt;String, Object&gt; model) &#123; // 默认 Spring Boot 可以去找到一个页面 error/404 String errorViewName = &quot;error/&quot; + viewName; // 模板引擎可以解析这个页面地址就用模板引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); if (provider != null) &#123; // 模板引擎可用的情况下返回到 errorViewName 指定的视图地址 return new ModelAndView(errorViewName, model); &#125; //模板引擎不可用，就在静态资源文件夹下找 errorViewName 对应的页面 error/404.html return resolveResource(errorViewName, model);&#125; 一但系统出现 4xx 或者 5xx 之类的错误，ErrorPageCustomizer 就会生效（定制错误的响应规则），就会来到 /error 请求，就会被BasicErrorController处理。 错误页面的定制 有模板引擎的情况下：error/状态码（将错误页面命名为 错误状态码 .html 放在模板引擎文件夹里面的 error 文件夹下）发生此状态码的错误就会来到对应的页面。 可以使用 4xx 和 5xx 作为错误页面的文件名来匹配这种类型的所有错误，精确优先。（优先寻找精确的 [状态码].html） 1234567页面能获取的信息： timestamp：时间戳 status：状态码 error：错误提示 exception：异常对象 message：异常消息 errors：JSR303 数据校验的错误都在这里 没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹下找，以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面。 12345678910111213141516// 自定义异常处理 &amp; 返回定制 Json 数据@ControllerAdvicepublic class MyExceptionHandler &#123; @ResponseBody @ExceptionHandler(UserNotExistException.class) public Map&lt;String,Object&gt; handleException(Exception e)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 传入我们自己的错误状态码 4xx 5xx，否则就不会进入定制错误页面的解析流程 request.setAttribute(&quot;javax.servlet.error.status_code&quot;,500); map.put(&quot;code&quot;,&quot;user.notexist&quot;); map.put(&quot;message&quot;,e.getMessage()); // 用于错误信息的读取 request.setAttribute(&quot;userNotExist&quot;,map); return &quot;forward:/error&quot;; &#125;&#125; 12345678910111213141516// 给容器中加入我们自己定义的 ErrorAttributes@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes &#123; @Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; // 这个 map 就是页面和 Json 能获取的所有字段 Map&lt;String, Object&gt; map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put(&quot;company&quot;, &quot;Welab&quot;); // 异常处理器携带的数据 Map&lt;String, Object&gt; map userNotExist = (Map&lt;String, Object&gt;)requestAttributes.getAttribute(&quot;userNotExist&quot;,0); map.put(&quot;userNotExist&quot;, userNotExist) return map; &#125;&#125; 嵌入式 Servlet 容器 由于 Spring Boot 默认是以 Jar 包的方式启动嵌入式的 Servlet 容器来启动 Spring Boot 的 web 应用，没有 web.xml 文件。 Spring Boot 修改和 server 有关的配置： 123456# 方式一：propertiesserver.port=8081server.context-path=/crudserver.tomcat.uri-encoding=UTF-8# 通用的 Servlet 容器设置 server.xxx# Tomcat 的设置 server.tomcat.xxx 1234567891011// 编写嵌入式的 Servlet 容器的定制器来修改 Servlet 容器的配置@Bean //一定要将这个定制器加入到容器中public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer()&#123; return new EmbeddedServletContainerCustomizer() &#123; //定制嵌入式的Servlet容器相关的规则 @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.setPort(8083); &#125; &#125;;&#125; Servlet 三大组件的注册 123456@Beanpublic ServletRegistrationBean myServlet()&#123; ServletRegistrationBean registrationBean = new ServletRegistrationBean( new MyServlet(),&quot;/myServlet&quot;); return registrationBean;&#125; 1234567@Beanpublic FilterRegistrationBean myFilter()&#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(new MyFilter()); registrationBean.setUrlPatterns(Arrays.asList(&quot;/hello&quot;,&quot;/myServlet&quot;)); return registrationBean;&#125; 123456@Beanpublic ServletListenerRegistrationBean myListener()&#123; ServletListenerRegistrationBean&lt;MyListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new MyListener()); return registrationBean;&#125; 替换 Servlet 容器 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 引入 web 模块默认就是使用嵌入式的 Tomcat 作为 Servlet 容器 --&gt;&lt;/dependency&gt; 123456789101112131415161718&lt;!-- 引入web模块 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--引入其他的Servlet容器--&gt;&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;!-- spring-boot-starter-undertow --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;/dependency&gt; 外置 Servlet 容器 ！！！创建的必须是一个 war 项目（利用 IDEA 创建好目录结构） 123456&lt;!-- 将嵌入式的 Tomcat 指定为 provided --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 必须要编写一个 SpringBootServletInitializer 的子类，并调用 configure() 方法。 123456789public class ServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; // 传入 SpringBoot 应用的主程序 return application.sources(SpringBootMyApplication.class); &#125;&#125; REST 架构风格","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Web","slug":"Web","permalink":"https://wingowen.github.io/tags/Web/"}]},{"title":"Spring Boot 日志","slug":"后台技术/Spring Boot/Spring Boot 日志框架","date":"2020-03-02T02:27:24.000Z","updated":"2023-09-20T06:44:49.037Z","comments":true,"path":"2020/03/02/后台技术/Spring Boot/Spring Boot 日志框架/","link":"","permalink":"https://wingowen.github.io/2020/03/02/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/","excerpt":"Spring Boot 日志框架的使用。","text":"Spring Boot 日志框架的使用。 日志框架市面上常见的日志框架： 日志门面 （日志的抽象层） 日志实现 JCL: Jakarta Commons Logging;SLF4j: Simple Logging Facade for Java;Jboss-Logging Log4JJUL: java.util.logging;Log4J2 Logback Spring Boot 选用 SLF4J 和 Logback Q：Spring Boot 整合了很多框架，若整合的框架默认使用的日志框架与 Spring Boot 默认使用的日志框架不一样时，要怎么做？ A：排除整合框架的默认日志框架，用中间包来替代原有的日志框架。 SLF4J在开发的中，日志记录方法的调用不应该来直接调用日志的实现类，而是调用日志抽象层里面的方法。 12345678910// 给系统里面导入 SLF4J 的 JAR 和 Logback 的实现 JARimport org.slf4j.Logger;import org.slf4j.LoggerFactory;public class HelloWorld &#123; public static void main(String[] args) &#123; Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(&quot;Hello World&quot;); &#125;&#125; SLF4J 做为不同的日志实现框架的门面的配置结构： 统一日志记录： 将系统中其他日志框架先排除出去； 用中间包来替换原有的日志框架； 导入 SLF4J 的其他的实现。 Spring Boot 日志关系1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot 内配置的排除 Spring 的 Commons-Logging： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 日志的使用日志的级别： 12345678// 由低到高 trace &lt; debug &lt; info &lt; warn &lt; error// 可以调整输出的日志级别；日志就只会在这个级别以以后的高级别生效logger.trace(&quot;这是trace日志...&quot;);logger.debug(&quot;这是debug日志...&quot;);// SpringBoot 默认给我们使用的是 info 级别（root 级别）logger.info(&quot;这是info日志...&quot;);logger.warn(&quot;这是warn日志...&quot;);logger.error(&quot;这是error日志...&quot;); 日志输出格式： 1234567日志输出格式： %d表示日期时间; %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %thread表示线程名; %-5level：级别从左显示5个字符宽度; %logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割; %msg：日志消息; %n是换行符。 Spring Boot 修改日志的默认设置 1234567891011121314logging.level.com.wingo=trace # 指定某个包路径下的日志输出等级# logging.path=# 不指定路径在当前项目下生成 springboot.log 日志# 可以指定完整的路径；# logging.file=G:/springboot.log# 在当前磁盘的根路径下创建 spring 文件夹和里面的 log 文件夹；使用 spring.log 作为默认文件# logging.path=/spring/log# 在控制台输出的日志的格式logging.pattern.console=%d&#123;yyyy-MM-dd&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n# 指定文件中日志输出的格式logging.pattern.file=%d&#123;yyyy-MM-dd&#125; === [%thread] === %-5level === %logger&#123;50&#125; ==== %msg%n 指定配置 给类路径下放上每个日志框架自己的配置文件即可，SpringBoot 就不使用他默认配置的了。 Logging System Customization Logback logback-spring.xml, logback-spring.groovy, logback.xml or logback.groovy Log4J2 log4j2-spring.xml or log4j2.xml JDK (Java Util Logging) logging.properties logback.xml：直接就被日志框架识别了； logback-spring.xml：日志框架就不直接加载日志的配置项，由 SpringBoot 解析日志配置，可以使用 SpringBoot 的高级 Profile 功能。 SpringBoot 的高级 Profile 功能： 123&lt;springProfile name=&quot;staging&quot;&gt; &lt;!-- configuration to be enabled when the &quot;staging&quot; profile is active --&gt;&lt;/springProfile&gt; 配置文件12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;!-- 先定义所有的 appender --&gt; &lt;appenders&gt; &lt;!-- 输出控制台的配置 --&gt; &lt;console name=&quot;Console target=SYSTEM_OUT&quot;&gt; &lt;!-- 输出日志的格式 --&gt; &lt;patternlayout pattern=&quot;[%p] &gt;&gt;&gt; %m%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;!-- 定义各种包下的日志的输出级别 --&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;!-- 输出到控制台 --&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;logger name=&quot;org.springframework&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;com.baomidou&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;org.hibernate&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;com. alibaba druid&quot; level=&quot;ERROR&quot;/&gt; &lt;/loggers&gt;&lt;/configuration&gt; 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;property name=&quot;log.path&quot; value=&quot;C:\\Users\\wingo\\Documents\\Recent\\log&quot; /&gt; &lt;!--输出到控制台--&gt; &lt;appender name=&quot;console&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;!-- &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt;--&gt; &lt;encoder&gt; &lt;pattern&gt;[%p] &gt;&gt;&gt; %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--输出到文件--&gt; &lt;appender name=&quot;file&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/logback.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %contextName [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;console&quot; /&gt; &lt;appender-ref ref=&quot;file&quot; /&gt; &lt;/root&gt; &lt;!-- logback为java中的包 --&gt; &lt;logger name=&quot;com.wingo.controller&quot;/&gt; &lt;!--logback.LogbackDemo：类的全路径 --&gt; &lt;logger name=&quot;com.wingo.controller.LearnController&quot; level=&quot;WARN&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt;&lt;/configuration&gt;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Log","slug":"Log","permalink":"https://wingowen.github.io/tags/Log/"}]},{"title":"Spring Boot 入门配置","slug":"后台技术/Spring Boot/Spring Boot 入门配置","date":"2020-02-29T02:48:07.000Z","updated":"2023-09-20T06:44:49.030Z","comments":true,"path":"2020/02/29/后台技术/Spring Boot/Spring Boot 入门配置/","link":"","permalink":"https://wingowen.github.io/2020/02/29/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E5%85%A5%E9%97%A8%E9%85%8D%E7%BD%AE/","excerpt":"Spring Boot 2.x 的 Hello World 以及配置详解。","text":"Spring Boot 2.x 的 Hello World 以及配置详解。 背景：J2EE 笨重的开发、繁多的配置、底下的开发效率吧、复杂的部署流程、第三方技术集成难度大。 解决：Spring 全家桶 👉 Spring Boot J2EE 一站式解决方案 👉 Spring Cloud 分布式整体解决方案 Spring Boot 简介 简化 Spring 应用开发的一个框架； 整合 Spring 技术栈的一个大集合； J2EE 一站式解决方案。 微服务：一种架构风格，即一个应用应该是一组小型服务，这些小服务之间可以通过 HTTP 的方式进行通信，每一个功能元素最终都是一个可独立替换和独立升级的软件单元。 Spring Boot HelloWorld 功能：浏览器发送 hello 请求，服务器接受请求并处理，响应 Hello World 字符串 项目生成项目生成地址：Spring Initiizr 填写项目的基本信息后点击 Generate 下载生成的项目。 将项目解压到相应目录，打开 IDEA 导入项目。（File 👉 New 👉 Project from Existing Sources） 选择以 Maven 项目的方式导入。 设置 JDK ，点击 Environment settings 配置 Maven。 项目导入成功后的项目目录。 resources文件夹中目录结构 static：保存所有的静态资源； js css images 等； templates：保存所有的模板页面；（Spring Boot 默认 jar 包使用嵌入式的 Tomcat，默认不支持 JSP 页面）；可以使用模板引擎（freemarker、thymeleaf）； application.properties：Spring Boot 应用的配置文件；可以修改一些默认设置； 代码编写在 pom.xml 中加入 Spring Boot 的相关依赖。 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 功能场景抽取成 Starter 来导入相关依赖 --&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启动 Spring Boot 应用的主程序 1234567891011/** * @author wingo */@SpringBootApplicationpublic class JavaSpringbootWingoApplication &#123; public static void main(String[] args) &#123; run(JavaSpringbootWingoApplication.class, args); &#125;&#125; 编写相关的 Controller 123456789@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping(&quot;/hello&quot;) public String hello()&#123; return &quot;Hello World!&quot;; &#125;&#125; 运行测试控制台打印中可看到 Tomcat 在其默认端口 8080 被开启。 访问：localhost:8080/hello 访问成功，此 Spring Boot 项目可正常运行。 简化部署123456789&lt;!-- 这个插件可以将应用打包成一个可执行的jar包，在 cmd 直接用 java -jar 命令执行 --&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Hello World 探究项目依赖spring-boot-starter-Xxx：启动器 Spring Boot 场景启动器，用于自动导入了场景运行所依赖的组件。 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- Starter 启动器的父项目 --&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt; 点击进入 spring-boot-starter-parent，它的父项目是 spring-boot-dependencies。 1234567&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 定义了各种依赖的版本号 --&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt; 主程序类（入口类）Spring Boot 用@SpringBootApplication来标注一个主程序类，说明这是一个 Spring Boot 应用。 123456@SpringBootConfiguration // 这是一个 Spring Boot 配置类@EnableAutoConfiguration // 开启自动配置功能@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;&#125; Spring Boot 在启动的时候从类路径下的 META-INF/spring.factories 中获取 EnableAutoConfiguration 指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作。 按住 Ctrl 点击一个自动装配类： 发现有大量的@Conditional派生类，必须是在@Conditional指定的条件成立的情况下，才给容器中添加组件，配置里面的所有内容才生效。 可以通过 debug=true 属性让控制台打印自动配置报告。 Spring Boot 配置文件 application.properties application.yml YAML以数据为中心。 123server: port: 8081 path: /hello 基本语法： k:(空格)v 表示一对键值对（大小写敏感）； 以空格的缩进来控制层级关系；只要是左对齐的一列数据，都是同一个层级的。 字符串字符串默认不用加上单引号或者双引号： 单引号：会转义特殊字符，特殊字符最终只是一个普通的字符串数据； 双引号：不会转义字符串里面的特殊字符，特殊字符会作为本身想表示的意思。 对象、Map： 123456friends: lastName: Wen age: 23# 行内写法friends: &#123;lastName: Wen,age: 23&#125; 数组、List、Set： 123456pets: - cat - dog # 行内写法friends: [cat,dog] 配置文件值的注入配置文件： 123456789101112person: lastName: hello age: 18 boss: false birth: 2017/12/12 maps: &#123;k1: v1,k2: 12&#125; lists: - lisi - zhaoliu dog: name: 小狗 age: 12 Java Bean 将配置文件中配置的每一个属性的值，映射到这个组件中。 只有这个组件是容器中的组件，才能使用容器提供的@ConfigurationProperties功能 @ConfigurationProperties告诉 SpringBoot 将本类中的所有属性和配置文件中相关的配置进行绑定； prefix = &quot;person&quot;配置文件中哪个下面的所有属性进行一一映射。 123456789101112131415161718192021/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = &quot;person&quot;：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能使用容器提供的@ConfigurationProperties功能； * */@Component@ConfigurationProperties(prefix = &quot;person&quot;)public class Person &#123; private String lastName; private Integer age; private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog;&#125; 导入配置文件处理器，这样编写配置时会有提示。 123456&lt;!-- 导入配置文件处理器，配置文件进行绑定就会有提示 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; properties 配置文件在 IDEA 中默认 UTF-8 可能会乱码，勾选 Setting - File Encodings - Transparent native-to-ascii conversion。 @Value获取值和@ConfigurationProperties获取值比较 @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 不支持 SpEL 不支持 支持 JSR303数据校验 支持 不支持 复杂类型封装 支持 不支持 配置文件 yml 还是 properties 它们都能获取到值： 只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value； 专门编写了一个 JavaBean 来和配置文件进行映射，我们就直接使用@ConfigurationProperties； 配置文件注入校验数据。 12345678//...@Validated // 开启校验public class Person &#123; //... @Email // 必须为邮件格式 private String lastName; //...&#125; 配置文件的加载@PropertySource：加载指定的配置文件。 1@PropertySource(value = &#123;&quot;classpath:person.properties&quot;&#125;) @ImportResource：让 Spring 的配置文件生效，加载进来。Spring Boot 里面没有 Spring 的配置文件，我们自己编写的配置文件，也不能自动识别。 1@ImportResource(locations = &#123;&quot;classpath:beans.xml&quot;&#125;) Spring Boot 推荐使用全注解的方式。 12345678910@Configuration // 指明当前类是一个 Spring 配置类public class MyAppConfig &#123; //将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名 @Bean public HelloService helloService()&#123; System.out.println(&quot;配置类 @Bean 给容器中添加组件了...&quot;); return new HelloService(); &#125;&#125; 配置文件占位符随机数： 12$&#123;random.value&#125;、$&#123;random.int&#125;、$&#123;random.long&#125;$&#123;random.int(10)&#125;、$&#123;random.int[1024,65536]&#125; 占位符获取之前配置过的值，可用:指定默认值 123456789person.last-name=张三$&#123;random.uuid&#125;person.age=$&#123;random.int&#125;person.birth=2017/12/15person.boss=falseperson.maps.k1=v1person.maps.k2=14person.lists=a,b,cperson.dog.name=$&#123;person.hello:hello&#125;_dog // 指定默认值person.dog.age=15 Profile多 Profile 文件：我们在主配置文件编写的时候，文件名可以是 application-{profile}.properties / yml，默认使用 application.properties 的配置。 Yaml 支持多文档块的方式： 12345678910111213141516171819server: port: 8081spring: profiles: active: prod # 指定激活哪个环境--- # 文档块分隔符server: port: 8083spring: profiles: dev # 属于 dev 环境---server: port: 8084spring: profiles: prod # 属于 prod 环境 激活指定的 Profile 在配置文件中指定spring.profiles.active=dev 命令行运行java -jar [项目 JAR 包] --spring.profiles.active=dev; 虚拟机参数-Dspring.profiles.active=dev 配置文件加载位置springboot 启动会扫描以下位置的 application.properties 或者 application.yml 文件作为 Spring boot 的默认配置文件 file:./config/ file:./ classpath:/config/ classpath:/ 优先级由高到底，高优先级的配置会覆盖低优先级的配置；SpringBoot 会从这四个位置加载全部配置文件形成互补配置。 项目打包好后，若要改变配置文件，可以使用命令行参数的形式 java -jar [项目 JAR 包] --spring.config.location=D:/application.properties 或者直接使用 --配置项=值的方式改变某一项配置 java -jar [项目 JAR 包] --server.port=8087 --server.context-path=/abc 优先级：外部 &gt; 内部、带 profile &gt; 不带 profile 启动原理解析","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"}]},{"title":"Netty 常用配置","slug":"后台技术/Netty/Netty 常用配置","date":"2020-02-27T08:35:34.000Z","updated":"2023-09-20T06:44:49.018Z","comments":true,"path":"2020/02/27/后台技术/Netty/Netty 常用配置/","link":"","permalink":"https://wingowen.github.io/2020/02/27/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"在使用 Netty 的过程中会对 serverBootStrap 的 option 和 childOption 进行自定义的一些配置，具体就是使用ChannelOption 里面声明的常量进行配置，在此介绍一下这些常量。","text":"在使用 Netty 的过程中会对 serverBootStrap 的 option 和 childOption 进行自定义的一些配置，具体就是使用ChannelOption 里面声明的常量进行配置，在此介绍一下这些常量。 ChannelOption 服务端： option() 用于设置 ServerChannel 的选项，负责监听和接收连接； childOption()：用于设置 SocketChannel，负责处理 I/O 操作。 客户端： 没有 childOption()。 ChannelOption.SO_BACKLOG SO_BACKLOG 对应的是 TCP/IP 协议 listen() 函数中的 backlog 参数，函数 listen(int socketfd, int backlog) 用来初始化服务端可连接队列，服务端处理客户端链接是顺序处理的，所以同一时间只能处理一个链接，多客户端请求过来时，服务端会将未处理的请求放入请求队列，backlogb就是制定了队列的大小。 ChannelOption.SO_REUSEADDR SO_REUSEADDR 对应的是 Socket 选项中 SO_REUSEADDR，这个参数表示允许重复使用本地地址和端口，例如，某个服务占用了 TCP 的 8080 端口，其他服务再对这个端口进行监听就会报错，SO_REUSEADDR 这个参数就是用来解决这个问题的，该参数允许服务公用一个端口，这个在服务器程序中比较常用，例如某个进程非正常退出，对一个端口的占用可能不会立即释放，这时候如果不设置这个参数，其他进程就不能立即使用这个端口。 ChannelOption.SO_KEEPALIVE SO_KEEPALIVE 这个参数对应 Socket 中的 SO_KEEPALIVE，当设置这个参数为 true 后，TCP 连接会测试这个连接的状态，如果该连接长时间没有数据交流，TCP 会自动发送一个活动探测数据报文，来检测链接是否存活。 ChannelOption.TCP_NODELAY TCP_NODELAY 对应于 socket 选项中的 TCP_NODELAY，该参数的使用和 Nagle 算法有关，Nagle 算法是将小的数据包组装为更大的帧进行发送，而不会来一个数据包发送一次，目的是为了提高每次发送的效率，因此在数据包没有组成足够大的帧时，就会延迟该数据包的发送，虽然提高了网络负载却造成了延时，TCP_NODELAY 参数设置为 true，就可以禁用 Nagle 算法，即使用小数据包即时传输。与 TCP_NODELAY 对应的就是 TCP_CORK，该选项会等到发送的数据量最大的时候，一次性发送，适合进行文件传输。 ChannelOption.SO_SNDBUF 和 ChannelOption.SO_RCVBUF SO_SNDBUF 和 SO_RCVBUF 对应 Socket 中的 SO_SNDBUF 和 SO_RCVBUF 参数，即设置发送缓冲区和接收缓冲区的大小，发送缓冲区用于保存发送数据，直到发送成功，接收缓冲区用于保存网络协议站内收到的数据，直到程序读取成功。 CodecHttp 服务HttpServerCodec 将二进制报文转为 HTTP 文本报文。 HttpObjectAggregator(512*1024) 合并分批次过来的 HTTP 报文，合并后 Handler 拿到的将是一个完整的 HTTP 报文。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"WebSocket","slug":"后台技术/Netty/WebSocket","date":"2020-02-17T04:12:30.000Z","updated":"2023-09-20T07:00:03.843Z","comments":true,"path":"2020/02/17/后台技术/Netty/WebSocket/","link":"","permalink":"https://wingowen.github.io/2020/02/17/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/WebSocket/","excerpt":"","text":"WebSocket 入门介绍以及 Netty WebSocket 协议开发。 HTTP 请求 / 响应模式：客户端加载一个网页，然后直到用户点击下一页之前，什么都不会发生。 Ajax：网络开始变得更加动态了，但所有的 HTTP 通信仍然由客户端控制，这就需要用户进行互动或定期轮询，以便从服务器加载新数据。 长期以来存在着各种技术让服务器得知有新数据可用时，立即将数据发送到客户端这些技术种类繁多。最常用的一种黑客手段是对服务器发起链接创建假象，被称为长轮询。利用长轮询，客户端可以打开指向服务器的 HTTP 连接，而服务器会一直保持连接打开，直到发送响应。服务器只要实际拥有新数据，就会发送响应。 问题：由于 HTTP 协议的开销，导致它们不适用于低延迟应用。 WebSocket：将网络套接字引入到了客户端和服务端来解决这一问题，浏览器和服务器之间可以通过套接字建立持久的连接，双方随时都可以互发数据给对方，而不是之前由客户端控制的一请求一应答模式。 WebSocket 入门 在 WebSocket API 中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道，两者就可以直接互相传送数据了。WebSocket 基于 TCP 双向全双工进行消息传递，在同一时刻，既可以发送消息，也可以接收消息，相比于 HTTP 的半双工协议，性能得到很大提升。 WebSocket 连接 建立 WebSocket 连接时，需要通过客户端或者浏览器发出握手请求。 握手请求消息如下： 为了建立一个 WebSocket 连接，客户端浏览器首先要向服务器发起一个 HTTP 请求，这个请求和通常的 HTTP 请求不同，包含了一些附加头信息，其中附加头信息 Upgrade: WebSocket表明这是一个申请协议升级的 HTTP 请求。服务器端解析这些附加的头信息，然后生成应答信息返回给客户端，客户端和服务器端的 WebSocket 连接就建立起来了，双方可以通过这个连接通道自由地传递信息，并且这个连接会持续存在直到客户端或者服务器端的某一方主动关闭连接。 握手应答消息如下： 请求消息中的Sec-WebSocket-Key是随机的,服务器端会用这些数据来构造出一个 SHA-1 的信息摘要。使用 SHA-1 加密,然后进行 BASE-64 编码，将结果做为Sec- WebSocket- Accept头的值，返回给客户端。 WebSocket 生命周期 握手成功之后,服务端和客户端就可以通过 messages 的方式进行通信了，一个消息由一个或者多个帧组成，WebSocket的消息并不一定对应一个特定网络层的帧，它可以被分割成多个帧或者被合并。 WebSocket 的握手关闭消息带有一个状态码和一个可选的关闭原因，它必须按照协议要求发送一个 Close 控制帧，当对端接收到关闭控制帧指令时，需要主动关闭 WebSocket连接。 Netty WebSocket 协议开发 Netty 基于 HTTP 协议栈开发了 WebSocket 协议栈,利用 Netty 的 WebSocket 协议栈可以非常方便地开发出 WebSocket 客户端和服务端。 Netty 服务端实例 功能介绍 支持 WebSocket 的浏览器通过 WebSocket 协议发送请求消息给服务端，服务端对请求消息进行判断； 如果是合法的 WebSocket 请求,则获取请求消息体（文本）并在后面追加字符串“欢迎使用 Netty WebSocket服务,现在时刻:[系统时间]”； 客户端 HTML 通过内嵌的 JS 脚本创建 WebSocket 连接，握手成功 / 失败，在文本框中打印“打开 Web Socket服务正常,浏览器支持 Web Socket!” / “抱歉，您的浏览器不支持 Web Socket协议!”。 功能开发 12345678&lt;!-- pom.xml --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// WebSocket 服务端启动类public class WebSocketServer&#123; public void run(int port) throws Exception&#123; EventLoopGroup bossGroup = new NioEventLoopGroup (); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b= new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline (); // 将请求和应答消息编码 / 解码为 HTTP 消息 pipeline.addLast(&quot;http-codec&quot;, new HttpServerCodec()); // 将 HTTP 消息的多个部分组合成一条完整的 HTTP 消息 pipeline.addLast (&quot;aggregator&quot;, new HttpObjectAggregator(65536)); // 主要用于支持浏览器和服务端进行 WebSocket 通信 ch.pipeline().addLast(&quot;http-chunked&quot;, new ChunkedWriteHandler()); // 增加 WebSocket 服务端 handler pipeline.addLast(&quot;handler&quot;, new WebSocketServerHandler()); &#125; &#125;); Channel ch = b.bind(port).sync().channel(); System.out.println(&quot;Web socket server started at port&quot; + port +&#x27;.&#x27;); System.out.println (&quot;Open our browser and navigate to http://localhost:&quot; + port + &#x27;/&#x27;); ch.closeFuture().sync(); &#125; finally &#123; bossGroup. shutdownGracefully(); workerGroup. shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws Exception &#123; int port = 8080; if (args.length &gt;0) &#123; try&#123; port = Integer.parseInt(args[0]); &#125; catch(NumberFormatException e) &#123; e.printStackTrace(); &#125; &#125; new WebSocketServer().run(port); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// Websocket 服务端处理类public class WebSocketServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; private static final Logger logger = Logger.getLogger(WebSocketServerHandler.class.getName()); private WebSocketServerHandshaker handshaker; @Override protected void messageReceived(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 传统的 HTTP 接入 if (msg instanceof FullHttpRequest)&#123; // 判断请求消息有中是否包含 Upgrade: websocket handleHttpRequest(ctx,(FullHttpRequest)msg); &#125;// WebSocket 接入 else if (msg instanceof WebSocketFrame)&#123; handleWebSocketFrame(ctx,(WebSocketFrame)msg); &#125; &#125; private void handleHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) throws Exception&#123; // 如果 HTTP 解码失败，返回 HTTP 400 响应异常 if (!req.getDecoderResult().isSuccess() || (!&quot;websocket&quot;.equals(req.headers().get(&quot;Upgrade&quot;))))&#123; sendHttpResponse(ctx, req,new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.BAD_REQUEST)); return; &#125; // 构造握手工厂，本机测试 WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory(&quot;ws://localhost:8080/websocket&quot;,null,false); // 创建握手处理类 handshaker = wsFactory.newHandshaker(req); if (handshaker == null)&#123; WebSocketServerHandshakerFactory.sendUnsupportedWebSocketVersionResponse(ctx.channel()); &#125;else &#123; // 构造握手响应消息返回给客户端 // 将 WebSocket 相关的编码和解码类动态添加到 ChannelPipeline 中用于 WebSocket 消息的编解码 handshaker.handshake(ctx.channel(),req); &#125; &#125; // 链路建立成功之后分别对控制帧进行判断 private void handleWebSocketFrame(ChannelHandlerContext ctx, WebSocketFrame frame) &#123; // 判断是否关闭链路指令 if (frame instanceof CloseWebSocketFrame)&#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) frame.retain()); return; &#125; // 判断是否是维持链路的 Ping 消息 if (frame instanceof PingWebSocketFrame)&#123; // 构造 pong 消息返回 ctx.channel().write(new PongWebSocketFrame(frame.content()).retain()); return; &#125; // 本例程仅支持文本信息，故对非文本消息抛出异常 if (!(frame instanceof TextWebSocketFrame))&#123; throw new UnsupportedOperationException(String.format(&quot;%s frame types not supported&quot;,frame.getClass().getName())); &#125; // 返回应答消息 String request = ((TextWebSocketFrame) frame).text(); if (logger.isLoggable(Level.FINE))&#123; logger.fine(String.format(&quot;%s received %s&quot;,ctx.channel(),request)); &#125; // 构造新的 TextWebSocketFrame 消息返回给客户端 // 由于握手应答时动态增加了 TextWebSocketframe 的编码类,所以可以直接发送 TextWebSocketFrame对象 ctx.channel().write(new TextWebSocketFrame(request+&quot;欢迎使用Netty WebSocket服务，现在时刻:&quot; + new Date().toString())); &#125; private static void sendHttpResponse(ChannelHandlerContext ctx,FullHttpRequest req,FullHttpResponse res)&#123; // 返回应答给客户端 if (res.getStatus().code() != 200)&#123; ByteBuf buf = Unpooled.copiedBuffer(res.getStatus().toString(), CharsetUtil.UTF_8); res.content().writeBytes(buf); // 明确释放ByteBuf的引用计数 buf.release(); setContentLength(res,res.content().readableBytes()); &#125; // 如果是非 Keep-Alive，关闭连接 ChannelFuture f = ctx.channel().writeAndFlush(res); if (!isKeepAlive(req) || res.getStatus().code() != 200)&#123; f.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;WebSoket Demo&lt;/title&gt; &lt;script type=&quot;text/JavaScript&quot;&gt; var WebSocket = WebSocket || window.WebSocket || window.MozWebSocket; if (!WebSocket) &#123; alert(&quot;WebSocket not supported by this browser!&quot;); &#125; else &#123; var ws = null; function Display() &#123; console.log(&quot;websocket 测试&quot;); &#125; var log = function (s) &#123; if (document.readyState !== &quot;complete&quot;) &#123; log.buffer.push(s); &#125; else &#123; document.getElementById(&quot;contentId&quot;).value += (s + &quot;/n&quot;); &#125; &#125; function CreateConnect () &#123; var msg = document.getElementById(&quot;wsUrlId&quot;); console.log(&quot;CreateConnect(), url: &quot; + msg.value); if (ws == null) &#123; var wsUrlValue = msg.value; try &#123; ws = new WebSocket(wsUrlValue); ws.onmessage = function (event) &#123; log(&quot;onmessage(), 接收到服务器消息: &quot; + event.data); &#125;; ws.onclose = function (event) &#123; log(&quot;onclose(), Socket 已关闭!&quot;); ws = null; &#125;; ws.onopen = function (event) &#123; log(&quot;onopen(), Socket 连接成功!&quot;); &#125;; ws.onerror = function (event) &#123; &#125;; &#125; catch (e) &#123; ws = null; log(&quot;连接异常, 重置 websocket&quot;); &#125; &#125; &#125; function SendMsg() &#123; var msg = document.getElementById(&quot;messageId&quot;); console.log(&quot;SendMsg(), msg: &quot; + msg.value); if (ws != null) &#123; log(&quot;发送 Socket 消息: &quot; + msg.value); ws.send(msg.value); &#125; else &#123; log(&quot;Socket 还未创建!, msg: &quot; + msg.value); &#125; &#125; function CloseConnect () &#123; console.log(&quot;CloseConnect()&quot;); if (ws != null) &#123; ws.close(); &#125; &#125; &#125; &lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;Display()&quot; &gt; &lt;div id=&quot;valueLabel&quot;&gt;&lt;/div&gt; &lt;textarea rows=&quot;10&quot; cols=&quot;40&quot; id=&quot;contentId&quot;&gt;&lt;/textarea&gt; &lt;br/&gt; &lt;input name=&quot;wsUrl&quot; id=&quot;wsUrlId&quot; value=&quot;ws://localhost:8080/websocket&quot;/&gt; &lt;button id=&quot;createButton&quot; onClick=&quot;javascript:CreateConnect()&quot;&gt;Create&lt;/button&gt; &lt;button id=&quot;closeButton&quot; onClick=&quot;javascript:CloseConnect()&quot;&gt;Close&lt;/button&gt; &lt;br/&gt; &lt;input name=&quot;message&quot; id=&quot;messageId&quot; value=&quot;Hello, Server!&quot;/&gt; &lt;button id=&quot;sendButton&quot; onClick=&quot;javascript:SendMsg()&quot;&gt;Send&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; 测试结果 控制台打印 浏览器调试","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://wingowen.github.io/tags/WebSocket/"}]},{"title":"Netty 引导","slug":"后台技术/Netty/Netty 引导","date":"2020-01-25T16:25:24.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/26/后台技术/Netty/Netty 引导/","link":"","permalink":"https://wingowen.github.io/2020/01/26/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%BC%95%E5%AF%BC/","excerpt":"引导一个应用程序是指对它进行配置，并使它运行起来的过程。Netty处理引导的方式使你的应用程序和网络层相隔离，无论它是客户端还是服务器。所有的框架组件都将会在后台结合在 一起并且启用。","text":"引导一个应用程序是指对它进行配置，并使它运行起来的过程。Netty处理引导的方式使你的应用程序和网络层相隔离，无论它是客户端还是服务器。所有的框架组件都将会在后台结合在 一起并且启用。 Bootstrap 类 引导类的层次结构包括一个抽象的父类和两个具体的引导子类。 相对于将具体的引导类分别看作用于服务器和客户端的引导来说，记住它们的本意是用来支 撑不同的应用程序的功能。也就是说，服务器致力于使用一个父 Channel 来接受来自客户端的连接，并创建子 Channel 以用于它们之间的通信；而客户端将最可能只需要一个单独的、没有父 Channel 的 Channel 来用于所有的网络交互。 为什么引导类是 Cloneable 的： 你有时可能会需要创建多个具有类似配置或者完全相同配置的Channel。为了支持这种模式而又不需要为每个 Channel 都创建并配置一个新的引导类实例， AbstractBootstrap 被标记为了 Cloneable。 注意，这种方式只会创建引导类实例的 EventLoopGroup 的一个浅拷贝，所以，后者将在所有克隆的 Channel 实例之间共享。这是可以接受的，因为通常这些克隆的 Channel 的生命周期都很短暂，一个典型的场景是：创建一个 Channel 以进行一次HTTP请求。 引导客户端 Bootstrap 类负责为客户端和使用无连接协议的应用程序创建 Channel。 12345678910111213141516171819202122232425262728293031323334// 引导一个客户端EventLoopGroup group = new NioEventLoopGroup();// 创建一个 Bootstrap 类的实例用来创建和连接新的客户端 ChannelBootstrap bootstrap = new Bootstrap();// 设置 EventLoopGroup 提供用于处理 Channel 事件的 EventLoopbootstrap.group(group) // 指定要使用的 Channel 实现 .channel(NioSocketChannel.class) .handler( // 设置用于 Channel 事件和数据的 ChannelInboundHandler new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channeRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; );ChannelFuture future = bootstrap.connect( // 连接到远程主机 new InetSocketAddress(&quot;www.manning.com&quot;, 80));future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Connection established&quot;); &#125; else &#123; System.err.println(&quot;Connection attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 引导服务器 负责引导 ServerChannel 的 ServerBootstrap 提供了负责创建子 Channel 的方法 childXxx()，这些子 Channel 代表了已被接收的连接。 1234567891011121314151617181920212223242526272829// 引导服务器NioEventLoopGroup group = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(group) .channel(NioServerSocketChannel.class) // 子线程 .childHandler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf byteBuf) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; );// 监听端口ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Server bound&quot;); &#125; else &#123; System.err.println(&quot;Bound attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 从 Channel 引导客户端 假设你的服务器正在处理一个客户端的请求，这个请求需要它充当第三方系统的客户端。 1234567891011121314151617181920212223242526272829303132333435ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(new NioEventLoopGroup(), new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; ChannelFuture connectFuture; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class).handler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; ); // 使用与分配给已被接受的子 Channel 相同的 EventLoop bootstrap.group(ctx.channel().eventLoop()); connectFuture = bootstrap.connect( new InetSocketAddress(&quot;www.manning.com&quot;, 80)); &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; if (connectFuture.isDone()) &#123; // do something with the data &#125; &#125; &#125; );ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture)throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Server bound&quot;); &#125; else &#123; System.err.println(&quot;Bind attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 尽可能地重用 EventLoop，以减少线程创建所带来的开销。 在引导过程中添加多个 ChannelHandler 在前面的几个例子中，在引导的过程中调用了 handler() 或者 childHandler() 方法来添加单个的 ChannelHandler。这对于简单的应用程序来说可能已经足够了，但是它不能满足更加复杂的需求。 protected abstract void initChannel(C ch) throws Exception; 这个方法提供了一种将多个 ChannelHandler 添加到一个 ChannelPipeline 中的简便方法 12345678910111213141516// 引导和使用 ChannelInitializerServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(new NioEventLoopGroup(), new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializerImpl());ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.sync();// ChannelInitializer 的实现final class ChannelInitializerImpl extends ChannelInitializer&lt;Channel&gt; &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new HttpClientCodec()); pipeline.addLast(new HttpObjectAggregator(Integer.MAX_VALUE)); &#125; &#125; 如果你的应用程序使用了多个 ChannelHandler，请定义你自己的 ChannelInitializer 实现来将它们安装到 ChannelPipeline 中。 ChannelOption 和属性 你可以使用 option() 方法来将 ChannelOption 应用到引导。你所提供的值将会被自动应用到引导所创建的所有 Channel。可用的 ChannelOption 包括了底层连接的详细信息，如 keep-alive 或者超时属性以及缓冲区设置。 Netty 提供了 AttributeMap 抽象（一个由 Channel 和引导类提供的集合）以及 AttributeKey（一 个用于插入和获取属性值的泛型类）。使用这些工具，便可以安全地将任何类型的数据项与客户端和服务器 Channel（包含 ServerChannel 的子 Channel）相关联了。 123456789101112131415161718192021222324252627// 使用属性值// 创建一个 AttributeKey 以标识该属性final AttributeKey&lt;Integer&gt; id = new AttributeKey&lt;Integer&gt;(&quot;ID&quot;);Bootstrap bootstrap = new Bootstrap();bootstrap.group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // 使用 AttributeKey 检索属性以及它的值 Integer idValue = ctx.channel().attr(id).get(); // do something with the idValue &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; );// 引导的每一个 Channel 都将具有这些属性bootstrap.option(ChannelOption.SO_KEEPALIVE,true) .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000);// 存储该 id 属性bootstrap.attr(id, 123456);ChannelFuture future = bootstrap.connect(new InetSocketAddress(&quot;www.manning.com&quot;, 80));future.syncUninterruptibly(); 引导 DatagramChannel 除了基于 TCP 协议的 SocketChannel，Bootstrap 类也可以被用于无连接协议。为此。Netty 提供了各种 DatagramChannel 的实现。唯一区别就是，不再调用 connect() 方法，而是只调用 bind() 方法。 1234567891011121314151617181920212223242526// 使用 Bootstrap 和 DatagramChannelBootstrap bootstrap = new Bootstrap();bootstrap.group(new OioEventLoopGroup()) .channel(OioDatagramChannel.class) .handler( new SimpleChannelInboundHandler&lt;DatagramPacket&gt;()&#123; @Override public void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; // Do something with the packet &#125; &#125; );// 调用 bind() 方法，因为该协议是无连接的ChannelFuture future = bootstrap.bind(new InetSocketAddress(0));future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Channel bound&quot;); &#125; else &#123; System.err.println(&quot;Bind attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125;&#125;); 关闭 引导使你的应用程序启动并且运行起来，但是迟早你都需要优雅地将它关闭。 123456789// 优雅的关闭EventLoopGroup group = new NioEventLoopGroup();Bootstrap bootstrap = new Bootstrap();bootstrap.group(group) .channel(NioSocketChannel.class);...;Future&lt;?&gt; future = group.shutdownGracefully();// block until the group has shutdownfuture.syncUninterruptibly(); 注意，shutdownGracefully() 方法也是一个异步的操作，所以你需要阻塞等待直到它完成，或者向所返回的 Future 注册一个监听器以在关闭完成时获得通知。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty EventLoop 和线程模型","slug":"后台技术/Netty/Netty EventLoop 和线程模型","date":"2020-01-25T08:09:20.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/25/后台技术/Netty/Netty EventLoop 和线程模型/","link":"","permalink":"https://wingowen.github.io/2020/01/25/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20EventLoop%20%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"线程模型指定了操作系统、编程语言、框架或者应用程序的上下文中的线程管理的关键方面，如何以及何时创建线程将对应用程序代码的执行产生显著的影响。","text":"线程模型指定了操作系统、编程语言、框架或者应用程序的上下文中的线程管理的关键方面，如何以及何时创建线程将对应用程序代码的执行产生显著的影响。 线程模型概述 多核心或多个 CPU 的计算机现在已经司空见惯，大多数的现代应用程序都利用了 复杂的多线程处理技术以有效地利用系统资源。 在早期的 Java 语言中，我们使用多线程处理的主要方式无非是按需创建和启动新的 Thread 来执行并发的任务单元，这是一种在高负载下工作得很差的原始方式。Java 5 随后引入了 Executor API，其线程池通过缓存和重用 Thread 极大地提高了性能。 虽然池化和重用线程相对于简单地为每个任务都创建和销毁线程是一种进步，但是它并不能 消除由上下文切换所带来的开销，其将随着线程数量的增加很快变得明显，并且在高负载下愈演愈烈。Netty 框架帮助简化了这一处理。 EventLoop 接口 运行任务来处理在连接的生命周期内发生的事件是任何网络框架的基本功能。与之相应的编程上的构造通常被称为事件循环：Netty 使用了 interface io.netty.channel.EventLoop 来适配的术语。 1234567// 在事件循环中执行任务while (!terminated) &#123; List&lt;Runnable&gt; readyEvents = blockUntilEventsReady(); for (Runnable ev: readyEvents) &#123; ev.run(); &#125; &#125; Netty 的 EventLoop 是协同设计的一部分，它采用了两个基本的 API：并发和网络编程。首先，io.netty.util.concurrent 包构建在 JDK 的 java.util.concurrent 包上，用来提供线程执行器。其次，io.netty.channel 包中的类，为了与 Channel 的事件进行交互，扩展了这些接口/类。 在这个模型中，一个 EventLoop 将由一个永远都不会改变的 Thread 驱动，同时任务 （Runnable 或者 Callable）可以直接提交给 EventLoop 实现，以立即执行或者调度执行。根据配置和可用核心的不同，可能会创建多个 EventLoop 实例用以优化资源的使用，并且单个 EventLoop 可能会被指派用于服务多个 Channel。 事件/任务的执行顺序 ： 事件和任务是以先进先出（FIFO）的顺序执行的。这样可以通过保证字节内容总是按正确的顺序被处理，消除潜在的数据损坏的可能性。 任务调度 使用核心的 Java API 和 Netty 的 EventLoop 来调度任务 JDK 的任务调度 API 12345678910111213// 使用 ScheduledExecutorService 来在 60 秒的延迟之后执行一个任务ScheduledExecutorService executor = Executors.newScheduledThreadPool(10);ScheduledFuture&lt;?&gt; future = executor.schedule( new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;60 seconds later&quot;); &#125; &#125;, 60, TimeUnit.SECONDS); // 调度任务在从现在开始的 60 秒之后执行...;executor.shutdown(); 使用 EventLoop 调度任务 12345678910// 使用 EventLoop 来在 60 秒的延迟之后执行一个任务Channel ch = ...;ScheduledFuture&lt;?&gt; future = ch.eventLoop().schedule( new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;60 seconds later&quot;); &#125; &#125;, 60, TimeUnit.SECONDS); 123456789101112// 使用 EventLoop 调度周期性的任务Channel ch = ...;ScheduledFuture&lt;?&gt; future = ch.eventLoop().scheduleAtFixedRate( new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Run every 60 seconds&quot;); &#125; &#125;, 60, 60, TimeUnit.Seconds);// 利用每个异步操作所返回的 ScheduledFuture 取消任务future.cancel(false); 实现细节 更加详细地探讨 Netty 的线程模型和任务调度实现的主要内容。 线程管理 Netty 线程模型的卓越性能取决于对于当前执行的 Thread 的身份的确定。 永远不要将一个长时间运行的任务放入到执行队列中，因为它将阻塞需要在同一线程上执行的任何其他任务。如果必须要进行阻塞调用或者执行长时间运行的任务，我们建议使用一个专门的 EventExecutor。 EventLoop / 线程的分配 服务于 Channel 的 I/O 和事件的 EventLoop 包含在 EventLoopGroup 中。根据不同的传输实现，EventLoop 的创建和分配方式也不同。 异步传输 异步传输实现只使用了少量的 EventLoop（以及和它们相关联的 Thread），而且在当前的线程模型中，它们可能会被多个 Channel 所共享。这使得可以通过尽可能少量的 Thread 来支撑大量的 Channel，而不是每个 Channel 分配一个 Thread。 一旦一个 Channel 被分配给一个 EventLoop，它将在它的整个生命周期中都使用这个 EventLoop（以及相关联的 Thread）。 阻塞传输 这里每一个 Channel 都将被分配给一个 EventLoop（以及它的 Thread）。 每个 Channel 的 I/O 事件都将只会被一个 Thread （用于支撑该 Channel 的 EventLoop 的那个 Thread）处理（Netty 的一致性体现）。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty ChannelHandler 和 ChannelPipeline","slug":"后台技术/Netty/Netty ChannelHandler 和 ChannelPipeline","date":"2020-01-24T14:09:35.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/24/后台技术/Netty/Netty ChannelHandler 和 ChannelPipeline/","link":"","permalink":"https://wingowen.github.io/2020/01/24/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20ChannelHandler%20%E5%92%8C%20ChannelPipeline/","excerpt":"ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑，还有一个重要的关系 ChannelHandlerContext。","text":"ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑，还有一个重要的关系 ChannelHandlerContext。 ChannelHandler 家族 Channel 的生命周期 当 Channel 的状态发生改变时，将会生成对应的事件。这些事件将会被转发给 ChannelPipeline 中的 ChannelHandler，其可以随后对它们做出响应。 ChannelHandler 的生命周期 类型 描述 handlerAdded 当把 ChannelHandler 添加到 ChannelPipeline 中时被调用 handlerRemoved 当从 ChannelPipeline 中移除 ChannelHandler 时被调用 exceptionCaught 当处理过程中在 ChannelPipeline 中有错误产生时被调用 Netty 定义了两个重要的 Channelhandler 子接口： ChannelInboundHandler——处理入站数据以及各种状态变化； ChannelOutboundHandler——处理出站数据并且允许拦截所有的操作。 ChannelInboundHandler 接口 12345678910// 释放消息资源@Sharable// 扩展了 ChannelInboundHandlerAdapterpublic class DiscardHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; // 丢弃已接收的消息 ReferenceCountUtil.release(msg); &#125; &#125; 123456789// 使用 SimpleChannelInboundHandler 会自动释放资源@Sharablepublic class SimpleDiscardHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; @Override public void channelRead0(ChannelHandlerContext ctx, Object msg) &#123; // 不需要任何显式的资源释放 // No need to do anything special &#125; &#125; ChannelOutboundHandler 接口 ChannelOutboundHandler 的一个强大的功能是可以按需推迟操作或者事件，这使得可 以通过一些复杂的方法来处理请求。 ChannelPromise 与 ChannelFuture：ChannelOutboundHandler 中的大部分方法都需要一个 ChannelPromise 参数，以便在操作完成时得到通知。ChannelPromise 是 ChannelFuture 的一个子类，其定义了一些可写的方法，如 setSuccess() 和 setFailure()，从而使 ChannelFuture 不可变。 ChannelHandler 适配器 适配器对 ChannelHandler 进行了简单的实现，只需要简单地扩展它们，并且重写那些你想要自定义的方法。 ChannelInboundHandlerAdapter 和 ChannelOutboundHandlerAdapter 中所提供的方法体调用了其相关联的 ChannelHandlerContext 上的等效方法，从而将事件转发到了 ChannelPipeline 中的下一个 ChannelHandler 中。 资源管理 每当通过调用 ChannelInboundHandler.channelRead() 或者 ChannelOutboundHandler.write() 方法来处理数据时，你都需要确保没有任何的资源泄漏。 Netty 使用引用计数来处理池化的 ByteBuf。所以在完全使用完某个 ByteBuf 后，调整其引用计数是很重要的。 12345678910111213// 丢弃并释放出战消息@Sharablepublic class DiscardOutboundHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) &#123; // 释放资源 ReferenceCountUtil.release(msg); // 通知 ChannelPromise 数据已经被处理了 promise.setSuccess(); &#125;&#125; 重要的是，不仅要释放资源，还要通知 ChannelPromise。否则可能会出现 ChannelFutureListener 收不到某个消息已经被处理了的通知的情况。 如果一个消息被消费或者丢弃了，并且没有传递给 ChannelPipeline 中的下一个 ChannelOutboundHandler，那么用户就有责任调用 ReferenceCountUtil.release()。 ChannelPipeline 接口 每一个新创建的 Channel 都将会被分配一个新的 ChannelPipeline。这项关联是永久性的；Channel 既不能附加另外一个 ChannelPipeline，也不能分离其当前的。 根据事件的起源，事件将会被 ChannelInboundHandler 或者 ChannelOutboundHandler 处理。随后，通过调用 ChannelHandlerContext 实现，它将被转发给同一超类型的下一个 ChannelHandler。 修改 ChannelPipeline ChannelHandler 可以通过添加 addXxx()、删除 remove() 或者替换 replace() 其他的 ChannelHandler 来实时地修改 ChannelPipeline 的布局。 ChannelHandler 的执行和阻塞 通常 ChannelPipeline 中的每一个 ChannelHandler 都是通过它的 EventLoop（I/O 线程）来处理传递给它的事件的。所以至关重要的是不要阻塞这个线程，因为这会对整体的 I/O 处理产生负面的影响。 但有时可能需要与那些使用阻塞 API 的遗留代码进行交互。对于这种情况，ChannelPipeline 有一些接受一个 EventExecutorGroup 的 add()方法。如果一个事件被传递给一个自定义的 EventExecutorGroup，它将被包含在这个 EventExecutorGroup 中的某个 EventExecutor 所处理，从而被从该 Channel 本身的 EventLoop 中移除。对于这种用例，Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。 getXxx()：通过类型或者名称来访问 ChannelHandler 的方法。 context()：返回和 ChannelHandler 绑定的 ChannelHandlerContext。 names()：返回 ChannelPipeline 中所有 ChannelHandler 的名称。 触发事件 ChannelPipeline 的 API 公开了用于调用入站和出站操作的附加方法。 小结 ChannelPipeline 保存了与 Channel 相关联的 ChannelHandler； ChannelPipeline 可以根据需要，通过添加或者删除 ChannelHandler 来动态地修改； ChannelPipeline 有着丰富的 API 用以被调用，以响应入站和出站事件。 ChannelHandlerContext 接口 ChannelHandlerContext 代表了 ChannelHandler 和 ChannelPipeline 之间的关联，每当有 ChannelHandler 添加到 ChannelPipeline 中时，都会创建 ChannelHandlerContext。 ChannelHandlerContext 的主要功能是管理它所关联的 ChannelHandler 和在同一个 ChannelPipeline 中的其他 ChannelHandler 之间的交互。 ChannelHandlerContext 和 ChannelHandler 之间的关联（绑定）是永远不会改变的，所以缓存对它的引用是安全的； 使用 ChannelHandlerContext Channel、ChannelPipeline、ChannelHandler 以及 ChannelHandlerContext 之间的关系 1234// 从 ChannelHandlerContext 访问 ChannelChannelHandlerContext ctx = ..;Channel channel = ctx.channel();channel.write(Unpooled.copiedBuffer(&quot;Netty in Action&quot;, CharsetUtil.UTF_8)); 1234// 通过 ChannelHandlerContext 访问 ChannelPipelineChannelHandlerContext ctx = ..;ChannelPipeline pipeline = ctx.pipeline();pipeline.write(Unpooled.copiedBuffer(&quot;Netty in Action&quot;, CharsetUtil.UTF_8)); 重要的是要注意到，虽然被调用的 Channel 或 ChannelPipeline 上的 write()方法将一直传播事件通 过整个 ChannelPipeline，但是在 ChannelHandler 的级别上，事件从一个 ChannelHandler 到下一个 ChannelHandler 的移动是由 ChannelHandlerContext 上的调用完成的. 1234// 调用 ChannelHandlerContext 的 write()方法ChannelHandlerContext ctx = ..;// write()方法将把缓冲区数据发送到下一个 ChannelHandlerctx.write(Unpooled.copiedBuffer(&quot;Netty in Action&quot;, CharsetUtil.UTF_8)); 高级用法 缓存 ChannelHandlerContext 的引用以供稍后使用，这可能会发生在任何的 ChannelHandler 方法之外，甚至来自于不同的线程。 12345678910111213// 缓存到 ChannelHandlerContext 的引用public class WriteHandler extends ChannelHandlerAdapter &#123; private ChannelHandlerContext ctx; @Override public void handlerAdded(ChannelHandlerContext ctx) &#123; //存储到 ChannelHandlerContext 的引用以供稍后使用 this.ctx = ctx; &#125; public void send(String msg) &#123; // 使用之前存储的到 ChannelHandlerContext 的引用来发送消息 ctx.writeAndFlush(msg); &#125; &#125; 因为一个 ChannelHandler 可以从属于多个 ChannelPipeline，所以它也可以绑定到多个 ChannelHandlerContext 实例。对于这种用法指在多个 ChannelPipeline 中共享同一 个 ChannelHandler，对应的 ChannelHandler 必须要使用 @Sharable 注解标注；否则， 试图将它添加到多个 ChannelPipeline 时将会触发异常。显而易见，为了安全地被用于多个 并发的 Channel（即连接），这样的 ChannelHandler 必须是线程安全的。 123456789// 可共享的 ChannelHandler@Sharable public class SharableHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; System.out.println(&quot;Channel read message: &quot; + msg); ctx.fireChannelRead(msg); &#125; &#125; 异常处理 Netty 提供了几种方式用于处理入站或者出站处理过程中所抛出的异常。 处理入站异常 12345678// 基本的入站异常处理public class InboundExceptionHandler extends ChannelInboundHandlerAdapter &#123; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; &#125; 处理出站异常 出站的异常处理基于一下通知机制： 每个出站操作都将返回一个 ChannelFuture。注册到 ChannelFuture 的 ChannelFutureListener 将在操作完成时被通知该操作是成功了还是出错了。 几乎所有的 ChannelOutboundHandler 上的方法都会传入一个 ChannelPromise 的实例。作为 ChannelFuture 的子类，ChannelPromise 也可以被分配用于异步通知的监听器。但是，ChannelPromise 还具有提供立即通知的可写方法。 ChannelPromise 的可写方法 通过调用 ChannelPromise 上的 setSuccess() 和 setFailure() 方法，可以使一个操作的状态在 ChannelHandler 的方法返回给其调用者时便即刻被感知到。 12345678910// 添加 ChannelFutureListener 到 ChannelFutureChannelFuture future = channel.write(someMessage);future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture f) &#123; if (!f.isSuccess()) &#123; f.cause().printStackTrace(); f.channel().close(); &#125; &#125;&#125;); 123456789101112131415// 添加 ChannelFutureListener 到 ChannelPromisepublic class OutboundExceptionHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) &#123; promise.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture f) &#123; if (!f.isSuccess()) &#123; f.cause().printStackTrace(); f.channel().close(); &#125; &#125; &#125;); &#125; &#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty ByteBuf","slug":"后台技术/Netty/Netty ByteBuf","date":"2020-01-24T03:29:09.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/24/后台技术/Netty/Netty ByteBuf/","link":"","permalink":"https://wingowen.github.io/2020/01/24/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20ByteBuf/","excerpt":"总所周知，网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。因此，Netty 提供了一个替代品 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。","text":"总所周知，网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。因此，Netty 提供了一个替代品 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。 ByteBuf 的 API Netty 的数据处理 API 通过两个组件暴露：abstract class ByteBuf 和 interface ByteBufHolder。 ByteBuf 类 ByteBuf 类是 Netty 的数据容器，通过使用不同的索引来简化对它所包含的数据的访问。 How ByteBuf 维护了两个不同的索引：一个用于读取，一个用于写入。读取时，readerIndex 将会被递增已经被读取的字节数；写入时，writerIndex 也会被递增。readerIndex == writerIndex 时则到达可读取数据的末尾。 名称以 read 或者 write 开头的 ByteBuf 方法，将会推进其对应的索引，而名称以 set 或者 get 开头的操作则不会。后面的这些方法将在作为一个参数传入的一个相对索引上执行操作。 Mode 在使用 Netty 时，有几种常见的围绕 ByteBuf 而构建的使用模式。 堆缓冲区 最常用的 ByteBuf 模式是将数据存储在 JVM 的堆空间中。这种模式被称为支撑数组。 12345678910111213// 支撑数组ByteBuf heapBuf = ...;// 检查 ByteBuf 是否有一个支撑数组if (heapBuf.hasArray()) &#123; // 获取对该数组的引用 byte[] array = heapBuf.array(); // 计算第一个字节的偏移量 int offset = heapBuf.arrayOffset() + heapBuf.readerIndex(); // 获取可读字节数 int length = heapBuf.readableBytes(); // 使用获取的信息作为参数调用自定义方法 handleArray(array, offset, length);&#125; 直接缓冲区（Updating） 复合缓冲器（Updating） 字节级操作 ByteBuf 提供了许多超出基本读、写操作的方法用于修改它的数据。 随机访问索引 123456// 访问数据ByteBuf buffer = ...;for (int i = 0; i &lt; buffer.capacity(); i++) &#123; byte b = buffer.getByte(i); System.out.println((char)b);&#125; 顺序访问索引 虽然 ByteBuf 同时具有读索引和写索引，但是 JDK 的 ByteBuffer 却只有一个索引，这也就是为什么必须调用 flip() 方法来在读模式和写模式之间进行切换的原因 ByteBuf 的内部分段 可丢弃字节 可丢弃字节的分段包含了已经被读过的字节。通过调用 discardReadBytes() 方法，可以丢弃它们并回收空间。这个分段的初始大小为 0，存储在 readerIndex 中，会随着 read 操作的执行而增加（get*操作不会移动 readerIndex）。 可读字节 ByteBuf 的可读字节分段存储了实际数据。 12345// 读取所有数据ByteBuf buffer = ...;while (buffer.isReadable()) &#123; System.out.println(buffer.readByte());&#125; 可写字节 可写字节分段是指一个拥有未定义内容的、写入就绪的内存区域。 12345// 写数据ByteBuf buffer = ...;while (buffer.writableBytes() &gt;= 4) &#123; buffer.writeInt(random.nextInt());&#125; 索引管理 可以通过调用 markReaderIndex()、markWriterIndex()、resetWriterIndex() 和 resetReaderIndex() 来标记和重置 ByteBuf 的 readerIndex 和 writerIndex。 也可以通过调用 readerIndex(int) 或者 writerIndex(int) 来将索引移动到指定位置。试图将任何一个索引设置到一个无效的位置都将导致一个 IndexOutOfBoundsException。 可以通过调用 clear()方法来将 readerIndex 和 writerIndex 都设置为 0（不会清除内存中的内容）。 查找操作（Updating） 123// 使用 ByteBufProcessor 来寻找\\rByteBuf buffer = ...;int index = buffer.forEachByte(ByteBufProcessor.FIND_CR); 派生缓冲区 派生缓冲区为 ByteBuf 提供了以专门的方式来呈现其内容的视图。修改了其中一个，源实例也会被修改（共享）。 12345678910// 对 ByteBuf 进行切片Charset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);ByteBuf sliced = buf.slice(0, 15);// 打印 Netty in ActionSystem.out.println(sliced.toString(utf8));// 更改索引 0 处的字节buf.setByte(0, (byte)&#x27;J&#x27;);// True 因为数据是共享关系assert buf.getByte(0) == sliced.getByte(0); ByteBuf 复制 如果需要一个现有缓冲区的真实副本，请使用 copy() 或者 copy(int, int) 方法。不同于派生缓冲区，由这个调用所返回的 ByteBuf 拥有独立的数据副本 12345678// 复制一个 ByteBufCharset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);ByteBuf copy = buf.copy(0, 15);System.out.println(copy.toString(utf8));buf.setByte(0, (byte) &#x27;J&#x27;);// True 因为数据是非共享关系assert buf.getByte(0) != copy.getByte(0); 除了修改原始 ByteBuf 的切片或者副本的效果以外，这两种场景是相同的。只要有可能， 使用 slice() 方法来避免复制内存的开销。 读写操作 getXxx() 和 setXxx() 操作，从给定的索引开始，并且保持索引不变。 12345678910111213// getXxx() And setXxx()Charset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);// 打印第一个字符 NSystem.out.println((char)buf.getByte(0));int readerIndex = buf.readerIndex();int writerIndex = buf.writerIndex();// 更改索引 0 处的字节buf.setByte(0, (byte)&#x27;B&#x27;);System.out.println((char)buf.getByte(0));// True 不会影响相应的索引assert readerIndex == buf.readerIndex();assert writerIndex == buf.writerIndex(); readXxx() 和 writeXxx() 操作，从给定的索引开始，并且会根据已经访问过的字节数对索 引进行调整。 1234567891011// readXxx() And writeXxx()Charset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);System.out.println((char)buf.readByte());int readerIndex = buf.readerIndex();int writerIndex = buf.writerIndex();// 将字符 ？ 追加到缓存区，writerIndex 改变buf.writeByte((byte)&#x27;?&#x27;);// Trueassert readerIndex == buf.readerIndex();assert writerIndex != buf.writerIndex(); ByteBufHolder 接口 如果想要实现一个将其有效负载存储在 ByteBuf 中的消息对象，那么 ByteBufHolder 将是个不错的选择。 ByteBuf 分配 常见的几种管理 ByteBuf 实例的不同方式。 按需分配：ByteBufAllocator 接口 为了降低分配和释放内存的开销，Netty 通过 interface ByteBufAllocator 实现了（ByteBuf 的）池化，它可以用来分配我们所描述过的任意类型的 ByteBuf 实例。 可以通过 Channel（每个都可以有一个不同的 ByteBufAllocator 实例）或者绑定到 ChannelHandler 的 ChannelHandlerContext 获取一个到 ByteBufAllocator 的引用。 1234567// 获取一个到 ByteBufAllocator 的引用Channel channel = ...;ByteBufAllocator allocator = channel.alloc();....ChannelHandlerContext ctx = ...;ByteBufAllocator allocator2 = ctx.alloc();... Netty提供了两种ByteBufAllocator的实现：PooledByteBufAllocator 和 UnpooledByteBufAllocator。 Unpooled 缓冲池 可能某些情况下，你不需要一个 ByteBufAllocator 的引用。对于这种情况，Netty 提供了一个简单的称为 Unpooled 的工具类，它提供了静态的辅助方法来创建未池化的 ByteBuf 实例。 ByteBufUtil 类 ByteBufUtil 提供了用于操作 ByteBuf 的静态的辅助方法。 引用计数 引用计数是一种通过在某个对象所持有的资源不再被其他对象引用时释放该对象所持有的资源来优化内存使用和性能的技术。Netty 在第 4 版中为 ByteBuf 和 ByteBufHolder 引入了引用计数技术，它们都实现了 interface ReferenceCounted。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty 传输","slug":"后台技术/Netty/Netty 传输","date":"2020-01-22T11:03:21.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/22/后台技术/Netty/Netty 传输/","link":"","permalink":"https://wingowen.github.io/2020/01/22/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E4%BC%A0%E8%BE%93/","excerpt":"流经网络的数据总是具有相同的类型：字节。这些字节是如何流动的主要取决于我们所说的网络传输（一个帮助我们抽象底层数据传输机制的概念）","text":"流经网络的数据总是具有相同的类型：字节。这些字节是如何流动的主要取决于我们所说的网络传输（一个帮助我们抽象底层数据传输机制的概念） 案例研究：JDK VS. Netty 原生 JDK 123456789101112131415161718192021222324252627282930313233343536// JDK 的阻塞网络编程public class PlainOioServer &#123; public void serve(int port) throws IOException &#123; final ServerSocket socket = new ServerSocket(port); try &#123; for (;;) &#123; final Socket clientSocket = socket.accept(); System.out.println(&quot;Accepted connection from &quot; + clientSocket); new Thread( // 创建一个新的线程来处理该连接 new Runnable() &#123; @Override public void run() &#123; OutputStream out; try &#123; out = clientSocket.getOutputStream(); // 将消息写给已连接的客户端 out.write(&quot;Hi!\\r\\n&quot;.getBytes(Charset.forName(&quot;UTF-8&quot;))); out.flush(); clientSocket.close(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; clientSocket.close(); &#125;catch (IOException ex) &#123; // ignore on close &#125; &#125; &#125; &#125;).start(); // 启动线程 &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 随着应用程序的用户越来越多，阻塞 I/O 并不能很好地伸缩到支撑成千上万地并发连入连接。此时异步 I/O 可以解决这个问题，但异步 I/O 的 API 是完全不同的，因此不得不重写此应用程序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// JDK 非阻塞网络处理public class PlainNioServer &#123; public void serve(int port) throws IOException &#123; ServerSocketChannel serverChannel = ServerSocketChannel.open(); serverChannel.configureBlocking(false); ServerSocket ssocket = serverChannel.socket(); InetSocketAddress address = new InetSocketAddress(port); ssocket.bind(address); // 打开 Selector 来处理 Channel Selector selector = Selector.open(); // 将 ServerChannel 注册到 Selector 以便接受连接（报个名） serverChannel.register(selector, SelectionKey.OP_ACCEPT); final ByteBuffer msg = ByteBuffer.wrap(&quot;Hi!\\r\\n&quot;.getBytes()); for (;;) &#123; try &#123; // 等待 select() 需要处理的新事件，阻塞将一直持续到下一个传入事件 selector.select(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); // handle exception break; &#125; // 获取所有接收事件的标识 SelectionKey() Set&lt;SelectionKey&gt; readyKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; // 取出标识，以便标识所对应的事件进行业务操作 SelectionKey key = iterator.next(); // 此事件已经取出处理，将其标识符移除 iterator.remove(); try &#123; // 检查事件类型 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel)key.channel(); // 接受客户端 SocketChannel client = server.accept(); client.configureBlocking(false); // 将客户端注册到选择器 client.register(selector, SelectionKey.OP_WRITE | SelectionKey.OP_READ, msg.duplicate()); System.out.println(&quot;Accepted connection from &quot; + client); &#125; // 检查事件类型 if (key.isWritable()) &#123; SocketChannel client = (SocketChannel)key.channel(); ByteBuffer buffer =(ByteBuffer)key.attachment(); while (buffer.hasRemaining()) &#123; // 将数据写到已连接的客户端 if (client.write(buffer) == 0) &#123; break; &#125; &#125; client.close(); &#125; &#125; catch (IOException ex) &#123; key.cancel(); try &#123; key.channel().close(); &#125; catch (IOException cex) &#123; // ignore on close &#125; &#125; &#125; // while &#125; // for &#125; &#125; Netty 框架 因为 Netty 为每种传输的实现都暴露了相同的 API，阻塞与非阻塞网络传输的实现都依赖于 interface Channel、 ChannelPipeline 和 ChannelHandler。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 使用 Netty 的网络处理public class NettyOioServer &#123; public void server(int port) throws Exception &#123; final ByteBuf buf = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer(&quot;Hi!\\r\\n&quot;, Charset.forName(&quot;UTF-8&quot;)) ); // 阻塞模式 EventLoopGroup group = new OioEventLoopGroup(); // 非阻塞模式 // EventLoopGroup group = new OioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(group) // 非阻塞模式 // .channel(NioServerSocketChannel.class) .channel(OioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) // 每个连接的通道都需要调用此初始化方法 .childHandler( new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast( // 拦截和处理事件 new ChannelInboundHandlerAdapter() &#123; @Override public void channelActive(ChannelHandlerContext ctx)throws Exception &#123; ctx.writeAndFlush(buf.duplicate()) // 监听，以便在消息写完后关闭连接 .addListener(ChannelFutureListener.CLOSE); &#125; &#125; ); &#125; &#125; ); ChannelFuture f = b.bind().sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; &#125; 传输 API 传输 API 的核心是 interface Channel，它被用于所有的 I/O 操作 每个 Channel 都将会被分配一个 ChannelPipeline 和 ChannelConfig。ChannelConfig 包含了该 Channel 的所有配置设置，并且支持热更新。 由于 Channel 是独一无二的，为了保证顺序将 Channel 声明为 java.lang. Comparable 的一个子接口。 ChannelPipeline 持有所有将应用于入站和出站数据以及事件的 ChannelHandler 实例，这些 ChannelHandler 实现了应用程序用于处理状态变化以及数据处理的逻辑。你也可以根据需要通过添加或者移除ChannelHandler实例来修改 ChannelPipeline。通过利用Netty的这项能力可以构建出高度灵活的应用程序。 123456789101112131415161718192021// 写出到 ChannelChannel channel = ...;// 创建持有要写数据的 ByteBufByteBuf buf = Unpooled.copiedBuffer(&quot;your data&quot;, CharsetUtil.UTF_8);// 写数据并且冲刷它ChannelFuture cf = channel.writeAndFlush(buf);// 添加监听器以便在操作完成后接收到通知cf.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; // 操作完成并且没有发生错误 if (future.isSuccess()) &#123; System.out.println(&quot;Write successful&quot;); &#125; else &#123; System.err.println(&quot;Write error&quot;); future.cause().printStackTrace(); &#125; &#125; &#125;); Netty 的 Channel 实现是线程安全的，因此你可以存储一个到 Channel 的引用，并且每当你需要向远程节点写数据时，都可以使用它，即使当时许多线程都在使用它。 1234567891011121314151617// 从多个线程使用同一个 Channelfinal Channel channel = ...;final ByteBuf buf = Unpooled.copiedBuffer(&quot;your data&quot;,CharsetUtil.UTF_8).retain();// 创建将数据写到 Channel 的 RunnableRunnable writer = new Runnable() &#123; @Override public void run() &#123; channel.writeAndFlush(buf.duplicate()); &#125;&#125;;// 获取到线程池 Executor 的引用Executor executor = Executors.newCachedThreadPool();// write in one threadexecutor.execute(writer);// write in another threadexecutor.execute(writer);... 内置的传输 Netty 内置了一些可开箱即用的传输。因为并不是它们所有的传输都支持每一种协议，所以你必须选择一个和你的应用程序所使用的协议相容的传输。 NIO NIO 提供了一个所有 I/O 操作的全异步的实现，利用了 JDK1.4 时便可用的基于选择器的 API。 选择器背后的基本概念是充当一个注册表，在那里你将可以请求在 Channel 的状态发生变化时得到通知。 class java.nio.channels.SelectionKey 定义了一组应用程序正在请求通知的状态变化集。 选择器选择并处理状态的变化 零拷贝（zero-copy）：它使你可以快速高效地将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间，其在像 FTP 或者 HTTP 这样的协议中可以显著地提升性能。 Epoll 用于 Linux 的本地非阻塞传输，Linux JDK NIO API 使用了这些 epoll 调用。 OIO Netty 的 OIO 传输实现代表了一种折中：它可以通过常规的传输 API 使用，但是由于它是建立在 java.net 包的阻塞实现之上的，所以它不是异步的。但是，它仍然非常适合于某些用途。 Local 传输 用于在同一个 JVM 中运行的客户端和服务器程序之间的异步通信。 Embedded 传输 Netty 提供了一种额外的传输，使得你可以将一组 ChannelHandler 作为帮助器类嵌入到其他的 ChannelHandler 内部。通过这种方式，你将可以扩展一个 ChannelHandler 的功能，而又不需要修改其内部代码。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty 组件和设计","slug":"后台技术/Netty/Netty 组件和设计","date":"2020-01-22T06:09:03.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/22/后台技术/Netty/Netty 组件和设计/","link":"","permalink":"https://wingowen.github.io/2020/01/22/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E7%BB%84%E4%BB%B6%E5%92%8C%E8%AE%BE%E8%AE%A1/","excerpt":"Channel、EventLoop 和 ChannelFuture Channel、EventLoop 和 ChannelFuture 类合在一起可以被认为是 Netty 的网络抽象代表","text":"Channel、EventLoop 和 ChannelFuture Channel、EventLoop 和 ChannelFuture 类合在一起可以被认为是 Netty 的网络抽象代表 Channel：Socket EventLoop：控制流、多线程处理、并发 ChannelFuture：异步通知 Channel 接口 基本的 I/O 操作（bind()、connect()、read()和 write()）依赖于底层网络传输所提供的原语。在基于 Java 的网络编程中，其基本的构造是 class Socket。Netty 的 Channel 接口所提供的 API，大大地降低了直接使用 Socket 类的复杂性。此外，Channel 也是拥有许多预定义的、专门化实现的广泛类层次结构的根。 EvenLoop 接口 一个 EventLoopGroup 包含一个或者多个 EventLoop； 一个 EventLoop 在它的生命周期内只和一个 Thread 绑定； 所有由 EventLoop 处理的 I/O 事件都将在它专有的 Thread 上被处理； 一个 Channel 在它的生命周期内只注册于一个 EventLoop； 一个 EventLoop 可能会被分配给一个或多个 Channel。 在这种设计中，一个给定 Channel 的 I/O 操作都是由相同的 Thread 执行的，实际上消除了对于同步的需要。 channelFuture 接口 Netty 中所有的 I/O 操作都是异步的，因为一个操作可能不会立即返回，所以我们需要一种用于在之后的某个时间点确定其结果的方法。为此，Netty 提供了 ChannelFuture 接口，其 addListener() 方法注册了一个 ChannelFutureListener，以便在某个操作完成时（无论是否成功）得到通知。 ChannelHandler 和 ChannelPipeline 接下来，让我们更加细致的看一看哪那些管理数据流以及执行应用程序处理逻辑的组件。 ChannelHandler 接口 从应用程序开发人员的角度来看，Netty 的主要组件是 ChannelHandler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。这是可行的，因为 ChannelHandler 的方法是由网络事件（其中术语“事件”的使用非常广泛）触发的。事实上，ChannelHandler 可专门用于几乎任何类型的动作，例如将数据从一种格式转换为另外一种格式，或者处理转换过程中所抛出的异常。 你的应用程序的业务逻辑通常驻留在一个或者多个 ChannelInboundHandler 中。 ChannelPipeline 接口 ChannelPipeline 提供了 ChannelHandler 链的容器，并定义了用于在该链上传播入站和出站事件流的 API。当 Channel 被创建时，它会被自动地分配到它专属的 ChannelPipeline。 ChannelHandler 安装到 ChannelPipeline 中的过程： 一个ChannelInitializer的实现被注册到了ServerBootstrap中； 当 ChannelInitializer.initChannel() 方法被调用时，ChannelInitializer 将在 ChannelPipeline 中安装一组自定义的 ChannelHandler； ChannelInitializer 将它自己从 ChannelPipeline 中移除。 ChannelHandler 是专为支持广泛的用途而设计的，可以将它看作是处理往来 ChannelPipeline 事件（包括数据）的任何代码的通用容器。使得事件流经 ChannelPipeline 是 ChannelHandler 的工作，它们是在应用程序的初始化或者引导阶段被安装的。这些对象接收事件、执行它们所实现的处理逻辑，并将数据传递给链中的下一个 ChannelHandler。它们的执行顺序是由它们被添加的顺序所决定的。实际上，被我们称为 ChannelPipeline 的是这些 ChannelHandler 的编排顺序。 如同上图所示，入站和出站 ChannelHandler 可以被安装到同一个 ChannelPipeline 中。虽然 ChannelInboundHandle 和 ChannelOutboundHandle 都扩展自 ChannelHandler，但是 Netty 能区分 ChannelInboundHandler 实现和 ChannelOutboundHandler 实现，并确保数据只会在具有相同定向类型的两个 ChannelHandler 之间传递。 当 ChannelHandler 被添加到 ChannelPipeline 时，它将会被分配一个 ChannelHandlerContext，其代表了 ChannelHandler 和 ChannelPipeline 之间的绑定。虽然这个对象可以被用于获取底层的 Channel，但是它主要还是被用于写出站数据。 在 Netty 中，有两种发送消息的方式。你可以直接写到 Channel 中，也可以写到和 ChannelHandler 相关联的 ChannelHandlerContext 对象中。前一种方式将会导致消息从 ChannelPipeline 的尾端开始流动，而后者将导致消息从 ChannelPipeline 中的下一个 ChannelHandler 开始流动。 编码器和解码器 正如有用来简化 ChannelHandler 的创建的适配器类一样，所有由 Netty 提供的编码器/解码器适配器类都实现 了 ChannelOutboundHandler 或者 ChannelInboundHandler 接口。 通常来说，这些基类的名称将类似于 ByteToMessageDecoder 或 MessageToByteEncoder。对于特殊的类型，你可能会发现类似于 ProtobufEncoder 和 ProtobufDecoder 这样的名称——预置的用来支持 Google 的 Protocol Buffers。 抽象类 SimpleChannelInboundHandler 最常见的情况是，你的应用程序会利用一个 ChannelHandler 来接收解码消息，并对该数据应用业务逻辑。要创建一个这样的 ChannelHandler，你只需要扩展基类 SimpleChannelInboundHandler&lt;T&gt;，其中 T 是你要处理的消息的 Java 类型 。在这个 ChannelHandler 中，你将需要重写基类的一个或者多个方法，并且获取一个到 ChannelHandlerContext 的引用， 这个引用将作为输入参数传递给 ChannelHandler 的所有方法。 引导 Netty 的引导类为应用程序的网络层配置提供了容器，这涉及将一个进程绑定到某个指定的端口，或者将一个进程连接到另一个运行在某个指定主机的指定端口上的进程。 “服务器”和“客户端”实际上表示了不同的网络行为；换句话说，是监听传入的连接还是建立到一个或者多个进程的连接。 因此，有两种类型的引导：一种用于客户端（简单地称为 Bootstrap），而另一种（ServerBootstrap）用于服务器。无论你的应用程序使用哪种协议或者处理哪种类型的数据，唯一决定它使用哪种引导类的是它是作为一个客户端还是作为一个服务器。 Bootstrap 将连接远程主机和端口，数量通常为一个。 ServerBootstrap 将绑定到一个端口，因为服务器必须要监听连接，数量通常为两个。 第一组将只包含一个 ServerChannel，代表服务器自身的已绑定到某个本地端口的正在监听的套接字。 第二组将包含所有已创建的用来处理传入客户端连接（对于每个服务器已经接受的连接都有一个）的 Channel。 与 ServerChannel 相关联的 EventLoopGroup 将分配一个负责为传入连接请求创建 Channel 的 EventLoop。一旦连接被接受，第二个 EventLoopGroup 就会给它的 Channel 分配一个 EventLoop。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"一个简单的 Netty 应用程序","slug":"后台技术/Netty/Netty 简单应用程序","date":"2020-01-21T14:46:53.000Z","updated":"2023-09-20T06:44:49.021Z","comments":true,"path":"2020/01/21/后台技术/Netty/Netty 简单应用程序/","link":"","permalink":"https://wingowen.github.io/2020/01/21/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"应用程序 EchoEcho 客户端和服务器之间的交互是非常简单的；在客户端建立一个连接之后，它会向服务器发送一个或多个消息，反过来，服务器又会将每个消息回送给客户端。这一简单的功能充分体现了客户端/服务器系统中典型的请求-响应交互模式。","text":"应用程序 EchoEcho 客户端和服务器之间的交互是非常简单的；在客户端建立一个连接之后，它会向服务器发送一个或多个消息，反过来，服务器又会将每个消息回送给客户端。这一简单的功能充分体现了客户端/服务器系统中典型的请求-响应交互模式。 编写 Echo 服务器所有的 Netty 服务器都需要以下两部分 至少一个 ChannelHandler：该组件实现了服务器对从客户端接收的数据的处理，即它的业务逻辑。 引导：这是配置服务器的启动代码。至少，它会将服务器绑定到它要监听连接请求的端口上。 ChannelHandler 和业务逻辑因为你的 Echo 服务器会响应传入的消息，所以它需要实现 ChannelInboundHandler 接口，用来定义响应入站事件的方法。这个简单的应用程序只需要用到少量的这些方法，所以继承 ChannelInboundHandlerAdapter 类也就足够了，它提供了 ChannelInboundHandler 的默认实现。 123456789101112131415161718192021222324// EchoServerHandler 类@Sharable // 标示一个 ChannelHandler 可以被多个 Channel 安全地共享public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; // 对于每个传入的消息都要调用 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf in = (ByteBuf) msg; System.out.println(&quot;Server received: &quot; + in.toString(CharsetUtil.UTF_8)); // 将接收到的数据写给发送者 ctx.write(in); &#125; // 通知 ChannelInboundHandler 最后一次对 channelRead() 的调用是当前批量读取中的最后一条消息 @Override public void channelReadComplete(ChannelHandlerContext ctx) &#123; // 将消息冲刷到远程节点并且关闭该 Channel ctx.writeAndFlush(Unpooled.EMPTY_BUFFER).addListener(ChannelFutureListener.CLOSE); &#125; // 在读取操作期间，有异常抛出时会调用 @Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; ChannelHandler 的一些关键点： 针对不同类型的事件来调用 ChannelHandler； 应用程序通过实现或者扩展 ChannelHandler 来挂钩到事件的生命周期，并且提供自定义的应用程序逻辑； 在架构上，ChannelHandler 有助于保持业务逻辑与网络处理代码的分离。这简化了开发过程，因为代码必须不断地演化以响应不断变化的需求。 引导服务器绑定到服务器将在其上监听并接受传入连接请求的端口； 配置 Channel，以将有关的入站消息通知给 EchoServerHandler 实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// EchoServer 类public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.println(&quot;Usage: &quot; + EchoServer.class.getSimpleName() + &quot; &lt;port&gt;&quot;); &#125; int port = Integer.parseInt(args[0]); new EchoServer(port).start(); &#125; public void start() throws Exception &#123; final EchoServerHandler serverHandler = new EchoServerHandler(); // 创建一个循环组 EventLoopGroup EventLoopGroup group = new NioEventLoopGroup(); try &#123; // 创建一个 ServerBootstrap 的实例以引导和绑定服务器 ServerBootstrap b = new ServerBootstrap(); b.group(group) .channel(NioServerSocketChannel.class) // 指定服务器绑定的本地的 InetSocketAddress .localAddress(new InetSocketAddress(port)) .childHandler( // 接受一个新连接则一个新的子 Channel 将会被创建 new ChannelInitializer&lt;SocketChannel&gt;()&#123; // 使用一个 EchoServerHandler 的实例初始化每一个新的 Channel @Override public void initChannel(SocketChannel ch)throws Exception &#123; ch.pipeline().addLast(serverHandler); &#125; &#125; ); // 异步地绑定服务器，sync() 方法的调用将导致当前 Thread 阻塞，一直到绑定操作完成为止 ChannelFuture f = b.bind().sync(); // 异步地关闭服务器 f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; &#125; 编写 Echo 客户端 Echo 客户端将会： 连接到服务器； 发送一个或者多个消息； 对于每个消息，等待并接受从服务器发送回的相同消息 关闭连接。 通过 ChannelHandler 实现客户端逻辑如同服务器，客户端将拥有一个用来处理数据的 ChannelInboundHandler。 123456789101112131415161718192021// 客户端的 ChannelHandler// ByteBuf 为 Nutty 的字节容器@Sharablepublic class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 在到服务器的连接已经建立之后将被调用 @Override public void channelActive(ChannelHandlerContext ctx) &#123; ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;Netty rocks!&quot;,CharsetUtil.UTF_8)); &#125; // 当从服务器接收到一条消息时被调用 @Override public void channelRead0(ChannelHandlerContext ctx, ByteBuf in) &#123; System.out.println(&quot;Client received: &quot; + in.toString(CharsetUtil.UTF_8)); &#125; // 在处理过程中引发异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; SimpleChannelInboundHandler 与 ChannelInboundHandler 你可能会想：为什么我们在客户端使用的是 SimpleChannelInboundHandler，而不是在 EchoServerHandler 中所使用的 ChannelInboundHandlerAdapter 呢？这和两个因素的相互作用有关：业务逻辑如何处理消息以及 Netty 如何管理资源。 在客户端，当 channelRead0() 方法完成时，你已经有了传入消息，并且已经处理完它了。当该方法返回时，SimpleChannelInboundHandler 负责释放指向保存该消息的 ByteBuf 的内存引用。 在 EchoServerHandler 中，你仍然需要将传入消息回送给发送者，而 write()操作是异步的，直到 channelRead() 方法返回后可能仍然没有完成。为此，EchoServerHandler 扩展了 ChannelInboundHandlerAdapter，其在这个时间点上不会释放消息。 消息在 EchoServerHandler 的 channelReadComplete() 方法中，当 writeAndFlush() 方法被调用时被释放 引导客户端引导客户端类似于引导服务器，不同的是，客户端是使用主机和端口参数来连接远程地址，也就是这里的 Echo 服务器的地址，而不是绑定到一个一直被监听的端口。 12345678910111213141516171819202122232425262728293031323334353637383940// 客户端的主类public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .remoteAddress(new InetSocketAddress(host, port)) .handler( new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler()); &#125; &#125; ); ChannelFuture f = b.connect().sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println(&quot;Usage: &quot; + EchoClient.class.getSimpleName() + &quot; &lt;host&gt; &lt;port&gt;&quot;); return; &#125; String host = args[0]; int port = Integer.parseInt(args[1]); new EchoClient(host, port).start(); &#125; &#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty 入门简介","slug":"后台技术/Netty/Netty 入门简介","date":"2020-01-21T01:37:11.000Z","updated":"2023-09-20T07:00:03.842Z","comments":true,"path":"2020/01/21/后台技术/Netty/Netty 入门简介/","link":"","permalink":"https://wingowen.github.io/2020/01/21/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B/","excerpt":"Java IO 无论是 C 语言，还是 Java，在进行网络编程的开发时都较为不友好。早期的 Java API（ java.net ）只支持由本地系统套接字库提供所谓的阻塞函数。","text":"Java IO 无论是 C 语言，还是 Java，在进行网络编程的开发时都较为不友好。早期的 Java API（ java.net ）只支持由本地系统套接字库提供所谓的阻塞函数。 12345678910111213141516171819202122// 阻塞 I/O 示例I/O// 创建一个 ServerSocket 用以监听端口上的连接请求ServerSocket serverSocket = new ServerSocket(portNumber);// accept() 方法调用将被阻塞，直到一个连接建立Socket clientSocket = serverSocket.accept();BufferReader in = new BufferReader( new InputStreamReader( clientSocket.getInputStream() ));// PrintWriter(OutputStream out, boolean autoFlush) ???PrintWriter out = new PrintWriter(clientSocket.getOutputStream(), true);String request, response;while ((request = in.readLine()) != null) &#123; if (&quot;Done&quot;.equals(request)) &#123; break; // 客户端发送了 Done 则退出处理循环 &#125; // You can use ProcessRequest to handle your request response = processRequest(request); out.println(response);&#125; 这段代码片段将只能同时处理一个连接，要管理多个并发客户端，需要为每个新的客户端 Socket 创建一个新的 Thread。 使用阻塞 I/O处理多个连接： Java NIO class java.nio.channels.Selector 是 Java 的非阻塞 I/O 实现的关键。它使用了事件通知 API 以确定在一组非阻塞套接字中有哪些已经就绪能够进 行 I/O 相关的操作。因为可以在任何的时间检查任意 的读操作或者写操作的完成状态。 使用 Selector 的非阻塞 I/O Netty 简介 在网络编程领域，Netty 是 Java 的卓越框架。它驾驭了 Java 高 API 的能力，并将其隐藏在一个易于使用的 API 之后。Netty 使你可以专注于自己真正感兴趣的：你的应用程序的独一无二的价值。 一个既是异步的又是事件驱动的系统会表现出一种特殊的、对我们来说极具价值的行为：它可以以任意的顺序响应在任意的时间点产生的事件。 完全异步的 I/O：非阻塞网络调用使得我们可以不必等待一个操作的完成。异步方法会立即返回，并且在它完成时，会直接或者在稍后的某个时间点通知用户。 选择器使得我们能够通过较少的线程便可监视许多连接上的事件。 Netty 的核心组件 Channel Channel 时 Java NIO 的一个基本构造。 它代表一个到实体（如一个硬件设备、一个文件、一个网络套接字或者一个能够执 行一个或者多个不同的I/O操作的程序组件）的开放连接，如读操作和写操作。 在一定程度上可以把 Channel 看作是传入（入站）或者传出（出站）数据的载体。因此，它可以被打开或者被关闭，连接或者断开连接。 回调 Netty 在内部使用了回调来处理事件；当一个回调被触发时，相关的事件可以被一个 interfaceChannelHandler 的实现处理 12345678910// 被回调触发的 ChannelHandlerpublic class ConnectHandler extends ChannelInboundHandlerAdapter &#123; @Override // 当一个新的连接被建立完成时，此方法将会被调用 public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println( &quot;Client &quot; + ctx.channel().remoteAddress() + &quot; connected&quot; ); &#125;&#125; Future Future 提供了另一种在操作完成时通知应用程序的方式。这个对象可以看作是一个异步操作的结果的占位符；它将在未来的某个时刻完成，并提供对其结果的访问。 JDK 预置了 interface java.util.concurrent.Future，但是其所提供的实现，只允许手动检查对应的操作是否已经完成，或者一直阻塞直到它完成。这是非常繁琐的，所以 Netty 提供了它自己的实现：ChannelFuture，用于在执行异步操作的时候使用。 ChannelFuture提供了几种额外的方法，这些方法使得我们能够注册一个或者多个 ChannelFutureListener 实例。由 ChannelFutureListener 提供的通知机制消除了手动检查对应的操作是否完成的必要。 监听器的回调方法 operationComplete()：将会在对应的操作完成时被调用。 监听器可以判断操作是成功地完成了还是出错了（ 检索产生的 Throwable ）。 每个 Netty 的出站 I/O 操作都将返回一个 ChannelFuture；也就是说，它们都不会阻塞。正如我们前面所提到过的一样，Netty 完全是异步和事件驱动的。 12345678910111213141516171819202122232425262728// 异步的建立连接Channel channel = ...;// Does not blockChannelFuture future = channel.connect( // 异步地连接到远程节点 new InetSocketAddress(&quot;192.168.0.1&quot;, 25));// 注册一个 ChannelFutureListener 以便在操作完成时获得通知future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; // 检查操作的状态 if (future.isSuccess())&#123; ByteBuf buffer = Unpooled.copiedBuffer( &quot;Hello&quot;,Charset.defaultCharset() ); // 将数据异步发送到远程节点 ChannelFuture wf = future.channel().writeAndFlush(buffer); .... &#125; else &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; &#125; &#125;); 回调和 Future 是相互补充的机制；它们相互结合，构成了 Netty 本身的关键构件块之一。 事件和 ChannelHandler Netty 是一个网络编程框架，所以事件是按照它们与入站或出站数据流的相关性进行分类的。 可能由入站数据或者相关的状态更改而触发的事件： 连接已被激活或者连接失活； 数据读取； 用户事件； 错误事件。 出站事件是未来将会触发的某个动作的操作结果，这些动作包括： 打开或者关闭到远程节点的连接； 将数据写到或者冲刷到套接字。 每个事件都可以被分发给 ChannelHandler 类中的某个用户实现的方法，后面会对此类进行更进一步的说明，目前你可以每个 ChannelHandler 的实例都类似于一种为了响应特定事件而被执行的回调。 流经 ChannelHandler 链的入站事件和出站事件 各组件的整合 Future、回调和 ChannelHandler Netty 的异步编程模型是建立在 Future 和回调的概念之上的，而将事件派发到 ChannelHandler 的方法则发生在更深的层次上。结合在一起，这些元素就提供了一个处理环境，使你的应用程序逻 辑可以独立于任何网络操作相关的顾虑而独立地演变。这也是 Netty 的设计方式的一个关键目标。 拦截操作以及高速地转换入站数据和出站数据，都只需要你提供回调或者利用操作所返回的 Future。这使得链接操作变得既简单又高效，并且促进了可重用的通用代码的编写。 选择器、事件和 EventLoop Netty 通过触发事件将 Selector 从应用程序中抽象出来，消除了所有本来将需要手动编写 的派发代码。在内部，将会为每个 Channel 分配一个 EventLoop，用以处理所有事件。 EventLoop 本身只由一个线程驱动，其处理了一个 Channel 的所有 I/O 事件，并且在该 EventLoop 的整个生命周期内都不会改变（ 无需顾虑同步 ）。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]}],"categories":[{"name":"杂项","slug":"杂项","permalink":"https://wingowen.github.io/categories/%E6%9D%82%E9%A1%B9/"},{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"基础科学","slug":"基础科学","permalink":"https://wingowen.github.io/categories/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/"},{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"日常","slug":"日常","permalink":"https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"},{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"},{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://wingowen.github.io/tags/Hexo/"},{"name":"InooDB","slug":"InooDB","permalink":"https://wingowen.github.io/tags/InooDB/"},{"name":"Hive","slug":"Hive","permalink":"https://wingowen.github.io/tags/Hive/"},{"name":"Kudu","slug":"Kudu","permalink":"https://wingowen.github.io/tags/Kudu/"},{"name":"Impala","slug":"Impala","permalink":"https://wingowen.github.io/tags/Impala/"},{"name":"Pandas","slug":"Pandas","permalink":"https://wingowen.github.io/tags/Pandas/"},{"name":"SQL","slug":"SQL","permalink":"https://wingowen.github.io/tags/SQL/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"},{"name":"脚本命令","slug":"脚本命令","permalink":"https://wingowen.github.io/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"name":"网络相关","slug":"网络相关","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"},{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"},{"name":"网站收集","slug":"网站收集","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"},{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"MQ","slug":"MQ","permalink":"https://wingowen.github.io/tags/MQ/"},{"name":"Dubbo","slug":"Dubbo","permalink":"https://wingowen.github.io/tags/Dubbo/"},{"name":"Aysnc","slug":"Aysnc","permalink":"https://wingowen.github.io/tags/Aysnc/"},{"name":"Scheduled","slug":"Scheduled","permalink":"https://wingowen.github.io/tags/Scheduled/"},{"name":"Email","slug":"Email","permalink":"https://wingowen.github.io/tags/Email/"},{"name":"Security","slug":"Security","permalink":"https://wingowen.github.io/tags/Security/"},{"name":"Shiro","slug":"Shiro","permalink":"https://wingowen.github.io/tags/Shiro/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://wingowen.github.io/tags/ElasticSearch/"},{"name":"Cache","slug":"Cache","permalink":"https://wingowen.github.io/tags/Cache/"},{"name":"Java","slug":"Java","permalink":"https://wingowen.github.io/tags/Java/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wingowen.github.io/tags/MyBatis/"},{"name":"JPA","slug":"JPA","permalink":"https://wingowen.github.io/tags/JPA/"},{"name":"Druid","slug":"Druid","permalink":"https://wingowen.github.io/tags/Druid/"},{"name":"JDBC","slug":"JDBC","permalink":"https://wingowen.github.io/tags/JDBC/"},{"name":"Web","slug":"Web","permalink":"https://wingowen.github.io/tags/Web/"},{"name":"Log","slug":"Log","permalink":"https://wingowen.github.io/tags/Log/"},{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://wingowen.github.io/tags/WebSocket/"}]}