<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>深度学习入门 | WINGO BLOG</title>
  <meta name="description" content="深度学习入门：基于 Python 的理论与实现。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门">
<meta property="og:url" content="https://wingowen.github.io/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="WINGO&#39;S BLOG">
<meta property="og:description" content="深度学习入门：基于 Python 的理论与实现。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/Broadcast.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/sincos.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/OR.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/OR01.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/ActivationFunction01.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/NeuralNet02.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/NeuralNet03.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/Predict.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/ManualToAuto.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/NeuralNetGradient.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815133610758.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815134405670.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815142539203.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815143539792.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815145429545.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815151401193.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815151419201.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815172530363.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815224431801.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815233559503.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816114932375.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816122708920.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816123300153.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816142103382.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816153901765.png">
<meta property="og:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816164002361.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817203500281.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817204419693.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817204619420.png">
<meta property="og:image" content="c:/Users/wingo/Pictures/Screenshot/深度学习入门/image-20200817204809164.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817205614567.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817210738055.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817211245102.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817211354403.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817224332415.png">
<meta property="og:image" content="c:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817231530172.png">
<meta property="article:published_time" content="2020-08-08T02:16:57.000Z">
<meta property="article:modified_time" content="2023-09-20T07:29:43.104Z">
<meta property="article:author" content="Wingo Wen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://welab-wingo.gitee.io/image/2020/08/DL/Broadcast.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://wingowen.github.io/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/index.html">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="WINGO'S BLOG" type="application/atom+xml">
</head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">WINGO.WEN</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
      </ul>
      
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>得失从缘 心无增减</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/">后台技术</a><span class="category-list-count">23</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/">基础科学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%BB%B4/">运维</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Aysnc/" style="font-size: 13px;">Aysnc</a> <a href="/tags/Cache/" style="font-size: 13px;">Cache</a> <a href="/tags/Druid/" style="font-size: 13px;">Druid</a> <a href="/tags/Dubbo/" style="font-size: 13px;">Dubbo</a> <a href="/tags/ElasticSearch/" style="font-size: 13px;">ElasticSearch</a> <a href="/tags/Email/" style="font-size: 13px;">Email</a> <a href="/tags/Flume/" style="font-size: 13px;">Flume</a> <a href="/tags/HBase/" style="font-size: 13px;">HBase</a> <a href="/tags/Hadoop/" style="font-size: 13px;">Hadoop</a> <a href="/tags/Hive/" style="font-size: 13.25px;">Hive</a> <a href="/tags/Impala/" style="font-size: 13px;">Impala</a> <a href="/tags/InooDB/" style="font-size: 13px;">InooDB</a> <a href="/tags/JDBC/" style="font-size: 13px;">JDBC</a> <a href="/tags/JPA/" style="font-size: 13px;">JPA</a> <a href="/tags/Java/" style="font-size: 13px;">Java</a> <a href="/tags/Kafka/" style="font-size: 13px;">Kafka</a> <a href="/tags/Kudu/" style="font-size: 13px;">Kudu</a> <a href="/tags/Log/" style="font-size: 13px;">Log</a> <a href="/tags/MQ/" style="font-size: 13.25px;">MQ</a> <a href="/tags/MyBatis/" style="font-size: 13px;">MyBatis</a> <a href="/tags/Netty/" style="font-size: 14px;">Netty</a> <a href="/tags/Pandas/" style="font-size: 13px;">Pandas</a> <a href="/tags/Python/" style="font-size: 13px;">Python</a> <a href="/tags/SQL/" style="font-size: 13px;">SQL</a> <a href="/tags/Scheduled/" style="font-size: 13px;">Scheduled</a> <a href="/tags/Security/" style="font-size: 13px;">Security</a> <a href="/tags/Shiro/" style="font-size: 13px;">Shiro</a> <a href="/tags/Spring-Boot/" style="font-size: 14px;">Spring Boot</a> <a href="/tags/Web/" style="font-size: 13px;">Web</a> <a href="/tags/WebSocket/" style="font-size: 13px;">WebSocket</a> <a href="/tags/gRPC/" style="font-size: 13px;">gRPC</a> <a href="/tags/sklearn/" style="font-size: 13px;">sklearn</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 13.75px;">机器学习</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/" style="font-size: 13px;">网络相关</a> <a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 13.5px;">考研</a> <a href="/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/" style="font-size: 13px;">脚本命令</a> <a href="/tags/%E9%83%A8%E7%BD%B2/" style="font-size: 13px;">部署</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">九月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">七月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">8</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc collapse   in  " id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BF%85%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text"> 必备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#anaconda"><span class="toc-number">1.1.</span> <span class="toc-text"> Anaconda</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#numpy"><span class="toc-number">1.2.</span> <span class="toc-text"> NumPy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#matplotlib"><span class="toc-number">1.3.</span> <span class="toc-text"> Matplotlib</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">2.</span> <span class="toc-text"> 感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E9%80%BB%E8%BE%91%E7%94%B5%E8%B7%AF"><span class="toc-number">2.1.</span> <span class="toc-text"> 简单逻辑电路</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">2.2.</span> <span class="toc-text"> 多层感知机</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text"> 神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.</span> <span class="toc-text"> 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.1.</span> <span class="toc-text"> Sigmoid 函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.2.</span> <span class="toc-text"> 阶跃函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#relu-%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.3.</span> <span class="toc-text"> ReLU 函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%81%92%E7%AD%89%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.4.</span> <span class="toc-text"> 恒等函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#softmax-%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.5.</span> <span class="toc-text"> softmax 函数</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.</span> <span class="toc-text"> 简单实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="toc-number">3.3.</span> <span class="toc-text"> 手写数字识别</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-number">3.3.1.</span> <span class="toc-text"> 批处理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text"> 神经网络的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.1.</span> <span class="toc-text"> 从数据中学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text"> 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.2.1.</span> <span class="toc-text"> 均方误差</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.2.2.</span> <span class="toc-text"> 交叉熵误差</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86"><span class="toc-number">4.3.</span> <span class="toc-text"> 数值微分</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">4.3.1.</span> <span class="toc-text"> 导数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.3.2.</span> <span class="toc-text"> 偏导数和梯度</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-number">4.4.</span> <span class="toc-text"> 神经网络的梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.5.</span> <span class="toc-text"> 学习算法的实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">5.</span> <span class="toc-text"> 误差反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.1.</span> <span class="toc-text"> 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B9%98%E6%B3%95%E5%B1%82"><span class="toc-number">5.1.1.</span> <span class="toc-text"> 乘法层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8A%A0%E6%B3%95%E5%B1%82"><span class="toc-number">5.1.2.</span> <span class="toc-text"> 加法层</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.2.</span> <span class="toc-text"> 激活函数层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#relu-%E5%B1%82"><span class="toc-number">5.2.1.</span> <span class="toc-text"> ReLU 层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sigmoid-%E5%B1%82"><span class="toc-number">5.2.2.</span> <span class="toc-text"> Sigmoid 层</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#affine-softmax-%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.3.</span> <span class="toc-text"> Affine &#x2F; Softmax 层的实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#softmax-with-loss-%E5%B1%82"><span class="toc-number">5.4.</span> <span class="toc-text"> Softmax-with-Loss 层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">5.5.</span> <span class="toc-text"> 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8A%80%E5%B7%A7"><span class="toc-number">6.</span> <span class="toc-text"> 与学习相关的技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="toc-number">6.1.</span> <span class="toc-text"> 参数更新</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#sgd"><span class="toc-number">6.1.1.</span> <span class="toc-text"> SGD</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#momentum"><span class="toc-number">6.1.2.</span> <span class="toc-text"> Momentum</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#adagrad"><span class="toc-number">6.1.3.</span> <span class="toc-text"> AdaGrad</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%80%BC"><span class="toc-number">6.2.</span> <span class="toc-text"> 权重初始值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#batch-normalization"><span class="toc-number">6.3.</span> <span class="toc-text"> Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.4.</span> <span class="toc-text"> 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">6.4.1.</span> <span class="toc-text"> 过拟合</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9D%83%E5%80%BC%E8%A1%B0%E9%80%80"><span class="toc-number">6.4.2.</span> <span class="toc-text"> 权值衰退</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#dropout"><span class="toc-number">6.4.3.</span> <span class="toc-text"> Dropout</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%AA%8C%E8%AF%81"><span class="toc-number">6.5.</span> <span class="toc-text"> 超参数的验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text"> 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-number">7.1.</span> <span class="toc-text"> 整体结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">7.2.</span> <span class="toc-text"> 卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97"><span class="toc-number">7.2.1.</span> <span class="toc-text"> 卷积运算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-number">7.2.2.</span> <span class="toc-text"> 填充</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85"><span class="toc-number">7.2.3.</span> <span class="toc-text"> 步幅</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E7%BB%B4%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97"><span class="toc-number">7.2.4.</span> <span class="toc-text"> 三维数据的卷积计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%B9%E5%A4%84%E7%90%86-2"><span class="toc-number">7.2.5.</span> <span class="toc-text"> 批处理</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">7.3.</span> <span class="toc-text"> 池化层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">7.4.</span> <span class="toc-text"> 代码实现</span></a></li></ol></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-机器学习/深度学习入门" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      深度学习入门
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" class="article-date">
	  <time datetime="2020-08-08T02:16:57.000Z" itemprop="datePublished">2020-08-08</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </span>

        

        

        <!-- <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/#comments" class="article-comment-link">评论</a></span> -->
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p>深度学习入门：基于 Python 的理论与实现。</p>
<span id="more"></span>
<h3 id="必备知识"><a class="markdownIt-Anchor" href="#必备知识"></a> 必备知识</h3>
<p>在正式开始学习学习深度学习之前，我们先来了解一些必须要掌握的基本知识。（Python 的基础语法请参照本人的另一篇博客：<a target="_blank" rel="noopener" href="https://welab-wingo.gitee.io/2020/08/01/Python/%E7%88%B6%E4%B8%8E%E5%AD%90%E7%BC%96%E7%A8%8B%E4%B9%8B%E6%97%85/">父与子编程之旅</a>）</p>
<h4 id="anaconda"><a class="markdownIt-Anchor" href="#anaconda"></a> Anaconda</h4>
<p>Anaconda 可以统一管理开发环境，以及便捷的获取及管理包。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建一个名为 py37 的开发环境，并指定 python 版本</span></span><br><span class="line">conda create -n py37 python=3.7</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">激活 py37 环境</span></span><br><span class="line">conda activate py37</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">退出 py37 环境</span></span><br><span class="line">conda deactivate py37</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除</span></span><br><span class="line">conda remove -n py37 --all</span><br></pre></td></tr></table></figure>
<h4 id="numpy"><a class="markdownIt-Anchor" href="#numpy"></a> NumPy</h4>
<p>在深度学习的实现中，经常出现数组和矩阵的计算。NumPy 的数组类 <code>numpy.array</code> 中提供了很多便捷的方法，在实现深度学习时，我们将使用这些方法。本节我们来简单介绍一下后面会用到的 NumPy。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 导入 NumPy 库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">## 生成 NumPy 数组</span></span><br><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># [ 1. 2. 3.]</span></span><br><span class="line"><span class="built_in">type</span>(x) <span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## NumPy 数组的算术运算</span></span><br><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">y = np.array([<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>])</span><br><span class="line">x + y <span class="comment"># array([ 3., 6., 9.])</span></span><br><span class="line">x - y <span class="comment"># array([ -1., -2., -3.])</span></span><br><span class="line">x * y <span class="comment"># array([ 2., 8., 18.])</span></span><br><span class="line">x / y <span class="comment"># array([ 0.5, 0.5, 0.5])</span></span><br></pre></td></tr></table></figure>
<p>注意，在上面的 NumPy 算数运算中，数组 x 和数组 y 的元素个数是相同的（对应元素计算），如果元素个数不同，程序就会报错，所以元素个数保持一致非常重要。</p>
<p>此外，NumPy 数组不仅可以进行 element-wise 运算，也可以和单一的数值（标量）组合器来进行运算。 此时，需要在 NumPy 数组的各个元素和标量之间进行运算。 这个功能也被称为广播。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/08/DL/Broadcast.png" alt="" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">x / <span class="number">2.0</span> <span class="comment"># array([ 0.5, 1. , 1.5])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## NumPy 的 N 维数组</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(A)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1 2]</span></span><br><span class="line"><span class="string"> [3 4]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">A.shape <span class="comment"># (2, 2) 第一个维度有两个元素 第二维度有两个元素</span></span><br><span class="line">A.dtype <span class="comment"># dtype(&#x27;int64&#x27;) 矩阵元素的数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 矩阵的算术运算</span></span><br><span class="line">B = np.array([[<span class="number">3</span>, <span class="number">0</span>],[<span class="number">0</span>, <span class="number">6</span>]])</span><br><span class="line">A + B</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 4, 2],</span></span><br><span class="line"><span class="string"> 	   [ 3, 10]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">A * B</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 3, 0],</span></span><br><span class="line"><span class="string">       [ 0, 24]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 与标量相乘的广播功能</span></span><br><span class="line">A * <span class="number">10</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 10, 20],</span></span><br><span class="line"><span class="string"> 	   [ 30, 40]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>NumPy 数组 <code>np.array</code> 可以生成 N 维数组，即可以生成一维数组、 二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量， 将二维数组称为矩阵。另外，可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为矩阵，将三维数组及三维以上的数组称为张量或多维数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 访问元素</span></span><br><span class="line"><span class="comment"># 索引访问</span></span><br><span class="line">X = np.array([[<span class="number">51</span>, <span class="number">55</span>], [<span class="number">14</span>, <span class="number">19</span>], [<span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[51 55]</span></span><br><span class="line"><span class="string"> [14 19]</span></span><br><span class="line"><span class="string"> [ 0  4]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">X[<span class="number">0</span>] <span class="comment"># 第 0 行 array([51, 55])</span></span><br><span class="line">X[<span class="number">0</span>][<span class="number">1</span>] <span class="comment"># (0,1) 的元素 55</span></span><br><span class="line"><span class="comment"># for 语句访问</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> X:</span><br><span class="line">	<span class="built_in">print</span>(row)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[51 55]</span></span><br><span class="line"><span class="string">[14 19]</span></span><br><span class="line"><span class="string">[ 0  4]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 数组访问</span></span><br><span class="line">X = X.flatten() <span class="comment"># 将 X 转换为一维数组</span></span><br><span class="line"><span class="built_in">print</span>(X) <span class="comment"># [51 55 14 19 0 4]</span></span><br><span class="line">X[np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>])] <span class="comment"># 获取索引为 0、2、4 的元素 array([51, 14, 0])</span></span><br><span class="line"><span class="comment"># 运用标记法，从 X 中抽出大于 15 的元素</span></span><br><span class="line">X &gt; <span class="number">15</span> <span class="comment"># array([ True, True, False, True, False, False], dtype=bool)</span></span><br><span class="line">X[X&gt;<span class="number">15</span>] <span class="comment"># array([51, 55, 19]) 取出 True 对应的元素</span></span><br></pre></td></tr></table></figure>
<p>矩阵的运算。（注意！！！矩阵的点乘得遵循一定的法则 (A, B) · (B, A) = (A, A)）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">np.ndim(A) <span class="comment"># 2 有两个维度</span></span><br><span class="line">A.shape <span class="comment"># (2, 3)</span></span><br><span class="line">B = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">B.shape <span class="comment"># (3, 2)</span></span><br><span class="line">np.dot(A, B) <span class="comment"># 点乘，矩阵的乘法</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[22, 28],</span></span><br><span class="line"><span class="string"> 	   [49, 64]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">B = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">np.ndim(B) <span class="comment"># 2</span></span><br><span class="line">B.shape <span class="comment"># (3, 2)</span></span><br></pre></td></tr></table></figure>
<h4 id="matplotlib"><a class="markdownIt-Anchor" href="#matplotlib"></a> Matplotlib</h4>
<p>深度学习实验中，图形的绘制和数据的可视化非常重要。Matplotlib 是用于绘制图形的库，使用 Matplotlib 可以轻松地绘制图形和实现数据的可视化。这里，我们来介绍一下图形的绘制方法和图像的显示方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">## 绘制 sin 以及 cos 函数的图形</span></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 以 0.1 为步长单位，生成 0 到 6 的数据</span></span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)</span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">plt.plot(x, y1, label=<span class="string">&quot;sin&quot;</span>)</span><br><span class="line">plt.plot(x, y2, linestyle = <span class="string">&quot;--&quot;</span>, label=<span class="string">&quot;cos&quot;</span>) <span class="comment"># 用虚线绘制</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;x&quot;</span>) <span class="comment"># x 轴标签</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;y&quot;</span>) <span class="comment"># y 轴标签</span></span><br><span class="line">plt.title(<span class="string">&#x27;sin &amp; cos&#x27;</span>) <span class="comment"># 标题</span></span><br><span class="line">plt.legend() <span class="comment"># 自动对上诉添加的标签布局，复杂时可以自定义</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/sincos.png" style="zoom:50%;" />
<p>pyplot 中还提供了用于显示图像的方法 <code>imshow()</code>。另外，可以使用 <code>matplotlib.image</code> 模块的 <code>imread()</code> 方法读入图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 显示图像</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.image <span class="keyword">import</span> imread</span><br><span class="line"></span><br><span class="line">img = imread(<span class="string">&#x27;lena.png&#x27;</span>) <span class="comment"># 读入图像（设定合适的路径！）</span></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="感知机"><a class="markdownIt-Anchor" href="#感知机"></a> 感知机</h3>
<p>感知机可以说是神经网络（深度学习）的起源的算法，因此， 学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。</p>
<p>感知机接收多个输入信号，输出一个信号。</p>
<h4 id="简单逻辑电路"><a class="markdownIt-Anchor" href="#简单逻辑电路"></a> 简单逻辑电路</h4>
<p>形式一：x<sub>1</sub>w<sub>1 </sub> + x<sub>2</sub>w<sub>2</sub> ≤ θ	👉	y = 0 （θ 代表阈值、x 代表输入信号、w 代表权重）</p>
<p>形式二：x<sub>1</sub>w<sub>1 </sub> + x<sub>2</sub>w<sub>2</sub> + b ≤ 0 （其中 b = -θ）	👉	y = 0 （b 代表偏置）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 与门</span></span><br><span class="line"><span class="comment"># 简单实现 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AND</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    w1, w2, theta = <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.7</span></span><br><span class="line">    tmp = x1*w1 + x2*w2</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= theta:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> tmp &gt; theta:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="comment"># 使用权重和偏置的实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">AND</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    x = np.array([x1, x2])</span><br><span class="line">    w = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">    b = -<span class="number">0.7</span></span><br><span class="line">    tmp = np.<span class="built_in">sum</span>(w*x) + b</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 非门：与门反过来即可</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">NAND</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    x = np.array([x1, x2])</span><br><span class="line">    w = np.array([-<span class="number">0.5</span>, -<span class="number">0.5</span>]) <span class="comment"># 仅权重和偏置与 AND 不同！</span></span><br><span class="line">    b = <span class="number">0.7</span></span><br><span class="line">    tmp = np.<span class="built_in">sum</span>(w*x) + b</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 或门</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">OR</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    x = np.array([x1, x2])</span><br><span class="line">    w = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>]) <span class="comment"># 仅权重和偏置与AND不同！</span></span><br><span class="line">    b = -<span class="number">0.2</span></span><br><span class="line">    tmp = np.<span class="built_in">sum</span>(w*x) + b</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="comment">## 异或门（无法用上面类似的形式进行实现）</span></span><br></pre></td></tr></table></figure>
<p>为什么异或门无法用上述相似的形式（线性）进行实现呢？先来看看异或门的结果分部。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/OR.png" style="zoom:50%;" />
<p>通过分析上图可以发现，无法用线性空间（直线）将 0 和 1 这两种情况进行区域的划分。但用非线性的空间（曲线）可以，那如何划分非线性空间呢？答案是：叠加感知机。即多层感知机。</p>
<h4 id="多层感知机"><a class="markdownIt-Anchor" href="#多层感知机"></a> 多层感知机</h4>
<p>通过组合与门、与非门、或门实现异或门。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/OR01.png" style="zoom:50%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异或门</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">XOR</span>(<span class="params">x1, x2</span>):</span><br><span class="line">    s1 = NAND(x1, x2)</span><br><span class="line">    s2 = OR(x1, x2)</span><br><span class="line">    y = AND(s1, s2)</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h3 id="神经网络"><a class="markdownIt-Anchor" href="#神经网络"></a> 神经网络</h3>
<p>在上一章中，我们使用感知机实现了简单的逻辑电路。从实现的逻辑中我们可以发现，即使是复杂的函数，感知机也隐含着能够代表它的可能性。但有一个很令人头疼的问题，那便是权重的设定。在实现简单逻辑电路等简单函数时，我们可以通过观察归纳得出符合条件的权重，但如果函数特别的复杂，那么光靠人脑去猜测其权重显然是不合适的。</p>
<p>神经网络的出现就是为了解决这个问题的。具体地讲，神经网络的一 个重要性质是它可以自动地从数据中学习到合适的权重参数。</p>
<h4 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h4>
<p>激活函数是连接感知机和神经网络的桥梁。</p>
<p>隐藏（中间层）的激活函数。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/08/DL/ActivationFunction01.png" alt="" /></p>
<h5 id="sigmoid-函数"><a class="markdownIt-Anchor" href="#sigmoid-函数"></a> Sigmoid 函数</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure>
<h5 id="阶跃函数"><a class="markdownIt-Anchor" href="#阶跃函数"></a> 阶跃函数</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 简单实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># 支持 NumPy 数组实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">step_function</span>(<span class="params">x</span>):</span><br><span class="line">    y = x &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> y.astype(np.<span class="built_in">int</span>) <span class="comment"># 把数组 y 的元素类型从 boole 型转换为 int 型</span></span><br></pre></td></tr></table></figure>
<h5 id="relu-函数"><a class="markdownIt-Anchor" href="#relu-函数"></a> ReLU 函数</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br></pre></td></tr></table></figure>
<p>输出层的激活函数。</p>
<blockquote>
<p>如何选择输出层的激活函数？</p>
<p>一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数， 多元分类问题可以使用 softmax 函数。</p>
<p>机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题（寻找决策边界）。比如，区分图像中的人是男性还是女性的问题就是分类问题。而回归问题是根据某个输入预测一个（连续的） 数值的问题（找到最优拟合）。</p>
</blockquote>
<h5 id="恒等函数"><a class="markdownIt-Anchor" href="#恒等函数"></a> 恒等函数</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">identity_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h5 id="softmax-函数"><a class="markdownIt-Anchor" href="#softmax-函数"></a> softmax 函数</h5>
<p><code>exp(x)</code> 是表示 e<sup>x</sup> 的指数函数（e 是纳皮尔常数2.7182 …），容易出现很大的值从而导致数值的溢出问题。在进行 softmax 的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果，为了防止溢出，一般会使用输入信号中的最大值。</p>
<p>如上所示，softmax 函数的输出是 0.0 到 1.0 之间的实数。并且，softmax 函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质。正因为有了这个性质，我们才可以把 softmax 函数的输出解释为概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">a</span>):</span><br><span class="line">    exp_a = np.exp(a)</span><br><span class="line">    sum_exp_a = np.<span class="built_in">sum</span>(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"><span class="comment"># 优化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">a</span>):</span><br><span class="line">    c = np.<span class="built_in">max</span>(a)</span><br><span class="line">    exp_a = np.exp(a - c) <span class="comment"># 溢出对策</span></span><br><span class="line">    sum_exp_a = np.<span class="built_in">sum</span>(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="简单实现"><a class="markdownIt-Anchor" href="#简单实现"></a> 简单实现</h4>
<p>了解了激活函数，让我们来看一个简单的神经网络。这个神经网络省略了偏置和激活函数，只有权重。</p>
<p>我们需要了解的重点是：神经网络的运算可以作为矩阵运算打包进行。</p>
<blockquote>
<p>隐藏（中间）层的激活函数表示为：<code>h()</code></p>
<p>输出层的激活函数表示为：<code>σ()</code></p>
</blockquote>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/NeuralNet02.png" style="zoom:50%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">X.shape <span class="comment"># (2,)</span></span><br><span class="line">W = np.array([[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(W)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1 3 5]</span></span><br><span class="line"><span class="string"> [2 4 6]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">W.shape <span class="comment"># (2, 3)</span></span><br><span class="line">Y = np.dot(X, W)</span><br><span class="line"><span class="built_in">print</span>(Y) <span class="comment"># [ 5 11 17]</span></span><br></pre></td></tr></table></figure>
<p>如上所示，使用 <code>np.dot</code> （多维数组的点积），可以一次性计算出 Y 的结果。 这意味着，即便 Y 的元素个数为 100 或 1000，也可以通过一次运算就计算出结果！如果不使用 <code>np.dot</code>，就必须单独计算 Y 的每一个元素（或者说必须使用 for 语句），非常麻烦。因此，通过矩阵的乘积一次性完成计算的技巧，在实现的层面上可以说是非常重要的。</p>
<p>现在我们将偏置还有激活函数考虑进来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 第一层处理</span></span><br><span class="line"><span class="comment"># 设置信号、权重、偏置为某一任意值</span></span><br><span class="line">X = np.array([<span class="number">1.0</span>, <span class="number">0.5</span>])</span><br><span class="line">W1 = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>]])</span><br><span class="line">B1 = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line"><span class="built_in">print</span>(W1.shape) <span class="comment"># (2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(X.shape) <span class="comment"># (2,)</span></span><br><span class="line"><span class="built_in">print</span>(B1.shape) <span class="comment"># (3,)</span></span><br><span class="line">A1 = np.dot(X, W1) + B1 <span class="comment"># 一次得出第一层的所有的 a</span></span><br><span class="line">Z1 = sigmoid(A1)</span><br><span class="line"><span class="built_in">print</span>(A1) <span class="comment"># [0.3, 0.7, 1.1] # 第一层的结果</span></span><br><span class="line"><span class="built_in">print</span>(Z1) <span class="comment"># [0.57444252, 0.66818777, 0.75026011] # sigmoid 函数进行处理</span></span><br><span class="line"><span class="comment">## 第二层处理，与第一层类似</span></span><br><span class="line">W2 = np.array([[<span class="number">0.1</span>, <span class="number">0.4</span>], [<span class="number">0.2</span>, <span class="number">0.5</span>], [<span class="number">0.3</span>, <span class="number">0.6</span>]])</span><br><span class="line">B2 = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line"><span class="built_in">print</span>(Z1.shape) <span class="comment"># (3,)</span></span><br><span class="line"><span class="built_in">print</span>(W2.shape) <span class="comment"># (3, 2)</span></span><br><span class="line"><span class="built_in">print</span>(B2.shape) <span class="comment"># (2,)</span></span><br><span class="line">A2 = np.dot(Z1, W2) + B2</span><br><span class="line">Z2 = sigmoid(A2)</span><br><span class="line"><span class="comment">## 第二层到输出层的处理</span></span><br><span class="line"><span class="comment"># 输出层的激活函数与之前隐藏层的有所不同</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">identity_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x <span class="comment"># 恒等函数</span></span><br><span class="line">W3 = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>]])</span><br><span class="line">B3 = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">A3 = np.dot(Z2, W3) + B3</span><br><span class="line">Y = identity_function(A3) <span class="comment"># 或者 Y = A3，此处按照规范使用一个返回自身的激活函数</span></span><br></pre></td></tr></table></figure>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/NeuralNet03.png" style="zoom:60%;" />
<p>最后我们将本章的代码流程进行整理，方便阅读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 权重及偏置的初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_network</span>():</span><br><span class="line">    network = &#123;&#125; <span class="comment"># 保存所有初始化参数的字典</span></span><br><span class="line">    network[<span class="string">&#x27;W1&#x27;</span>] = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>]])</span><br><span class="line">    network[<span class="string">&#x27;b1&#x27;</span>] = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">    network[<span class="string">&#x27;W2&#x27;</span>] = np.array([[<span class="number">0.1</span>, <span class="number">0.4</span>], [<span class="number">0.2</span>, <span class="number">0.5</span>], [<span class="number">0.3</span>, <span class="number">0.6</span>]])</span><br><span class="line">    network[<span class="string">&#x27;b2&#x27;</span>] = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">    network[<span class="string">&#x27;W3&#x27;</span>] = np.array([[<span class="number">0.1</span>, <span class="number">0.3</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>]])</span><br><span class="line">    network[<span class="string">&#x27;b3&#x27;</span>] = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"><span class="comment">## 封住了将输入信号转换为输出信号的处理过程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">network, x</span>):</span><br><span class="line">    W1, W2, W3 = network[<span class="string">&#x27;W1&#x27;</span>], network[<span class="string">&#x27;W2&#x27;</span>], network[<span class="string">&#x27;W3&#x27;</span>]</span><br><span class="line">    b1, b2, b3 = network[<span class="string">&#x27;b1&#x27;</span>], network[<span class="string">&#x27;b2&#x27;</span>], network[<span class="string">&#x27;b3&#x27;</span>]</span><br><span class="line">    a1 = np.dot(x, W1) + b1</span><br><span class="line">    z1 = sigmoid(a1)</span><br><span class="line">    a2 = np.dot(z1, W2) + b2</span><br><span class="line">    z2 = sigmoid(a2)</span><br><span class="line">    a3 = np.dot(z2, W3) + b3</span><br><span class="line">    y = identity_function(a3)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">network = init_network()</span><br><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">0.5</span>])</span><br><span class="line">y = forward(network, x)</span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># [ 0.31682708 0.69627909]</span></span><br></pre></td></tr></table></figure>
<h4 id="手写数字识别"><a class="markdownIt-Anchor" href="#手写数字识别"></a> 手写数字识别</h4>
<p>介绍完神经网络的结构之后，现在我们来试着解决实际问题。</p>
<p>假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的推理处理。这个推理处理也称为神经网络的前向传播（forward propagation）。</p>
<blockquote>
<p>求解机器学习问题的步骤可以分为学习和推理两个阶段。</p>
<p>首先，在学习阶段进行模型的学习，</p>
<p>然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。</p>
<p>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。 并且，即便使用 softmax 函数，输出值最大的神经元的位置也不会变。因此， 神经网络在进行分类时，输出层的 softmax 函数可以省略。</p>
<p>在输出层使用 softmax 函数是因为它和神经网络的学习有关系。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 让我们先尝试着将数据集 MINST 进行导入</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir) <span class="comment"># 为了导入父目录中的文件而进行的设定</span></span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="comment"># 读入 MNIST 数据集（第一次调用会花费几分钟）</span></span><br><span class="line"><span class="comment"># flatten 将形状压成一维</span></span><br><span class="line"><span class="comment"># normalize 将输入图像正规化为 0.0～1.0 的值</span></span><br><span class="line"><span class="comment"># (训练图像 ,训练标签)，(测试图像，测试标签)</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="literal">True</span>, normalize=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 输出各个数据的形状</span></span><br><span class="line"><span class="built_in">print</span>(x_train.shape) <span class="comment"># (60000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(t_train.shape) <span class="comment"># (60000,)</span></span><br><span class="line"><span class="built_in">print</span>(x_test.shape) <span class="comment"># (10000, 784)</span></span><br><span class="line"><span class="built_in">print</span>(t_test.shape) <span class="comment"># (10000,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 接下来试着使用 PIL（Python Image Library） 模块来显示图片</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">img_show</span>(<span class="params">img</span>): <span class="comment"># 定义显示图片的函数</span></span><br><span class="line">    pil_img = Image.fromarray(np.uint8(img))</span><br><span class="line">    pil_img.show()</span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="literal">True</span>, normalize=<span class="literal">False</span>)</span><br><span class="line">img = x_train[<span class="number">0</span>]</span><br><span class="line">label = t_train[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(label) <span class="comment"># 5</span></span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment"># (784,)</span></span><br><span class="line">img = img.reshape(<span class="number">28</span>, <span class="number">28</span>) <span class="comment"># 把图像的形状变成原来的尺寸</span></span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment"># (28, 28)</span></span><br><span class="line">img_show(img)</span><br></pre></td></tr></table></figure>
<p>好了，到这一步我们已经对这个数据集有一定的了解，接下来我们来思考一下对这个 MINST 数据集实现神经网络的推理处理。</p>
<p>神经网络的输入层有 784 个神经元，输出层有 10 个神经元。输入层的 784 这个数字来源于图像大小的 <code>28 × 28 = 784</code>，输出层的10 这个数字来源于 10 类别分类（数字 0 到 9，共 10 个类别）。此外，这个神经网络有 2 个隐藏层，第 1 个隐藏层有 50 个神经元，第 2 个隐藏层有 100 个神经元。这个 50 和 100 可以设置为任何值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 获取测试数据（测试集与标签集）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_data</span>():</span><br><span class="line">    (x_train, t_train), (x_test, t_test) = \</span><br><span class="line">    load_mnist(normalize=<span class="literal">True</span>, flatten=<span class="literal">True</span>, one_hot_label=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> x_test, t_test</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取权重（数据集自带的）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_network</span>():</span><br><span class="line">    <span class="comment"># Binary Mode 返回 Bytes</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;sample_weight.pkl&quot;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        network = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"><span class="comment">## 预测过程（模型）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">network, x</span>):</span><br><span class="line">    W1, W2, W3 = network[<span class="string">&#x27;W1&#x27;</span>], network[<span class="string">&#x27;W2&#x27;</span>], network[<span class="string">&#x27;W3&#x27;</span>]</span><br><span class="line">    b1, b2, b3 = network[<span class="string">&#x27;b1&#x27;</span>], network[<span class="string">&#x27;b2&#x27;</span>], network[<span class="string">&#x27;b3&#x27;</span>]</span><br><span class="line">    a1 = np.dot(x, W1) + b1</span><br><span class="line">    z1 = sigmoid(a1)</span><br><span class="line">    a2 = np.dot(z1, W2) + b2</span><br><span class="line">    z2 = sigmoid(a2)</span><br><span class="line">    a3 = np.dot(z2, W3) + b3</span><br><span class="line">    y = softmax(a3)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment">## 评价识别精度</span></span><br><span class="line">x, t = get_data()</span><br><span class="line">network = init_network()</span><br><span class="line">accuracy_cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    y = predict(network, x[i])</span><br><span class="line">    p = np.argmax(y) <span class="comment"># 获取概率最高的元素的索引</span></span><br><span class="line">    <span class="keyword">if</span> p == t[i]:</span><br><span class="line">        accuracy_cnt += <span class="number">1</span> <span class="comment"># 记录正确的次数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span> + <span class="built_in">str</span>(<span class="built_in">float</span>(accuracy_cnt) / <span class="built_in">len</span>(x)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>把数据限定到某个范围内的处理称为正规化（normalization）。此外，对神经网络的输入数据进行某种既定的转换称为预处理（preprocessing）。</p>
<p>很多预处理都会考虑到数据的整体分布。比如，利用数据整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。除此之外，还有将数据整体的分布形状均匀化的方法，即数据白化（whitening）等。</p>
</blockquote>
<h5 id="批处理"><a class="markdownIt-Anchor" href="#批处理"></a> 批处理</h5>
<p>在上面的预测中，我们一次只预测一个输入。其实还有一种更简单快捷的方式：批处理。</p>
<blockquote>
<p>批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。</p>
</blockquote>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/Predict.png" style="zoom: 50%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x, t = get_data()</span><br><span class="line">network = init_network()</span><br><span class="line">batch_size = <span class="number">100</span> <span class="comment"># 批数量</span></span><br><span class="line">accuracy_cnt = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x), batch_size): [<span class="number">0</span>, <span class="number">100</span>, <span class="number">200</span> ...]</span><br><span class="line">    x_batch = x[i:i+batch_size] <span class="comment"># 切片 [0, 100) [100, 200)</span></span><br><span class="line">    y_batch = predict(network, x_batch)</span><br><span class="line">    p = np.argmax(y_batch, axis=<span class="number">1</span>) <span class="comment"># 取得所有第一个维度上的最大值</span></span><br><span class="line">    accuracy_cnt += np.<span class="built_in">sum</span>(p == t[i:i+batch_size]) <span class="comment"># 批量判断是否准确并累加</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy:&quot;</span> + <span class="built_in">str</span>(<span class="built_in">float</span>(accuracy_cnt) / <span class="built_in">len</span>(x)))</span><br></pre></td></tr></table></figure>
<h3 id="神经网络的学习"><a class="markdownIt-Anchor" href="#神经网络的学习"></a> 神经网络的学习</h3>
<p>上一章我们了解神经网络的推测过程，在推测的过程中我们使用到了权重与偏置参数，这些参数在上一章中并没有提及是如何得出的。</p>
<p>而神经网络的学习，其学习过程便是指从训练数据中自动获取最优权重参数的过程。</p>
<p>本章中，为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。为了找出尽可能小的损失函数的值，本章我们将介绍利用了函数斜率的梯度法。</p>
<h4 id="从数据中学习"><a class="markdownIt-Anchor" href="#从数据中学习"></a> 从数据中学习</h4>
<p>在实际的神经网络中，参数的数量可能是成千上万个，在层数更深的深度学习中，参数的数量甚至可能上亿，想要人工决定这些参数的值是不可能的。</p>
<p>所以，我们要完成一件非常了不起的事情：<strong>数据自动决定权重参数的值。</strong></p>
<p>一般来讲，需要在数据中提取特征量，这里所说的特征量是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。特征的量的提取可以分为人工提取（机器学习）与机器提取（深度学习）两大类。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/ManualToAuto.png" style="zoom:67%;" />
<p>机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。</p>
<p>为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的<strong>泛化</strong>能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。</p>
<p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。顺便说一下，只对某个数据集过度拟合的状态称为过拟合（Over Fitting）。避免过拟合也是机器学习的一个重要课题。</p>
<h4 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h4>
<p>损失函数是表示神经网络性能的恶劣程度的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。</p>
<h5 id="均方误差"><a class="markdownIt-Anchor" href="#均方误差"></a> 均方误差</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>((y-t)**<span class="number">2</span>) <span class="comment"># y 代表神经网络输出 t 代表监督数据</span></span><br><span class="line"><span class="comment"># 设 2 为正确解</span></span><br><span class="line">t = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 2 的概率最高的情况（0.6）</span></span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">mean_squared_error(np.array(y), np.array(t)) <span class="comment"># 0.097500000000000031</span></span><br><span class="line"><span class="comment"># 7 的概率最高的情况（0.6）</span></span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">mean_squared_error(np.array(y), np.array(t)) <span class="comment"># 0.59750000000000003</span></span><br></pre></td></tr></table></figure>
<h5 id="交叉熵误差"><a class="markdownIt-Anchor" href="#交叉熵误差"></a> 交叉熵误差</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    delta = <span class="number">1e-7</span> <span class="comment"># 微小值保护 np.log(0) 会变为负无限大的 -inf</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta))</span><br><span class="line">t = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">cross_entropy_error(np.array(y), np.array(t))</span><br><span class="line"><span class="number">0.51082545709933802</span></span><br><span class="line">y = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">cross_entropy_error(np.array(y), np.array(t))</span><br><span class="line"><span class="number">2.3025840929945458</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## mini-batch 版本实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 用 batch 的个数进行正规化，计算单个数据的平均交叉熵误差</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + <span class="number">1e-7</span>)) / batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 监督数据是标签形式，即非 one-hot</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">y, t</span>):</span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># np.arange(batch_size) 生成从 0 到 batch_size-1 的数组</span></span><br><span class="line">    <span class="comment"># y[np.arange(batch_size), t] 生成 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure>
<p>在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。</p>
<h4 id="数值微分"><a class="markdownIt-Anchor" href="#数值微分"></a> 数值微分</h4>
<p>梯度法的核心就是数值微分，在了解梯度法之前我们先来讨论一下数值微分中的导数和偏导数。</p>
<h5 id="导数"><a class="markdownIt-Anchor" href="#导数"></a> 导数</h5>
<p>只存在一个变量的情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_diff</span>(<span class="params">f, x</span>):</span><br><span class="line">    h = <span class="number">1e-4</span> <span class="comment"># 0.0001</span></span><br><span class="line">    <span class="keyword">return</span> (f(x+h) - f(x-h)) / (<span class="number">2</span>*h) <span class="comment"># 中心差分</span></span><br></pre></td></tr></table></figure>
<h5 id="偏导数和梯度"><a class="markdownIt-Anchor" href="#偏导数和梯度"></a> 偏导数和梯度</h5>
<p>存在多个变量。</p>
<p>由全部变量的偏导数汇总而成的向量称为<strong>梯度</strong>（gradient）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">f, x</span>):</span><br><span class="line">    h = <span class="number">1e-4</span> <span class="comment"># 0.0001</span></span><br><span class="line">    grad = np.zeros_like(x) <span class="comment"># 生成和 x 形状相同的数组</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(x.size): <span class="comment"># 每次只对一个变量进行中心差分求导</span></span><br><span class="line">        tmp_val = x[idx]</span><br><span class="line">        <span class="comment"># f(x+h)的计算</span></span><br><span class="line">        x[idx] = tmp_val + h</span><br><span class="line">        fxh1 = f(x)</span><br><span class="line">        <span class="comment"># f(x-h)的计算</span></span><br><span class="line">        x[idx] = tmp_val - h</span><br><span class="line">        fxh2 = f(x)</span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span>*h)</span><br><span class="line">        x[idx] = tmp_val <span class="comment"># 还原值</span></span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>实际上， 梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向。通过巧妙地使用梯度来寻找函数最小值 （或者尽可能小的值）的方法就是梯度法。</p>
<p>在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进， 如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进， 逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。</p>
<p>不断的前进的过程中，我们需要考虑一次前进多少？这就是神经网络的学习中的<strong>学习率</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降法 学习率为 0.01 学习次数为 100 次</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">f, init_x, lr=<span class="number">0.01</span>, step_num=<span class="number">100</span></span>):</span><br><span class="line">    x = init_x</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step_num):</span><br><span class="line">        grad = numerical_gradient(f, x)</span><br><span class="line">        x -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<blockquote>
<p>像学习率这样的由人工设定的参数称为<strong>超参数</strong>。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。</p>
</blockquote>
<h4 id="神经网络的梯度"><a class="markdownIt-Anchor" href="#神经网络的梯度"></a> 神经网络的梯度</h4>
<p>神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/NeuralNetGradient.png" style="zoom:75%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> softmax, cross_entropy_error</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">simpleNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.W = np.random.randn(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 用高斯分布进行初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> np.dot(x, self.W)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        z = self.predict(x)</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        loss = cross_entropy_error(y, t) <span class="comment"># 交叉熵误差</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 求这个简单神经网络的损失</span></span><br><span class="line">net = simpleNet()</span><br><span class="line"><span class="built_in">print</span>(net.W) <span class="comment"># 权重参数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 0.47355232 0.9977393 0.84668094],</span></span><br><span class="line"><span class="string"> [ 0.85557411 0.03563661 0.69422093]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">x = np.array([<span class="number">0.6</span>, <span class="number">0.9</span>])</span><br><span class="line">p = net.predict(x)</span><br><span class="line"><span class="built_in">print</span>(p) <span class="comment"># [ 1.05414809 0.63071653 1.1328074]</span></span><br><span class="line">np.argmax(p) <span class="comment"># 最大值的索引</span></span><br><span class="line">t = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># 正确解标签</span></span><br><span class="line">net.loss(x, t) <span class="comment"># 0.92806853663411326</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 求梯度</span></span><br><span class="line">f = <span class="keyword">lambda</span> w: net.loss(x, t) <span class="comment"># 此处传参 w 为伪参数，求梯度中会进行传参</span></span><br><span class="line">dW = numerical_gradient(f, net.W)</span><br><span class="line"><span class="built_in">print</span>(dW)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 0.21924763 0.14356247 -0.36281009]</span></span><br><span class="line"><span class="string"> [ 0.32887144 0.2153437 -0.54421514]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>到目前为止，我们已经得到了这个神经网络的损失函数所对应的梯度了，让我们来观察一下这个梯度矩阵。首先，形状上梯度矩阵与权重矩阵的形状相同，并且应该是对应的关系：w<sub>(2,3)</sub> 对应的梯度大约为 -0.5，这意味着这个方向上的权重每增加 h，损失函数的值将减小 0.5*h</p>
<p>求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可。</p>
<h4 id="学习算法的实现"><a class="markdownIt-Anchor" href="#学习算法的实现"></a> 学习算法的实现</h4>
<p>步骤 1（mini-batch）：</p>
<p>从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小 mini-batch 的损失函数的值。</p>
<p>步骤 2（计算梯度）：</p>
<p>为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。 梯度表示损失函数的值减小最多的方向。</p>
<p>步骤 3（更新参数）：</p>
<p>将权重参数沿梯度方向进行微小更新。</p>
<p>步骤 4（重复）：</p>
<p>重复步骤1、步骤2、步骤3。</p>
<blockquote>
<p>因为这里使用的数据是随机选择的 mini batch 数据，所以又称为随机梯度下降法 SGD（Stochastic Gradient Descent）。</p>
</blockquote>
<p>下面，我们来实现手写数字识别的神经网络。这里以 2 层神经网络（隐藏层为 1 层的网络）为对象，使用 MNIST 数据集进行学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 两层神经网络的类</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">from</span> common.functions <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, weight_init_std=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># 初始化权重</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        W1, W2 = self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">        b1, b2 = self.params[<span class="string">&#x27;b1&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>]</span><br><span class="line">        a1 = np.dot(x, W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1, W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> cross_entropy_error(y, t)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W2&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>mini-batch 的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, one_hot_laobel = <span class="literal">True</span>)</span><br><span class="line">train_loss_list = []</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">iters_num = <span class="number">10000</span> <span class="comment"># 循环次数</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    <span class="comment"># 获取 mini-batch，随机获取 100 个</span></span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    grad = network.numerical_gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment"># grad = network.gradient(x_batch, t_batch) # 高速版!</span></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">        <span class="comment"># 记录学习过程</span></span><br><span class="line">        loss = network.loss(x_batch, t_batch)</span><br><span class="line">        train_loss_list.append(loss)</span><br></pre></td></tr></table></figure>
<p>这里给出的算法记录了每一次的损失，但其实这是没有必要的，我们其实只要对一次 epoch 做一次评价就可以。</p>
<blockquote>
<p>epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被（宏观上）使用过一次时的更新次数。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line">(x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=<span class="literal">True</span>, one_hot_</span><br><span class="line">                                                    laobel = <span class="literal">True</span>)</span><br><span class="line">train_loss_list = []</span><br><span class="line">train_acc_list = []</span><br><span class="line">test_acc_list = []</span><br><span class="line"><span class="comment"># 平均每个 epoch 的重复次数</span></span><br><span class="line">iter_per_epoch = <span class="built_in">max</span>(train_size / batch_size, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">iters_num = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    <span class="comment"># 获取 mini-batch</span></span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    grad = network.numerical_gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment"># grad = network.gradient(x_batch, t_batch) # 高速版!</span></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">        loss = network.loss(x_batch, t_batch)</span><br><span class="line">        train_loss_list.append(loss)</span><br><span class="line">        <span class="comment"># 计算每个 epoch 的识别精度</span></span><br><span class="line">        <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">            train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">            test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">            train_acc_list.append(train_acc)</span><br><span class="line">            test_acc_list.append(test_acc)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;train acc, test acc | &quot;</span> + <span class="built_in">str</span>(train_acc) + <span class="string">&quot;, &quot;</span> + <span class="built_in">str</span>(test_acc))</span><br></pre></td></tr></table></figure>
<h3 id="误差反向传播"><a class="markdownIt-Anchor" href="#误差反向传播"></a> 误差反向传播</h3>
<p>在前面的章节中，我们用数值微分计算了神经网络的权重参数的梯度。数值微分简单易实现，但在计算上是比较耗费时间的。本章我们将学习一个高效计算权重参数的梯度方法：误差反向传播。</p>
<p>首先我们通过图片来理解这一方法。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815133610758.png" alt="image-20200815133610758" /></p>
<p>上图表示的是基于反向传播的导数传递。</p>
<p>让我们来看看图中的各数值代表什么。在这个例子中，反向传播从右向左传递导数的值（1 👉 1.1 👉2.2）。这表示支付金额关于苹果价格的导数的值是 2.2，即苹果每上涨 ð 元，最终的支付金额会增加 2.2ð  元。</p>
<p>通过观察我们可以发现一个更为通用的法则，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数（链式法则）。</p>
<blockquote>
<p>反向传播的节点除了 <code>x</code> 还有 <code>+</code>。加法节点的变化在传播的过程中不对上一层的链式传播数值产生影响。</p>
</blockquote>
<p>来看一个比较复杂的例子。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815134405670.png" alt="image-20200815134405670" /></p>
<blockquote>
<p>苹果个数对最终价格的影响为：110。</p>
<p>推导过程：从右到左 <code>715 = 650 x 1.1</code> 这里得到影响因子 1.1 👉 加法节点直接继承上一层的影响因子 1.1 👉 <code>200 = 2 x 100</code> 这里得到影响因子 <code>1.1 x 100 = 110</code>。</p>
</blockquote>
<h4 id="代码实现"><a class="markdownIt-Anchor" href="#代码实现"></a> 代码实现</h4>
<p>在这里我们用 Python 实现上述的例子。乘法节点为乘法层（MulLayer），加法节点称为加法层（AddLayer）。</p>
<h5 id="乘法层"><a class="markdownIt-Anchor" href="#乘法层"></a> 乘法层</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MulLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 正向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        out = x * y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="comment"># 反向传播 dout 为上游传来的导数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * self.y <span class="comment"># 翻转 x 和 y</span></span><br><span class="line">        dy = dout * self.x</span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>
<p>那我们买苹果的计算图的正向传播可以用以下代码表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apple = <span class="number">100</span></span><br><span class="line">apple_num = <span class="number">2</span></span><br><span class="line">tax = <span class="number">1.1</span></span><br><span class="line"><span class="comment"># layer</span></span><br><span class="line">mul_apple_layer = MulLayer()</span><br><span class="line">mul_tax_layer = MulLayer()</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">apple_price = mul_apple_layer.forward(apple, apple_num)</span><br><span class="line">price = mul_tax_layer.forward(apple_price, tax)</span><br><span class="line"><span class="built_in">print</span>(price) <span class="comment"># 220</span></span><br></pre></td></tr></table></figure>
<p>关于各个变量的导数可由 <code>backward()</code> 求出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backward</span></span><br><span class="line">dprice = <span class="number">1</span></span><br><span class="line">dapple_price, dtax = mul_tax_layer.backward(dprice)</span><br><span class="line">dapple, dapple_num = mul_apple_layer.backward(dapple_price)</span><br><span class="line"><span class="built_in">print</span>(dapple, dapple_num, dtax) <span class="comment"># 2.2 110 200</span></span><br></pre></td></tr></table></figure>
<h5 id="加法层"><a class="markdownIt-Anchor" href="#加法层"></a> 加法层</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        out = x + y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * <span class="number">1</span></span><br><span class="line">        dy = dout * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>
<p>我们来用 Python 代码实现一下买苹果和橘子的计算图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apple = <span class="number">100</span></span><br><span class="line">apple_num = <span class="number">2</span></span><br><span class="line">orange = <span class="number">150</span></span><br><span class="line">orange_num = <span class="number">3</span></span><br><span class="line">tax = <span class="number">1.1</span></span><br><span class="line"><span class="comment"># layer</span></span><br><span class="line">mul_apple_layer = MulLayer()</span><br><span class="line">mul_orange_layer = MulLayer()</span><br><span class="line">add_apple_orange_layer = AddLayer()</span><br><span class="line">mul_tax_layer = MulLayer()</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">apple_price = mul_apple_layer.forward(apple, apple_num) <span class="comment"># (1)</span></span><br><span class="line">orange_price = mul_orange_layer.forward(orange, orange_num) <span class="comment"># (2)</span></span><br><span class="line">all_price = add_apple_orange_layer.forward(apple_price, orange_price) <span class="comment"># (3)</span></span><br><span class="line">price = mul_tax_layer.forward(all_price, tax) <span class="comment">#( 4)</span></span><br><span class="line"><span class="comment"># backward</span></span><br><span class="line">dprice = <span class="number">1</span></span><br><span class="line">dall_price, dtax = mul_tax_layer.backward(dprice) <span class="comment"># (4)</span></span><br><span class="line">dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) <span class="comment"># (3)</span></span><br><span class="line">dorange, dorange_num = mul_orange_layer.backward(dorange_price) <span class="comment"># (2)</span></span><br><span class="line">dapple, dapple_num = mul_apple_layer.backward(dapple_price) <span class="comment"># (1)</span></span><br><span class="line"><span class="built_in">print</span>(price) <span class="comment"># 715</span></span><br><span class="line"><span class="built_in">print</span>(dapple_num, dapple, dorange, dorange_num, dtax) <span class="comment"># 110 2.2 3.3 165 650</span></span><br></pre></td></tr></table></figure>
<h4 id="激活函数层的实现"><a class="markdownIt-Anchor" href="#激活函数层的实现"></a> 激活函数层的实现</h4>
<p>现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 ReLU 层和 Sigmoid 层。</p>
<h5 id="relu-层"><a class="markdownIt-Anchor" href="#relu-层"></a> ReLU 层</h5>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815142539203.png" alt="image-20200815142539203" style="zoom:50%;" />
<p>如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815143539792.png" alt="image-20200815143539792" style="zoom:50%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Relu</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.mask = (x &lt;= <span class="number">0</span>)</span><br><span class="line">        out = x.copy()</span><br><span class="line">        out[self.mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dout[self.mask] = <span class="number">0</span></span><br><span class="line">        dx = dout</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>ReLU 类有实例变量 mask。这个变量 mask 是由 True / False 构成的 NumPy 数 组，它会把正向传播时的输入 x  的元素中小于等于 0 的地方保存为 True，其他地方（大于 0 的元素）保存为 False。</p>
<p>如果正向传播时的输入值小于等于 0，则反向传播的值为 0。 因此，反向传播中会使用正向传播时保存的 mask，将从上游传来的 dout 的 mask 中的元素为 True 的地方设为 0。</p>
<h5 id="sigmoid-层"><a class="markdownIt-Anchor" href="#sigmoid-层"></a> Sigmoid 层</h5>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815145429545.png" alt="image-20200815145429545" style="zoom:50%;" />
<blockquote>
<p>这里 <code>/</code> 用倒数的 <code>x</code> 计算。</p>
</blockquote>
<p>对结果进行进一步的整理。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815151401193.png" alt="image-20200815151401193" style="zoom:50%;" />
<p>让我们将推导的过程隐蔽，我们可以得到一个 Sigmoid 节点。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815151419201.png" alt="image-20200815151419201" style="zoom:50%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Sigmoid</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.out = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">        self.out = out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = dout * (<span class="number">1.0</span> - self.out) * self.out</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h4 id="affine-softmax-层的实现"><a class="markdownIt-Anchor" href="#affine-softmax-层的实现"></a> Affine / Softmax 层的实现</h4>
<p>在很前面的章节中（忘了赶紧回去给我复习：神经网络 / 简单实现]]），我们利用公式 <code>Y = np.dot(X,W) + B</code> 计算出神经元的加权。然后 <code>Y</code> 经过激活函数转换后，传递给下一层，这就是神经网络正向传播过程。</p>
<blockquote>
<p>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为仿射变换。因此，这里将进行仿射变换的处理实现为 Affine 层。</p>
</blockquote>
<p>让我们来看看将这个过程转换为计算图是什么样子的。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815172530363.png" alt="image-20200815172530363" style="zoom:50%;" />
<p>正向传播时，偏置会被加到每一个数据上。因此，反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。</p>
<blockquote>
<p>Affine 层：神经网络的正向传播中，进行的矩阵的乘积运算，在几何学领域被称为仿射变换。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Affine</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, W, b</span>):</span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.dW = <span class="literal">None</span></span><br><span class="line">        self.db = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        out = np.dot(x, self.W) + self.b</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout</span>):</span><br><span class="line">        dx = np.dot(dout, self.W.T)</span><br><span class="line">        self.dW = np.dot(self.x.T, dout)</span><br><span class="line">        self.db = np.<span class="built_in">sum</span>(dout, axis=<span class="number">0</span>) <span class="comment"># 公式 3 把 b 当成</span></span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h4 id="softmax-with-loss-层"><a class="markdownIt-Anchor" href="#softmax-with-loss-层"></a> Softmax-with-Loss 层</h4>
<p>在神经网络的学习中，我们一般需要将输出正规化后输入损失函数进行处理，一般会使用 Softmax 层；当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。</p>
<p>Softmax-with-Loss：这里的 Loss 指交交叉熵误差层 Cross Entropy Error。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815224431801.png" alt="image-20200815224431801" /></p>
<p>这图看到人就傻了，让我们来简化一下。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200815233559503.png" alt="image-20200815233559503" style="zoom: 50%;" />
<p>让我们来看看各个字母所代表的含义。<code>（y1, y2, y3）</code> 是 Softmax 层的输出，<code>（t1, t2, t3）</code> 是监督数据，所以 <code>（y1 − t1, y2 − t2, y3 − t3）</code> 是 Softmax 层的输出和教师标签的差分。所以 <code>（y1 − t1, y2 − t2, y3 − t3）</code> 是 Softmax 层的输出和教师标签的差分（即输出与标签的误差）。</p>
<blockquote>
<p>使用交叉熵误差作为 softmax 函数的损失函数后，反向传播得到 <code>（y1 − t1, y2 − t2, y3 − t3）</code> 这样漂亮的结果。实际上，这样漂亮的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。</p>
<p>回归问题中输出层使用恒等函数，损失函数使用平方和误差，也是出于同样的理由（忘了就给我翻回去复习！！！）。也就是说，使用平方和误差作为恒等函数的损失函数，反向传播才能得到 <code>（y1 − t1, y2 − t2, y3 − t3）</code> 这样漂亮的结果。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SoftmaxWithLoss</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.loss = <span class="literal">None</span> <span class="comment"># 损失</span></span><br><span class="line">        self.y = <span class="literal">None</span> <span class="comment"># softmax 的输出</span></span><br><span class="line">        self.t = <span class="literal">None</span> <span class="comment"># 监督数据（one-hot vector）</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        self.t = t</span><br><span class="line">        self.y = softmax(x)</span><br><span class="line">        self.loss = cross_entropy_error(self.y, self.t)</span><br><span class="line">        <span class="keyword">return</span> self.loss</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, dout=<span class="number">1</span></span>):</span><br><span class="line">        batch_size = self.t.shape[<span class="number">0</span>]</span><br><span class="line">        dx = (self.y - self.t) / batch_size</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h4 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h4>
<p>现在来进行神经网络的实现。这里我们要把 2 层神经网络实现为 TwoLayerNet。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> common.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> common.gradient <span class="keyword">import</span> numerical_gradient</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>:</span><br><span class="line">    <span class="comment"># 输入层的神经元数、隐藏层的神经元数、输出层的神经元数、初始化权重时的高斯分布的规模</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size, weight_init_std=<span class="number">0.01</span></span>):</span><br><span class="line">        <span class="comment"># 初始化权重 params 为参数字典</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">&#x27;W1&#x27;</span>] = weight_init_std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">&#x27;W2&#x27;</span>] = weight_init_std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">&#x27;b2&#x27;</span>] = np.zeros(output_size)</span><br><span class="line">        <span class="comment"># 生成层 layers 有序字典型变量 保存神经网络的层</span></span><br><span class="line">        self.layers = OrderedDict()</span><br><span class="line">        self.layers[<span class="string">&#x27;Affine1&#x27;</span>] = Affine(self.params[<span class="string">&#x27;W1&#x27;</span>], self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        self.layers[<span class="string">&#x27;Relu1&#x27;</span>] = Relu()</span><br><span class="line">        self.layers[<span class="string">&#x27;Affine2&#x27;</span>] = Affine(self.params[<span class="string">&#x27;W2&#x27;</span>], self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="comment"># 神经网络的最后一层</span></span><br><span class="line">        self.lastLayer = SoftmaxWithLoss()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers.values():</span><br><span class="line">            x = layer.forward(x)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> self.lastLayer.forward(y, t)</span><br><span class="line">    <span class="comment"># 计算识别精度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> t.ndim != <span class="number">1</span> : t = np.argmax(t, axis=<span class="number">1</span>)</span><br><span class="line">        accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">    <span class="comment"># x:输入数据, t:监督数据 通过数值微分计算关于权重的梯度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">numerical_gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b1&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;W2&#x27;</span>])</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, self.params[<span class="string">&#x27;b2&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line">    <span class="comment"># 通过误差反向传播计算关于权重参数的梯度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, x, t</span>):</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        self.loss(x, t)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dout = <span class="number">1</span></span><br><span class="line">        dout = self.lastLayer.backward(dout)</span><br><span class="line">        layers = <span class="built_in">list</span>(self.layers.values())</span><br><span class="line">        layers.reverse()</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">            dout = layer.backward(dout)</span><br><span class="line">        <span class="comment"># 设定</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        grads[<span class="string">&#x27;W1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].db</span><br><span class="line">        grads[<span class="string">&#x27;W2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].dW</span><br><span class="line">        grads[<span class="string">&#x27;b2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].db</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，将神经网络的层保存在 <code>OrderedDict</code> 这一点非常重要（正向与反向都需要按顺序调用各层）。</p>
</blockquote>
<p>可能在座的各位会有一个疑问：既然我们有了计算较为简单的误差反向传播，为什么还要由数值微分的方法呢？误差反向传播的计算虽然简单，但是实现较为复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度确认</span></span><br><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="literal">True</span>, one_hot_label = <span class="literal">True</span>)</span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line">x_batch = x_train[:<span class="number">3</span>]</span><br><span class="line">t_batch = t_train[:<span class="number">3</span>]</span><br><span class="line">grad_numerical = network.numerical_gradient(x_batch, t_batch)</span><br><span class="line">grad_backprop = network.gradient(x_batch, t_batch)</span><br><span class="line"><span class="comment"># 求各个权重的绝对误差的平均值</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> grad_numerical.keys():</span><br><span class="line">    diff = np.average( np.<span class="built_in">abs</span>(grad_backprop[key] - grad_numerical[key]) )</span><br><span class="line">    <span class="built_in">print</span>(key + <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(diff))</span><br></pre></td></tr></table></figure>
<p>最后，我们利用反向传播误差来实现一下神经网络的学习（就是把前面的数值微分替换成反向误差传播）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> two_layer_net <span class="keyword">import</span> TwoLayerNet</span><br><span class="line"><span class="comment"># 读入数据</span></span><br><span class="line">(x_train, t_train), (x_test, t_test) = \</span><br><span class="line">load_mnist(normalize=<span class="literal">True</span>, one_hot_label=<span class="literal">True</span>)</span><br><span class="line">network = TwoLayerNet(input_size=<span class="number">784</span>, hidden_size=<span class="number">50</span>, output_size=<span class="number">10</span>)</span><br><span class="line">iters_num = <span class="number">10000</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">train_loss_list = []</span><br><span class="line">train_acc_list = []</span><br><span class="line">test_acc_list = []</span><br><span class="line">iter_per_epoch = <span class="built_in">max</span>(train_size / batch_size, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    <span class="comment"># 通过误差反向传播法求梯度</span></span><br><span class="line">    grad = network.gradient(x_batch, t_batch)</span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">&#x27;W1&#x27;</span>, <span class="string">&#x27;b1&#x27;</span>, <span class="string">&#x27;W2&#x27;</span>, <span class="string">&#x27;b2&#x27;</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(train_acc, test_acc)</span><br></pre></td></tr></table></figure>
<h3 id="与学习相关的技巧"><a class="markdownIt-Anchor" href="#与学习相关的技巧"></a> 与学习相关的技巧</h3>
<p>本章将介绍一下神经网络学习中的一些重要观点，让我们可以更高效地进行神经网络地学习，提高识别精度。</p>
<h4 id="参数更新"><a class="markdownIt-Anchor" href="#参数更新"></a> 参数更新</h4>
<p>我们已经知道，神经网络的学习目的就是找到使损失函数的尽可能小的参数，这个寻找最优参数的问题称为最优化。随机梯度下降法（stochastic gradient descent）， 简称 SGD 就是一个简单最优化的方法。但是，根据不同的问题，也存在比 SGD 更加聪明的方法。接下来我们会介绍这些方法。</p>
<h5 id="sgd"><a class="markdownIt-Anchor" href="#sgd"></a> SGD</h5>
<p>先来看看 SGD 吧。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816114932375.png" alt="image-20200816114932375" style="zoom:50%;" />
<p>SGD 可以简单的用上面的图表示（右边的式子更新左边的式子）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SGD</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, lr=<span class="number">0.01</span></span>):</span><br><span class="line">        self.lr = lr</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, params, grads</span>):</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</span><br><span class="line">            params[key] -= self.lr * grads[key]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = SGD()</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">optimizer.update(params, grads)</span><br></pre></td></tr></table></figure>
<h5 id="momentum"><a class="markdownIt-Anchor" href="#momentum"></a> Momentum</h5>
<p>Momentum 是动量的意思，和物理有关。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816122708920.png" alt="image-20200816122708920" style="zoom:50%;" />
<p><code>v</code> 对应物理上的速度。<code>αv</code> 这一项表示在物体不受任何力时，该项承担使物体逐渐减速的任务（<code>α</code> 设定为 0.9 之类的值），对应物理上的地面摩擦或空气阻力。</p>
<p>对这两个公式的大致理解：小的叠加，大的抵消（减少震荡）。</p>
<p><img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816123300153.png" alt="image-20200816123300153" /></p>
<h5 id="adagrad"><a class="markdownIt-Anchor" href="#adagrad"></a> AdaGrad</h5>
<p>前面提到的学习过程，其学习率是固定的，有些优化方式是动态的调节学习率，比如：学习率衰减。逐渐减小学习率的想法，相当于将全体参数的学习率值一起降低。 而 AdaGrad 进一步发展了这个想法，针对一个一个的参数，赋予其定制的值。 AdaGrad 会为参数的每个元素适当地调整学习率，与此同时进行学习。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816142103382.png" alt="image-20200816142103382" style="zoom:50%;" />
<p><code>h</code> 保存了以前所有梯度的平方和，我们用这个参数来动态的调整梯度的变化程度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AdaGrad</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, lr=<span class="number">0.01</span></span>):</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.h = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, params, grads</span>):</span><br><span class="line">        <span class="keyword">if</span> self.h <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.h = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> key, val <span class="keyword">in</span> params.items():</span><br><span class="line">            self.h[key] = np.zeros_like(val)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> params.keys():</span><br><span class="line">        self.h[key] += grads[key] * grads[key]</span><br><span class="line">        params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<h4 id="权重初始值"><a class="markdownIt-Anchor" href="#权重初始值"></a> 权重初始值</h4>
<p>权重的初始值也决定了这个神经网络的学习是否可以成功，让我们来看看权重初始值的一些推荐值。</p>
<p>然我们先来考虑一下权值应该越大越好？还是越小越好？若权值越大，这个模型的泛化能力就差，所以权值应该是一个较小的数，在这之前的权重初始值都是像 <code>0.01 * np.random.randn(10, 100)</code> 这样，使用由高斯分布生成的值乘以 0.01 后得到的值（标准差为0.01的高斯分布）。</p>
<p>那初始权值可以设为 0 吗？答案是不可的，若初始权重都为 0，这使得神经网络拥有许多不同的权重的意义丧失了。为了防止权重均一化（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。</p>
<blockquote>
<p>各层的激活值的分布都要求有适当的广度。通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者表现力受限的问题，导致学习可能无法顺利进行。</p>
</blockquote>
<p>现在，在一般的深度学习框架中，Xavier 初始值已被作为标准使用。Xavier 初始值是以激活函数是线性函数为前提而推导出来的。因为 sigmoid 函数和 tanh 函数左右对称，且中央附近可以视作线性函数，所以适合使用 Xavier 初始值。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816153901765.png" alt="image-20200816153901765" style="zoom: 67%;" />
<p>其实，权重初始值的设定与激活函数也有一定的关系。当激活函数使用 ReLU 时，一般推荐使用 ReLU 专用的初始值 He 初始值。</p>
<img src="http://welab-wingo.gitee.io/image/2020/08/DL/image-20200816164002361.png" alt="image-20200816164002361" style="zoom: 50%;" />
<h4 id="batch-normalization"><a class="markdownIt-Anchor" href="#batch-normalization"></a> Batch Normalization</h4>
<p>前面讲到我们需要在各层拥有适当的广度，这样才可以使学习顺利进行。那么，为了使各层拥有适当的广度，强制性地调整激活值的分布会怎样呢？Batch Normalization 就是基于这种想法产生的。</p>
<h4 id="正则化"><a class="markdownIt-Anchor" href="#正则化"></a> 正则化</h4>
<h5 id="过拟合"><a class="markdownIt-Anchor" href="#过拟合"></a> 过拟合</h5>
<p>学习后生成的模型泛化能力弱：对样本数据识别良好，对测试数据识别很差。</p>
<h5 id="权值衰退"><a class="markdownIt-Anchor" href="#权值衰退"></a> 权值衰退</h5>
<p>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</p>
<h5 id="dropout"><a class="markdownIt-Anchor" href="#dropout"></a> Dropout</h5>
<p>作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的 L2 范数的权值衰减方法。该方法可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情 况下，我们经常会使用 Dropout 方法。</p>
<p>Dropout 是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。</p>
<p>可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习，进而达到集成学习的效果。</p>
<h4 id="超参数的验证"><a class="markdownIt-Anchor" href="#超参数的验证"></a> 超参数的验证</h4>
<p>神经网络中，除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。超参数的决定过程中常常伴随着很多试错。</p>
<p>因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。我们使用这个验证数据来评估超参数的好坏。</p>
<h3 id="卷积神经网络"><a class="markdownIt-Anchor" href="#卷积神经网络"></a> 卷积神经网络</h3>
<p>卷积神经网络（Convolutional Neural Network，CNN）常被用于图像识别、语音识别等场合，在图像识别的比赛中，基于深度学习的方法几乎都以 CNN 为基础。本章就让我们来一睹 CNN 的 风采。</p>
<h4 id="整体结构"><a class="markdownIt-Anchor" href="#整体结构"></a> 整体结构</h4>
<p>先让我们大致了解一下 CNN 的框架，其实它和神经网络一样都是通过组装层来构建的。不过，在 CNN 中还出现了新的层：卷积层（Convolution 层）和池化层（Pooling 层）。</p>
<p>CNN 中出现了一些特有的术语，比如填充、步幅等。此外，各层中传递的数据是有形状的数据（比如，3 维数据），这与之前的全连接网络不同，因此刚开始学习 CNN 时可能会感到难以理解。</p>
<h4 id="卷积层"><a class="markdownIt-Anchor" href="#卷积层"></a> 卷积层</h4>
<p>前面的神经网络中我们使用了全连接层（Affine 层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。但是，全连接层牺牲掉了数据的形状。。比如，输 入数据是图像时，图像通常是高、长、通道方向上的 3 维形状。但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。</p>
<p>3 维的形状，其中可能包含由重要的空间信息。比如，空间上邻近的像素为相似的值、RBG 的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3 维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。</p>
<p>而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以 3 维数据的形式接收输入数据，并同样以 3 维数据的形式输出至下一层。因此， 在 CNN 中，可以（有可能）正确理解图像等具有形状的数据。</p>
<p>另外，CNN 中，有时将卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。</p>
<h5 id="卷积运算"><a class="markdownIt-Anchor" href="#卷积运算"></a> 卷积运算</h5>
<p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的滤波器运算。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817203500281.png" alt="image-20200817203500281" /></p>
<blockquote>
<p>将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。</p>
</blockquote>
<h5 id="填充"><a class="markdownIt-Anchor" href="#填充"></a> 填充</h5>
<p>在卷积运算中，我们经常会向输入数据的周围填入固定的数据（比如 0 等），这成为填充。填充通常用于保持数据在卷积运算后仍然保持空间大小不变的将数据传给下一层。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817204419693.png" alt="image-20200817204419693" /></p>
<h5 id="步幅"><a class="markdownIt-Anchor" href="#步幅"></a> 步幅</h5>
<p>应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是 1，如果将步幅设为 2，则情况如下图所示。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817204619420.png" alt="image-20200817204619420" /></p>
<p>综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。这里，假设输入大小为 <code>(H, W)</code>，滤波器大小为 <code>(FH, FW)</code>，输出大小为 <code>(OH, OW)</code>，填充为 <code>P</code>，步幅为 <code>S</code>。可以得到如下关系。</p>
<img src="C:\Users\wingo\Pictures\Screenshot\深度学习入门\image-20200817204809164.png" alt="image-20200817204809164" style="zoom:50%;" />
<h5 id="三维数据的卷积计算"><a class="markdownIt-Anchor" href="#三维数据的卷积计算"></a> 三维数据的卷积计算</h5>
<p>3 维数据和 2 维数据时相比，可以发现纵深方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817205614567.png" alt="image-20200817205614567" /></p>
<p>需要注意的是，在 3 维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。在这个例子中，输入数据和滤波器的通道数一致，均为 3。 滤波器大小可以设定为任意值（不过，每个通道的滤波器大小要全部相同）。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817210738055.png" alt="image-20200817210738055" /></p>
<p>通过应用 <code>FN</code> 个滤波器，输出特征图也生成了 <code>FN</code> 个。如果将这 <code>FN</code> 个特征图汇集在一起，就得到了形状为 <code>(FN, OH, OW)</code> 的方块。将这个方块传给下一层，就是 CNN 的处理流。</p>
<p>这里需要注意的是，作为 4 维数据，滤波器的权重数据要按 <code>(output_channel, input_ channel, height, width)</code> 的顺序书写。比如，通道数为 3、大小为 <code>5 × 5</code> 的滤波器有 20 个时，可以写成 <code>(20, 3, 5, 5)</code>。</p>
<p>3 维的卷积运算也存在偏置项，让我们来看看添加偏置的情况。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817211245102.png" alt="image-20200817211245102" /></p>
<h5 id="批处理-2"><a class="markdownIt-Anchor" href="#批处理-2"></a> 批处理</h5>
<p>让我们来看看 3 维卷积计算的批处理示意图。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817211354403.png" alt="image-20200817211354403" /></p>
<h4 id="池化层"><a class="markdownIt-Anchor" href="#池化层"></a> 池化层</h4>
<p>池化是缩小高、长方向上的空间的运算。比如：Max 池化如下图所示。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817224332415.png" alt="image-20200817224332415" /></p>
<p>除了 Max 池化之外，还有 Average 池化等。相对于 Max 池化是从目标区域中取出最大值，Average 池化则是计算目标区域的平均值。在图像识别领域，主要使用 Max 池化（默认）。</p>
<h4 id="代码实现-2"><a class="markdownIt-Anchor" href="#代码实现-2"></a> 代码实现</h4>
<p>在实现具体的代码之前，让我们来看看计算的过程的转化图示。</p>
<p><img src="C:%5CUsers%5Cwingo%5CPictures%5CScreenshot%5C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%5Cimage-20200817231530172.png" alt="image-20200817231530172" /></p>
<p><code>im2col</code> 会考虑滤波器大小、步幅、填充，将输入数据展开为 2 维数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">from</span> common.util <span class="keyword">import</span> im2col</span><br><span class="line">x1 = np.random.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line"><span class="comment">## im2col (input_data, filter_h, filter_w, stride=1, pad=0)</span></span><br><span class="line"><span class="comment"># input_data 由（数据量，通道，高，长）的 4 维数组构成的输入数据</span></span><br><span class="line"><span class="comment"># filter_h 滤波器的高 filter_w 滤波器的长</span></span><br><span class="line"><span class="comment"># stride 步幅 pad 填充</span></span><br><span class="line">col1 = im2col(x1, <span class="number">5</span>, <span class="number">5</span>, stride=<span class="number">1</span>, pad=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># batch = 1</span></span><br><span class="line"><span class="built_in">print</span>(col1.shape) <span class="comment"># (9, 75)</span></span><br><span class="line">x2 = np.random.rand(<span class="number">10</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line"><span class="comment"># batch = 10</span></span><br><span class="line">col2 = im2col(x2, <span class="number">5</span>, <span class="number">5</span>, stride=<span class="number">1</span>, pad=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(col2.shape) <span class="comment"># (90, 75)</span></span><br></pre></td></tr></table></figure>
<p>了解完 <code>im2col</code>，让我们利用它来实现卷积层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Convolution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, W, b, stride=<span class="number">1</span>, pad=<span class="number">0</span></span>):</span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.pad = pad</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        FN, C, FH, FW = self.W.shape</span><br><span class="line">        N, C, H, W = x.shape</span><br><span class="line">        out_h = <span class="built_in">int</span>(<span class="number">1</span> + (H + <span class="number">2</span>*self.pad - FH) / self.stride)</span><br><span class="line">        out_w = <span class="built_in">int</span>(<span class="number">1</span> + (W + <span class="number">2</span>*self.pad - FW) / self.stride)</span><br><span class="line">        </span><br><span class="line">        col = im2col(x, FH, FW, self.stride, self.pad)</span><br><span class="line">        col_W = self.W.reshape(FN, -<span class="number">1</span>).T <span class="comment"># 滤波器的展开 (10, 3, 5, 5) 👉 (10, 75)</span></span><br><span class="line">        out = np.dot(col, col_W) + self.b</span><br><span class="line">        </span><br><span class="line">        out = out.reshape(N, out_h, out_w, -<span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

      
    </div>
    <!-- <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://wingowen.github.io/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" title="深度学习入门" target="_blank" rel="external">https://wingowen.github.io/2020/08/08/机器学习/深度学习入门/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">WINGO.WEN</span><small class="ml-1x"></small></a></h3>
        <div>一个疯子。</div>
      </div>
    </figure>
  </div>
</div>


    </div> -->
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/" title="机器学习基础教程"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2020/05/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20%E5%AE%9E%E6%88%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="Spring 实战学习笔记"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn " data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">    <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
    <div class="copyright">
    	
        <div class="publishby">
        	<!-- Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>. -->
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   






</body>
</html>