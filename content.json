{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"https://wingowen.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-07-29T10:17:24.000Z","updated":"2022-07-29T10:18:07.390Z","comments":true,"path":"categories/index.html","permalink":"https://wingowen.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-07-29T10:18:35.000Z","updated":"2022-07-29T10:18:54.011Z","comments":true,"path":"tags/index.html","permalink":"https://wingowen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"线性回归与逻辑回归","slug":"算法/线性回归与逻辑回归","date":"2022-08-12T02:35:16.000Z","updated":"2022-08-13T14:34:12.821Z","comments":true,"path":"2022/08/12/算法/线性回归与逻辑回归/","link":"","permalink":"https://wingowen.github.io/2022/08/12/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等","text":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等 线性回归线性回归分析 Linear Regression Analysis 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。 所以线性的定义为：自变量之间只存在线性关系，即自变量只能通过相加、或者相减进行组合。 监督学习 如果现在有一个房子 H1，面积是 S，监督学习如何估算它的价格？ 监督学习从训练集中找到面积最接近 S 的房子 H2，预测 H1 的价格等于 H2 的价格。 监督学习根据训练集，找到一个数学表达式，对任意的面积的房子都可以估算出其价格。 h 代表假设函数：Training Set → Learning Algorithm → h；Size of House + h → Estimated Price。 线性回归的假设模型 h_{\\theta}(x)=\\theta_{0}+\\theta_{1} x如何求解模型，有以下两种思路。 尝试一些 $\\theta{0}$ 和 $\\theta{1}$ 的组合，选择能使得画出的直线正好穿过训练集的 $\\theta{0}$ 和 $\\theta{1}$ 。 尝试一些 $\\theta{0}$ 和 $\\theta{1}$ 的组合，然后在训练集上进行预测，选能使得预测值与真实的房子价格最接近的 $\\theta{0}$ 和 $\\theta{1}$ 。 选择最佳的 $\\theta{0}$ 和 $\\theta{1}$，使得 $h_{\\theta}(x)$ 对所有的训练样本 $(x, y)$ 来说，尽可能的接近 $y$。 损失函数 Train Set: $\\left{\\left(x^{(1)}, y^{(1)}\\right),\\left(x^{(2)}, y^{(2)}\\right), \\cdots,\\left(x^{(m)}, y^{(m)}\\right)\\right}$ \\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}最小化损失函数，得到的 $\\theta{0}$ 和 $\\theta{1}$ 是最佳的。 12345678910# 房屋的价格和面积数据import numpy as npdata = np.array([[2104, 460], [1416, 232], [1534, 315], [852,178]])# 使用线性回归模型计算预测值def get_predict(x, theta0, theta1): h = theta0 + theta1 * x #todo return h 梯度下降算法梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合 $(\\theta{0},\\theta{1},……,\\theta_{n})$ 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合， 可能会找到不同的局部最小值。 实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，需要同时更新 $\\theta{0}$ 和 $\\theta{1}$，实现方法是：你应该计算公式右边的部分，通过那一部分计算出 $\\theta{0}$ 和 $\\theta{1}$的值，然后同时更新 $\\theta{0}$ 和 $\\theta{1}$。 \\text { temp0 }:=\\theta_{0}-\\alpha \\frac{\\partial}{\\partial \\theta_{0}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\text { temp1 }:=\\theta_{1}-\\alpha \\frac{\\partial}{\\partial \\theta_{1}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\theta_{0}:=\\text { temp0 } \\\\ \\theta_{1}:=\\text { temp1 }逻辑回归二分类问题下，采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。它适用于标签 y 取值离散的情况。 逻辑函数 Logistic Function，一个最常用的逻辑函数是 Sigmoid Function，以 Z=0 为决策界限，公式如下： g(z)=\\frac{1}{1+e^{-z}}123import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 逻辑回归模型假设 h_\\theta(x)=g(\\theta^TX)$h\\theta(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性 （estimated probablity），即 $ h\\theta(x) = P(y=1|x;\\theta)$。 例如，如果对于给定的 x，通过已经确定的参数计算得出 $h_\\theta(x)$=0.7，则表示有 70% 的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7 = 0.3。 损失函数 线性回归模型的代价函数是所有模型误差的平方和，若逻辑回归的假设模型沿用这个定义，得到的函数将是一个非凸函数 non-convex function。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)}) \\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{aligned} -\\log \\left(h_{\\theta}(x)\\right) & \\text { if } y=1 \\\\ -\\log \\left(1-h_{\\theta}(x)\\right) & \\text { if } y=0 \\end{aligned}\\right. Cost(h_\\theta(x), y)=-y\\times{log(h_\\theta(x))}-(1-y)\\times{log(1-h_\\theta(x))} J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) return np.sum(first - second) / (len(X)) 当实际的 y=1 且 $h\\theta(x)$ 也为1 时误差为 0，当 y=1 但 $h\\theta(x)$ 不为 1 时误差随着 $h_\\theta(x)$ 的变小而变大； 当实际的 y=0 且 $h\\theta(x)$ 也为 0 时代价为 0，当 y=0 但 $h\\theta(x)$ 不为 0 时误差随着 $h_\\theta(x)$ 的变大而变大。 同样使用梯度下降法对参数进行更新： \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j123456789import numpy as np# 返回某一轮训练中的梯度def _gradient(X, Y_label, theta): # _f用来计算 y 的值 y_pred = _f(X, theta, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) return w_grad 多元分类 我们将多个类中的一个类标记为正向类 y=1，然后将其他所有类都标记为负向类，这个模型记作 $h^{(1)\\theta(x)}$。接着，类似地第我们选择另一个类标记为正向类 y=2，再将其它类都标记为负向类，将这个模型记作 $h^{(2)\\theta(x)}$，依此类推。 最后我们得到一系列的模型简记为： h^{(i)_\\theta(x)} = p(y=i|x;\\theta)最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h^{(i)\\theta(x)}$， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 $h^{(i)\\theta(x)}$ 最大的 i，即 $\\maxih^{(i)\\theta(x)}$。 过拟合化和正则化过拟合在训练数据上的表现非常好；对于非训练的数据点，过拟合的模型表现与我们的期望有较大的偏。 减少拟合化的方法： 减少选取变量的数量：选取最重要的参数； 正则化：一种减小参数大小的办法。 正则化 回归：岭回归。 分类：L1 正则化，L2 正则化。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"恋练有词 Unit01","slug":"考研/恋练有词-Unit01","date":"2022-08-02T01:16:03.000Z","updated":"2022-08-03T05:41:59.963Z","comments":true,"path":"2022/08/02/考研/恋练有词-Unit01/","link":"","permalink":"https://wingowen.github.io/2022/08/02/%E8%80%83%E7%A0%94/%E6%81%8B%E7%BB%83%E6%9C%89%E8%AF%8D-Unit01/","excerpt":"第一次学习时间：2022年08月02日。","text":"第一次学习时间：2022年08月02日。 state n. 状态 情况 国 州 v. 陈述 说明 The current state of affairs may have been encouraged — though not justified — by the lack of legal penalty (in American, bur not in Europe) for data leakage. As a condition of receiving state approval for the sale, the company agreed to seek permission from state regulators to operate past 2012. statute n. 法令 法规 manifestation n. 显示 表现 示威运动 statistic n. 统计数值 statistical adj. 统计学的 a statistical population distribution among age peers. stationary adj. 固定的 静止的 不动的 statement n. 陈述 声明 表达 A string of accidents, including the partial collapse of a colling tower in 2007 and the discovery of an underground pipe system leakage, raised serious questions about both A’s safety and B’s management especically after company made misleading statements about the pipe. understatement n. 保守陈述 轻描淡写 overstate v. 夸大陈述 It was banks that were on the wrong planet, with account that vastly overvalued assets. Today they argue that market prices overstate losses, because they largely reflect the temporary illiquidity of markets, not the likely extent of bad debts. statesman n. 政治家 estate n. 房地产 身份 财产 devastate v. 毁灭 毁坏 devastating adj. 毁灭的 极具破坏力的 The wear-and-tear that come from these longer relationships can be quite devastating. workstation n. 工作台","categories":[{"name":"英语","slug":"英语","permalink":"https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"}],"tags":[{"name":"恋练有词","slug":"恋练有词","permalink":"https://wingowen.github.io/tags/%E6%81%8B%E7%BB%83%E6%9C%89%E8%AF%8D/"}]},{"title":"Python 编程","slug":"编程/Python-编程","date":"2022-08-01T03:17:02.000Z","updated":"2022-08-11T08:53:33.680Z","comments":true,"path":"2022/08/01/编程/Python-编程/","link":"","permalink":"https://wingowen.github.io/2022/08/01/%E7%BC%96%E7%A8%8B/Python-%E7%BC%96%E7%A8%8B/","excerpt":"Python 编程开发查漏补缺。 gRPC Redis","text":"Python 编程开发查漏补缺。 gRPC Redis GRPCGoogle 开发的基于 HTTP/2 和 Protocol Buffer 3 的 RPC 框架。 Protocol Buffers, protobuf：结构数据序列化机制。 gRPC 默认使用 Brotocol Buffers，用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。 定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 GRPC 服务器来处理客户端调用。在客户端拥有一个存根 Stub，存根负责接收本地方法调用，并将它们委派给各自的具体实现对象（在远程服务器上）。 简单实现实现一个简单的 gRPC HelloWorld。 proto file定义 Protocol Buffers 规则文件。 123456789101112131415161718syntax = &quot;proto3&quot;;package helloworld;service Greeter &#123; // 定义方法参数和返回类型 rpc SayHello (HelloRequest) returns (HelloResponse) &#123;&#125;&#125;// 请求结构声明message HelloRequest &#123; string name = 1;&#125;// 响应结构声明message HelloResponse &#123; string message = 1;&#125; 运行 grpc_tools 工具。 1python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto 生成 python 代码。 helloworld_pb2.py 为 Protocol Buffers 的 Python 实现。 1234567891011121314151617181920# helloworld_pb2_grpc.py 用于 gRPC 实现的 Python 方法实现# 客户端存根class GreeterStub(object): def __init__(self, channel): self.SayHello = channel.unary_unary( &#x27;/helloworld.Greeter/SayHello&#x27;, request_serializer=helloworld__pb2.HelloRequest.SerializeToString, response_deserializer=helloworld__pb2.HelloResponse.FromString, )# 服务端服务class GreeterServicer(object): def SayHello(self, request, context): context.set_code(grpc.StatusCode.UNIMPLEMENTED) context.set_details(&#x27;Method not implemented!&#x27;) raise NotImplementedError(&#x27;Method not implemented!&#x27;) def add_GreeterServicer_to_server(servicer, server): # ...... server自定义 gRPC 服务端。 1234567891011121314151617181920212223import grpcimport randomfrom concurrent import futuresimport helloworld_pb2import helloworld_pb2_grpc# 实现定义的方法，继承并实现方法class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloResponse(message=&#x27;Hello &#123;msg&#125;&#x27;.format(msg=request.name))def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) # 绑定处理器 helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) # 未使用 SSL，所以是不安全的 server.add_insecure_port(&#x27;[::]:50054&#x27;) server.start() print(&#x27;gRPC 服务端已开启，端口为 50054...&#x27;) server.wait_for_termination()if __name__ == &#x27;__main__&#x27;: serve() client自定义客户端。 1234567891011121314import grpcimport helloworld_pb2, helloworld_pb2_grpcdef run(): # 本次不使用 SSL，所以 channel 是不安全的 channel = grpc.insecure_channel(&#x27;localhost:50054&#x27;) # 客户端实例 stub = helloworld_pb2_grpc.GreeterStub(channel) # 调用服务端方法 response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#x27;World&#x27;)) print(&quot;Greeter client received: &quot; + response.message)if __name__ == &#x27;__main__&#x27;: run() RedisREmote DIctionary Server, Redis 是一个 key-value 存储系统，是跨平台的非关系型数据库。 123456789101112131415pip install redisimport redis # 导入redis 模块# 获取连接r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True) # Redis 实例会维护一个自己的连接池，建立连接池，从连接池获取连接pool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)r = redis.Redis(connection_pool=pool)r.set(&#x27;name&#x27;, &#x27;runoob&#x27;, nx, xx) # 设置 name 对应的值, 当 nx = Ture 则只有 Key 不存在才执行插入; xx 相反print(r[&#x27;name&#x27;])print(r.get(&#x27;name&#x27;), px, ex) # 取出键 name 对应的值, px 毫秒 ex 秒 为过期时间print(type(r.get(&#x27;name&#x27;))) # 查看类型","categories":[{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"}]},{"title":"决策树算法","slug":"算法/决策树算法","date":"2022-07-31T02:58:15.000Z","updated":"2022-08-10T15:03:29.129Z","comments":true,"path":"2022/07/31/算法/决策树算法/","link":"","permalink":"https://wingowen.github.io/2022/07/31/%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/","excerpt":"基本概念。","text":"基本概念。 决策树的基本概念决策树节点： 叶节点表示一份类别或者一个值； 非叶节点表示一个属性划分。 决策树的有向边代表了属性划分的不同取值： 当属性是离散时，可将属性的每一个取值用一条边连接到子结点； 当属性是连续时，需要特殊处理。 决策树是一种描述实例进行分类的树形结构。 对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。 决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略；决策树的学习过程本质上是从训练数据中寻找一组分类规则；决策树学习也可以看做是由训练数据集估计条件概率模型。 由上述描述可以得知，决策树学习是一个递归过程，有三种情况会导致递归返回： 当前结点包含的所有样本属于同一类别。 当前属性结合为空，或所有样本在所有属性上的取值都相同：将当前结点标记为叶结点，其类别为该节点包含的样本最多的类别。 当前结点包含的样本基本为空：将当前结点标记为叶结点，其类别为父节点包含样本最多的类别 决策树的学习结果为：树结构 + 叶节点的取值（类别） 信息增益熵，又称信息熵，是信息论的重要概念。熵是度量样本集合纯度的指标，熵越大，样本的纯度越低。假设当前样本集合 $D$ 中第 $i$ 类样本所占比例的为 $p_i(i = 1,2,…,C)$，则 $D$ 的熵定义为： H(D)=-\\sum_{i=1}^{C} p_{i} \\log _{2} p_{i}信息增益表示特征对于当前样本集纯度提升的程度。某属性的信息增益越大，说明使用改属性进行划分获得的纯度提升越大。因此使用信息增益进行决策树属性选择时，选择属性信息增益最大的作为当前节点。 G(D, a)=H(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} H\\left(D^{v}\\right)123456789101112131415161718192021222324252627282930# 信息增益计算def get_G(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data 中所有样本属于同一类，由于 xlnx 在 x = 1和 x-&gt;0 是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE return H - Eprint(&#x27;各个属性的信息增益为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_G(data,i,len(data.columns)-1)) 信息增益比以信息增益作为划分数据集的特征，会导致对可取值数目较多的属性有所偏好。为了缓解这种不良影响，采用信息增益比作为选择属性的准则。 123456789101112131415161718192021222324252627282930def get_GR(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益比 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益比, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 IV = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data中所有样本属于同一类，由于xlnx 在x = 1和x-&gt;0是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE IV = IV - new_data.shape[0] / data.shape[0] * (np.log2(new_data.shape[0] / data.shape[0])) G = H - E return G / IVprint(&#x27;各个属性的信息增益比为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_GR(data,i,len(data.columns)-1)) 基尼系数纯度使用基尼指数来度量，在使用基尼系数作为指标时，应该选择基尼指数最小的属性。 1234567891011121314151617181920212223def get_Gini(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的基尼系数 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的基尼系数, type:float, e.g.:0.32 &#x27;&#x27;&#x27; Gini = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] Gini_v = 1 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] Gini_v = Gini_v - p * p Gini = Gini + new_data.shape[0] / data.shape[0] * Gini_v return Giniprint(&#x27;各个属性的基尼指数为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_Gini(data,i,len(data.columns)-1)) ID3ID3 算法的核心是在决策树各结点上使用信息增益准则选择特征：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，根据特征的不同取值建立子结点。递归地调用以上方法，构建决策树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def build_tree_id3(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用 ID3 算法在 data 数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_G(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_id3(new_data, nu, new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)root_id3 = build_tree_id3(data, None, ori_ppt, len(data.columns)-1)# 可视化createPlot(root_id3) C4.5C4.5 算法对 ID3 算法进行了改进，即使用信息增益比来选择特征，其余和 ID3 算法基本相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def build_tree_c45(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用C4.5算法在data数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_GR(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_c45(new_data, nu,new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)# print(data)root_c45 = build_tree_c45(data, None ,ori_ppt, len(data.columns)-1)createPlot(root_c45) 损失函数与剪枝决策树的剪枝往往通过最小化决策树的损失函数实现。 123456789101112131415161718192021def cal_loss(root, alpha): &#x27;&#x27;&#x27; 计算以root为根结点的决策树的损失值 :param root: 根结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 以root为根结点的决策树的损失值, type:float, e.g.:0.24 &#x27;&#x27;&#x27; loss = 0 for leaf in root.leaf_ch: data = leaf.data for v in np.unique(data.iloc[:,len(data.columns)-1]): ntk = data[data.iloc[:,len(data.columns)-1] == v].shape[0] loss = loss - ntk * np.log2(ntk / data.shape[0]) loss = loss + alpha * len(root.leaf_ch) return lossori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)cal_loss(root_c45, 0.3) 决策树生成算法递归地产生决策树，直到无法继续。这种做法会带来过拟合问题。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，构建过于复杂的决策树。因此，一种解决方法是考虑决策树的复杂程度，从而对决策树进行简化。对决策树进行简化的过程称为剪枝。即从决策树中裁掉一些子树或叶结点，将其根节点或父节点作为新的叶结点。 剪枝算法的实现 计算每个节点的经验熵。 递归地从树的叶结点向上回缩，若回缩后的损失值 &gt; 回缩前的损失值，则进行剪枝，父节点变为叶节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def tree_pruning(root, leaf, alpha): &#x27;&#x27;&#x27; 决策树剪枝 :param root: 根结点, type:Node :param leaf: 叶结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 剪枝后的决策树根结点, type:Node &#x27;&#x27;&#x27; new_root = copy.deepcopy(root) pre_loss = cal_loss(root, alpha) flag = 1 for nl in leaf.fa.leaf_ch.copy(): nl.can_delete = 1 new_set = set() for leaf_ch in root.leaf_ch: if leaf_ch.can_delete != 1: new_set.add(leaf_ch) root.leaf_ch = new_set leaf.fa.set_leaf(leaf.fa.data.iloc[:,len(root.data.columns)-1].value_counts().keys()[0]) root.add_leaf_ch(leaf.fa) after_loss = cal_loss(root, alpha) if after_loss &gt;= pre_loss: #不剪枝 root = new_root flag = 0 return root, flag ori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)# createPlot(root_c45)#设置超参数alpha = 0.3update = 1while update == 1: update = 0 for leaf in root_c45.leaf_ch.copy(): root_c45,flag = tree_pruning(root_c45, leaf, alpha) if flag: update = 1# root_c45createPlot(root_c45) 连续值处理 缺失值处理","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"计算机组成","slug":"考研/计算机组成","date":"2022-07-30T07:15:43.000Z","updated":"2022-08-03T05:41:57.995Z","comments":true,"path":"2022/07/30/考研/计算机组成/","link":"","permalink":"https://wingowen.github.io/2022/07/30/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/","excerpt":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。","text":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}]},{"title":"模型评估与选择","slug":"算法/模型评估与选择","date":"2022-07-29T10:24:58.000Z","updated":"2022-08-02T01:19:57.843Z","comments":true,"path":"2022/07/29/算法/模型评估与选择/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。","text":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。 错误率和精度123456789101112131415161718import numpy as np# 真实的数据标签real_label = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]) \\ \\ \\# 分类器的预测标签classifier_pred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])compare_result = (real_label == classifier_pred)compare_result = compare_result.astype(np.int)# m 为样本数量 b 为预测错误样本m = len(real)b = m - np.sum(cmp)# 错误率error_rate = (b / m)*100# 精确度 accaccuracy = (1 - b / m)*100 误差模型在训练样本上的误差称为训练误差或经验误差；模型在新样本上的误差称为泛化误差。 过拟合模型：虽然训练误差接近 0，泛化误差非常大。 欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。 偏差和方差偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。 偏差 bias，与真实值的偏离程度； 方差 variance，该随机变量在其期望值附近的波动程度。 评估方法评估：对学习器的泛化误差进行评估并进而做出选择。 留出法以一定比例划分训练集和测试集。 1234567891011121314151617181920212223# 导入包import numpy as npfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): &#x27;&#x27;&#x27; return: 返回随机生成 200 个点的坐标 &#x27;&#x27;&#x27; dots = 200 # 样本数 dim = 2 # 数据维度 X = np.random.randn(dots,dim) # 建立数据集，shape(200,2) # 建立样本 X 的类别 Y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): Y[i] = 1 return X, Y# 加载数据X,Y = load_pts()# 使用train_test_split划分训练集和测试集train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0) 交叉验证交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。 12345678910111213# 导入包from sklearn.model_selection import KFoldimport numpy as np# 生成数据集，随机生成40个点data = np.random.randn(40,2)# 交叉验证法kf = KFold(n_splits = 4, shuffle = False, random_state = None) for train, test in kf.split(data): print(train) print(test,&#x27;\\n&#x27;) 自助法有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ： 每次随机从 D 中挑选一个样本； 将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中； 重复执行 m 次该过程； 最后得到包含 m 个样本数据集 D’。 由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。 123456789101112131415161718# 导入包import numpy as np# 任意设置一个数据集X = [1,4,3,23,4,6,7,8,9,45,67,89,34,54,76,98,43,52]# 通过产生的随机数获得抽取样本的序号 bootstrapping = []for i in range(len(X)): bootstrapping.append(np.random.randint(0,len(X),(1)))# 通过序号获得原始数据集中的数据D_1 = []for i in range(len(X)): print(int(bootstrapping[i])) D_1.append(X[int(bootstrapping[i])]) print(D_1) 总结 采样方法 与原始数据集的分布是否相同 相比原始数据集的容量 是否适用小数据集 是否适用大数据集 是否存在估计偏差 留出法 分层抽样 否 变小 否 是 是 交叉验证法 分层抽样 否 变小 否 是 是 自助法 放回抽样 否 不变 是 否 是 性能度量性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。 性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。 回归任务常用性能度量：MSE mean square error，均方差。 分类任务常用性能度量：acc accuracy，精度；错误率 查准率、查全率、F1对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为： 真正例 true positive 假正例 false positive 真反例 true negative 假反例 false negative P(\\text { Precision })=\\frac{T P}{T P+F P} \\\\R(\\text { Recall })=\\frac{T P}{T P+F N}Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。 Precision，查准率：在预测为正的样本中，实际为正的概率。 P-R 曲线，BRP，Break Even Point：平衡单 P = R。 由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。BRP 还是过于简单，更常用的是 F1 度量。 F 1=\\frac{2 \\times P \\times R}{P+R}=\\frac{2 T P}{n+T P-T N}F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。 当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。 F_{\\beta}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R}β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。 如果使用了类似交叉验证法，我们会得到多个 confusion matrix： 宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1； 微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as np# 加载数据集def generate_data(random_state=2021): &quot;&quot;&quot; :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] &quot;&quot;&quot; noise_rate = 0.1 # 噪声比例 sample_num = 4096 # 总样本数 noise_sample_num = int(sample_num*noise_rate) # 噪声样本数 np.random.seed(random_state) Pred_Score = np.random.uniform(0,1,sample_num) GT_label = (Pred_Score&gt;0.5).astype(np.int) noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=False, p=None) for index in noise_ids: GT_label[index] = 1 if GT_label[index] == 0 else 0 return GT_label, Pred_ScoreGT_label, Pred_Score = generate_data()# 请你补全以下代码，计算查准率与查全率def get_PR(GT_label, Pred_Score, threshold, random_state=2021): &quot;&quot;&quot; 计算错误率和精度 :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] :threshold: 评估阈值 :random_state: 随机种子 :返回值: P: 查准率 R: 查全率 &quot;&quot;&quot; Pred_Label = list(map(lambda x: 1 if x &gt; threshold else 0, Pred_Score)) from sklearn.metrics import precision_score, recall_score P = precision_score(GT_label, Pred_Label) R = recall_score(GT_label, Pred_Label) &quot;&quot;&quot; TODO &quot;&quot;&quot; return P, R P, R = get_PR(GT_label, Pred_Score, 0.55, random_state=2021)print(&quot;查准率P ：&#123;:.2f&#125;&quot;.format(P))print(&quot;查全率R ：&#123;:.2f&#125;&quot;.format(R)) ROC 与 AUC 原理ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。 \\begin{aligned}T P R(\\text { Precision }) &=\\frac{T P}{T P+F N} \\\\F P R(\\text { Precision }) &=\\frac{F P}{F P+T N}\\end{aligned}TPR 真正率，真正样本与实际为正的样本的比率； FPR 假正率，加正样本与实际为负的样本的比率。 若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两 个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC Area Under ROC Curve。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): dots = 200 # 点数 X = np.random.randn(dots,2) * 15 # 建立数据集，shape(200,2)，坐标放大15倍 # 建立 X 的类别 y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): if X[i,0] &gt; -15 and X[i,0] &lt; 15 and X[i,1] &gt; -15 and X[i,1] &lt; 15: # 矩形框内的样本都是目标类（正例） y[i] = 1 if 0 == np.random.randint(i+1) % 10: # 对数据随机地插入错误，20 个左右 y[i] = 1 - y[i] # 数据集可视化 plt.scatter(X[np.argwhere(y==0).flatten(),0], X[np.argwhere(y==0).flatten(),1],s = 20, color = &#x27;blue&#x27;, edgecolor = &#x27;k&#x27;) plt.scatter(X[np.argwhere(y==1).flatten(),0], X[np.argwhere(y==1).flatten(),1],s = 20, color = &#x27;red&#x27;, edgecolor = &#x27;k&#x27;) plt.xlim(-40,40) plt.ylim(-40,40) plt.grid(False) plt.tick_params( axis=&#x27;x&#x27;, which=&#x27;both&#x27;, bottom=False, top=False) return X, yX, y = load_pts()plt.show()### 训练模型 ###from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.svm import SVC# 将数据集拆分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) # 建立模型 clf1 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=4,min_samples_split=4)clf2 = GradientBoostingClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=10)clf3 = SVC(kernel=&#x27;rbf&#x27;, gamma=0.001, probability=True)# 训练模型clf1.fit(X_train, y_train)clf2.fit(X_train, y_train)clf3.fit(X_train, y_train)### 评估模型 ###from sklearn.metrics import roc_curve# 模型预测y_score1 = clf1.predict_proba(X_test)y_score2 = clf2.predict_proba(X_test)y_score3 = clf3.predict_proba(X_test)# 获得 FPR、TPR 值fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,1])fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,1])fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,1])### 绘制 ROC 曲线 ###from sklearn.metrics import aucplt.figure()# 绘制 ROC 函数def plot_roc_curve(fpr, tpr, c, name): lw = 2 roc_auc = auc(fpr,tpr) plt.plot(fpr, tpr, color=c,lw=lw, label= name +&#x27; (area = %0.2f)&#x27; % roc_auc) plt.plot([0,1], [0,1], color=&#x27;navy&#x27;, lw=lw, linestyle=&#x27;--&#x27;) plt.xlim([0, 1.0]) plt.ylim([0, 1.05]) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.ylabel(&#x27;True Positive Rate&#x27;) #plt.title(&#x27;&#x27;) plt.legend(loc=&quot;lower right&quot;) plot_roc_curve(fpr1, tpr1, &#x27;red&#x27;,&#x27;DecisionTreeClassifier &#x27;) plot_roc_curve(fpr2, tpr2, &#x27;navy&#x27;,&#x27;GradientBoostingClassifier &#x27;) plot_roc_curve(fpr3, tpr3, &#x27;green&#x27;,&#x27;SVC &#x27;) plt.show() 比较检验（TODO）模型性能比较的重要因素： 实验评估得到的性能不等于泛化性能； 测试集上的性能与测试集本身的选择有很大关系； 很多机器学习算法本身有一定的随机性。 统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。 假设检验由样本推测总体的方法。 交叉验证 t 检验McNemar 检验Friedman 检验与 Nemenyi 后续检验类别不平衡在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。 如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。 解决不平衡分类问题的策略可以分为两大类： 从数据层面入手 , 通过改变训练集样本分布降低不平衡程度； 从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。 数据层面解决类别不平衡扩大数据样本。 重采样：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。 过采样：复制稀有样本； 123456789101112131415161718192021222324252627282930313233# 导入包from sklearn.datasets import make_classificationfrom collections import Counterfrom imblearn.over_sampling import RandomOverSampler# 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)# 打印每个类别样本数print(Counter(y))# 过采样ros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)# 打印过采样后每个类别样本数print(sorted(Counter(y_resampled).items()))# 生成新的稀有样本# 导入包from imblearn.over_sampling import SMOTE# 过采样sm = SMOTE(random_state=42)X_res, y_res = sm.fit_resample(X, y)# 打印过采样后每个类别样本数print(&#x27;Resampled dataset shape %s&#x27; % Counter(y_res)) 欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。 123456789# 导入包from imblearn.under_sampling import RandomUnderSampler# 欠采样rus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)# 打印欠采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。 因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。 12345678# 导入包from imblearn.combine import SMOTEENN# 过采样与欠采样结合smote_enn = SMOTEENN(random_state=0)X_resampled, y_resampled = smote_enn.fit_resample(X, y)# 打印采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 算法层面解决类别不平衡惩罚项方法：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，错分稀有类的样本需要付出更大的代价。 通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。 例如，调整 SVM 以惩罚稀有类别的错误分类。 1234567# LABEL 0 4000# LABEL 1 200# 导入相关包from sklearn.svm import SVC# 添加惩罚项clf = SVC(C=0.8, probability=True, class_weight=&#123;0:0.25, 1:0.75&#125;) 特征选择方法 样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。 选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"贝叶斯算法","slug":"算法/贝叶斯算法","date":"2022-07-29T10:24:58.000Z","updated":"2022-08-02T01:27:32.849Z","comments":true,"path":"2022/07/29/算法/贝叶斯算法/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/","excerpt":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。","text":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。 条件概率P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率： P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率： P(B \\mid A)=\\frac{P(A \\cap B)}{P(A)}那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出贝叶斯公式： P(A \\mid B)=\\frac{P(B \\mid A) \\times P(A)}{P(B)}{\\scriptsize }贝叶斯基础思想： 已知类条件概率密度参数表达式和先验概率； 利用贝叶斯公式转换成后验概率； 根据后验概率大小进行决策分类。 根据以上基本思想，可以得到贝叶斯概率计算公式表达为：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）。 优点： 贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断； 它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信； 如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了； 它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。 缺点： 它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出； 有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。 扩展阅读： 一文读懂概率论学习：贝叶斯理论 贝叶斯决策论&amp;朴素贝叶斯算法 朴素贝叶斯法讲解 sklearn 贝叶斯方法 贝叶斯推断：广告邮件自动识别的代码实现若邮件包含某个关键词，求此邮件是广告的概率。 12345678910111213141516171819# 广告邮件数量ad_number = 4000# 正常邮件数量normal_number = 6000# 所有广告邮件中，出现 “红包” 关键词的邮件的数量ad_hongbao_number = 1000# 所有正常邮件中，出现 “红包” 关键词的邮件的数量normal_hongbao_number = 6# 广告的先验概率 P(A)P_ad = ad_number / (ad_number + normal_number)# 包含红包的先验概率 P(B)P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)# 广告 包含红包的似然概率 P(B|A)P_hongbao_ad = ad_hongbao_number / ad_number# 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbaoprint(P_ad_hongbao) 10.9940357852882705 极大似然估计极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。 极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 极大似然估计与贝叶斯推断是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。 极大似然估计的简单计算一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。 \\begin{array}{c} \\frac{d}{d p}\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{61}(1-p)^{39}=\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right)\\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\\right) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61-100 p) \\\\ =0 \\end{array}当 $P = \\frac{61}{100}, 0$ 时，导数为零。因为 1 &lt; P &lt; 0，所以 $P = \\frac{61}{100}$。 极大似然估计的简单应用求极大似然估计 MLE 的一般步骤： 由总体分布导出样本的联合概率函数（或联合密度）； 把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 $θ$ 看作自变量，得到似然函数 $l(θ)$； 求似然函数 $l(θ)$ 的最大值点，常常转化为求 $lnl(θ)$ 的最大值点，即 $θ$ 的 MLE； 在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。 若随机变量 $x$ 服从一个数学期望为 $μ$、方差为 $σ^2$ 的正态分布，记为 $N(μ,σ^2)$，假设 $μ=30, σ=2$。 1234567891011import numpy as npfrom scipy.stats import normimport matplotlib.pyplot as pltμ = 30 # 数学期望σ = 2 # 方差x = μ + σ * np.random.randn(10000) # 正态分布plt.hist(x, bins=100) # 直方图显示plt.show()print(norm.fit(x)) # 返回极大似然估计，估计出参数约为 30 和 2 朴素贝叶斯分类器朴素贝叶斯分类器是一系列假设特征之间强（朴素）独立条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。 MNIST 手写体数字识别123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import warningswarnings.filterwarnings(&quot;ignore&quot;)# numpy 库import numpy as np# tensorflow 库中的 mnist 数据集import tensorflow as tfmnist = tf.keras.datasets.mnist# sklearn 库中的 BernoulliNBfrom sklearn.naive_bayes import BernoulliNB# 绘图工具库 pltimport matplotlib.pyplot as pltprint(&quot;读取数据中 ...&quot;)# 载入数据(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量train_images = train_images.reshape(len(train_images),784)test_images = test_images.reshape(len(test_images),784)print(&#x27;读取完毕!&#x27;)def plot_images(imgs): &quot;&quot;&quot;绘制几个样本图片 :param show: 是否显示绘图 :return: &quot;&quot;&quot; sample_num = min(9, len(imgs)) img_figure = plt.figure(1) img_figure.set_figwidth(5) img_figure.set_figheight(5) for index in range(0, sample_num): ax = plt.subplot(3, 3, index + 1) ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;) ax.grid(False) plt.margins(0, 0) plt.show()plot_images(train_images)print(&quot;初始化并训练贝叶斯模型...&quot;)# 定义 朴素贝叶斯模型classifier_BNB = BernoulliNB()# 训练模型classifier_BNB.fit(train_images,train_labels)print(&#x27;训练完成!&#x27;)print(&quot;测试训练好的贝叶斯模型...&quot;)# 分类器在测试集上的预测值test_predict_BNB = classifier_BNB.predict(test_images)print(&quot;预测完成!&quot;)# 计算准确率accuracy = classifier_BNB.score(test_images, test_labels)print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy) 对结果进行统计比较分析。 12345678910111213141516171819202122# 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 class_num = &#123;&#125;# 每个类别预测为 0-9 类别的个数，predict_num = []# 每个类别预测的准确率class_accuracy = &#123;&#125;for i in range(10): # 找到类别是 i 的下标 class_is_i_index = np.where(test_labels == i)[0] # 统计类别是 i 的个数 class_num[i] = len(class_is_i_index) # 统计类别 i 预测为 0-9 各个类别的个数 predict_num.append( [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)]) # 统计类别 i 预测的准确率 class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100 print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % ( i, class_num[i], predict_num[i][i], class_accuracy[i])) 用热力图对结果进行分析。 123456789101112import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) np.random.seed(0)uniform_data = predict_numax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)ax.set_xlabel(&#x27;真实值&#x27;)ax.set_ylabel(&#x27;预测值&#x27;)plt.show()","categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"考研","slug":"考研/考研","date":"2022-04-21T06:19:23.000Z","updated":"2022-08-03T05:41:56.366Z","comments":true,"path":"2022/04/21/考研/考研/","link":"","permalink":"https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/%E8%80%83%E7%A0%94/","excerpt":"考研院校信息整理汇总。","text":"考研院校信息整理汇总。 报考专业 深大 - 人工智能与金融科技 初试科目 101 思想政治理论 201 英语一 301 数学一 408 计算机学科专业基础综合 复试科目 FSX8 机器学习 计算机考研 408 包括（150） 数据结构 45 计算机组成原理 45 操作系统 35 计算机网络 25","categories":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]}],"categories":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"英语","slug":"英语","permalink":"https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"},{"name":"编程","slug":"编程","permalink":"https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"恋练有词","slug":"恋练有词","permalink":"https://wingowen.github.io/tags/%E6%81%8B%E7%BB%83%E6%9C%89%E8%AF%8D/"},{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/tags/Python/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"},{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]}