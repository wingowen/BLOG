{"meta":{"title":"WINGO'S BLOG","subtitle":"","description":"","author":"Wingo Wen","url":"https://wingowen.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-07-29T10:17:24.000Z","updated":"2022-07-29T10:18:07.390Z","comments":true,"path":"categories/index.html","permalink":"https://wingowen.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-07-29T10:18:35.000Z","updated":"2022-07-29T10:18:54.011Z","comments":true,"path":"tags/index.html","permalink":"https://wingowen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hexo","slug":"杂项/Hexo","date":"2023-09-20T05:41:38.000Z","updated":"2023-09-20T07:31:56.138Z","comments":true,"path":"2023/09/20/杂项/Hexo/","link":"","permalink":"https://wingowen.github.io/2023/09/20/%E6%9D%82%E9%A1%B9/Hexo/","excerpt":"","text":"文章置顶 修改 \\node_modules\\hexo-generator-index-pin-top\\lib\\generator.js 123456789101112131415161718192021// ... const posts = locals.posts.sort(config.index_generator.order_by); // 添加以下根据 Top 排序代码 posts.data = posts.data.sort(function (a, b) &#123; if (a.top &amp;&amp; b.top) &#123; if (a.top == b.top) return b.date - a.date; else return b.top - a.top; &#125; else if (a.top &amp;&amp; !b.top) &#123; return -1; &#125; else if (!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); sort(posts.data, (a, b) =&gt; (b.sticky || 0) - (a.sticky || 0)); //...&#125;;","categories":[{"name":"杂项","slug":"杂项","permalink":"https://wingowen.github.io/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[]},{"title":"MySQL","slug":"数据库/MySQL","date":"2023-09-20T01:53:41.000Z","updated":"2023-09-20T03:45:37.292Z","comments":true,"path":"2023/09/20/数据库/MySQL/","link":"","permalink":"https://wingowen.github.io/2023/09/20/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/","excerpt":"","text":"单进程多线程 SQL 解析器 &gt; SQL 查询优化器 &gt; 执行器 &gt; 存储引擎 SQL Select 语句完整的执行顺序： from 子句组装来自不同数据源的数据； where 子句基于指定的条件对记录行进行筛选； group by 子句将数据划分为多个分组； 使用聚集函数进行计算； 使用 having 子句筛选分组； 计算所有的表达式； select 的字段； 使用 order by 对结果集进行排序。 InnoDB 存储引擎 unzip 单页 16k。 支持事务，设计目标主要面向在线事务处理 OLAP 应用。其特点是行锁设计、支持外间，并支持类似于 Oracle 的非锁定读，即默认读取操作不会产生锁。 MVCC（Multi-Version Concurrency Control）是一种数据库并发控制机制，用于在多个事务并发执行时保证数据的一致性和隔离性。MVCC 基于版本控制的思想，为每个事务提供一个独立的数据版本，从而避免了读写冲突和锁竞争。 在数据库中，“undo 页”（也称为回滚段或回滚段页）是用于实现事务的回滚和并发控制的一种数据结构。它用于存储已提交事务的旧版本数据，以便在需要回滚或提供多版本并发控制时使用。 脏页（Dirty Page）是指在数据库缓冲区中已经被修改但尚未写回到磁盘的数据页。当数据库执行写操作时，相应的数据页会被标记为脏页，表示该页的内容已经被修改且与磁盘上的数据不一致。 123select * from innodb_buffer_page_lruwhere oldest_modification &gt; 0; 缓冲池 多缓冲池实例，根据 hash 将页分配到不同的实例 LRU Latest Recent Used 最近最少使用算法进行管理 123show variables like &#x27;innodb_buffer_pool_size&#x27;\\G;show variables like &#x27;innodb_buffer_pool_instance&#x27;\\G;show engine innodb status&#x27;\\G; Redo Log Buffer（重做日志缓冲区）是数据库系统中用于存储事务操作的日志记录的缓冲区。它是实现事务的持久性和恢复能力的关键组件。 参考资料 MySQL 技术内幕 InnoDB 存储引擎 - 第二版","categories":[{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"InooDB","slug":"InooDB","permalink":"https://wingowen.github.io/tags/InooDB/"}]},{"title":"大数据组件","slug":"大数据/大数据组件","date":"2023-09-19T09:11:59.000Z","updated":"2023-09-20T06:20:59.054Z","comments":true,"path":"2023/09/19/大数据/大数据组件/","link":"","permalink":"https://wingowen.github.io/2023/09/19/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6/","excerpt":"简单描述组件之间的关系文件系统：HDFS数据库存储：HBase、Kudu查询引擎：Hive、Impala","text":"简单描述组件之间的关系文件系统：HDFS数据库存储：HBase、Kudu查询引擎：Hive、Impala Hive Hive 存储的是纯逻辑表，即表的元数据 Hive 本身不存储数据，完全依赖于 HDFS 和 MapReduce，将结构化的数据文件映射为一张张数据库表 Hive 基于 MapReduce 处理数据，基于行模式处理 OLTP &amp; OLAP Hase Google‘s Big Table 的开源实现 NoSQL 分布式 KV 数据库 Hbase 存储的是物理表，适合存放非结构化数据 Hbase 基于列模式处理数据，适合海量数据的随机访问 OLTP 底层存储引擎是基于 LSM-Tree 数据结构设计的。写入数据时会先写 WAL 日志，再将数据写到写缓存 MemStore中，等写缓存达到一定规模后或满足其他触发条件才会 flush 刷写到磁盘，每一次刷写磁盘都会生成新的 HFile 文件，HBase 会定期执行 compaction 操作以合并 HFile 读取数据时会依次从 BlockCache、MemStore 以及 HFile 中 seek 数据，再加上一些其他设计比如布隆过滤器、索引等 Spark 在线数据分析 HbaseClient 直接访问 Hbase Spark 引入 Hbase Connector 后通过 Spark SQL / DDL 直接操作数据 Hive 在线数据分析 建 Hbase 外表然后进行 SQL 查询 离线处理 通过 LTS 导出到 HDFS，存储为 Parquet 格式，使用 Spark 进行分析 通过 LTS 增量订阅 Hbase 数据，写入到 Kafka，使用 Spark Streaming 对 Kafka 进行流式计算 WAL（Write-Ahead Logging）是一种数据库系统中常用的日志记录技术，用于确保数据的持久性和一致性。WAL日志是在对数据库进行修改之前，首先将修改操作记录到日志中，然后再将修改应用到数据库文件中。 Impala 使用分布式查询引擎（由 Query Planner、Query Coordinator 和 Query Exec Engine 三部分组成），可以直接从 HDFS 或 Hase 中进行实时交互式 SQL 查询 kudu 同时提供低延迟的随机读写和高效的数据分析能力，是一个融合 HDFS 和 HBase 的功能的新组件，","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://wingowen.github.io/tags/Hive/"},{"name":"Kudu","slug":"Kudu","permalink":"https://wingowen.github.io/tags/Kudu/"},{"name":"Impala","slug":"Impala","permalink":"https://wingowen.github.io/tags/Impala/"}]},{"title":"Pandas 练习","slug":"Python/Pandas-练习","date":"2023-09-19T07:11:03.000Z","updated":"2023-09-20T07:39:38.777Z","comments":true,"path":"2023/09/19/Python/Pandas-练习/","link":"","permalink":"https://wingowen.github.io/2023/09/19/Python/Pandas-%E7%BB%83%E4%B9%A0/","excerpt":"","text":"12import numpy as npimport pandas as pd 12data = &#123;&quot;grammer&quot;:[&quot;Python&quot;,&quot;C&quot;,&quot;Java&quot;,&quot;GO&quot;,&quot;R&quot;,&quot;SQL&quot;,&quot;PHP&quot;,&quot;Python&quot;], &quot;score&quot;:[1,2,np.nan,4,5,6,7,10]&#125; 1df = pd.DataFrame(data=data) 1df[df[&quot;grammer&quot;]==&#x27;Python&#x27;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; grammer score 0 Python 1.0 7 Python 10.0 1df.columns Index(['grammer', 'score'], dtype='object') 1df.rename(columns=&#123;&#x27;score&#x27;:&#x27;popularity&#x27;&#125;, inplace=True) 1df[&#x27;grammer&#x27;].value_counts() Python 2 GO 1 Java 1 C 1 R 1 PHP 1 SQL 1 Name: grammer, dtype: int64 12# 线性插值填充df[&#x27;popularity&#x27;] = df[&#x27;popularity&#x27;].fillna(df[&#x27;popularity&#x27;].interpolate()) 1df[df[&#x27;popularity&#x27;]&gt;3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; grammer popularity 3 GO 4.0 4 R 5.0 5 SQL 6.0 6 PHP 7.0 7 Python 10.0 1df.drop_duplicates(subset=[&#x27;grammer&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; grammer popularity 0 Python 1.0 1 C 2.0 2 Java 3.0 3 GO 4.0 4 R 5.0 5 SQL 6.0 6 PHP 7.0 1df[&#x27;popularity&#x27;].mean() 4.75 1df[&#x27;grammer&#x27;].to_list() ['Python', 'C', 'Java', 'GO', 'R', 'SQL', 'PHP', 'Python'] 1df.shape (8, 2) 123# 改变列的顺序col = df.columns[[1,0]]df = df[col] 1df[df[&#x27;popularity&#x27;] == df[&#x27;popularity&#x27;].max()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; popularity grammer 7 10.0 Python 1df[-5:df.shape[0]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; popularity grammer 3 4.0 GO 4 5.0 R 5 6.0 SQL 6 7.0 PHP 7 10.0 Python 1df.tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; popularity grammer 3 4.0 GO 4 5.0 R 5 6.0 SQL 6 7.0 PHP 7 10.0 Python 1df.drop(index=len(df)-1, inplace=True) 12row = &#123;&#x27;popularity&#x27;: 6.6, &#x27;grammer&#x27; : &#x27;Perl&#x27;&#125;df = df.append(row, ignore_index=True) 1df = df.sort_values(&quot;popularity&quot;) 1df[&#x27;grammer&#x27;].map(lambda x: len(x)) 0 6 1 1 2 4 3 2 4 1 5 3 7 4 6 3 Name: grammer, dtype: int64 PART 02 1df = pd.read_excel(&quot;pandas120.xlsx&quot;) 1df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 2020-03-16 11:30:18 本科 20k-35k 1 2020-03-16 10:58:48 本科 20k-40k 2 2020-03-16 10:46:39 不限 20k-35k 3 2020-03-16 10:45:44 本科 13k-20k 4 2020-03-16 10:20:41 本科 10k-20k 1234def cal_ave(x): min, max = x.split(&#x27;-&#x27;) return int((int(min[:-1])+int(max[:-1]))/2*1000)df[&#x27;salary&#x27;] = df[&#x27;salary&#x27;].apply(cal_ave) 1df.groupby(df[&#x27;education&#x27;]).mean() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; salary education 不限 19600.000000 大专 10000.000000 本科 19361.344538 硕士 20642.857143 1df[&#x27;createTime&#x27;] = df[&#x27;createTime&#x27;].map(lambda x : x.strftime(&#x27;%m-%d&#x27;)) 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 135 entries, 0 to 134 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 createTime 135 non-null object 1 education 135 non-null object 2 salary 135 non-null int64 dtypes: int64(1), object(2) memory usage: 3.3+ KB 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; salary count 135.000000 mean 19159.259259 std 8661.686922 min 3500.000000 25% 14000.000000 50% 17500.000000 75% 25000.000000 max 45000.000000 123bins = [0,5000, 20000, 50000]group_names = [&#x27;低&#x27;, &#x27;中&#x27;, &#x27;高&#x27;]df[&#x27;categories&#x27;] = pd.cut(df[&#x27;salary&#x27;], bins, labels=group_names) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary categories 0 03-16 本科 27500 高 1 03-16 本科 30000 高 2 03-16 不限 27500 高 3 03-16 本科 16500 中 4 03-16 本科 15000 中 ... ... ... ... ... 130 03-16 本科 14000 中 131 03-16 硕士 37500 高 132 03-16 本科 30000 高 133 03-16 本科 19000 中 134 03-16 本科 30000 高 135 rows × 4 columns 1df.sort_values(&#x27;salary&#x27;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary categories 53 03-16 本科 45000 高 37 03-16 本科 40000 高 101 03-16 本科 37500 高 16 03-16 本科 37500 高 131 03-16 硕士 37500 高 ... ... ... ... ... 123 03-16 本科 4500 低 126 03-16 本科 4000 低 110 03-16 本科 4000 低 96 03-16 不限 3500 低 113 03-16 本科 3500 低 135 rows × 4 columns 1df.loc[32] createTime 03-16 education 硕士 salary 22500 categories 高 Name: 32, dtype: object 1np.median(df[&#x27;salary&#x27;]) 17500.0 1df.salary.plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel='Frequency'&gt; 12# KED Kernel Density Estimationdf.salary.plot(kind=&#x27;kde&#x27;) &lt;AxesSubplot:ylabel='Density'&gt; 1del df[&#x27;categories&#x27;] 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 03-16 本科 27500 1 03-16 本科 30000 2 03-16 不限 27500 3 03-16 本科 16500 4 03-16 本科 15000 ... ... ... ... 130 03-16 本科 14000 131 03-16 硕士 37500 132 03-16 本科 30000 133 03-16 本科 19000 134 03-16 本科 30000 135 rows × 3 columns 1df.salary.max() - df.salary.min() 41500 1pd.concat([df[:1], df[-2:-1]]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 03-16 本科 27500 133 03-16 本科 19000 1df.append(df.loc[7]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary 0 03-16 本科 27500 1 03-16 本科 30000 2 03-16 不限 27500 3 03-16 本科 16500 4 03-16 本科 15000 ... ... ... ... 131 03-16 硕士 37500 132 03-16 本科 30000 133 03-16 本科 19000 134 03-16 本科 30000 7 03-16 本科 12500 136 rows × 3 columns 1df.set_index(&quot;createTime&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; education salary createTime 03-16 本科 27500 03-16 本科 30000 03-16 不限 27500 03-16 本科 16500 03-16 本科 15000 ... ... ... 03-16 本科 14000 03-16 硕士 37500 03-16 本科 30000 03-16 本科 19000 03-16 本科 30000 135 rows × 2 columns 1df_r = pd.DataFrame(np.random.randint(1, 10, 135), columns=[&#x27;random&#x27;]) 1df = pd.concat([df, df_r], axis=1) 1df[&#x27;sub&#x27;] = df.salary - df.random 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary random sub 0 03-16 本科 27500 2 27498 1 03-16 本科 30000 6 29994 2 03-16 不限 27500 5 27495 3 03-16 本科 16500 3 16497 4 03-16 本科 15000 8 14992 ... ... ... ... ... ... 130 03-16 本科 14000 1 13999 131 03-16 硕士 37500 5 37495 132 03-16 本科 30000 8 29992 133 03-16 本科 19000 7 18993 134 03-16 本科 30000 6 29994 135 rows × 5 columns 1df.isna().sum() createTime 0 education 0 salary 0 random 0 sub 0 dtype: int64 1df.isna().values.any() False 1df.salary.astype(float) 0 27500.0 1 30000.0 2 27500.0 3 16500.0 4 15000.0 ... 130 14000.0 131 37500.0 132 30000.0 133 19000.0 134 30000.0 Name: salary, Length: 135, dtype: float64 1len(df[df.salary &gt; 10000].salary) 119 12df.education.value_counts() 本科 119 硕士 7 不限 5 大专 4 Name: education, dtype: int64 1df.education.unique() array(['本科', '不限', '硕士', '大专'], dtype=object) 1df.education.nunique() 4 1df[(df.salary + df[&#x27;sub&#x27;]) &gt; 60000].tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; createTime education salary random sub 92 03-16 本科 35000 6 34994 101 03-16 本科 37500 4 37496 131 03-16 硕士 37500 5 37495 金融数据处理 1df = pd.read_excel(&#x27;./600000.SH.xls&#x27;) WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero 1df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 0 600000.SH 浦发银行 2016-01-04 16.1356 16.1444 16.1444 15.4997 15.7205 42240610 754425783 -0.4151 -2.5725 17.8602 0.2264 3.320318e+11 3.320318e+11 1.865347e+10 6.5614 1 600000.SH 浦发银行 2016-01-05 15.7205 15.4644 15.9501 15.3672 15.8618 58054793 1034181474 0.1413 0.8989 17.8139 0.3112 3.350163e+11 3.350163e+11 1.865347e+10 6.6204 2 600000.SH 浦发银行 2016-01-06 15.8618 15.8088 16.0208 15.6234 15.9855 46772653 838667398 0.1236 0.7795 17.9307 0.2507 3.376278e+11 3.376278e+11 1.865347e+10 6.6720 1df.isna().sum() 代码 1 简称 2 日期 2 前收盘价(元) 2 开盘价(元) 2 最高价(元) 2 最低价(元) 2 收盘价(元) 2 成交量(股) 2 成交金额(元) 2 涨跌(元) 2 涨跌幅(%) 2 均价(元) 2 换手率(%) 2 A股流通市值(元) 2 总市值(元) 2 A股流通股本(股) 2 市盈率 2 dtype: int64 1df[df[&#x27;日期&#x27;].isna()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 327 NaN NaN NaT NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 328 数据来源：Wind资讯 NaN NaT NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 12345data = dffor columname in data.columns: if data[columname].count() != len(data): loc = data[columname][data[columname].isnull().values==True].index.tolist() print(&#x27;列名：&quot;&#123;&#125;&quot;, 第&#123;&#125;行位置有缺失值&#x27;.format(columname,loc)) 列名：&quot;代码&quot;, 第[327]行位置有缺失值 列名：&quot;简称&quot;, 第[327, 328]行位置有缺失值 列名：&quot;日期&quot;, 第[327, 328]行位置有缺失值 列名：&quot;前收盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;开盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;最高价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;最低价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;收盘价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;成交量(股)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;成交金额(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;涨跌(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;涨跌幅(%)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;均价(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;换手率(%)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;A股流通市值(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;总市值(元)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;A股流通股本(股)&quot;, 第[327, 328]行位置有缺失值 列名：&quot;市盈率&quot;, 第[327, 328]行位置有缺失值 1df.dropna(axis=0, how=&#x27;any&#x27;, inplace=True) 12# df = df.set_index(&#x27;日期&#x27;)df[&#x27;收盘价(元)&#x27;].plot(kind=&#x27;line&#x27;) &lt;AxesSubplot:&gt; 1df[[&#x27;收盘价(元)&#x27;, &#x27;开盘价(元)&#x27;]].plot(kind=&#x27;line&#x27;) &lt;AxesSubplot:&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 25910 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 25910 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0, flags=flags) 1df[&#x27;涨跌幅(%)&#x27;].plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel='Frequency'&gt; 1df[&#x27;涨跌幅(%)&#x27;].plot(kind=&#x27;hist&#x27;, bins=30) &lt;AxesSubplot:ylabel='Frequency'&gt; 12345temp = pd.DataFrame(columns = data.columns.to_list())for i in range(len(data)): if type(data.iloc[i,13]) != float: temp = temp.append(data.loc[i])temp.head(1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 代码 简称 日期 前收盘价(元) 开盘价(元) 最高价(元) 最低价(元) 收盘价(元) 成交量(股) 成交金额(元) 涨跌(元) 涨跌幅(%) 均价(元) 换手率(%) A股流通市值(元) 总市值(元) A股流通股本(股) 市盈率 26 600000.SH 浦发银行 2016-02-16 16.2946 16.2946 16.2946 16.2946 16.2946 -- -- 0.0 0.0 -- -- 3.441565e+11 3.441565e+11 1.865347e+10 6.801 1i = df[df[&#x27;换手率(%)&#x27;].isin([&#x27;--&#x27;])].index 1i Int64Index([26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], dtype='int64') 1df = df.drop(index=i) 1df[&#x27;换手率(%)&#x27;].plot(kind=&#x27;kde&#x27;) &lt;AxesSubplot:ylabel='Density'&gt; 1df[&#x27;收盘价(元)&#x27;].diff() 0 NaN 1 0.1413 2 0.1237 3 -0.5211 4 -0.0177 ... 322 -0.0800 323 -0.1000 324 -0.0600 325 -0.0600 326 -0.1000 Name: 收盘价(元), Length: 309, dtype: float64 1df[&#x27;收盘价(元)&#x27;] - df[&#x27;收盘价(元)&#x27;].shift(1) 0 NaN 1 0.1413 2 0.1237 3 -0.5211 4 -0.0177 ... 322 -0.0800 323 -0.1000 324 -0.0600 325 -0.0600 326 -0.1000 Name: 收盘价(元), Length: 309, dtype: float64 1df[&#x27;收盘价(元)&#x27;].pct_change() 0 NaN 1 0.008988 2 0.007799 3 -0.032598 4 -0.001145 ... 322 -0.005277 323 -0.006631 324 -0.004005 325 -0.004021 326 -0.006729 Name: 收盘价(元), Length: 309, dtype: float64 1df = data.set_index(&#x27;日期&#x27;) 1df[&#x27;收盘价(元)&#x27;].rolling(5).mean() 日期 2016-01-04 NaN 2016-01-05 NaN 2016-01-06 NaN 2016-01-07 NaN 2016-01-08 15.69578 ... 2017-05-03 15.14200 2017-05-04 15.12800 2017-05-05 15.07000 2017-05-08 15.00000 2017-05-09 14.92000 Name: 收盘价(元), Length: 327, dtype: float64 1df[&#x27;收盘价(元)&#x27;].rolling(5).sum() 日期 2016-01-04 NaN 2016-01-05 NaN 2016-01-06 NaN 2016-01-07 NaN 2016-01-08 78.4789 ... 2017-05-03 75.7100 2017-05-04 75.6400 2017-05-05 75.3500 2017-05-08 75.0000 2017-05-09 74.6000 Name: 收盘价(元), Length: 327, dtype: float64 1234temp = pd.DataFrame()temp[&#x27;m5&#x27;] = df[&#x27;收盘价(元)&#x27;].rolling(5).mean()temp[&#x27;m20&#x27;] = df[&#x27;收盘价(元)&#x27;].rolling(20).mean()temp.plot() &lt;AxesSubplot:xlabel='日期'&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) 1df[&#x27;收盘价(元)&#x27;].resample(rule=&#x27;W&#x27;).max() 日期 2016-01-10 15.9855 2016-01-17 15.8265 2016-01-24 15.6940 2016-01-31 15.0405 2016-02-07 16.2328 ... 2017-04-16 15.9700 2017-04-23 15.5600 2017-04-30 15.2100 2017-05-07 15.1600 2017-05-14 14.8600 Freq: W-SUN, Name: 收盘价(元), Length: 71, dtype: float64 12df[&#x27;收盘价(元)&#x27;].plot()df[&#x27;收盘价(元)&#x27;].resample(&#x27;7D&#x27;).max().plot() &lt;AxesSubplot:xlabel='日期'&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) 1data[&#x27;开盘价(元)&#x27;].expanding(min_periods=1).mean() 0 16.144400 1 15.804400 2 15.805867 3 15.784525 4 15.761120 ... 322 16.055594 323 16.052552 324 16.049159 325 16.045266 326 16.041122 Name: 开盘价(元), Length: 327, dtype: float64 12df[&#x27;expanding Open mean&#x27;]=df[&#x27;开盘价(元)&#x27;].expanding(min_periods=1).mean()df[[&#x27;开盘价(元)&#x27;, &#x27;expanding Open mean&#x27;]].plot(figsize=(16, 6)) &lt;AxesSubplot:xlabel='日期'&gt; /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0.0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26085 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 26399 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 24320 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 30424 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20215 missing from current font. font.set_text(s, 0, flags=flags) /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20803 missing from current font. font.set_text(s, 0, flags=flags) pandas &amp; numpy 123456# 随机df1 = pd.DataFrame(np.random.randint(0, 100, 20))# 等步长df2 = pd.DataFrame(np.arange(0, 100, 5))# 正态分布df3 = pd.DataFrame(np.random.normal(0, 1, 20)) 1df3.plot(kind=&#x27;hist&#x27;) &lt;AxesSubplot:ylabel='Frequency'&gt; 1df = pd.concat([df1,df2,df3],ignore_index=True) 1df = pd.concat([df1,df2,df3], axis=1, ignore_index=True) 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 1 2 0 25 0 0.573071 1 86 5 -0.474405 2 34 10 -0.910378 3 18 15 0.713983 4 9 20 0.732289 5 67 25 -2.087864 6 5 30 -1.126019 7 51 35 -1.512201 8 3 40 -0.697655 9 10 45 -1.082615 10 59 50 -0.809815 11 28 55 -0.158042 12 21 60 -1.753240 13 96 65 0.687153 14 77 70 -1.534696 15 25 75 1.169113 16 26 80 -0.015986 17 52 85 0.517278 18 60 90 1.045848 19 35 95 0.856043 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 1 2 count 20.000000 20.000000 20.000000 mean 39.350000 47.500000 -0.293407 std 27.642978 29.580399 1.034218 min 3.000000 0.000000 -2.087864 25% 20.250000 23.750000 -1.093466 50% 31.000000 47.500000 -0.316223 75% 59.250000 71.250000 0.693860 max 96.000000 95.000000 1.169113 1df.columns = [&#x27;col1&#x27;, &#x27;col2&#x27;, &#x27;col3&#x27;] 1df[&#x27;col1&#x27;][~df[&#x27;col1&#x27;].isin(df[&#x27;col2&#x27;])] 1 86 2 34 3 18 4 9 5 67 7 51 8 3 10 59 11 28 12 21 13 96 14 77 16 26 17 52 Name: col1, dtype: int64 1df[&#x27;col1&#x27;].append(df[&#x27;col2&#x27;]).value_counts().head(3) 25 3 60 2 35 2 dtype: int64 1np.where(df[&#x27;col1&#x27;] % 5==0) (array([ 0, 6, 9, 15, 18, 19]),) 1df[df.columns[::-1]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; col3 col2 col1 0 0.573071 0 25 1 -0.474405 5 86 2 -0.910378 10 34 3 0.713983 15 18 4 0.732289 20 9 5 -2.087864 25 67 6 -1.126019 30 5 7 -1.512201 35 51 8 -0.697655 40 3 9 -1.082615 45 10 10 -0.809815 50 59 11 -0.158042 55 28 12 -1.753240 60 21 13 0.687153 65 96 14 -1.534696 70 77 15 1.169113 75 25 16 -0.015986 80 26 17 0.517278 85 52 18 1.045848 90 60 19 0.856043 95 35 1df[&#x27;col1&#x27;].take([1,10,15]) 1 86 10 59 15 25 Name: col1, dtype: int64 12tem = np.diff(np.sign(np.diff(df[&#x27;col1&#x27;])))np.where(tem == -2)[0] + 1 array([ 1, 5, 7, 10, 13, 18]) 1df.mean(axis=1) 0 8.524357 1 30.175198 2 14.363207 3 11.237994 4 9.910763 5 29.970712 6 11.291327 7 28.162600 8 14.100782 9 17.972462 10 36.063395 11 27.613986 12 26.415587 13 53.895718 14 48.488435 15 33.723038 16 35.328005 17 45.839093 18 50.348616 19 43.618681 dtype: float64 1np.convolve(df[&#x27;col2&#x27;], np.ones(3)/3, mode=&#x27;valid&#x27;) array([ 5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60., 65., 70., 75., 80., 85., 90.]) 1df[&#x27;col2&#x27;].rolling(window=3).mean() 0 NaN 1 NaN 2 5.0 3 10.0 4 15.0 5 20.0 6 25.0 7 30.0 8 35.0 9 40.0 10 45.0 11 50.0 12 55.0 13 60.0 14 65.0 15 70.0 16 75.0 17 80.0 18 85.0 19 90.0 Name: col2, dtype: float64 1df.sort_values(&quot;col3&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; col1 col2 col3 5 67 25 -2.087864 12 21 60 -1.753240 14 77 70 -1.534696 7 51 35 -1.512201 6 5 30 -1.126019 9 10 45 -1.082615 2 34 10 -0.910378 10 59 50 -0.809815 8 3 40 -0.697655 1 86 5 -0.474405 11 28 55 -0.158042 16 26 80 -0.015986 17 52 85 0.517278 0 25 0 0.573071 13 96 65 0.687153 3 18 15 0.713983 4 9 20 0.732289 19 35 95 0.856043 18 60 90 1.045848 15 25 75 1.169113 1df.col1[df[&#x27;col1&#x27;] &gt; 50]= &#x27;高&#x27; &lt;ipython-input-97-d0c96ec6289a&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://img/pandas.pydata.org/img/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df.col1[df['col1'] &gt; 50]= '高' 1np.linalg.norm(df[&#x27;col2&#x27;]-df[&#x27;col3&#x27;]) 248.99392792632952 123456789df1= pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],&#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K0&#x27;, &#x27;K1&#x27;],&#x27;A&#x27;: [&#x27;A0&#x27;, &#x27;A1&#x27;, &#x27;A2&#x27;, &#x27;A3&#x27;],&#x27;B&#x27;: [&#x27;B0&#x27;, &#x27;B1&#x27;, &#x27;B2&#x27;, &#x27;B3&#x27;]&#125;)df2= pd.DataFrame(&#123;&#x27;key1&#x27;: [&#x27;K0&#x27;, &#x27;K1&#x27;, &#x27;K1&#x27;, &#x27;K2&#x27;],&#x27;key2&#x27;: [&#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;, &#x27;K0&#x27;],&#x27;C&#x27;: [&#x27;C0&#x27;, &#x27;C1&#x27;, &#x27;C2&#x27;, &#x27;C3&#x27;],&#x27;D&#x27;: [&#x27;D0&#x27;, &#x27;D1&#x27;, &#x27;D2&#x27;, &#x27;D3&#x27;]&#125;) 1pd.merge(df1, df2, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 1pd.merge(df1, df2, how=&#x27;inner&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K1 K0 A2 B2 C1 D1 2 K1 K0 A2 B2 C2 D2 1pd.merge(df1, df2, how=&#x27;left&#x27;, on=[&#x27;key1&#x27;, &#x27;key2&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key1 key2 A B C D 0 K0 K0 A0 B0 C0 D0 1 K0 K1 A1 B1 NaN NaN 2 K1 K0 A2 B2 C1 D1 3 K1 K0 A2 B2 C2 D2 4 K2 K1 A3 B3 NaN NaN 123df = pd.DataFrame(&#123;0: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0), 1: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0), 2: pd.Series(np.random.random_sample(100)).map(lambda x: 1 if x&gt;=0.5 else 0)&#125;) 1na = np.array(df) 1np.argwhere(na==1) array([[ 0, 0], [ 1, 1], [ 1, 2], [ 3, 0], [ 3, 1], [ 3, 2], [ 4, 2], [ 5, 0], [ 5, 1], [ 6, 0], [ 6, 2], [ 7, 0], [ 8, 0], [ 8, 2], [ 9, 2], [10, 2], [11, 2], [12, 0], [12, 1], [14, 0], [14, 1], [15, 0], [15, 2], [17, 0], [17, 2], [18, 2], [19, 0], [19, 2], [21, 1], [21, 2], [22, 0], [22, 1], [23, 2], [24, 0], [24, 1], [25, 0], [28, 1], [29, 1], [30, 0], [30, 1], [31, 2], [32, 0], [33, 0], [33, 1], [33, 2], [34, 0], [35, 0], [36, 1], [36, 2], [37, 1], [37, 2], [38, 0], [38, 1], [39, 0], [39, 1], [40, 0], [41, 1], [42, 1], [43, 0], [43, 1], [43, 2], [44, 2], [45, 2], [47, 1], [47, 2], [48, 1], [49, 0], [49, 1], [50, 0], [50, 1], [51, 2], [52, 2], [53, 1], [54, 0], [55, 0], [55, 2], [56, 0], [57, 2], [58, 0], [58, 1], [58, 2], [59, 2], [61, 1], [61, 2], [62, 1], [63, 0], [64, 1], [65, 0], [65, 1], [66, 0], [66, 1], [66, 2], [67, 2], [68, 0], [68, 1], [68, 2], [69, 0], [69, 2], [70, 2], [71, 0], [71, 1], [71, 2], [72, 1], [73, 1], [74, 2], [76, 1], [76, 2], [77, 1], [77, 2], [78, 1], [78, 2], [79, 0], [79, 2], [80, 1], [81, 0], [81, 1], [81, 2], [83, 1], [84, 0], [84, 1], [84, 2], [85, 0], [86, 0], [86, 1], [86, 2], [87, 2], [89, 0], [90, 0], [90, 2], [91, 1], [91, 2], [92, 1], [93, 0], [96, 0], [96, 2], [98, 0], [98, 1]]) 1pd.pivot_table(df,values=[0,1],index=0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 1 2 0 0 0.418182 0.490909 1 0.488889 0.444444 1df = pd.read_csv(&quot;./数据.csv&quot;, encoding=&#x27;gbk&#x27;) 1!file -i 数据.csv 数据.csv: regular file 1!yum install enca zsh:1: command not found: yum 1df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; positionId positionName companyId companyLogo companySize industryField financeStage companyLabelList firstType secondType ... plus pcShow appShow deliver gradeDescription promotionScoreExplain isHotHire count aggregatePositionIds famousCompany 0 6802721 数据分析 475770 i/image2/M01/B7/3E/CgoB5lwPfEaAdn8WAABWQ0Jgl5s... 50-150人 移动互联网,电商 A轮 ['绩效奖金', '带薪年假', '定期体检', '弹性工作'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 1 5204912 数据建模 50735 image1/M00/00/85/CgYXBlTUXeeAR0IjAABbroUk-dw97... 150-500人 电商 B轮 ['年终奖金', '做五休二', '六险一金', '子女福利'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] False 2 6877668 数据分析 100125 image2/M00/0C/57/CgqLKVYcOA2ADcFuAAAE8MukIKA74... 2000人以上 移动互联网,企业服务 上市公司 ['节日礼物', '年底双薪', '股票期权', '带薪年假'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 3 6496141 数据分析 26564 i/image2/M01/F7/3F/CgoB5lyGAQGAZeI-AAAdOqXecnw... 500-2000人 电商 D轮及以上 ['生日趴', '每月腐败基金', '每月补贴', '年度旅游'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] True 4 6467417 数据分析 29211 i/image2/M01/77/B8/CgoB5l1WDyGATNP5AAAlY3h88SY... 2000人以上 物流丨运输 上市公司 ['技能培训', '免费班车', '专项奖金', '岗位晋升'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 100 6884346 数据分析师 21236 i/image/M00/43/F6/CgqKkVeEh76AUVPoAAA2Bj747wU6... 500-2000人 移动互联网,医疗丨健康 C轮 ['技能培训', '年底双薪', '节日礼物', '绩效奖金'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] False 101 6849100 商业数据分析 72076 i/image2/M01/92/A4/CgotOV2LPUmAR_8dAAB_DlDMiXA... 500-2000人 移动互联网,电商 C轮 ['节日礼物', '股票期权', '带薪年假', '年度旅游'] 市场|商务类 市场|营销 ... NaN 0 0 0 NaN NaN 0 0 [] False 102 6803432 奔驰·耀出行-BI数据分析专家 751158 i/image3/M01/64/93/Cgq2xl48z2mAeYRoAAD6Qf_Jeq8... 150-500人 移动互联网 不需要融资 [] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] False 103 6704835 BI数据分析师 52840 i/image2/M00/26/CA/CgoB5lofsguAfk9ZAACoL3r4p24... 2000人以上 电商 上市公司 ['技能培训', '年底双薪', '节日礼物', '绩效奖金'] 开发|测试|运维类 数据开发 ... NaN 0 0 0 NaN NaN 0 0 [] True 104 6728058 数据分析专家-LQ(J181203029) 2474 i/image2/M01/14/4D/CgoB5lyq5fqAAHHzAAAa148hbk8... 2000人以上 汽车丨出行 不需要融资 ['弹性工作', '节日礼物', '岗位晋升', '技能培训'] 产品|需求|项目类 数据分析 ... NaN 0 0 0 NaN NaN 0 0 [] True 105 rows × 53 columns 1pd.pivot_table(df,values=[&quot;companyId&quot;,&quot;salary&quot;,&quot;score&quot;],index=&quot;positionId&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; companyId salary score positionId 5203054 329 30000 4.0 5204912 50735 15000 176.0 5269002 50576 37500 1.0 5453691 166666 30000 4.0 5519962 50735 37500 14.0 ... ... ... ... 6882983 7461 27500 15.0 6884346 21236 25000 0.0 6886661 321001 37500 5.0 6888169 751158 42500 1.0 6896403 285786 30000 3.0 95 rows × 3 columns 1pip install nbconvert pandoc Requirement already satisfied: nbconvert in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (6.0.7) Collecting pandoc Downloading pandoc-2.3.tar.gz (33 kB) Requirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.5.1) Requirement already satisfied: traitlets&gt;=4.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (5.0.5) Requirement already satisfied: nbformat&gt;=4.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (5.0.8) Requirement already satisfied: pygments&gt;=2.4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (2.7.2) Requirement already satisfied: defusedxml in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.6.0) Requirement already satisfied: jupyter-core in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (4.6.3) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (1.4.3) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.8.4) Requirement already satisfied: jupyterlab-pygments in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.1.2) Requirement already satisfied: testpath in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.4.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (0.3) Requirement already satisfied: bleach in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (3.2.1) Requirement already satisfied: jinja2&gt;=2.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbconvert) (2.11.2) Collecting plumbum Downloading plumbum-1.8.2-py3-none-any.whl (127 kB) \u001b[K |████████████████████████████████| 127 kB 14 kB/s eta 0:00:01 \u001b[?25hRequirement already satisfied: ply in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from pandoc) (3.11) Requirement already satisfied: async-generator in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (1.10) Requirement already satisfied: jupyter-client&gt;=6.1.5 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (6.1.7) Requirement already satisfied: nest-asyncio in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (1.4.2) Requirement already satisfied: ipython-genutils in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from traitlets&gt;=4.2-&gt;nbconvert) (0.2.0) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from nbformat&gt;=4.4-&gt;nbconvert) (3.2.0) Requirement already satisfied: webencodings in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (0.5.1) Requirement already satisfied: six&gt;=1.9.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (1.15.0) Requirement already satisfied: packaging in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from bleach-&gt;nbconvert) (20.4) Requirement already satisfied: MarkupSafe&gt;=0.23 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jinja2&gt;=2.4-&gt;nbconvert) (1.1.1) Requirement already satisfied: python-dateutil&gt;=2.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (2.8.1) Requirement already satisfied: pyzmq&gt;=13 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (19.0.2) Requirement already satisfied: tornado&gt;=4.1 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client&gt;=6.1.5-&gt;nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert) (6.0.4) Requirement already satisfied: pyrsistent&gt;=0.14.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (0.17.3) Requirement already satisfied: setuptools in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (50.3.1.post20201107) Requirement already satisfied: attrs&gt;=17.4.0 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.4-&gt;nbconvert) (20.3.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /Users/wingo.wen/opt/anaconda3/lib/python3.8/site-packages (from packaging-&gt;bleach-&gt;nbconvert) (2.4.7) Building wheels for collected packages: pandoc Building wheel for pandoc (setup.py) ... \u001b[?25ldone \u001b[?25h Created wheel for pandoc: filename=pandoc-2.3-py3-none-any.whl size=33271 sha256=22e3e077eb44a4ba73332321d2656d5d67e118abef8947040a2719d096f363f4 Stored in directory: /Users/wingo.wen/Library/Caches/pip/wheels/90/3a/a8/3237a93e3a6261bd24edabf3277ca59f64c1710b3d8c7c72a0 Successfully built pandoc Installing collected packages: plumbum, pandoc Successfully installed pandoc-2.3 plumbum-1.8.2 Note: you may need to restart the kernel to use updated packages. 1!jupyter nbconvert --to markdown pandas.ipynb","categories":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/categories/Python/"}],"tags":[{"name":"Pandas","slug":"Pandas","permalink":"https://wingowen.github.io/tags/Pandas/"}]},{"title":"SQL 练习","slug":"数据库/SQL-练习","date":"2023-09-19T07:03:55.000Z","updated":"2023-09-19T07:25:20.863Z","comments":true,"path":"2023/09/19/数据库/SQL-练习/","link":"","permalink":"https://wingowen.github.io/2023/09/19/%E6%95%B0%E6%8D%AE%E5%BA%93/SQL-%E7%BB%83%E4%B9%A0/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381CREATE TABLE `course` ( `c_id` text, `c_name` text, `t_id` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `score` ( `s_id` text, `c_id` text, `s_score` int(11) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `student` ( `s_id` text, `s_name` text, `s_birth` text, `s_sex` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `teacher` ( `t_id` text, `t_name` text) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `course` (`c_id`, `c_name`, `t_id`) VALUES(&#x27;02&#x27;, &#x27;数学&#x27;, &#x27;01&#x27;),(&#x27;03&#x27;, &#x27;英语&#x27;, &#x27;03&#x27;),(&#x27;01&#x27;, &#x27;语文&#x27;, &#x27;02&#x27;);INSERT INTO `score` (`s_id`, `c_id`, `s_score`) VALUES(&#x27;01&#x27;, &#x27;02&#x27;, &#x27;90&#x27;),(&#x27;01&#x27;, &#x27;03&#x27;, &#x27;99&#x27;),(&#x27;02&#x27;, &#x27;01&#x27;, &#x27;70&#x27;),(&#x27;02&#x27;, &#x27;02&#x27;, &#x27;60&#x27;),(&#x27;02&#x27;, &#x27;03&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;01&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;02&#x27;, &#x27;80&#x27;),(&#x27;03&#x27;, &#x27;03&#x27;, &#x27;80&#x27;),(&#x27;04&#x27;, &#x27;01&#x27;, &#x27;50&#x27;),(&#x27;04&#x27;, &#x27;02&#x27;, &#x27;30&#x27;),(&#x27;04&#x27;, &#x27;03&#x27;, &#x27;20&#x27;),(&#x27;05&#x27;, &#x27;01&#x27;, &#x27;76&#x27;),(&#x27;05&#x27;, &#x27;02&#x27;, &#x27;87&#x27;),(&#x27;06&#x27;, &#x27;01&#x27;, &#x27;31&#x27;),(&#x27;06&#x27;, &#x27;03&#x27;, &#x27;34&#x27;),(&#x27;07&#x27;, &#x27;02&#x27;, &#x27;89&#x27;),(&#x27;07&#x27;, &#x27;03&#x27;, &#x27;98&#x27;),(&#x27;01&#x27;, &#x27;01&#x27;, &#x27;80&#x27;);INSERT INTO `student` (`s_id`, `s_name`, `s_birth`, `s_sex`) VALUES(&#x27;02&#x27;, &#x27;钱电&#x27;, &#x27;1990-12-21&#x27;, &#x27;男&#x27;),(&#x27;03&#x27;, &#x27;孙风&#x27;, &#x27;1990-05-20&#x27;, &#x27;男&#x27;),(&#x27;04&#x27;, &#x27;李云&#x27;, &#x27;1990-08-06&#x27;, &#x27;男&#x27;),(&#x27;05&#x27;, &#x27;周梅&#x27;, &#x27;1991-12-01&#x27;, &#x27;女&#x27;),(&#x27;06&#x27;, &#x27;吴兰&#x27;, &#x27;1992-03-01&#x27;, &#x27;女&#x27;),(&#x27;07&#x27;, &#x27;郑竹&#x27;, &#x27;1989-07-01&#x27;, &#x27;女&#x27;),(&#x27;08&#x27;, &#x27;王菊&#x27;, &#x27;1990-01-20&#x27;, &#x27;女&#x27;),(&#x27;01&#x27;, &#x27;赵雷&#x27;, &#x27;1990-01-01&#x27;, &#x27;男&#x27;);INSERT INTO `teacher` (`t_id`, `t_name`) VALUES(&#x27;02&#x27;, &#x27;李四&#x27;),(&#x27;03&#x27;, &#x27;王五&#x27;),(&#x27;01&#x27;, &#x27;张三&#x27;);# 查询&quot;01&quot;课程比&quot;02&quot;课程成绩高的学生的信息及课程分数select *from student st left join score sc01 on st.s_id = sc01.s_id and sc01.c_id = 01 left join score sc02 on st.s_id = sc02.s_id and sc02.c_id = 02where sc01.s_score &gt; sc02.s_score;# 查询平均成绩大于等于60分的同学的学生编号和学生姓名和平均成绩select *from student st left join ( select s_id,avg(s_score) as ms from score group by s_id ) st1 on st.s_id=st1.s_id WHERE st1.ms &gt; 60;# 查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩select *from student st left join ( SELECT s_id, count(1) as class_num, sum(s_score) score_sum FROM score group by s_id ) c on st.s_id = c.s_id;# 查询&quot;李&quot;姓老师的数量select count(1) from teacher where t_name like &quot;李%&quot;;# 查询学过&quot;张三&quot;老师授课的同学的信息select * FROM student stleft JOIN score sc on st.s_id = sc.s_idleft JOIN course c on c.c_id = sc.c_idleft JOIN teacher t on c.t_id = t.t_idwhere t.t_name=&quot;张三&quot;;# 查询学过编号为&quot;01&quot;并且也学过编号为&quot;02&quot;的课程的同学的信息select * FROM student stleft JOIN score sc on st.s_id = sc.s_idleft JOIN course c on c.c_id = sc.c_idwhere c.c_id=&#x27;01&#x27; or c.c_id=&#x27;02&#x27;;# 查询没有学全所有课程的同学的信息SELECT * FROM student st WHERE st.s_id in (select st.s_id as c_numFROM student st left JOIN score sc on st.s_id = sc.s_idGROUP BY st.s_idhaving count(sc.c_id) &lt; 3);select student.* from studentjoin (select count(c_id)num1 from course)tmp1left join( select s_id,count(c_id)num2 from score group by s_id)tmp2on student.s_id=tmp2.s_id and tmp1.num1=tmp2.num2where tmp2.s_id is null;# 查询至少有一门课与学号为&quot;01&quot;的同学所学相同的同学的信息select st.* from student stleft join score sc on sc.s_id = st.s_idwhere sc.c_id in (select c_id from score where s_id = 01)and st.s_id!=01group by st.s_id,s_name,s_birth,s_sex;select student.* from studentjoin (select c_id from score where score.s_id=01)tmp1join (select s_id,c_id from score)tmp2 on tmp1.c_id =tmp2.c_id and student.s_id =tmp2.s_idwhere student.s_id not in(&#x27;01&#x27;)group by student.s_id,s_name,s_birth,s_sex;select * from studentjoin (select c_id from score where score.s_id=01)tmp1join (select s_id,c_id from score)tmp2 on tmp1.c_id =tmp2.c_id and student.s_id =tmp2.s_id;# 查询和&quot;01&quot;号的同学学习的课程完全相同的其他同学的信息:# hive不支持group_concat方法,可用 concat_ws(’|’, collect_set(str)) 实现select * FROM(select st.*, GROUP_CONCAT(tmp.c_id SEPARATOR &#x27;;&#x27;) as all_c_id from student stjoin (select s_id,c_id from score)tmpon st.s_id =tmp.s_idWHERE st.s_id != 01group by st.s_id,s_name,s_birth,s_sex) aleft join (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01 group by score.s_id) tmp2on a.s_id = a.s_idwhere a.all_c_id = tmp2.all_c_id; select student.*,tmp1.course_id from studentjoin (select s_id ,GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id from score group by s_id having s_id not in (1))tmp1 on student.s_id = tmp1.s_idjoin (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id2 from score where s_id=1)tmp2 on tmp1.course_id = tmp2.course_id2;select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) course_id2 from score where s_id=1;select * FROM(select st.*, GROUP_CONCAT(tmp.c_id SEPARATOR &#x27;;&#x27;) as all_c_id from student stjoin (select s_id,c_id from score)tmpon st.s_id =tmp.s_idWHERE st.s_id != 01group by st.s_id,s_name,s_birth,s_sex) aleft join (select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01 group by score.s_id) tmp2on a.s_id = a.s_idwhere a.all_c_id = tmp2.all_c_id;select GROUP_CONCAT(c_id SEPARATOR &#x27;;&#x27;) as all_c_id, s_id from score where score.s_id=01;# 查询没学过&quot;张三&quot;老师讲授的任一门课程的学生姓名SELECT st.* FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idLEFT JOIN course c ON c.c_id = sc.c_idLEFT JOIN teacher t ON t.t_id = c.t_idWHERE t.t_name=&#x27;张三&#x27;;# 查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩SELECT st.s_id FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idWHERE sc.s_score &lt; 60;SELECT st.s_id, st.s_name, round(avg(sc.s_score)) FROM student stLEFT JOIN score sc ON sc.s_id = st.s_idWHERE sc.s_score &lt; 60GROUP BY s_id, s_nameHAVING count(sc.s_score) &gt;= 2;select student.s_id,student.s_name,tmp.avg_score from studentinner join (select s_id from score where s_score&lt;60 group by score.s_id having count(s_id)&gt;1)tmp2on student.s_id = tmp2.s_idleft join ( select s_id,round(AVG (score.s_score)) avg_score from score group by s_id)tmp on tmp.s_id=student.s_id;# 检索&quot;01&quot;课程分数小于60，按分数降序排列的学生信息:SELECT st.*, sc.s_score from student stLEFT JOIN score sc on sc.s_id = st.s_idwhere sc.c_id = 01 and sc.s_score &lt; 60order by sc.s_score DESC;# 按平均成绩从高到低显示所有学生的所有课程的成绩以及平均成绩select a.s_id,tmp1.s_score as chinese,tmp2.s_score as math,tmp3.s_score as english, round(avg (a.s_score),2) as avgScorefrom score aleft join (select s_id,s_score from score s1 where c_id=&#x27;01&#x27;)tmp1 on tmp1.s_id=a.s_idleft join (select s_id,s_score from score s2 where c_id=&#x27;02&#x27;)tmp2 on tmp2.s_id=a.s_idleft join (select s_id,s_score from score s3 where c_id=&#x27;03&#x27;)tmp3 on tmp3.s_id=a.s_idgroup by a.s_id,tmp1.s_score,tmp2.s_score,tmp3.s_score order by avgScore desc;select * from course;SELECT sc.s_id, a.s_score as chinese, b.s_score as math, c.s_score as english, round(avg(a.s_score)) as avg_score FROM score scLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;01&#x27;) a on a.s_id=sc.s_idLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;02&#x27;) b on b.s_id=sc.s_idLEFT JOIN (SELECT s_id, s_score FROM score where c_id=&#x27;03&#x27;) c on c.s_id=sc.s_idgroup by sc.s_id, a.s_score, b.s_score, c.s_scoreorder by avg_score desc;# 查询各科成绩最高分、最低分和平均分：以如下形式显示：课程ID，课程name，最高分，最低分，平均分，及格率，中等率，优良率，优秀率:# 及格为&gt;=60，中等为：70-80，优良为：80-90，优秀为：&gt;=90select c.c_id, c.c_name, max(s_score), min(s_score),round(avg(s_score)),round(sum(case when sc.s_score&gt;=60 and sc.s_score&lt;70 then 1 else 0 end)/count(1),2) &quot;及格率&quot;,round(sum(case when sc.s_score&gt;=70 and sc.s_score&lt;80 then 1 else 0 end)/count(1),2) &quot;中等率&quot;,round(sum(case when sc.s_score&gt;=80 and sc.s_score&lt;90 then 1 else 0 end)/count(1),2) &quot;优良率&quot;,round(sum(case when sc.s_score&gt;=90 then 1 else 0 end)/count(1)) &quot;优秀率&quot;from score scLEFT JOIN course c on c.c_id=sc.c_idgroup by c_id, c.c_name;# 按各科成绩进行排序，并显示排名select s1.*,row_number()over(order by s1.s_score desc) Ranking from score s1 where s1.c_id=&#x27;01&#x27;union all select s2.*,row_number()over(order by s2.s_score desc) Ranking from score s2 where s2.c_id=&#x27;02&#x27;union all select s3.*,row_number()over(order by s3.s_score desc) Ranking from score s3 where s3.c_id=&#x27;03&#x27;order by noRanking asc;SELECT sc.s_score, sc.s_id FROM score sc where sc.c_id = 1 GROUP BY sc.s_score, sc.s_id order by sc.s_score;# 查询学生的总成绩并进行排名select sum(s_score) as sum from scoregroup by s_idorder by sum;# 查询不同老师所教不同课程平均分从高到低显示(SELECT t_name, c.c_name, round(avg(sc.s_score)) from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_id where t.t_id=01 group by t_name, c.c_name )Union(SELECT t_name, c.c_name, round(avg(sc.s_score)) from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_id where t.t_id=02group by t_name, c.c_name;SELECT t_name, c.c_name, round(avg(sc.s_score)) avg from score scleft join course c on sc.c_id = c.c_idLEFT join teacher t on t.t_id=c.t_idgroup by t_name, c.c_nameorder by avg;# 查询所有课程的成绩第2名到第3名的学生信息及该课程成绩(SELECT * from (select * from score where c_id=&#x27;01&#x27; order by s_score desc limit 3)tmp1order by tmp1.s_score asc limit 2)UNION ALL(SELECT * from (select * from score where c_id=&#x27;02&#x27; order by s_score desc limit 3)tmp1order by tmp1.s_score asc limit 2);# 查询学生平均成绩及其名次SELECT @rownum := @rownum + 1 as rank, s.avgFROM (SELECT round(avg(s_score)) as avg FROM score GROUP by s_id order by avg desc) s,(SELECT @rownum := 0) r;# 查询各科成绩前三名的记录select score.c_id,course.c_name,student.s_name,s_score from scorejoin student on student.s_id=score.s_idjoin course on course.c_id=score.c_idWHERE score.c_id=&#x27;01&#x27;order by s_score desc limit 3;# 查询每门课程被选修的学生数SELECT c.c_name, count(1) from student stleft join score sc on sc.s_id = st.s_idleft join course c on c.c_id = sc.c_idgroup by c.c_name;SELECT * from student stleft join score sc on sc.s_id = st.s_idleft join course c on c.c_id = sc.c_id;# 查询所有学生的课程及分数情况select a.s_name, SUM(case c.c_name when &#x27;语文&#x27; then b.s_score else 0 end ) as chainese, SUM(case c.c_name when &#x27;数学&#x27; then b.s_score else 0 end ) as math, SUM(case c.c_name when &#x27;英语&#x27; then b.s_score else 0 end ) as english, SUM(b.s_score) as sumScore from student a join score b on a.s_id=b.s_id join course c on b.c_id=c.c_id group by s_name,a.s_id;# 查询每门课程成绩最好的前三名SELECT s_name, s_score FROM student stLEFT JOIN score sc on sc.s_id = st.s_id and sc.c_id=01order by s_score desc limit 3;# 统计每门课程的学生选修人数（超过5人的课程才统计）# 要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列select c.c_id, c_name, count(1) as num from course cleft join score sc on sc.c_id = c.c_idleft join student st on st.s_id = sc.s_idgroup by c_name, c.c_idhaving num&gt;5order by num,c.c_id;select distinct course.c_id,tmp.num from course join (select c_id,count(1) as num from score group by c_id)tmp where tmp.num&gt;=5 order by tmp.num desc ,course.c_id asc;# 查询各学生的年龄(周岁):# 按照出生日期来算，当前月日 &lt; 出生年月的月日则，年龄减一SELECT *,year(CURRENT_DATE())-year(s_birth) + (case when MONTH(CURRENT_DATE())&gt;MONTH(s_birth) then 1 when MONTH(CURRENT_DATE())=MONTH(s_birth) and DAY(CURRENT_DATE())&gt;DAY(s_birth) then 0 else 0end) as age FROM student;# 查询本周过生日的学生SELECT WEEKOFYEAR(STR_TO_DATE(year(CURRENT_DATE())+date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;),&#x27;%m-%d&#x27;));SELECT date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;);SELECT *, WEEKOFYEAR(concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(s_birth,&#x27;%m-%d&#x27;))) as a, WEEKOFYEAR(CURRENT_DATE()) as b,concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(s_birth,&#x27;%m-%d&#x27;)) a1,WEEKOFYEAR(s_birth) as cfrom student;# 细微差别SELECT WEEKOFYEAR(&#x27;2023-01-02&#x27;);SELECT WEEK(&#x27;2023-01-02&#x27;);select WEEKOFYEAR(concat(year(CURRENT_DATE()),&#x27;-&#x27;,date_format(CURRENT_DATE(),&#x27;%m-%d&#x27;)));select s_name,s_sex,s_birth from student where MONTH(s_birth)=&#x27;12&#x27;;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"https://wingowen.github.io/tags/SQL/"}]},{"title":"高等数学","slug":"基础科学/高等数学","date":"2023-01-04T09:13:09.000Z","updated":"2023-09-20T06:47:27.218Z","comments":true,"path":"2023/01/04/基础科学/高等数学/","link":"","permalink":"https://wingowen.github.io/2023/01/04/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/","excerpt":"函数与极限","text":"函数与极限 函数与极限 函数 x,yx,yx,y 为连个变量 (x∈D)(x \\in D)(x∈D)，∀x∈D\\forall x \\in D∀x∈D 若存在唯一确定的 yyy 与 xxx 对应，称 yyy 为 xxx 的函数，记 y=f(x)y=f(x)y=f(x)。 R={y∣y=f(x),x∈D}R = \\{ y | y=f(x), x \\in D \\} R={y∣y=f(x),x∈D} DDD - 定义域；RRR - 值域。 特殊的函数 符号函数。 y=sng⁡x={−1,x&lt;00,x=01,x&gt;0y=\\operatorname{sng} x=\\left\\{ \\begin{array}{rr} -1, &amp; x&lt;0 \\\\ 0, &amp; x=0 \\\\ 1, &amp; x&gt;0 \\end{array}\\right. y=sngx=⎩⎪⎨⎪⎧​−1,0,1,​x&lt;0x=0x&gt;0​ 狄利克雷函数，QQQ 表示有理数。 f(x)={1x∈Q0x∉Qf(x)=\\left\\{\\begin{array}{ll} 1 &amp; x \\in Q \\\\ 0 &amp; x \\notin Q \\end{array}\\right.f(x)={10​x∈Qx∈/Q​ 左取整函数 [X][X][X]。 反函数 y=f(x)y=f(x)y=f(x) 严格单调，⇒x=arcf(y)\\Rightarrow x=arcf(y)⇒x=arcf(y)。 例题：求 y=ln(x+x2+1)y=ln(x+ \\sqrt{x^2 + 1})y=ln(x+x2+1​) 的反函数。 ⇒ey=x+x2+1\\Rightarrow e^y = x+ \\sqrt{x^2 + 1}⇒ey=x+x2+1​ ① ∵(x+x2+1)(−x+x2+1)=1\\because (x+ \\sqrt{x^2 + 1})(-x+ \\sqrt{x^2 + 1}) = 1∵(x+x2+1​)(−x+x2+1​)=1 ∴−x+x2+1=1ey=e−y\\therefore -x+ \\sqrt{x^2 + 1} = \\frac{1}{e^y} = e^{-y}∴−x+x2+1​=ey1​=e−y ② ① - ② ⇒2x=ey−e−y⇒x=ey−e−y2\\Rightarrow 2x=e^y-e^{-y} \\Rightarrow x = \\frac{e^y-e^{-y}}{2}⇒2x=ey−e−y⇒x=2ey−e−y​ 基本初等函数 初等函数 基本初等函数的四则运算及复合运算而形成的式子。 初等性质 奇偶性 DDD 关于原点对称。 奇函数：∀x∈D,f(−x)=−f(x)\\forall x \\in D, f(-x) = -f(x)∀x∈D,f(−x)=−f(x)。 偶函数：∀x∈D,f(−x)=f(x)\\forall x \\in D, f(-x) = f(x)∀x∈D,f(−x)=f(x)。 单调性 ∀x1,x2∈D,x1&lt;x2⇒f(x1)&lt;f(x2)\\forall x_1, x_2 \\in D, x1 &lt; x2 \\Rightarrow f(x1)&lt;f(x2)∀x1​,x2​∈D,x1&lt;x2⇒f(x1)&lt;f(x2)，单调递增。 ∀x1,x2∈D,x1&lt;x2⇒f(x1)&lt;f(x2)\\forall x_1, x_2 \\in D, x1&lt;x2 \\Rightarrow f(x1)&lt;f(x2)∀x1​,x2​∈D,x1&lt;x2⇒f(x1)&lt;f(x2)，单调递减。 有界性 ∃M&gt;0,∀x∈D⇒∣f(x)∣&lt;M\\exists M&gt;0, \\forall x \\in D \\Rightarrow |f(x)|&lt;M∃M&gt;0,∀x∈D⇒∣f(x)∣&lt;M，则称 f(x)f(x)f(x) 在 DDD 上有界，即有下界和上界。 周期性 ∃T&gt;0,∀x∈D(x+T∈D)⇒f(x+T)=f(x)\\exists T&gt;0, \\forall x \\in D (x+T \\in D) \\Rightarrow f(x+T)=f(x)∃T&gt;0,∀x∈D(x+T∈D)⇒f(x+T)=f(x)，f(x)f(x)f(x) 为周期为 T 的周期函数。 TminT_{min}Tmin​ 为最小正周期。 数列极限 (ε−N)(\\varepsilon - N)(ε−N) an{a_n}an​ 为数列，AAA 为常数。∀ε&gt;0,∃N&gt;0,n&gt;N，∣an−A∣&lt;ε\\forall \\varepsilon &gt;0, \\exists N&gt;0, n&gt;N ，|a_n -A|&lt;\\varepsilon∀ε&gt;0,∃N&gt;0,n&gt;N，∣an​−A∣&lt;ε。 lim⁡n→∞an=A/an→A(n→∞)\\lim_{n \\to \\infty} a_n= A \\quad / \\quad a_n → A (n → {\\infty}) n→∞lim​an​=A/an​→A(n→∞) 例题：证明 lim⁡n→∞n−1n+1=1\\lim_{n \\to \\infty} \\frac{n-1}{n+1}=1limn→∞​n+1n−1​=1。 ∣n−1n+1−1∣=2n+1&lt;ε|\\frac{n-1}{n+1} -1| = \\frac{2}{n+1} &lt; \\varepsilon∣n+1n−1​−1∣=n+12​&lt;ε n&gt;2ε−1n &gt; \\frac{2}{\\varepsilon} - 1n&gt;ε2​−1 取 N=[2ε−1]N=[\\frac{2}{\\varepsilon} - 1]N=[ε2​−1]，当 n&gt;N→2n+1&lt;εn&gt;N \\to \\frac{2}{n+1} &lt; \\varepsilonn&gt;N→n+12​&lt;ε 成立。 数列计算性质 唯一性，使用反证法证明 假设 lim⁡n→∞an=A\\lim_{n \\to \\infty} a_n = Alimn→∞​an​=A 且 lim⁡n→∞an=B\\lim_{n \\to \\infty} a_n = Blimn→∞​an​=B 成立。 设 ε&lt;A−B2&gt;0\\varepsilon &lt; \\frac{A-B}{2} &gt; 0ε&lt;2A−B​&gt;0 ∃Na&gt;0\\exists N_a &gt; 0∃Na​&gt;0 使之成立，∣an−A∣&lt;A−B2⇒A+B2&lt;an&lt;3A−B2|a_n - A| &lt; \\frac{A-B}{2} \\Rightarrow \\frac{A+B}{2}&lt;a_{n}&lt;\\frac{3 A-B}{2}∣an​−A∣&lt;2A−B​⇒2A+B​&lt;an​&lt;23A−B​ ① ∃Nb&gt;0\\exists N_b &gt; 0∃Nb​&gt;0 使之成立，∣an−B∣&lt;A−B2⇒3B−A2&lt;an&lt;A+B2|a_n - B| &lt; \\frac{A-B}{2} \\Rightarrow \\frac{3 B-A}{2}&lt;a_{n}&lt;\\frac{A+B}{2}∣an​−B∣&lt;2A−B​⇒23B−A​&lt;an​&lt;2A+B​ ② ① 与 ② 冲突，故假设不成立。 有界性 iflim⁡n→∞an=Aif \\lim_{n \\to \\infty} a_n = Aiflimn→∞​an​=A，则 ∃M\\exists M∃M 使得 ∣an∣≤M|a_n|≤M∣an​∣≤M 成立。反之不成立。 证：有极限则一定有界。 取 ε=1\\varepsilon = 1ε=1，则 ∃N&gt;0\\exists N&gt;0∃N&gt;0，当 n&gt;Nn&gt;Nn&gt;N 时，∣an−A∣&lt;1|a_n - A| &lt; 1∣an​−A∣&lt;1 ∵∣∣an∣−∣A∣∣≤∣an−A∣\\because ||a_n|-|A|| ≤ |a_n-A|∵∣∣an​∣−∣A∣∣≤∣an​−A∣ ∴∣∣an∣−∣A∣∣&lt;1→∣an∣&lt;1+A\\therefore ||a_n|-|A|| &lt; 1 \\to |a_n| &lt; 1 + A∴∣∣an​∣−∣A∣∣&lt;1→∣an​∣&lt;1+A，注意，这是在 n&gt;Nn&gt;Nn&gt;N 的条件下成立的。 取 M=MAX{∣a1∣,∣a2∣,∣a3∣,...,∣an∣,1+A}M = MAX\\{|a_1|,|a_2|,|a_3|,...,|a_n|,1+A\\}M=MAX{∣a1​∣,∣a2​∣,∣a3​∣,...,∣an​∣,1+A} 则 ∀n,∣an∣≤M\\forall n, |a_n| ≤ M∀n,∣an​∣≤M 成立 保号性 iflim⁡n→∞an=A&gt;0if \\lim_{n \\to \\infty} a_n = A &gt; 0iflimn→∞​an​=A&gt;0，则 ∃N&gt;0\\exists N &gt; 0∃N&gt;0 当 n&gt;Nn&gt;Nn&gt;N 时，an&gt;0a_n&gt;0an​&gt;0 iflim⁡n→∞an=A&gt;0if \\lim_{n \\to \\infty} a_n = A &gt; 0iflimn→∞​an​=A&gt;0，则 ∃N&gt;0\\exists N &gt; 0∃N&gt;0 当 n&gt;Nn&gt;Nn&gt;N 时，an&lt;0a_n&lt;0an​&lt;0 证：取 ε=A2&gt;0\\varepsilon = \\frac{A}{2} &gt; 0ε=2A​&gt;0 函数极限 ∀ε&gt;0,∃δ&gt;0\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0∀ε&gt;0,∃δ&gt;0，当 0&lt;∣x−a∣&lt;δ0&lt;|x-a|&lt;\\delta0&lt;∣x−a∣&lt;δ 时，∣f(x)−A∣&lt;ε|f(x)-A|&lt;\\varepsilon∣f(x)−A∣&lt;ε。称 f(x)f(x)f(x) 当 x→ax \\to ax→a 时，以 AAA 为极限。 lim⁡n→af(x)=A/f(x)→A(x→a)\\lim_{n \\to a} f(x)= A \\quad / \\quad f(x) → A (x → a) n→alim​f(x)=A/f(x)→A(x→a) Case ∀ε&gt;0,∃X&gt;0,x&gt;X,∣f(x)−A∣&lt;ε→limx→∞+\\forall \\varepsilon&gt;0, \\exists X&gt;0, x&gt;X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty^+}∀ε&gt;0,∃X&gt;0,x&gt;X,∣f(x)−A∣&lt;ε→limx→∞+​ ∀ε&gt;0,∃X&lt;0,x&gt;−X,∣f(x)−A∣&lt;ε→limx→∞−\\forall \\varepsilon&gt;0, \\exists X&lt;0, x&gt;-X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty^-}∀ε&gt;0,∃X&lt;0,x&gt;−X,∣f(x)−A∣&lt;ε→limx→∞−​ ∀ε&gt;0,∃X&gt;0,∣x∣&gt;X,∣f(x)−A∣&lt;ε→limx→∞\\forall \\varepsilon&gt;0, \\exists X&gt;0, |x|&gt;X, |f(x)-A|&lt;\\varepsilon \\rightarrow lim_{x \\to \\infty}∀ε&gt;0,∃X&gt;0,∣x∣&gt;X,∣f(x)−A∣&lt;ε→limx→∞​ Note x→ax \\to ax→a 包含 x→a−x \\to a^-x→a− 表示左邻域；x→a+x \\to a^+x→a+ 表示右邻域。 u˚(a,δ)\\mathring{u}(a, \\delta)u˚(a,δ) 表示 aaa 的去心 δ\\deltaδ 邻域。 limn→af(x)lim_{n \\to a} f(x)limn→a​f(x) 与 f(a)f(a)f(a) 无关。 函数存在分段时，需要分开讨论，即分为左极限 x→a−x \\to a^-x→a− 与右极限 x→a+x \\to a^+x→a+。 唯一性 局部有界性 无穷小与无穷大 无穷小 0 为无穷小，但无穷小不一定为 0。 无穷小余自变量趋向有关。（去心邻域） 运算法则 α→0,β→0⇒α±β→0\\alpha \\rightarrow 0, \\beta \\rightarrow 0 \\Rightarrow \\alpha \\pm \\beta \\rightarrow 0α→0,β→0⇒α±β→0 α→0,k×α→0\\alpha \\rightarrow 0, k \\times \\alpha \\rightarrow 0α→0,k×α→0 α→0,β→0⇒α×β→0\\alpha \\rightarrow 0, \\beta \\rightarrow 0 \\Rightarrow \\alpha \\times \\beta \\rightarrow 0α→0,β→0⇒α×β→0 lim⁡x→x0f(x)=A⇔f(x)=A+α,α→0(x→x0)\\lim_{x \\to x_0}f(x)=A \\Leftrightarrow f(x)=A+\\alpha, \\alpha \\rightarrow 0 (x \\rightarrow x_0)limx→x0​​f(x)=A⇔f(x)=A+α,α→0(x→x0​) 无穷大 ∀M&gt;0,∃δ&gt;0,0&lt;∣x−x0∣&lt;δ,∣f(x)∣&gt;M⇒limx→x0f(x)=∞\\forall M&gt;0, \\exists \\delta&gt;0, 0&lt;|x-x_0|&lt;\\delta , |f(x)|&gt;M \\Rightarrow lim_{x \\to x_0}f(x)=\\infty∀M&gt;0,∃δ&gt;0,0&lt;∣x−x0​∣&lt;δ,∣f(x)∣&gt;M⇒limx→x0​​f(x)=∞ 无穷小与无穷大互为倒数。 极限地运算法则 四则运算 limx→x0f(x)=A,limx→x0g(x)=Blim_{x \\to x_0}f(x) = A, lim_{x \\to x_0}g(x) = Blimx→x0​​f(x)=A,limx→x0​​g(x)=B limx→x0[f(x)±g(x)]=limx→x0f(x)±limx→x0g(x)=A±Blim_{x \\to x_0}[f(x) \\pm g(x)] = lim_{x \\to x_0}f(x) \\pm lim_{x \\to x_0}g(x) = A \\pm Blimx→x0​​[f(x)±g(x)]=limx→x0​​f(x)±limx→x0​​g(x)=A±B limx→x0kf(x)=klimx→x0f(x)lim_{x \\to x_0}kf(x) = klim_{x \\to x_0}f(x)limx→x0​​kf(x)=klimx→x0​​f(x) limx→x0f(x)g(x)=limx→x0f(x)limx→x0g(x)=ABlim_{x \\to x_0}f(x)g(x) = lim_{x \\to x_0}f(x)lim_{x \\to x_0}g(x) = ABlimx→x0​​f(x)g(x)=limx→x0​​f(x)limx→x0​​g(x)=AB limx→x0f(x)g(x)=limx→x0f(x)limx→x0g(x)=AB,limx→x0g(x)≠0lim_{x \\to x_0}\\frac{f(x)}{g(x)} = \\frac{lim_{x \\to x_0}f(x)}{lim_{x \\to x_0}g(x)} = \\frac{A}{B}, lim_{x \\to x_0}g(x) \\ne 0limx→x0​​g(x)f(x)​=limx→x0​​g(x)limx→x0​​f(x)​=BA​,limx→x0​​g(x)=0 Note P(x)P(x)P(x) 为最大次数为 nnn 的队列，Q(x)Q(x)Q(x) 为最大次数为 mmm 的队列 若 n&gt;m,limx→x0P(x)Q(x)=∞n&gt;m, lim_{x \\to x_0}\\frac{P(x)}{Q(x)} = \\inftyn&gt;m,limx→x0​​Q(x)P(x)​=∞ 若 n&lt;m,limx→x0P(x)Q(x)=0n&lt;m, lim_{x \\to x_0}\\frac{P(x)}{Q(x)} = 0n&lt;m,limx→x0​​Q(x)P(x)​=0 复合函数极限（套娃）。 极限存在准则 准则一：收敛定理 / 夹逼定理 f(x)≤g(x)≤h(x),limf(x)=limg(x)=A⇒limg(x)=Af(x) \\le g(x) \\le h(x), limf(x)=limg(x)=A \\Rightarrow limg(x)=Af(x)≤g(x)≤h(x),limf(x)=limg(x)=A⇒limg(x)=A 准则二：单调有界函数必有极限 重要极限 limx→0sinxx=1lim_{x \\to 0}\\frac{sinx}{x}=1 limx→0​xsinx​=1 limx→0(1+1n)n=elim_{x \\to 0}(1+\\frac{1}{n})^n=e limx→0​(1+n1​)n=e 无穷小的比较","categories":[{"name":"基础科学","slug":"基础科学","permalink":"https://wingowen.github.io/categories/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"常用命令","slug":"运维/常用命令","date":"2023-01-04T09:03:22.000Z","updated":"2023-01-04T09:04:26.430Z","comments":true,"path":"2023/01/04/运维/常用命令/","link":"","permalink":"https://wingowen.github.io/2023/01/04/%E8%BF%90%E7%BB%B4/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"BASH 脚本 12345678910111213141516# 条件判断if [ $1 = &quot;test&quot; ]; then UNION=else UNION=fi# 判断文件是否存在if [[ ! -f [file_path] ]]; then echo &#x27;文件不存在&#x27;fi# 判断文件夹是否存在if [[ ! -d [dir_path] ]]; then echo &#x27;文件夹不存在&#x27;fi find 123find . | grep -v volumns | grep -v &quot;\\.tar&quot; | cpio -pdm ../wefe-v4/ 用户与用户组 12345678910111213141516171819# 查看系统中所有用户# 用户名:用户密码:用户 ID:群组 ID:用户信息:家目录:shell 类型cat /etc/passwd# 查看用户组信息# 用户组名:用户组密码:用户组 ID:用户列表cat /etc/group# 新建用户useradd username# 设置密码passwd username# 将普通用户放入管理员组提升为管理员usermod -G wheel username# 限制 su - 命令只有 wheel 组成员可以运行vi /etc/pam.d/su Swap 12345678910111213141516171819# 查看系统 swap 大小free -g# 查看系统的挂载盘df -h# 创建 swap 文件夹fallocate -l 48G /data/swap# 创建 swap 区域mkswap -L swap /data/swapchmod 600 /data/swap# 挂载 swap 分区swapon /data/swap# 卸载 swap 分区swapoff /data/swap 环境变量 12# 登陆系统时 shell 读取的顺序/etc/profile -&gt;/etc/enviroment --&gt;$HOME/.profile --&gt;$HOME/.env 开机自启 12345# 开机自启脚本# /etc/rc.d/rc.local 文件会在 Linux 系统各项服务都启动完毕之后再被运行chmod +x /etc/rc.d/rc.localchmod +x auto_run_script.shvi /etc/rc.d/rc.local SSH 免密登陆过程 场景：A 机器通过 SSH 免密登陆到机器 B。 条件： A 生成了私钥、公钥； B 拥有 A 的公钥（在 authorized_keys 中）。 过程： A → B：发送连接请求，信息包括用户名以及 IP 等 B 在 authorized_keys 中查找相应用户名和 IP 对应的 公钥；若有符合的公钥，则生成一个随机字符串 R，并使用 A 的公钥对 R 进行加密生成 F®； B → A：发送 F® A 对 F® 进行解密得到 R； A → B：发送 R B 检查 R=R，则允许此次登陆。 1234ssh-keygen -t rsa -P &quot;&quot;# /etc/ssh/sshd_configPermitRootLogin yes","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"脚本命令","slug":"脚本命令","permalink":"https://wingowen.github.io/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/"}]},{"title":"网络相关","slug":"运维/网络相关","date":"2022-09-01T02:45:43.000Z","updated":"2023-09-20T07:40:51.094Z","comments":true,"path":"2022/09/01/运维/网络相关/","link":"","permalink":"https://wingowen.github.io/2022/09/01/%E8%BF%90%E7%BB%B4/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/","excerpt":"iptables / netfilter","text":"iptables / netfilter iptables iptables 是 Linux 防火墙的管理工具而已，位于 /sbin/iptables；真正实现防火墙功能的是 netfilter，它是 Linux 内核中实现包过滤的内部结构。 iptables 传输数据包的过程： 当一个数据包进入网卡时，它首先进入 PREROUTING 链，内核根据数据包目的 IP 判断是否需要转送出去。 如果数据包就是进入本机的，它就会沿着图向下移动，到达 INPUT 链。数据包到了 INPUT 链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过 OUTPUT 链，然后到达 POSTROUTING 链输出。 如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出。 iptables的规则表和链： 表（tables）提供特定的功能，iptables 内置了 4 个表 filter 表，包过滤； nat 表，网络地址转换； mangle 表，包重构、修改； raw 表，数据跟踪处理。 链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一 条或数条规则。当一个数据包到达一个链时，iptables 就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则 iptables 将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。 五个链为： PREROUTING：路由选择前； INPUT：路由选择后，进入到主机中； FORWARD：路由选择后，转发； OUTPUT：路由选择后（判断用哪张网卡发出包）,流出； POSTROUTING：最后的数据流出。 常用命令： 123456789101112131415161718192021222324252627282930313233343536373839# 查看规则iptables -t 表名 -Liptables -t nat --line -nvL PREROUTING # --line 显示规则的行号# -n 不解析IP# -v 显示详细内容# 添加规则iptables -t filter -A INPUT -s 192.168.1.146 -j DROPiptables -t filter -I INPUT -s 192.168.1.146 -j ACCEPT# 指定位置iptables -t filter -I INPUT 5 -s 192.168.1.146 -j REJECT # 设置指定表的指定链的默认策略（默认动作），并非添加规则。iptables -t filter -P FORWARD ACCEPT# -I 插入到第一行# -A 插入到最后# 删除规则iptables -t filter -D INPUT 3iptables -t filter -D INPUT -s 192.168.1.146 -j DROPiptables -t filter -F INPUTiptables -t filter -F# -F 清空# 删除自定义链iptables -X WEB# 修改规则iptables -t filter -R INPUT 3 -s 192.168.1.146 -j ACCEPT# 清除包的计数iptabls -t nat -Z PREROUTING# 清除nat表所有链的计数iptabls -t nat -Z# 保存service iptables saveiptables-save &gt; /etc/sysconfig/iptables","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"服务安装","slug":"运维/服务安装","date":"2022-08-31T02:41:25.000Z","updated":"2023-09-20T06:27:26.756Z","comments":true,"path":"2022/08/31/运维/服务安装/","link":"","permalink":"https://wingowen.github.io/2022/08/31/%E8%BF%90%E7%BB%B4/%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/","excerpt":"","text":"JDK12345678wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27;yum install jdk-8u202-linux-x64.rpm -ycat &gt;&gt; ~/.bash_profile &lt;&lt; EOFJAVA_HOME=/usr/java/jdk1.8.0_202-amd64/EOFsource ~/.bash_profile MySQL123456789101112131415161718192021222324252627282930313233343536# mysql 服务下载启动# 方式一：若速度过慢采用离线下载方式yum install -y &lt;http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm&gt;yum install mysql-community-server.x86_64 -y --nogpgcheck# 方式二：离线下载mkdir /opt/mysqlcd /opt/mysqlwget &lt;http://mirrors.ustc.edu.cn/mysql-ftp/Downloads/MySQL-5.7/mysql-5.7.38-1.el7.x86_64.rpm-bundle.tar&gt;tar -xvf mysql-5.7.38-1.el7.x86_64.rpm-bundle.tarrm -rf mysql-5.7.38-1.el7.x86_64.rpm-bundle.taryum install createrepo -ycreaterepo ./cat &gt;&gt; /etc/yum.repos.d/mysql.repo &lt;&lt; EOF [mysql]name=mysqlbaseurl=file:///opt/mysql/gpgcheck=0enabled=1 EOFyum install mysql-server -y# 方式二结束systemctl start mysqldsystemctl enable mysqld# mysql 服务配置# 获取初始密码进行登陆并修改密码cat /var/log/mysqld.log | grep password# set global validate_password_policy=0;SET PASSWORD = PASSWORD(&#x27;12341234&#x27;);grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;12341234&quot;;flush privileges; 123456789# 获取初始密码cat /var/log/mysqld.log | grep password# 登陆控制台mysql -p# 设置密码及权限&gt; set global validate_password_policy=0;&gt; SET PASSWORD = PASSWORD(&#x27;wefe2022&#x27;);&gt; grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;wefe2022&quot;;&gt; flush privileges; YUMYUM 源替换。 123456yum install wget -ycd /etc/yum.repos.dwget -O /etc/yum.repos.d/CentOS-Base.repo &lt;http://mirrors.aliyun.com/repo/Centos-7.repo&gt;wget -O /etc/yum.repos.d/epel.repo &lt;http://mirrors.aliyun.com/repo/epel-7.repo&gt;yum clean allyum makecache Nacos12345678910111213141516mkdir /data/nacos &amp;&amp; cd /data/nacos# 检查依赖yum install -y whichjava -version# nacos 官网下载wget &lt;https://github.com/alibaba/nacos/releases/download/2.0.3/nacos-server-2.0.3.zip&gt;# 备用下载链接wget &lt;https://welab-wefe-release.oss-cn-shenzhen.aliyuncs.com/IAM/RES/nacos-server-2.0.3.zip&gt;yum install unzip -yunzip nacos-server-2.0.3.zipcd nacos/bin# 启动 nacos 单机版sh startup.sh -m standalone# 服务访问地址：ip:8848/nacos# 默认账号密码：nacos / nacos Nginx12rpm -Uvh &lt;http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm&gt;yum install nginx -y Redis123456yum install redis -ysystemctl start redis# 测试 redis 服务redis-cli ping# PONG K8S 部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 关闭防火墙、Swap、SELinuxsystemctl stop firewalldsystemctl disable firewalldswapoff -asetenforce 0cat /etc/selinux/config# SELINUX=disabled# 安装系统依赖以及 dockeryum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum list docker-ce --showduplicates | sort -ryum install docker-ce.x86_64 3:18.09.9 -ysystemctl start dockersystemctl enable docker# 需保证 docker 和 kubelet 的的 cgroupdriver 是相同的cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;graph&quot;: &quot;/data/docker-compose&quot;, &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;EOFsystemctl restart docker# 安装 kubelet、kubeadm、kubectlcat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetes Repositorybaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOFyum install -y kubelet-1.23.0-0.x86_64 kubeadm-1.23.0-0.x86_64 kubectl-1.23.0-0.x86_64 --disableexcludes=kubernetes # Kubernetes 集群网络有很多种实现，有很大一部分都用到了 Linux 网桥# 由于网桥是虚拟的二层设备，同节点的 Pod 之间通信直接走二层转发，跨节点通信才会经过宿主机 eth0# 创建/etc/sysctl.d/k8s.conf文件，添加如下内容。表示 bridge 设备在二层转发时也去调用 iptables 配置的三层规则 (包含 conntrack)cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF# 执行如下命令使修改生效modprobe br_netfiltersysctl -p /etc/sysctl.d/k8s.confsystemctl start kubeletsystemctl enable kubelet# Master 机器操作# 修改本机 IP、镜像拉取地址 registry.aliyuncs.com/google_containerskubeadm config print init-defaults &gt; init-config.yamlkubeadm config images pull --config=init-config.yamlkubeadm init --config=init-config.yamlcd ~mkdir .kubecp /etc/kubernetes/admin.conf config# Worker 按提示操作kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers[kubeadm join ...]# 若集群出问题则重置kubeadm reset# 安装网络插件 flannelkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlcat &gt; /run/flannel/subnet.env &lt;&lt; EOFFLANNEL_NETWORK=10.244.0.0/16FLANNEL_SUBNET=10.244.0.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=trueEOF# 注意！！！这里踩坑了，卡了好几天# flannel 启动后，会生成 cni.0 与 flannel.1，这两网卡需要在同一网段cni0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 10.244.0.1 netmask 255.255.255.0 broadcast 10.244.0.255 inet6 fe80::34b6:76ff:fe2c:7f58 prefixlen 64 scopeid 0x20&lt;link&gt; ether 36:b6:76:2c:7f:58 txqueuelen 1000 (Ethernet) RX packets 107164 bytes 8642460 (8.2 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 105234 bytes 9770864 (9.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 flannel.1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.244.0.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::203e:21ff:fe5f:575b prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:3e:21:5f:57:5b txqueuelen 0 (Ethernet) RX packets 196206 bytes 60155180 (57.3 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 233768 bytes 30816014 (29.3 MiB) TX errors 0 dropped 8 overruns 0 carrier 0 collisions 0 Spark on Yarn 集群部署假设三台机器的 hostname 以及 ip 如下： 1234# hostname ipnode0 10.11.0.2node1 10.11.0.3node2 10.11.0.4 以下操作如无特殊说明默认在 node0 机器上操作。 SSH 免密登陆配置 12345678910111213yum install ssh -y# 在三台机器分别生成钥匙，提示输入回车即可ssh-keygen -t rsa -P &quot;&quot;# 在三台机器分别添加 hosts 解析echo &quot;10.11.0.2 node010.11.0.3 node110.11.0.4 node2&quot; &gt;&gt; /etc/hosts;# 在 node0 上执行以下代码ssh-copy-id -i ~/.ssh/id_rsa.pub root@node0ssh-copy-id -i ~/.ssh/id_rsa.pub root@node1ssh-copy-id -i ~/.ssh/id_rsa.pub root@node2 JDK 安装 12345678910# 在 node0 节点上操作wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27; yum install jdk-8u202-linux-x64.rpm -y echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_202-amd64/PATH=\\$PATH:\\$JAVA_HOME/bin&quot; &gt; /etc/profile.d/jdk.shsource /etc/profile.d/jdk.shscp /etc/profile.d/jdk.sh node1:/etc/profile.d/jdk.shscp /etc/profile.d/jdk.sh node2:/etc/profile.d/jdk.sh 资源下载 12345678910mkdir -p /data/big-data-app/rescd /data/big-data-app/reswget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgzwget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gztar -xvf spark-3.1.3-bin-hadoop2.7.tgzmv spark-3.1.3-bin-hadoop2.7 ../spark3tar -xvf hadoop-2.7.7.tar.gzmv hadoop-2.7.7.tar.gz ../hadoop2 HDFS 1234567891011121314151617181920212223242526# 增量同步工具yum install -y ssh rsync# hadoop envecho &quot;export HADOOP_HOME=/data/big-data-app/hadoop2export HADOOP_INSTALL=\\$HADOOP_HOMEexport HADOOP_MAPRED_HOME=\\$HADOOP_HOMEexport HADOOP_HDFS_HOME=\\$HADOOP_HOMEexport HADOOP_COMMON_HOME=\\$HADOOP_HOMEexport HADOOP_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport YARN_HOME=\\$HADOOP_HOMEexport YARN_CONF_DIR=\\$HADOOP_HOME/etc/hadoopexport PATH=\\$PATH:\\$HADOOP_HOME/sbin:\\$HADOOP_HOME/bin&quot; &gt; /etc/profile.d/hadoop.shsource /etc/profile.d/hadoop.shscp /etc/profile.d/hadoop.sh node1:/etc/profile.d/hadoop.shscp /etc/profile.d/hadoop.sh node2:/etc/profile.d/hadoop.sh# spark envecho &quot;SPARK_HOME=/data/big-data-app/spark3PATH=\\$SPARK_HOME/bin:\\$PATH&quot; &gt; /etc/profile.d/spark.shsource /etc/profile.d/spark.shscp /etc/profile.d/spark.sh node1:/etc/profile.d/spark.sh************scp************ /etc/profile.d/spark.sh node2:/etc/profile.d/spark.sh 修改配置，配置文件在 $HADOOP_CONF_DIR 目录下。 capacity-scheduler.xml 12345&lt;!-- DefaultResourceCalculator only uses Memory --&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.resource-calculator&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.util.resource.DominantResourceCalculator&lt;/value&gt;&lt;/property&gt; core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node0:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node0:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;node0:50091&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 启动动态资源需依赖 jar 包资源，操作如下 12# 在每个节点上进行操作cp $SPARK_HOME/yarn/spark-3.1.3-yarn-shuffle.jar $HADOOP_HOME/share/hadoop/yarn/lib/ 1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 根据节点内存设置节点分配给 YARN 的内存，为 1024 的倍数--&gt; &lt;!-- 32G 服务器，则可分配 32*0.75*1024=24576，一般分配比例为 0.75~0.85 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;32768&lt;/value&gt; &lt;/property&gt; &lt;!-- 每个 Container 能申请的资源最大值 --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;16384&lt;/value&gt; &lt;/property&gt; &lt;!-- 动态资源相关配置 START --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt; &lt;/property&gt; &lt;!-- 动态资源相关配置 END --&gt;&lt;/configuration&gt; slaves 12node1node2 Spark 配置 1234567891011121314151617181920echo &quot;node1node2&quot; &gt; $SPARK_HOME/conf/workersecho &quot;spark.eventLog.enabled truespark.eventLog.compress truespark.eventLog.dir hdfs:///logsspark.yarn.historyServer.address node0:18080&quot; &gt;&gt; $SPARK_HOME/conf/spark-default.conf# 创建日志存储目录./bin/hdfs dfs -mkdir /logs# 声明 spark 日志服务启动参数export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 \\-Dspark.history.retainedApplications=3 \\-Dspark.history.fs.logDirectory=hdfs://node0:9000/logs&quot;# 启动日志服务cd $SPARK_HOME/sbinsh start-history-server.sh 分发大数据资源 12rsync -arv /data/big-data-app node1:/data/big-data-apprsync -arv /data/big-data-app node2:/data/big-data-app 格式化 NameNode 12cd $HADOOP_HOME/bin./hdfs namenode -format 启动服务并检查 1234567891011cd $HADOOP_HOME/binstart-all.sh# 到各节点检查进程jps# node0 NameNode SecondaryNode ResourceManager# node1 DataNode NodeManager# ndoe2 DataNode NodeManager# HDFS URL http://node0:50070# YARN URL http://node0:8088 提交集群任务测试 12345678cd $SPARK_HOME./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\\--master yarn \\\\--deploy-mode cluster \\\\--driver-memory 4g \\\\--executor-memory 2g \\\\--executor-cores 1 \\\\examples/jars/spark-examples*.jar \\\\ Hive 部署基于 Docker 部署 HiveHive Docker 部署资源 基于 Dockerfile 生成相应镜像1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354FROM ubuntu:14.04ADD sources.list /etc/apt/sources.listADD apache-hive-2.3.9-bin.tar.gz /ADD mysql-connector-java-5.1.38.jar /ADD hadoop-2.7.2.tar.gz /COPY config/* /tmp/WORKDIR /root# install openssh-server, openjdk and wget,install hadoop 2.7.2RUN apt-get update &amp;&amp; \\ apt-get install -y --reinstall software-properties-common &amp;&amp; \\ add-apt-repository -y ppa:openjdk-r/ppa &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y openssh-server openjdk-8-jdk &amp;&amp; \\ apt-get clean all &amp;&amp; \\ mv /hadoop-2.7.2 /usr/local/hadoop &amp;&amp; \\ mv /apache-hive-2.3.9-bin /usr/local/hive &amp;&amp; \\ cp /mysql-connector-java-5.1.38.jar /usr/local/hive/lib/ &amp;&amp; \\ apt-get -y --purge remove software-properties-common# set environment variableENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64ENV HADOOP_HOME=/usr/local/hadoopENV HIVE_HOME=/usr/local/hiveENV PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin # ssh without key and hadoop configRUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P &#x27;&#x27; &amp;&amp; \\ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; \\ mkdir -p ~/hdfs/namenode &amp;&amp; \\ mkdir -p ~/hdfs/datanode &amp;&amp; \\ mkdir $HADOOP_HOME/logs &amp;&amp; \\ mv /tmp/ssh_config ~/.ssh/config &amp;&amp; \\ mv /tmp/hadoop-env.sh /usr/local/hadoop/etc/hadoop/hadoop-env.sh &amp;&amp; \\ mv /tmp/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml &amp;&amp; \\ mv /tmp/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml &amp;&amp; \\ mv /tmp/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml &amp;&amp; \\ mv /tmp/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml &amp;&amp; \\ mv /tmp/slaves $HADOOP_HOME/etc/hadoop/slaves &amp;&amp; \\ mv /tmp/start-hadoop.sh ~/start-hadoop.sh &amp;&amp; \\ mv /tmp/run-wordcount.sh ~/run-wordcount.sh &amp;&amp; \\ mv /tmp/hive-site.xml /usr/local/hive/conf/ &amp;&amp; \\ chmod +x ~/start-hadoop.sh &amp;&amp; \\ chmod +x ~/run-wordcount.sh &amp;&amp; \\ chmod +x $HADOOP_HOME/sbin/start-dfs.sh &amp;&amp; \\ chmod +x $HADOOP_HOME/sbin/start-yarn.sh &amp;&amp; \\ /usr/local/hadoop/bin/hdfs namenode -format# format namenode#RUN /usr/local/hadoop/bin/hdfs namenode -formatCMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;service ssh start; bash&quot;]","categories":[{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"}]},{"title":"网站收集","slug":"杂项/网站收集","date":"2022-08-29T08:57:30.000Z","updated":"2023-09-20T07:31:44.348Z","comments":true,"path":"2022/08/29/杂项/网站收集/","link":"","permalink":"https://wingowen.github.io/2022/08/29/%E6%9D%82%E9%A1%B9/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/","excerpt":"收集一些有用的网站。","text":"收集一些有用的网站。 杂项 gitee update Hexo 博客主题 pure 使用说明 | Cofess - Web Developer &amp; Designer JAVA Spring All 刷题 HiveSQL 50 Pandas 120","categories":[{"name":"杂项","slug":"杂项","permalink":"https://wingowen.github.io/categories/%E6%9D%82%E9%A1%B9/"}],"tags":[]},{"title":"计算机科学","slug":"计算机科学/计算机科学","date":"2022-08-15T09:16:17.000Z","updated":"2023-09-20T06:44:20.191Z","comments":true,"path":"2022/08/15/计算机科学/计算机科学/","link":"","permalink":"https://wingowen.github.io/2022/08/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/","excerpt":"协程。","text":"协程。 协程操作系统在线程等待 IO 的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待 IO 的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。 协程刚好可以解决上述 2 个问题。协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。 协程只有在等待 IO 的过程中才能重复利用线程。 假设协程运行在线程之上，并且协程调用了一个阻塞 IO 操作，这时候会发生什么？实际上操作系统并不知道协程的存在，它只知道线程，因此在协程调用阻塞 IO 操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。 因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥其作用。","categories":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[]},{"title":"降维","slug":"机器学习/降维","date":"2022-08-14T08:48:21.000Z","updated":"2023-09-20T07:37:24.811Z","comments":true,"path":"2022/08/14/机器学习/降维/","link":"","permalink":"https://wingowen.github.io/2022/08/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%99%8D%E7%BB%B4/","excerpt":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效 信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。","text":"数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效 信息的提取综合及无用信息的摈弃。 数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。 低维嵌入介绍 在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个低维嵌入 embedding。 缓解维数灾难的一个重要途径是降维 dimension reduction。它是通过某种数学变换将原始高纬度属性空间转变为一个低维子空间，在这个子空间中样本密度大幅提高，计算距离也变得更为容易。低维嵌入的目的是解决 k 邻近学习方法可操作性弱的问题。 将一个三维问题垂直投影，变成一个二维问题。 这种方法叫做多维缩放 Multiple Dimensional Scaling，简称 MDS，这是一种经典的降维方法。 MDS 12345678910111213141516# 导入包import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasets,manifoldfrom collections import Counterdef load_data(): # 使用 scikit-learn 自带的 iris 数据集 iris=datasets.load_iris() return iris.data,iris.target# 产生用于降维的数据集X, y=load_data()print(X.shape)print(Counter(y))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"特征选择","slug":"机器学习/特征选择","date":"2022-08-13T14:41:12.000Z","updated":"2023-09-20T07:37:06.944Z","comments":true,"path":"2022/08/13/机器学习/特征选择/","link":"","permalink":"https://wingowen.github.io/2022/08/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/","excerpt":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。","text":"特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。 过滤式选择 先对数据集进行特征选择，然后再训练分类器，特征选择过程与后续训练无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。 Relief 选择法 Relief Relevant Features，该方法设计了一个相关统计量来度量特征的重要性，并且其是一个向量，其每个分量分别对应一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。最终只需要确定一个阈值 r，然后选择比 r 大的相关统计量分量所对应的特征即可。也可以指定选取相关统计量分量最大的前 k 个特征。 FInsher 选择法 计算数据集中每个类别样本的类内特征方差与类间特征方差。 类内特征方差越小，类间特征方差越大，越有利于后续的分类训练，即该特征需要保留，反之该特征需要滤除。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#导入包import pandas as pdimport numpy as np#创建示例样本sample = [[1,1,5],[10,0,1],[2,5,6]]label = [1, 0, 1]print(&quot;sample:&quot;,sample)print(&quot;label:&quot;,label)#判断样本长度与类标长度是否匹配if len(sample) != len(label): print(&#x27;Sample does not match label&#x27;) exit()#创建并保存计算过程中的变量df1 = pd.DataFrame(sample)df2 = pd.DataFrame(label, columns=[&#x27;label&#x27;])data = pd.concat([df1, df2], axis=1) # 合并成为一个dataframeprint(&quot;data:\\n&quot;,data,&#x27;\\n&#x27;)data0 = data[data.label == 0]#对标签分类，分成包含0和1的两个dataframedata1 = data[data.label == 1]n = len(label)#标签长度n1 = sum(label)#1类标签的个数n0 = n - n1#0类标签的个数lst = []#用于返回的列表features_list = list(data.columns)[:-1]print(&quot;特征维数:&quot;)print(features_list)#fisher score计算for feature in features_list: print(&#x27;\\nfeature&#x27;,feature,&#x27;:&#x27;) # 算关于类标0 m0_feature_mean = data0[feature].mean() # 0 类标签在第 m 维上的均值 print(&quot;m0_feature_mean&quot;,m0_feature_mean) m0_SW=sum((data0[feature] -m0_feature_mean )**2) # 0类在第 m 维上的类内方差 print(&quot;m0_SW&quot;,m0_SW) # 算关于类标1 m1_feature_mean = data1[feature].mean() # 1 类标签在第 m 维上的均值 print(&quot;m1_feature_mean&quot;,m1_feature_mean) m1_SW=sum((data1[feature] -m1_feature_mean )**2)# 1 类标签在第 m 维上的类内方差 print(&quot;m1_SW&quot;,m1_SW) # 算关于 data m_all_feature_mean = data[feature].mean() # 所有类标签在第 m 维上的均值 print(&quot;m_all_feature_mean&quot;,m_all_feature_mean) m0_SB = n0 / n * (m0_feature_mean - m_all_feature_mean) ** 2 # 0 类标签在第 m 维上的类间方差 print(&quot;m0_SB&quot;,m0_SB) m1_SB = n1 / n * (m1_feature_mean - m_all_feature_mean) ** 2 # 1 类标签在第 m 维上的类间方差 print(&quot;m1_SB&quot;,m1_SB) m_SB = m1_SB + m0_SB # 计算SB print(&quot;m_SB&quot;,m_SB) m_SW = (m0_SW + m1_SW) / n # 计算 SW print(&quot;m_SW&quot;,m_SW) if m_SW == 0: # 0/0类型也是返回nan m_fisher_score = np.nan else: # 计算Fisher score m_fisher_score = m_SB / m_SW #计算第m维特征的Fisher score #Fisher score值添加进列表 print(&quot;m_fisher_score&quot;,m_fisher_score) lst.append(m_fisher_score) 包裹式选择 包裹式特征选择直接将最终要使用的学习器的性能作为特征子集的评价准则，包裹式特征选择的目的就是为给定的学习器选择最有利于其性能而量身定做的特征子集。 一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，比过滤式特征选择更好，但由于在特征选择过程中要多次训练学习器，因此包裹式特征选择的计算开销比过滤式特征选择大得多。 LVW 是一个经典的包裹式特征选择方法，它在拉斯维加斯方法框架下使用随机策略进行子集搜索，以最终分类器误差作为特征子集评价标准。 除了 LVW 包裹式特征选择之外，RFE(递归特征消除)也是一种常见的包裹式特征选择方法，RFE特征选择使用模型准确率来判断哪些特征（或特征组合）对预测结果贡献较大，并且递归地去除贡献小的特征。 除了 RFE 之外，还有一种选择算法称为 RFECV，其是以 RFE 为基础进行改进得到的。 RFECV 通过交叉验证的方式执行 RFE，以此来选择最佳数量的特征，即不手动设置保留的特征数量。对于一个数量为 d 的 feature 的集合，他的所有的子集的个数是 2 的 d 次方减 1 (包含空集)。指定一个外部的学习算法，比如 SVM 之类。通过该算法计算所有子集的validation error。选择 error 最小的那个子集作为所挑选的特征。 123456789101112131415161718192021222324252627282930313233343536373839404142#导入包from sklearn.feature_selection import RFE,RFECVfrom sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn import model_selection&#x27;&#x27;&#x27;class RFE(BaseEstimator, MetyuanaEstimatorMixin, SelectorMixin): 参数： BaseEstimator: 基模型 n_features_to_select：目标特征数量 return：经过选择后的特征 比较经过特征选择和未经特征选择的数据集，对 LinearSVC 的预测性能的区别&#x27;&#x27;&#x27;### 加载数据iris = load_iris()X, y = iris.data, iris.target### 特征提取estimator = LinearSVC()selector = RFE(estimator=estimator, n_features_to_select=2)X_t = selector.fit_transform(X, y) #对样本进行特征选择，最终保留n_features_to_select个特征。print(&quot;\\n特征选择结果显示:&quot;)print(&quot;原数据样本X：&quot;,X[1])print(&quot;特征选择后样本X_t：&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0, stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))clf_t.fit(X_train_t, y_train_t)print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t))) 嵌入式选择 嵌入式特征选择是将特征选择过程与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。 正则化嵌入式选择 L1L1L1 范数与 L2L2L2 范数都有利于降低过拟合，但前者还会带来一个额外的好处，即 L1L1L1 范数比 L2L2L2 范数更容易获得稀疏解，即它求得的 www 会有更少的非零分量。 其中，基于 L1L1L1 正则化的学习方法是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。 123456789101112131415161718192021222324252627282930313233#导入包from sklearn.svm import LinearSVCfrom sklearn.datasets import load_irisfrom sklearn.feature_selection import SelectFromModelfrom sklearn import model_selection#导入数据集并打印示例iris = load_iris()X, y = iris.data, iris.targetprint(&quot;原始数据特征维数：&quot;,len(X[1])) # (150, 4)print(&quot;原始数据样本：&quot;,X[1])#特征选择lsvc = LinearSVC(C=0.01, penalty=&quot;l1&quot;, dual=False).fit(X, y) #设置分类器model = SelectFromModel(lsvc, prefit=True) #设置模型为特征选择X_t = model.transform(X) #获取经过筛选的数据print(&quot;特征选择数据特征维数&quot;,len(X_t[1])) #(150, 3)print(&quot;特征选择后数据样本&quot;,X_t[1])#### 切分测试集与验证集X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y, test_size=0.25, random_state=0,stratify=y)print(&quot;测试集与验证集切分完成&quot;)### 测试与验证clf = LinearSVC()clf_t = LinearSVC()clf.fit(X_train, y_train)clf_t.fit(X_train_t, y_train_t)print(&quot;\\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t)))","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"线性回归与逻辑回归","slug":"机器学习/线性回归与逻辑回归","date":"2022-08-12T02:35:16.000Z","updated":"2023-09-20T07:37:17.618Z","comments":true,"path":"2022/08/12/机器学习/线性回归与逻辑回归/","link":"","permalink":"https://wingowen.github.io/2022/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等","text":"监督学习。 线性回归 Linear Regress 是回归问题的基础。 逻辑回归 Logistic Regress 是分类问题的基础。 损失函数与梯度下降法。 过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等 线性回归 线性回归分析 Linear Regression Analysis 是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。 所以线性的定义为：自变量之间只存在线性关系，即自变量只能通过相加、或者相减进行组合。 监督学习 如果现在有一个房子 H1，面积是 S，监督学习如何估算它的价格？ 监督学习从训练集中找到面积最接近 S 的房子 H2，预测 H1 的价格等于 H2 的价格。 监督学习根据训练集，找到一个数学表达式，对任意的面积的房子都可以估算出其价格。 h 代表假设函数：Training Set → Learning Algorithm → h；Size of House + h → Estimated Price。 线性回归的假设模型 hθ(x)=θ0+θ1xh_{\\theta}(x)=\\theta_{0}+\\theta_{1} x hθ​(x)=θ0​+θ1​x 如何求解模型，有以下两种思路。 尝试一些 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 的组合，选择能使得画出的直线正好穿过训练集的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 。 尝试一些 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 的组合，然后在训练集上进行预测，选能使得预测值与真实的房子价格最接近的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 。 选择最佳的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​，使得 hθ(x)h_{\\theta}(x)hθ​(x) 对所有的训练样本 (x,y)(x, y)(x,y) 来说，尽可能的接近 yyy。 损失函数 Train Set: {(x(1),y(1)),(x(2),y(2)),⋯ ,(x(m),y(m))}\\left\\{\\left(x^{(1)}, y^{(1)}\\right),\\left(x^{(2)}, y^{(2)}\\right), \\cdots,\\left(x^{(m)}, y^{(m)}\\right)\\right\\}{(x(1),y(1)),(x(2),y(2)),⋯,(x(m),y(m))} 12m∑i=1m(hθ(x(i))−y(i))2\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2} 2m1​i=1∑m​(hθ​(x(i))−y(i))2 最小化损失函数，得到的 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​ 是最佳的。 12345678910# 房屋的价格和面积数据import numpy as npdata = np.array([[2104, 460], [1416, 232], [1534, 315], [852,178]])# 使用线性回归模型计算预测值def get_predict(x, theta0, theta1): h = theta0 + theta1 * x #todo return h 梯度下降算法 梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数的组合 (θ0,θ1,......,θn)(\\theta_{0},\\theta_{1},......,\\theta_{n})(θ0​,θ1​,......,θn​) 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合， 可能会找到不同的局部最小值。 实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，需要同时更新 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​，实现方法是：你应该计算公式右边的部分，通过那一部分计算出 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​的值，然后同时更新 θ0\\theta_{0}θ0​ 和 θ1\\theta_{1}θ1​。 temp0 :=θ0−α∂∂θ0J(θ0,θ1) temp1 :=θ1−α∂∂θ1J(θ0,θ1)θ0:= temp0 θ1:= temp1 \\text { temp0 }:=\\theta_{0}-\\alpha \\frac{\\partial}{\\partial \\theta_{0}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\text { temp1 }:=\\theta_{1}-\\alpha \\frac{\\partial}{\\partial \\theta_{1}} J\\left(\\theta_{0}, \\theta_{1}\\right) \\\\ \\theta_{0}:=\\text { temp0 } \\\\ \\theta_{1}:=\\text { temp1 } temp0 :=θ0​−α∂θ0​∂​J(θ0​,θ1​) temp1 :=θ1​−α∂θ1​∂​J(θ0​,θ1​)θ0​:= temp0 θ1​:= temp1 逻辑回归 二分类问题下，采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。它适用于标签 y 取值离散的情况。 逻辑函数 Logistic Function，一个最常用的逻辑函数是 Sigmoid Function，以 Z=0 为决策界限，公式如下： g(z)=11+e−zg(z)=\\frac{1}{1+e^{-z}} g(z)=1+e−z1​ 123import numpy as npdef sigmoid(z): return 1 / (1 + np.exp(-z)) 逻辑回归模型假设 hθ(x)=g(θTX)h_\\theta(x)=g(\\theta^TX) hθ​(x)=g(θTX) hθ(x)h_\\theta(x)hθ​(x) 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性 （estimated probablity），即 $ h_\\theta(x) = P(y=1|x;\\theta)$。 例如，如果对于给定的 x，通过已经确定的参数计算得出 hθ(x)h_\\theta(x)hθ​(x)=0.7，则表示有 70% 的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7 = 0.3。 损失函数 线性回归模型的代价函数是所有模型误差的平方和，若逻辑回归的假设模型沿用这个定义，得到的函数将是一个非凸函数 non-convex function。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。 J(θ)=1m∑i=1mCost(hθ(x(i)),y(i))J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)}) J(θ)=m1​i=1∑m​Cost(hθ​(x(i)),y(i)) Cost⁡(hθ(x),y)={−log⁡(hθ(x)) if y=1−log⁡(1−hθ(x)) if y=0\\operatorname{Cost}\\left(h_{\\theta}(x), y\\right)=\\left\\{\\begin{aligned} -\\log \\left(h_{\\theta}(x)\\right) &amp; \\text { if } y=1 \\\\ -\\log \\left(1-h_{\\theta}(x)\\right) &amp; \\text { if } y=0 \\end{aligned}\\right. Cost(hθ​(x),y)={−log(hθ​(x))−log(1−hθ​(x))​ if y=1 if y=0​ Cost(hθ(x),y)=−y×log(hθ(x))−(1−y)×log(1−hθ(x))Cost(h_\\theta(x), y)=-y\\times{log(h_\\theta(x))}-(1-y)\\times{log(1-h_\\theta(x))} Cost(hθ​(x),y)=−y×log(hθ​(x))−(1−y)×log(1−hθ​(x)) J(θ)=−1m∑i=1m[y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i)))]J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))] J(θ)=−m1​i=1∑m​[y(i)log(hθ​(x(i)))+(1−y(i))log(1−hθ​(x(i)))] 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T))) return np.sum(first - second) / (len(X)) 当实际的 y=1 且 hθ(x)h_\\theta(x)hθ​(x) 也为1 时误差为 0，当 y=1 但 hθ(x)h_\\theta(x)hθ​(x) 不为 1 时误差随着 hθ(x)h_\\theta(x)hθ​(x) 的变小而变大； 当实际的 y=0 且 hθ(x)h_\\theta(x)hθ​(x) 也为 0 时代价为 0，当 y=0 但 hθ(x)h_\\theta(x)hθ​(x) 不为 0 时误差随着 hθ(x)h_\\theta(x)hθ​(x) 的变大而变大。 同样使用梯度下降法对参数进行更新： θj:=θj−α1m∑i=1m(hθ(x(i))−y(i))xj(i)\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j θj​:=θj​−αm1​i=1∑m​(hθ​(x(i))−y(i))xj(i)​ 123456789import numpy as np# 返回某一轮训练中的梯度def _gradient(X, Y_label, theta): # _f用来计算 y 的值 y_pred = _f(X, theta, b) pred_error = Y_label - y_pred w_grad = -np.sum(pred_error * X.T, 1) return w_grad 多元分类 我们将多个类中的一个类标记为正向类 y=1，然后将其他所有类都标记为负向类，这个模型记作 h(1)θ(x)h^{(1)_\\theta(x)}h(1)θ​(x)。接着，类似地第我们选择另一个类标记为正向类 y=2，再将其它类都标记为负向类，将这个模型记作 h(2)θ(x)h^{(2)_\\theta(x)}h(2)θ​(x)，依此类推。 最后我们得到一系列的模型简记为： h(i)θ(x)=p(y=i∣x;θ)h^{(i)_\\theta(x)} = p(y=i|x;\\theta) h(i)θ​(x)=p(y=i∣x;θ) 最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：h(i)θ(x)h^{(i)_\\theta(x)}h(i)θ​(x)， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 h(i)θ(x)h^{(i)_\\theta(x)}h(i)θ​(x) 最大的 i，即 max⁡ih(i)θ(x)\\max_ih^{(i)_\\theta(x)}maxi​h(i)θ​(x)。 过拟合化和正则化 过拟合在训练数据上的表现非常好；对于非训练的数据点，过拟合的模型表现与我们的期望有较大的偏。 减少拟合化的方法： 减少选取变量的数量：选取最重要的参数； 正则化：一种减小参数大小的办法。 正则化 回归：岭回归。 分类：L1 正则化，L2 正则化。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Python 编程","slug":"Python/Python-编程","date":"2022-08-01T03:17:02.000Z","updated":"2023-09-20T07:39:47.524Z","comments":true,"path":"2022/08/01/Python/Python-编程/","link":"","permalink":"https://wingowen.github.io/2022/08/01/Python/Python-%E7%BC%96%E7%A8%8B/","excerpt":"Python 编程开发查漏补缺。 gRPC Redis","text":"Python 编程开发查漏补缺。 gRPC Redis GRPC Google 开发的基于 HTTP/2 和 Protocol Buffer 3 的 RPC 框架。 Protocol Buffers, protobuf：结构数据序列化机制。 gRPC 默认使用 Brotocol Buffers，用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。 定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 GRPC 服务器来处理客户端调用。在客户端拥有一个存根 Stub，存根负责接收本地方法调用，并将它们委派给各自的具体实现对象（在远程服务器上）。 简单实现 实现一个简单的 gRPC HelloWorld。 proto file 定义 Protocol Buffers 规则文件。 123456789101112131415161718syntax = &quot;proto3&quot;;package helloworld;service Greeter &#123; // 定义方法参数和返回类型 rpc SayHello (HelloRequest) returns (HelloResponse) &#123;&#125;&#125;// 请求结构声明message HelloRequest &#123; string name = 1;&#125;// 响应结构声明message HelloResponse &#123; string message = 1;&#125; 运行 grpc_tools 工具。 1python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto 生成 python 代码。 helloworld_pb2.py 为 Protocol Buffers 的 Python 实现。 1234567891011121314151617181920# helloworld_pb2_grpc.py 用于 gRPC 实现的 Python 方法实现# 客户端存根class GreeterStub(object): def __init__(self, channel): self.SayHello = channel.unary_unary( &#x27;/helloworld.Greeter/SayHello&#x27;, request_serializer=helloworld__pb2.HelloRequest.SerializeToString, response_deserializer=helloworld__pb2.HelloResponse.FromString, )# 服务端服务class GreeterServicer(object): def SayHello(self, request, context): context.set_code(grpc.StatusCode.UNIMPLEMENTED) context.set_details(&#x27;Method not implemented!&#x27;) raise NotImplementedError(&#x27;Method not implemented!&#x27;) def add_GreeterServicer_to_server(servicer, server): # ...... server 自定义 gRPC 服务端。 1234567891011121314151617181920212223import grpcimport randomfrom concurrent import futuresimport helloworld_pb2import helloworld_pb2_grpc# 实现定义的方法，继承并实现方法class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloResponse(message=&#x27;Hello &#123;msg&#125;&#x27;.format(msg=request.name))def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) # 绑定处理器 helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) # 未使用 SSL，所以是不安全的 server.add_insecure_port(&#x27;[::]:50054&#x27;) server.start() print(&#x27;gRPC 服务端已开启，端口为 50054...&#x27;) server.wait_for_termination()if __name__ == &#x27;__main__&#x27;: serve() client 自定义客户端。 1234567891011121314import grpcimport helloworld_pb2, helloworld_pb2_grpcdef run(): # 本次不使用 SSL，所以 channel 是不安全的 channel = grpc.insecure_channel(&#x27;localhost:50054&#x27;) # 客户端实例 stub = helloworld_pb2_grpc.GreeterStub(channel) # 调用服务端方法 response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#x27;World&#x27;)) print(&quot;Greeter client received: &quot; + response.message)if __name__ == &#x27;__main__&#x27;: run() Redis REmote DIctionary Server, Redis 是一个 key-value 存储系统，是跨平台的非关系型数据库。 123456789101112131415pip install redisimport redis # 导入redis 模块# 获取连接r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True) # Redis 实例会维护一个自己的连接池，建立连接池，从连接池获取连接pool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)r = redis.Redis(connection_pool=pool)r.set(&#x27;name&#x27;, &#x27;runoob&#x27;, nx, xx) # 设置 name 对应的值, 当 nx = Ture 则只有 Key 不存在才执行插入; xx 相反print(r[&#x27;name&#x27;])print(r.get(&#x27;name&#x27;), px, ex) # 取出键 name 对应的值, px 毫秒 ex 秒 为过期时间print(type(r.get(&#x27;name&#x27;))) # 查看类型 在使用中，Redis 存储可分为两大类： set 即 k v，这里的 v 通常是一个字符串。 hset 即 k Hash-v，这里的 v 是一个 Redis Hash，是一个 string 类型的 field（字段）和 value（值）的映射表。 缓存技术 缓存就是利用编程技术将数据存储在临时位置，而不是每次都从源数据去检索。 Flask 基于 Flask 实现自定义控制器规则。 TODO","categories":[{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/categories/Python/"}],"tags":[{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"}]},{"title":"决策树算法","slug":"机器学习/决策树算法","date":"2022-07-31T02:58:15.000Z","updated":"2023-09-20T07:37:37.499Z","comments":true,"path":"2022/07/31/机器学习/决策树算法/","link":"","permalink":"https://wingowen.github.io/2022/07/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/","excerpt":"基本概念。","text":"基本概念。 决策树的基本概念 决策树节点： 叶节点表示一份类别或者一个值； 非叶节点表示一个属性划分。 决策树的有向边代表了属性划分的不同取值： 当属性是离散时，可将属性的每一个取值用一条边连接到子结点； 当属性是连续时，需要特殊处理。 决策树是一种描述实例进行分类的树形结构。 对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。 决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略；决策树的学习过程本质上是从训练数据中寻找一组分类规则；决策树学习也可以看做是由训练数据集估计条件概率模型。 由上述描述可以得知，决策树学习是一个递归过程，有三种情况会导致递归返回： 当前结点包含的所有样本属于同一类别。 当前属性结合为空，或所有样本在所有属性上的取值都相同：将当前结点标记为叶结点，其类别为该节点包含的样本最多的类别。 当前结点包含的样本基本为空：将当前结点标记为叶结点，其类别为父节点包含样本最多的类别 决策树的学习结果为：树结构 + 叶节点的取值（类别） 信息增益 熵，又称信息熵，是信息论的重要概念。熵是度量样本集合纯度的指标，熵越大，样本的纯度越低。假设当前样本集合 DDD 中第 iii 类样本所占比例的为 pi(i=1,2,...,C)p_i(i = 1,2,...,C)pi​(i=1,2,...,C)，则 DDD 的熵定义为： H(D)=−∑i=1Cpilog⁡2piH(D)=-\\sum_{i=1}^{C} p_{i} \\log _{2} p_{i} H(D)=−i=1∑C​pi​log2​pi​ 信息增益表示特征对于当前样本集纯度提升的程度。某属性的信息增益越大，说明使用改属性进行划分获得的纯度提升越大。因此使用信息增益进行决策树属性选择时，选择属性信息增益最大的作为当前节点。 G(D,a)=H(D)−∑v=1V∣Dv∣∣D∣H(Dv)G(D, a)=H(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} H\\left(D^{v}\\right) G(D,a)=H(D)−v=1∑V​∣D∣∣Dv∣​H(Dv) 123456789101112131415161718192021222324252627282930# 信息增益计算def get_G(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data 中所有样本属于同一类，由于 xlnx 在 x = 1和 x-&gt;0 是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE return H - Eprint(&#x27;各个属性的信息增益为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_G(data,i,len(data.columns)-1)) 信息增益比 以信息增益作为划分数据集的特征，会导致对可取值数目较多的属性有所偏好。为了缓解这种不良影响，采用信息增益比作为选择属性的准则。 123456789101112131415161718192021222324252627282930def get_GR(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的信息增益比 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的信息增益比, type:float, e.g.:0.32 &#x27;&#x27;&#x27; H = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0] H = H - p * np.log2(p) E = 0 IV = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] # new_data中所有样本属于同一类，由于xlnx 在x = 1和x-&gt;0是都为0，故无需计算该项 if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1: continue TE = 0 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] TE = TE - p * np.log2(p) E = E + new_data.shape[0] / data.shape[0] * TE IV = IV - new_data.shape[0] / data.shape[0] * (np.log2(new_data.shape[0] / data.shape[0])) G = H - E return G / IVprint(&#x27;各个属性的信息增益比为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_GR(data,i,len(data.columns)-1)) 基尼系数 纯度使用基尼指数来度量，在使用基尼系数作为指标时，应该选择基尼指数最小的属性。 1234567891011121314151617181920212223def get_Gini(data, index, clss_idx): &#x27;&#x27;&#x27; 求样本集某个属性的基尼系数 :param data: 数据集, type:pandas.DataFrame :param index: 属性索引, type:int, e.g.: 1 :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: index属性的基尼系数, type:float, e.g.:0.32 &#x27;&#x27;&#x27; Gini = 0 for v in np.unique(data.iloc[:,index]): new_data = data[data.iloc[:,index] == v] Gini_v = 1 for value in np.unique(data.iloc[:,clss_idx]): p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0] Gini_v = Gini_v - p * p Gini = Gini + new_data.shape[0] / data.shape[0] * Gini_v return Giniprint(&#x27;各个属性的基尼指数为&#x27;)for i in range(len(data.columns)-1): print(data.columns[i],get_Gini(data,i,len(data.columns)-1)) ID3 ID3 算法的核心是在决策树各结点上使用信息增益准则选择特征：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，根据特征的不同取值建立子结点。递归地调用以上方法，构建决策树。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def build_tree_id3(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用 ID3 算法在 data 数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_G(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_id3(new_data, nu, new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)root_id3 = build_tree_id3(data, None, ori_ppt, len(data.columns)-1)# 可视化createPlot(root_id3) C4.5 C4.5 算法对 ID3 算法进行了改进，即使用信息增益比来选择特征，其余和 ID3 算法基本相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def build_tree_c45(data, fa, ppt_list, clss_idx): &#x27;&#x27;&#x27; 使用C4.5算法在data数据集上建立决策树 :param data: 数据集, type:pandas.DataFrame :param fa: 父结点, type:pandas.DataFrame :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2] :param clss_idx: 样本类别的索引, type:int, e.g.:4 :return: 决策树的根结点, type:Node &#x27;&#x27;&#x27; nu = Node(data, fa, ppt_list) if len(np.unique(data.iloc[:, clss_idx])) == 1: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu if len(ppt_list) == 0: kind = data.iloc[:, clss_idx].value_counts().keys()[0] nu.set_leaf(kind) return nu best = -10000000 best_ppt = -1 for ppt in ppt_list: G = get_GR(data, ppt, clss_idx) if G &gt; best: best = G best_ppt = ppt new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt)) for v in np.unique(data.iloc[:, best_ppt]): new_data = data[data.iloc[:, best_ppt] == v] ch_node = build_tree_c45(new_data, nu,new_ppt_list, clss_idx) nu.add_child(ch_node) if ch_node.is_leaf: nu.add_leaf_ch(ch_node) else : for nd in ch_node.leaf_ch: nu.add_leaf_ch(nd) return nuori_ppt = np.arange(len(data.columns)-1)# print(data)root_c45 = build_tree_c45(data, None ,ori_ppt, len(data.columns)-1)createPlot(root_c45) 损失函数与剪枝 决策树的剪枝往往通过最小化决策树的损失函数实现。 123456789101112131415161718192021def cal_loss(root, alpha): &#x27;&#x27;&#x27; 计算以root为根结点的决策树的损失值 :param root: 根结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 以root为根结点的决策树的损失值, type:float, e.g.:0.24 &#x27;&#x27;&#x27; loss = 0 for leaf in root.leaf_ch: data = leaf.data for v in np.unique(data.iloc[:,len(data.columns)-1]): ntk = data[data.iloc[:,len(data.columns)-1] == v].shape[0] loss = loss - ntk * np.log2(ntk / data.shape[0]) loss = loss + alpha * len(root.leaf_ch) return lossori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)cal_loss(root_c45, 0.3) 决策树生成算法递归地产生决策树，直到无法继续。这种做法会带来过拟合问题。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，构建过于复杂的决策树。因此，一种解决方法是考虑决策树的复杂程度，从而对决策树进行简化。对决策树进行简化的过程称为剪枝。即从决策树中裁掉一些子树或叶结点，将其根节点或父节点作为新的叶结点。 剪枝算法的实现 计算每个节点的经验熵。 递归地从树的叶结点向上回缩，若回缩后的损失值 &gt; 回缩前的损失值，则进行剪枝，父节点变为叶节点。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def tree_pruning(root, leaf, alpha): &#x27;&#x27;&#x27; 决策树剪枝 :param root: 根结点, type:Node :param leaf: 叶结点, type:Node :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3 :return: 剪枝后的决策树根结点, type:Node &#x27;&#x27;&#x27; new_root = copy.deepcopy(root) pre_loss = cal_loss(root, alpha) flag = 1 for nl in leaf.fa.leaf_ch.copy(): nl.can_delete = 1 new_set = set() for leaf_ch in root.leaf_ch: if leaf_ch.can_delete != 1: new_set.add(leaf_ch) root.leaf_ch = new_set leaf.fa.set_leaf(leaf.fa.data.iloc[:,len(root.data.columns)-1].value_counts().keys()[0]) root.add_leaf_ch(leaf.fa) after_loss = cal_loss(root, alpha) if after_loss &gt;= pre_loss: #不剪枝 root = new_root flag = 0 return root, flag ori_ppt = np.arange(len(data.columns)-1)root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)# createPlot(root_c45)#设置超参数alpha = 0.3update = 1while update == 1: update = 0 for leaf in root_c45.leaf_ch.copy(): root_c45,flag = tree_pruning(root_c45, leaf, alpha) if flag: update = 1# root_c45createPlot(root_c45) 连续值处理 缺失值处理","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"计算机组成","slug":"计算机科学/计算机组成","date":"2022-07-30T07:15:43.000Z","updated":"2023-09-20T06:47:40.247Z","comments":true,"path":"2022/07/30/计算机科学/计算机组成/","link":"","permalink":"https://wingowen.github.io/2022/07/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/","excerpt":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线","text":"北大计算机组成课程。 计算机基本结构：冯诺依曼结构，计算机执行指令的过程。 系统总线 计算机系统概论 计算机系统层次结构 微程序机器 M0 微指令系统： 由硬件直接执行微命令； 实际机器 M1 机器语言机器：用微程序解释机器指令； 虚拟机器 M2 操作系统机器：用机器语言解释操作系统； 虚拟机器 M3 汇编语言机器：用汇编程序翻译成机器语言程序； 虚拟机器 M4 高级语言机器：用编译程序翻译成汇编语言程序或其它中间语言程序。 计算机的基本组成 冯·诺依曼提出存储程序的概念，以此概念为基础的计算机通称为冯·诺依曼计算机，其具有如下特点： 计算机由运算器、存储器、控制器、输入设备和输出设备五大部件组成； 指令和数据以同地位存放于存储器内，可按地址寻访； 指令和数据均用二进制数表示； 指令由操作码和地址码组成，操作码用来表示操作的性质，地址码用来表示操作数在存储器中的位置； 指令在存储器内按顺序存放。通常，指令是顺序执行的，在特定条件下，可根据运算结果或根据设定的条件改变执行顺序。 机器以运算器为中心，输入输出设备与存储器间的数据传送通过运算器完成。 各部件的功能如下： 运算器用来完成算术运算和逻辑运算，并将运算的中间结果暂存在运算器内； 存储器用来存放数据和程序； 控制器用来控制、指挥程序的运行、程序的输入输出以及处理运算结果； 运算器和控制器在逻辑关系和电路结构上联系十分紧密，两大部件往往集成在同一芯片上，因此通常将它们合起来统称为中央处理器 CPU, Central Processing Unit。 现代计算机组成：CPU, I/O 以及 主存储器 Main Memory, MM。 CPU + MM 称为主机；I/O 有称为外部设备。 Arithmetic Logic Unit, ALU 算术逻辑部件，用来完成算术逻辑运算；Control Unit, CU 控制单元，用来解释存储器中的指令，并发出各种操作命令来执行指令。 ALU 和 CU 是 CPU 的核心部件； I/O 设备也受 CU控制，用来完成相应的输入、输出操作。 计算机的工作步骤 TODO 系统总线 计算机系统的五大部件之间的互连方式有两种： 各部件之间使用单独的连线，成为分散连接； 另一种是各部件连到一组公共信息传输线上，成为总线连接。 总线是连接多个部件的信息传输线，是各部件共享的传输介质。当多个部件与总线相连时，如果出现两个或两个以上部件同时向总线发送信息，势必导致信号冲突，传输无效。因此，在某一时刻，只允许有一个部件向总线发送信息，而多个部件可以同时从总线上接收相同信息。 以运算器为中心的结构。I/O 设备与主存交换信息时仍然要占用 CPU，因此还会影响 CPU 的工作效率。 单总线（系统总线）。I/O 设备与主存交换信息时，原则上不影响 CPU 的工作，CPU 仍可继续处理不访问主存或 I/O 设备的操作，提高了 CPU 的工作效率。 但当某一时刻各部件都要占用总线时，就会发生冲突，必须设置总线判优逻辑，让各部件按优先级高低来占用总线，这也会影响整机的工作速度。 以存储器为中心的双总线结构框图。存储中线只供主存与 CPU 之间传输信息，即提高了传输效率，又减轻了系统总线的负担，还保留了 I/O 设备与存储器交换信息不经过 CPU 的特点。 总线分类 片内总线：芯片内部总线； 系统总线：各大部件之间的信息数据信息；按系统总线传输信息的不同，又可分为三类**：数据总线、地址总线和控制总线**。 通信总线：计算机系统之间或与其他系统之间的通信。 总线特性及性能指标 TODO 总线结构 TODO 总线控制 判优控制（仲裁逻辑）和通信控制。 总线判优控制 主设备：对总线有控制选；从设备：只能响应从主设备发来的总线命令，对总线没有控制权。 若多个主设备同时要使用总线时，由总线的判优、仲裁逻辑按一定的优先等级顺序确定哪个主设备能使用总线，只有获得总线使用权的主设备才能开始传送数据。 总线判优控制可分集中式和分布式两种： 将控制逻辑集中在一处； 将控制逻辑分散在与总线连接的各个部件或设备上。 集中控制优先仲裁方式 链式查询方式 控制总线中有 3 根线用于总线控制：BS 总线忙、BR 总线请求、BG 总线同意。其中 BG 是串行从一个 I/O 接口送到下一个 I/O 接口。如果 BG 到达的接口有总线请求 BR，BG 信号就不再往下传，意味着该接口获得了总线使用权，并建立总线忙 BS 信号，表示它占用了总线。 离总线控制部件最近的设备具有最高优先级。 只需很少几根线就能按一定有限次序实现总线控制，并且很容易扩充设备，但对电路故障很敏感，且优先级别低的设备可能很难获得请求。 计数器定时查询方式 与链式查询方式相比，多了一组设备地址线，少了一根总线同意线 BG。 总线控制部件接到由 BR 送来的总线请求信号后，在 BS=0 时，总线控制部件中的计数器开始计数，并通过设备地址线向各设备发出一组地址信号。当某个请求占用总线的设备地址与计数值一致时，便获得总线使用权，此时终止计数查询。 初始值可由程序设置；终止计数后可以重头开始，也可以从上一次计数终点开始。 对电路故障敏感度小于链式查询方式，但增加了控制线数（设备地址）目，控制也较复杂。 独立请求方式 每一台设备均有一对总线请求线和总线同意线。当设备要求使用总线时，便发出改设备的请求信号。总线控制部件中有一排队电路，可根据优先次序确定响应哪一台设备的请求。 响应快，优先次序控制灵活（根据程序改变），控制线数量多，总线控制更复杂。 通信控制 通常将完成一次总线操作的时间称为总线周期，可分为以下 4 各阶段。 申请分配阶段：主模块提出申请，总线仲裁机构决定下一传数周期的总线使用权授予某一申请者； 寻址阶段：取得了使用权的主模块通过总线发出本次要访问的从模块的地址及有关命令。启动参与本次传数的从模块； 传数阶段：主模块与从模块进行数据交换，数据由源模块发出，经数据总线流入目的模块； 结束阶段：主模块的有关信息均从系统总线上撤除，让出总线使用权。 总线通信控制主要解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调如何配合。 同步通信 读命令：CPU 在 T1 上升沿发出地址信息；在 T2 的上升沿发出读命令；与地址信号相符合的输入设备按命令进行一系列内部操作，且必须在 T3 的上升沿到来之前将 CPU 所需数据送到数据总线上；CPU 在 T3 时钟周期内将数据上的信息传送到其内部寄存器；CPU 在 T4 上升沿撤销读命令，输入设备不再向数据总线上传送数据，撤销它对数据总线的驱动。 规定明确、统一，模板间配合简单一致。 缺点是主、从模块时间配合属于强制性“同步”，必须在限定时间内完成规定的要求。并且对所有从模块都用统一时限，各模块速度不同，必须以最慢速度的部件来设计公共时钟，严重影响总线工作效率，给设计带来局限性，缺乏灵活性。 异步通信 (a) 不互锁：主模块发出请求信号后，不必等待接到从模块的回答信号，而是经过一段时间，确认从模块已收到请求信号后，便撤销其请求信号；从模块接到请求信号后，在条件允许时发出回答信号，并且经过一段时间确认主模块已收到回答信号后，自动撤销回答信号。 (b) 半互锁：主模块发出请求信号，必须接待到从模块的回答信号后再撤销其请求信号，有旧互锁关系；而从模块接到请求信号后发出回答信号，但不必等待获知主模块的请求信号，而是隔一段时间后自动撤销其回答信号，无互锁关系。 © 全互锁：皆需获得回答信号后撤销。在网络通信中，通信双方采用的就是全互锁方式。 半同步通信 保留了同步通信的基本特点，同时又像异步通信那样允许不同速度的模块和谐地工作。 增设了一条等待响应信号线，采用插入等待周期的措施来协调通信双方的配合问题。 分离式通信 将一个传输周期（总线周期）分解为两个子周期，两个传输子周期都只有单方面的信息流，每个模块都变成了主模块。 存储器 概述 存储器分类 a) 按存储介质分类。 半导体存储器 由半导体器件组成，用超大规模集成电路工艺制成芯片，体积小、功耗低、存取时间短。当电源消失时，所存信息也随即像丢失，是一种易失性存储器。 非挥发性材料制成的半导体存储器，克服了信息易失的弊病。 双极型 TTL 半导体存储器：高速。 MOS 半导体存储器：高集成度，制造简单，成本低，故被广泛应用。 磁表面存储器 在金属或塑料基体的表面上涂一层磁性材料作为记录介质，工作时磁层随载磁体高速运转，用磁头在磁层上进行读、写操作。 用具有矩形磁滞回线特性的材料作次表面物质，按其剩磁状态的不同而区分 0 或 1，而且剩磁状态不会轻易丢失，故这类存储器具有非易失性的特点。 磁芯存储器 被半导体存储器取代。 光盘存储器 用激光在磁光材料上进行读、写的存储器，具有非易失性的特点。 记录密度高、耐用性好、可靠性高和可互换性强等特点。 b) 按存取方式分类 随机存储器 Random Access Memory RAM 是一种可读、写存储器，其特点是存储器的任何一个存储单元的内容都可以随机存取，且存取时间与存储单位的物理位置无关。计算机系统中的主存都采用这种随机存储器。 静态 RAM，以触发器原理寄存信息。 动态 RAM，以电容充放电原理寄存信息。 只读存储器 Read Only Memory ROM 掩模型只读存储器 Masked ROM，MROM；可编程只读存储器 Programmable ROM，PROM；可擦除可编程只读存储器 Erasable Programmable ROM，EPROM；用电可擦除可编程只读存储器 Electrically Erasable Programmable ROM， EEPROM；Flash Memory。 串行访问存储器 对存储单元进行读写操作时，需按其物理位置的先后顺序寻找地址，则这种存储器称为串行访问存储器。 由于信息所在位置不同，读写时间均不相同。 c) 按在计算机中的作用分类 主存储器 可以和 CPU 直接交换信息。 速度快、容量小、每位价格高。 辅助存储器 是主存储器的后援存储器，用来存放当时暂时不用的程序和书，不能与 CPU 直接交换信息。 速度慢、容量大、每位价格低。 缓冲存储器 用在两个速度不同的部件之中。 层次结构 由上至下，位价越来越低，速度越来越慢，容量越来越大。 寄存器中的数直接在 CPU 内部参与运算。 主存用来存放将要参与运行的程序和数据，其速度与 CPU 速度差距较大，为使它们匹配，在主存与 CPU 之间插入了一种比主存速度更快、容量跟更小的高速缓冲存储器 Cache。 寄存器、缓存、主存这三类存储器都是由速度不同、位价不等的半导体存储材料制成的，它们都设在主机内。 磁盘、磁带属于辅助存储器，其容量比主存大得多，大都用来存放暂时未用到的程序和数据文件。 CPU 不能直接访问辅存，辅存只能与主存交换信息，因此辅存的速度可以比主存慢很多。 缓存 - 主存层次主要解决 CPU 与主存速度不匹配的问题。 主存 - 辅存层次主要解决存储系统的容量问题。形成了虚拟存储系统，在这个系统中，程序员变成的地址范围与虚拟存储器的地址空间相对应。 程序员编程时，可用的地址空间远远大于主存空间，使程序员以为自己占有一个容量极大的主存（虚拟存储器）。其逻辑地址转变为物理地址的工作由计算机系统的硬件和操作系统自动完成的，对程序员是透明的。 当虚地址的内容在主存时，机器便可立即使用；若虚地址的内容不在主存，则必须先将此虚地址内容传递到主存的合适单元后再为机器所用。 主存储器 主存的基本组成。 根据 MAR 储存器地址寄存器中的地址访问某个存储单元时，还需要经过地址译码、驱动等电路，才能找到所需要访问的单元。 读出时，需经过读出放大器，才能将被选中单元的存储字送到 MDR 主存数据寄存器。 写入时，MDR 中的数据也必须经过写入电路才能真正写入到被选中的单元中。 现代计算机的主存都由半导体集成电路构成，驱动器、译码器和读写电路均制作在存储芯片中，而 MAR 和 MDR 制作在 CPU 芯片中。存储芯片和 CPU 芯片可通过总线连接。 当要从存储器读出来某一信息字时，首先由 CPU 将该字的地址送到 MAR，经地址总线送至主存，然后发出读命令，主存接到读命令后将该单元内容读至数据总线上。 若要向主存存入一个信息字时，首先 CPU 将该字所在主存单元的地址经 MAR 送到地址总线，并将信息字送入 MDR，然后向主存发出写命令，主存接到写命令后，便将数据线上的信息写入到对应地址线指出的主存单元中。 主存中存储单元地址的分配 不同的机器存储字长不同，常用 8 位二进制数代表一个字节，因此存储字长都取 8 的倍数。 通常计算机系统即可按字寻址，也可按字节寻址。 主存的技术指标 存储容量：存储单元 x 存储字长；1 字长 = 8 字节。 存储速度：由存取时间和存取周期来表示。 存取时间、访问时间 Memory Access Time，是指启动一次存储器操作（读或写）到完成该操作所需的全部时间。存取时间分为读出时间和写入时间两种。 存取周期 Memory Cycle Time 是指存储器进行连续两次独立的存储器操作所需的最小间隔时间，通常存储周期大于存储时间。 存储器带宽 与存储周期密切相关，表示单位时间内存储器存取的信息量。通过以下方式提高存储器带宽： 缩短存储周期； 增加存储字长，使每个存取周期可读 / 写更多的二进制数； 增加存储体。 半导体存储芯片简介 基本结构 半导体存储芯片采用超大规模集成电路制造工艺，在一个芯片内集成具有记忆功能的存储矩阵、译码驱动电路和读写电路等。 译码驱动能把地址总线送来的地址信号翻译成对应存储单元的选择信号，该信号在读 / 写电路的配合下完成对被选中单元的读写操作。 读写电路包括读出放大器和写入电路，用来完成读写操作。 地址线是单向输入的，数据线是双向的，位数与芯片容量有关。 地址线和数据线位数共同反映存储芯片的容量。例如，地址线为 10 根，数据线为 4 根，则芯片容量为 210x4 K 位。 控制线主要包括读写控制线和片选线两种（不同存储芯片不同，可共用一根或分用两根）。由于半导体存储器是由许多芯片组成的，为此需要用片选信号来确定哪个芯片被选中。 译码驱动方式 线选法：用一根字选择线直接选中一个存储单元的各位。 重合法： 随机存取存储器 静态 RAM、Static RAM、SRAM 存储器用于寄存 0 和 1 代码的电路成为存储器的基本单元电路。","categories":[{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"}],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"模型评估与选择","slug":"机器学习/模型评估与选择","date":"2022-07-29T10:24:58.000Z","updated":"2023-09-20T07:36:55.949Z","comments":true,"path":"2022/07/29/机器学习/模型评估与选择/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。","text":"错误率和精度，误差，偏差和方差。 评估方法：留出法，交叉验证，自助法。 二分类任务性能度量：查准率，查全率，F1，ROC，AUC。 数据层面解决类别不平衡：欠采样，过采样，~结合。 算法层面解决类别不平衡：惩罚项。 错误率和精度 123456789101112131415161718import numpy as np# 真实的数据标签real_label = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]) \\ \\ \\# 分类器的预测标签classifier_pred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])compare_result = (real_label == classifier_pred)compare_result = compare_result.astype(np.int)# m 为样本数量 b 为预测错误样本m = len(real)b = m - np.sum(cmp)# 错误率error_rate = (b / m)*100# 精确度 accaccuracy = (1 - b / m)*100 误差 模型在训练样本上的误差称为训练误差或经验误差；模型在新样本上的误差称为泛化误差。 过拟合模型：虽然训练误差接近 0，泛化误差非常大。 欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。 偏差和方差 偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。 偏差 bias，与真实值的偏离程度； 方差 variance，该随机变量在其期望值附近的波动程度。 评估方法 评估：对学习器的泛化误差进行评估并进而做出选择。 留出法 以一定比例划分训练集和测试集。 1234567891011121314151617181920212223# 导入包import numpy as npfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): &#x27;&#x27;&#x27; return: 返回随机生成 200 个点的坐标 &#x27;&#x27;&#x27; dots = 200 # 样本数 dim = 2 # 数据维度 X = np.random.randn(dots,dim) # 建立数据集，shape(200,2) # 建立样本 X 的类别 Y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): Y[i] = 1 return X, Y# 加载数据X,Y = load_pts()# 使用train_test_split划分训练集和测试集train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0) 交叉验证 交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。 ![Untitled](/img/模型评估与选择/Untitled 1.png) 12345678910111213# 导入包from sklearn.model_selection import KFoldimport numpy as np# 生成数据集，随机生成40个点data = np.random.randn(40,2)# 交叉验证法kf = KFold(n_splits = 4, shuffle = False, random_state = None) for train, test in kf.split(data): print(train) print(test,&#x27;\\n&#x27;) 自助法 有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ： 每次随机从 D 中挑选一个样本； 将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中； 重复执行 m 次该过程； 最后得到包含 m 个样本数据集 D’。 由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。 123456789101112131415161718# 导入包import numpy as np# 任意设置一个数据集X = [1,4,3,23,4,6,7,8,9,45,67,89,34,54,76,98,43,52]# 通过产生的随机数获得抽取样本的序号 bootstrapping = []for i in range(len(X)): bootstrapping.append(np.random.randint(0,len(X),(1)))# 通过序号获得原始数据集中的数据D_1 = []for i in range(len(X)): print(int(bootstrapping[i])) D_1.append(X[int(bootstrapping[i])]) print(D_1) 总结 采样方法 与原始数据集的分布是否相同 相比原始数据集的容量 是否适用小数据集 是否适用大数据集 是否存在估计偏差 留出法 分层抽样 否 变小 否 是 是 交叉验证法 分层抽样 否 变小 否 是 是 自助法 放回抽样 否 不变 是 否 是 性能度量 性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。 性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。 回归任务常用性能度量：MSE mean square error，均方差。 分类任务常用性能度量：acc accuracy，精度；错误率 查准率、查全率、F1 对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为： 真正例 true positive 假正例 false positive 真反例 true negative 假反例 false negative ![Untitled](/img/模型评估与选择/Untitled 2.png) P( Precision )=TPTP+FPR( Recall )=TPTP+FNP(\\text { Precision })=\\frac{T P}{T P+F P} \\\\R(\\text { Recall })=\\frac{T P}{T P+F N} P( Precision )=TP+FPTP​R( Recall )=TP+FNTP​ Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。 Precision，查准率：在预测为正的样本中，实际为正的概率。 P-R 曲线，BRP，Break Even Point：平衡单 P = R。 ![Untitled](/img/模型评估与选择/Untitled 3.png) 由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。 BRP 还是过于简单，更常用的是 F1 度量。 F1=2×P×RP+R=2TPn+TP−TNF 1=\\frac{2 \\times P \\times R}{P+R}=\\frac{2 T P}{n+T P-T N} F1=P+R2×P×R​=n+TP−TN2TP​ F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。 当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。 Fβ=(1+β2)×P×R(β2×P)+RF_{\\beta}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R} Fβ​=(β2×P)+R(1+β2)×P×R​ β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。 如果使用了类似交叉验证法，我们会得到多个 confusion matrix： 宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1； 微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。 123456789101112131415161718192021222324252627282930313233343536373839404142434445import numpy as np# 加载数据集def generate_data(random_state=2021): &quot;&quot;&quot; :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] &quot;&quot;&quot; noise_rate = 0.1 # 噪声比例 sample_num = 4096 # 总样本数 noise_sample_num = int(sample_num*noise_rate) # 噪声样本数 np.random.seed(random_state) Pred_Score = np.random.uniform(0,1,sample_num) GT_label = (Pred_Score&gt;0.5).astype(np.int) noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=False, p=None) for index in noise_ids: GT_label[index] = 1 if GT_label[index] == 0 else 0 return GT_label, Pred_ScoreGT_label, Pred_Score = generate_data()# 请你补全以下代码，计算查准率与查全率def get_PR(GT_label, Pred_Score, threshold, random_state=2021): &quot;&quot;&quot; 计算错误率和精度 :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果 :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1] :threshold: 评估阈值 :random_state: 随机种子 :返回值: P: 查准率 R: 查全率 &quot;&quot;&quot; Pred_Label = list(map(lambda x: 1 if x &gt; threshold else 0, Pred_Score)) from sklearn.metrics import precision_score, recall_score P = precision_score(GT_label, Pred_Label) R = recall_score(GT_label, Pred_Label) &quot;&quot;&quot; TODO &quot;&quot;&quot; return P, R P, R = get_PR(GT_label, Pred_Score, 0.55, random_state=2021)print(&quot;查准率P ：&#123;:.2f&#125;&quot;.format(P))print(&quot;查全率R ：&#123;:.2f&#125;&quot;.format(R)) ROC 与 AUC 原理 ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。 TPR( Precision )=TPTP+FNFPR( Precision )=FPFP+TN\\begin{aligned}T P R(\\text { Precision }) &amp;=\\frac{T P}{T P+F N} \\\\F P R(\\text { Precision }) &amp;=\\frac{F P}{F P+T N}\\end{aligned} TPR( Precision )FPR( Precision )​=TP+FNTP​=FP+TNFP​​ TPR 真正率，真正样本与实际为正的样本的比率； FPR 假正率，加正样本与实际为负的样本的比率。 若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两 个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC Area Under ROC Curve。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_split# 加载数据集def load_pts(): dots = 200 # 点数 X = np.random.randn(dots,2) * 15 # 建立数据集，shape(200,2)，坐标放大15倍 # 建立 X 的类别 y = np.zeros(dots, dtype=&#x27;int&#x27;) for i in range(X.shape[0]): if X[i,0] &gt; -15 and X[i,0] &lt; 15 and X[i,1] &gt; -15 and X[i,1] &lt; 15: # 矩形框内的样本都是目标类（正例） y[i] = 1 if 0 == np.random.randint(i+1) % 10: # 对数据随机地插入错误，20 个左右 y[i] = 1 - y[i] # 数据集可视化 plt.scatter(X[np.argwhere(y==0).flatten(),0], X[np.argwhere(y==0).flatten(),1],s = 20, color = &#x27;blue&#x27;, edgecolor = &#x27;k&#x27;) plt.scatter(X[np.argwhere(y==1).flatten(),0], X[np.argwhere(y==1).flatten(),1],s = 20, color = &#x27;red&#x27;, edgecolor = &#x27;k&#x27;) plt.xlim(-40,40) plt.ylim(-40,40) plt.grid(False) plt.tick_params( axis=&#x27;x&#x27;, which=&#x27;both&#x27;, bottom=False, top=False) return X, yX, y = load_pts()plt.show()### 训练模型 ###from sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.svm import SVC# 将数据集拆分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) # 建立模型 clf1 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=4,min_samples_split=4)clf2 = GradientBoostingClassifier(max_depth=8, min_samples_leaf=10, min_samples_split=10)clf3 = SVC(kernel=&#x27;rbf&#x27;, gamma=0.001, probability=True)# 训练模型clf1.fit(X_train, y_train)clf2.fit(X_train, y_train)clf3.fit(X_train, y_train)### 评估模型 ###from sklearn.metrics import roc_curve# 模型预测y_score1 = clf1.predict_proba(X_test)y_score2 = clf2.predict_proba(X_test)y_score3 = clf3.predict_proba(X_test)# 获得 FPR、TPR 值fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,1])fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,1])fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,1])### 绘制 ROC 曲线 ###from sklearn.metrics import aucplt.figure()# 绘制 ROC 函数def plot_roc_curve(fpr, tpr, c, name): lw = 2 roc_auc = auc(fpr,tpr) plt.plot(fpr, tpr, color=c,lw=lw, label= name +&#x27; (area = %0.2f)&#x27; % roc_auc) plt.plot([0,1], [0,1], color=&#x27;navy&#x27;, lw=lw, linestyle=&#x27;--&#x27;) plt.xlim([0, 1.0]) plt.ylim([0, 1.05]) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.ylabel(&#x27;True Positive Rate&#x27;) #plt.title(&#x27;&#x27;) plt.legend(loc=&quot;lower right&quot;) plot_roc_curve(fpr1, tpr1, &#x27;red&#x27;,&#x27;DecisionTreeClassifier &#x27;) plot_roc_curve(fpr2, tpr2, &#x27;navy&#x27;,&#x27;GradientBoostingClassifier &#x27;) plot_roc_curve(fpr3, tpr3, &#x27;green&#x27;,&#x27;SVC &#x27;) plt.show() 比较检验（TODO） 模型性能比较的重要因素： 实验评估得到的性能不等于泛化性能； 测试集上的性能与测试集本身的选择有很大关系； 很多机器学习算法本身有一定的随机性。 统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。 假设检验 由样本推测总体的方法。 交叉验证 t 检验 McNemar 检验 Friedman 检验与 Nemenyi 后续检验 类别不平衡 在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。 如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。 解决不平衡分类问题的策略可以分为两大类： 从数据层面入手 , 通过改变训练集样本分布降低不平衡程度； 从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。 数据层面解决类别不平衡 扩大数据样本。 重采样：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。 过采样：复制稀有样本； 123456789101112131415161718192021222324252627282930313233# 导入包from sklearn.datasets import make_classificationfrom collections import Counterfrom imblearn.over_sampling import RandomOverSampler# 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2X, y = make_classification(n_samples=5000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=1, weights=[0.01, 0.05, 0.94], class_sep=0.8, random_state=0)# 打印每个类别样本数print(Counter(y))# 过采样ros = RandomOverSampler(random_state=0)X_resampled, y_resampled = ros.fit_resample(X, y)# 打印过采样后每个类别样本数print(sorted(Counter(y_resampled).items()))# 生成新的稀有样本# 导入包from imblearn.over_sampling import SMOTE# 过采样sm = SMOTE(random_state=42)X_res, y_res = sm.fit_resample(X, y)# 打印过采样后每个类别样本数print(&#x27;Resampled dataset shape %s&#x27; % Counter(y_res)) 欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。 123456789# 导入包from imblearn.under_sampling import RandomUnderSampler# 欠采样rus = RandomUnderSampler(random_state=0)X_resampled, y_resampled = rus.fit_resample(X, y)# 打印欠采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。 因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。 12345678# 导入包from imblearn.combine import SMOTEENN# 过采样与欠采样结合smote_enn = SMOTEENN(random_state=0)X_resampled, y_resampled = smote_enn.fit_resample(X, y)# 打印采样后每个类别样本数print(sorted(Counter(y_resampled).items())) 算法层面解决类别不平衡 惩罚项方法：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，错分稀有类的样本需要付出更大的代价。 通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。 例如，调整 SVM 以惩罚稀有类别的错误分类。 1234567# LABEL 0 4000# LABEL 1 200# 导入相关包from sklearn.svm import SVC# 添加惩罚项clf = SVC(C=0.8, probability=True, class_weight=&#123;0:0.25, 1:0.75&#125;) 特征选择方法 样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。 选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"贝叶斯算法","slug":"机器学习/贝叶斯算法","date":"2022-07-29T10:24:58.000Z","updated":"2023-09-20T07:36:37.626Z","comments":true,"path":"2022/07/29/机器学习/贝叶斯算法/","link":"","permalink":"https://wingowen.github.io/2022/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/","excerpt":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。","text":"条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。 朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。 条件概率 P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率： P(A∣B)=P(A∩B)P(B)P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)} P(A∣B)=P(B)P(A∩B)​ P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率： P(B∣A)=P(A∩B)P(A)P(B \\mid A)=\\frac{P(A \\cap B)}{P(A)} P(B∣A)=P(A)P(A∩B)​ 那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出贝叶斯公式： P(A∣B)=P(B∣A)×P(A)P(B)P(A \\mid B)=\\frac{P(B \\mid A) \\times P(A)}{P(B)}{\\scriptsize } P(A∣B)=P(B)P(B∣A)×P(A)​ 贝叶斯 基础思想： 已知类条件概率密度参数表达式和先验概率； 利用贝叶斯公式转换成后验概率； 根据后验概率大小进行决策分类。 根据以上基本思想，可以得到贝叶斯概率计算公式表达为**：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）**。 优点： 贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断； 它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信； 如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了； 它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。 缺点： 它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出； 有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。 扩展阅读： 一文读懂概率论学习：贝叶斯理论 贝叶斯决策论&amp;朴素贝叶斯算法 朴素贝叶斯法讲解 sklearn 贝叶斯方法 贝叶斯推断：广告邮件自动识别的代码实现 若邮件包含某个关键词，求此邮件是广告的概率。 12345678910111213141516171819# 广告邮件数量ad_number = 4000# 正常邮件数量normal_number = 6000# 所有广告邮件中，出现 “红包” 关键词的邮件的数量ad_hongbao_number = 1000# 所有正常邮件中，出现 “红包” 关键词的邮件的数量normal_hongbao_number = 6# 广告的先验概率 P(A)P_ad = ad_number / (ad_number + normal_number)# 包含红包的先验概率 P(B)P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)# 广告 包含红包的似然概率 P(B|A)P_hongbao_ad = ad_hongbao_number / ad_number# 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbaoprint(P_ad_hongbao) 10.9940357852882705 极大似然估计 极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。 极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。 极大似然估计与贝叶斯推断是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。 极大似然估计的简单计算 一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。 ddp(10061)p61(1−p)39=(10061)(61p60(1−p)39−39p61(1−p)38)=(10061)p60(1−p)38(61(1−p)−39p)=(10061)p60(1−p)38(61−100p)=0\\begin{array}{c} \\frac{d}{d p}\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{61}(1-p)^{39}=\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right)\\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\\right) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\\\ =\\left(\\begin{array}{c} 100 \\\\ 61 \\end{array}\\right) p^{60}(1-p)^{38}(61-100 p) \\\\ =0 \\end{array} dpd​(10061​)p61(1−p)39=(10061​)(61p60(1−p)39−39p61(1−p)38)=(10061​)p60(1−p)38(61(1−p)−39p)=(10061​)p60(1−p)38(61−100p)=0​ 当 P=61100,0P = \\frac{61}{100}, 0P=10061​,0 时，导数为零。因为 1 &lt; P &lt; 0，所以 P=61100P = \\frac{61}{100}P=10061​。 极大似然估计的简单应用 求极大似然估计 MLE 的一般步骤： 由总体分布导出样本的联合概率函数（或联合密度）； 把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 θθθ 看作自变量，得到似然函数 l(θ)l(θ)l(θ)； 求似然函数 l(θ)l(θ)l(θ) 的最大值点，常常转化为求 lnl(θ)lnl(θ)lnl(θ) 的最大值点，即 θθθ 的 MLE； 在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。 若随机变量 xxx 服从一个数学期望为 μμμ、方差为 σ2σ^2σ2 的正态分布，记为 N(μ,σ2)N(μ,σ^2)N(μ,σ2)，假设 μ=30,σ=2μ=30, σ=2μ=30,σ=2。 1234567891011import numpy as npfrom scipy.stats import normimport matplotlib.pyplot as pltμ = 30 # 数学期望σ = 2 # 方差x = μ + σ * np.random.randn(10000) # 正态分布plt.hist(x, bins=100) # 直方图显示plt.show()print(norm.fit(x)) # 返回极大似然估计，估计出参数约为 30 和 2 朴素贝叶斯分类器 朴素贝叶斯分类器是一系列假设特征之间强（朴素）独立条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。 朴素贝叶斯的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。 对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。 MNIST 手写体数字识别 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import warningswarnings.filterwarnings(&quot;ignore&quot;)# numpy 库import numpy as np# tensorflow 库中的 mnist 数据集import tensorflow as tfmnist = tf.keras.datasets.mnist# sklearn 库中的 BernoulliNBfrom sklearn.naive_bayes import BernoulliNB# 绘图工具库 pltimport matplotlib.pyplot as pltprint(&quot;读取数据中 ...&quot;)# 载入数据(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量train_images = train_images.reshape(len(train_images),784)test_images = test_images.reshape(len(test_images),784)print(&#x27;读取完毕!&#x27;)def plot_images(imgs): &quot;&quot;&quot;绘制几个样本图片 :param show: 是否显示绘图 :return: &quot;&quot;&quot; sample_num = min(9, len(imgs)) img_figure = plt.figure(1) img_figure.set_figwidth(5) img_figure.set_figheight(5) for index in range(0, sample_num): ax = plt.subplot(3, 3, index + 1) ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;) ax.grid(False) plt.margins(0, 0) plt.show()plot_images(train_images)print(&quot;初始化并训练贝叶斯模型...&quot;)# 定义 朴素贝叶斯模型classifier_BNB = BernoulliNB()# 训练模型classifier_BNB.fit(train_images,train_labels)print(&#x27;训练完成!&#x27;)print(&quot;测试训练好的贝叶斯模型...&quot;)# 分类器在测试集上的预测值test_predict_BNB = classifier_BNB.predict(test_images)print(&quot;预测完成!&quot;)# 计算准确率accuracy = classifier_BNB.score(test_images, test_labels)print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy) 对结果进行统计比较分析。 12345678910111213141516171819202122# 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 class_num = &#123;&#125;# 每个类别预测为 0-9 类别的个数，predict_num = []# 每个类别预测的准确率class_accuracy = &#123;&#125;for i in range(10): # 找到类别是 i 的下标 class_is_i_index = np.where(test_labels == i)[0] # 统计类别是 i 的个数 class_num[i] = len(class_is_i_index) # 统计类别 i 预测为 0-9 各个类别的个数 predict_num.append( [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)]) # 统计类别 i 预测的准确率 class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100 print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % ( i, class_num[i], predict_num[i][i], class_accuracy[i])) 用热力图对结果进行分析。 123456789101112import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) np.random.seed(0)uniform_data = predict_numax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)ax.set_xlabel(&#x27;真实值&#x27;)ax.set_ylabel(&#x27;预测值&#x27;)plt.show()","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"考研","slug":"杂项/考研","date":"2022-04-21T06:19:23.000Z","updated":"2023-09-20T06:48:11.440Z","comments":true,"path":"2022/04/21/杂项/考研/","link":"","permalink":"https://wingowen.github.io/2022/04/21/%E6%9D%82%E9%A1%B9/%E8%80%83%E7%A0%94/","excerpt":"考研院校信息整理汇总。","text":"考研院校信息整理汇总。 报考专业 深大 - 人工智能与金融科技 初试科目 101 思想政治理论 201 英语一 301 数学一 408 计算机学科专业基础综合 复试科目 FSX8 机器学习 计算机考研 408 包括（150） 数据结构 45 计算机组成原理 45 操作系统 35 计算机网络 25","categories":[],"tags":[{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"}]},{"title":"机器学习基础教程","slug":"机器学习/机器学习基础教程","date":"2020-08-18T12:27:37.000Z","updated":"2023-09-20T07:35:51.893Z","comments":true,"path":"2020/08/18/机器学习/机器学习基础教程/","link":"","permalink":"https://wingowen.github.io/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/","excerpt":"深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。","text":"深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。 引言 机器学习（machine learning）是从数据中提取知识。它是统计学、人工智能和计算机科学交叉的研究领域，也被称为预测分析（predictive analytics）或统计学习（statistical learning）。 最成功的机器学习算法是能够将决策过程自动化的那些算法，这些决策过程是从已知示例中泛化得出的。 机器学习可以分为两大类： 监督学习算法：从输入 / 输出对中进行学习的机器学习算法。 无监督学习算法：只有输入数据是已知的，没有为算法提供输出数据。 无论是监督学习任务还是无监督学习任务，将输入数据表征为计算机可以理解的形式都是十分重要的。在机器学习中，这里的每个实体或每一行被称为一个样本（sample）或数据点，而每一列（用来描述这些实体的属性）则被称为特征（feature）。 构建良好的数据表征的方法称为特征提取（feature extraction）或特征工程（feature engineering）。 环境搭建 Anaconda 管理 Python 环境，已经预先安装好了 Python 科学计算包。 123456789101112conda --versionconda create -n python37 python=3.7conda env listconda activate python37conda remove --name python36 --all# 第三方包pip install requestspip uninstall requests# 查看环境包信息conda list# 若直接安装 Python 可用 pip 安装将要用到的所有包pip install numpy scipy matplotlib ipython scikit-learn pandas 必要的库和工具的使用。 NumPy 123456789## Numpy 数组import numpy as npx = np.array([[1, 2, 3], [4, 5, 6]])print(&quot;x:\\n&#123;&#125;&quot;.format(x))&#x27;&#x27;&#x27;x:[[1 2 3] [4 5 6]]&#x27;&#x27;&#x27; SciPy 12345678910111213141516171819202122232425262728293031323334353637## Scipy 是用于科学计算的函数集合# 最重要的是 scipy.sparse 给出稀疏矩阵from scipy import sparseimport numpy as np# 创建一个二维 NumPy 数组，对角线为 1，其余都为 0eye = np.eye(4)print(&quot;NumPy array:\\n&#123;&#125;&quot;.format(eye))&#x27;&#x27;&#x27;NumPy array:[[ 1. 0. 0. 0.] [ 0. 1. 0. 0.] [ 0. 0. 1. 0.] [ 0. 0. 0. 1.]]&#x27;&#x27;&#x27;# 将 NumPy 数组转换为CSR格式的SciPy稀疏矩阵# 只保存非零元素sparse_matrix = sparse.csr_matrix(eye)print(&quot;\\nSciPy sparse CSR matrix:\\n&#123;&#125;&quot;.format(sparse_matrix))&#x27;&#x27;&#x27;SciPy sparse CSR matrix: (0, 0) 1.0 (1, 1) 1.0 (2, 2) 1.0 (3, 3) 1.0&#x27;&#x27;&#x27;## 直接创建稀疏矩阵 COO 格式# 创建 [1. 1. 1. 1.]data = np.ones(4)# 创建 [0 1 2 3]row_indices = np.arange(4)# 创建 [0 1 2 3]col_indices = np.arange(4)# 直接创建稀疏矩阵，仅提供非 0 的格子与存储的数据eye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))print(&quot;COO representation:\\n&#123;&#125;&quot;.format(eye_coo)) matplotlib 1234567891011# 使用魔法命令来显示图像 否则需要 plt.show%matplotlib inlineimport numpy as npimport matplotlib.pyplot as plt# 在 -10 和 10 之间生成一个数列，共 100 个数x = np.linspace(-10, 10, 100)# 用正弦函数创建每个 x 的 y 值y = np.sin(x)# plot 函数绘制一个数组关于另一个数组的折线图 (marker=&quot;x&quot; 看起来是用字母 x 在曲线上标记一下而已)plt.plot(x, y, marker=&quot;x&quot;) pandas pandas 是用于处理和分析数据的 Python 库。它基于一种叫作 DataFrame 的数据结构，这种数据结构模仿了 R 语言中的 DataFrame。 简单来说，一个 pandas DataFrame 是一张表格，类似于 Excel 表格。pandas 中包含大量用于修改表格和操作表格的方法，尤其是可以像 SQL 一样对表格进行查询和连接。 12345678910111213141516import pandas as pdfrom IPython.display import display## 利用字典创建 DataFramedata = &#123; &#x27;Name&#x27;: [&quot;John&quot;, &quot;Anna&quot;, &quot;Peter&quot;, &quot;Linda&quot;], &#x27;Location&#x27; : [&quot;New York&quot;, &quot;Paris&quot;, &quot;Berlin&quot;, &quot;London&quot;], &#x27;Age&#x27; : [24, 13, 53, 33]&#125;# 创建表格data_pandas = pd.DataFrame(data)# 打印表格display(data_pandas)# 选择年龄大于 30 的所有行display(data_pandas[data_pandas.Age &gt; 30]) 鸢尾花分类 前提：有一些鸢尾花的测量数据，这些花之前已经被植物学专家鉴定为属于 setosa、versicolor 或 virginica 三个品种之一。另外，这些测量数据可以确定每朵鸢尾花所属的品种。 目标：构建一个机器学习模型，可以从这些已知品种的鸢尾花测量数据中进行学习，从而能够预测新鸢尾花的品种。 分析：因为我们有已知品种的鸢尾花的测量数据，所以这是一个监督学习问题。我们要在多个选项中预测其中一个（鸢尾花的品种）。这是一个分类（classification）问题，可能的输出（鸢尾花的不同品种）叫作类别（class）。数据集中的每朵鸢尾花都属于三个类别之一，所以这是一个三分类问题。单个数据点（一朵鸢尾花）的预期输出是这朵花的品种。对于一个数据点来说，它的品种叫作标签（label）。 取得数据 1234567891011121314151617181920212223from sklearn.datasets import load_iris# 加载数据iris_dataset = load_iris()# 数据集包含了哪些信息print(&quot;Keys of iris_dataset: \\n&#123;&#125;&quot;.format(iris_dataset.keys()))&#x27;&#x27;&#x27;Keys of iris_dataset:dict_keys([&#x27;target_names&#x27;, &#x27;feature_names&#x27;, &#x27;DESCR&#x27;, &#x27;data&#x27;, &#x27;target&#x27;])&#x27;&#x27;&#x27;# 数据集的描述信息 DESCRprint(iris_dataset[&#x27;DESCR&#x27;][:193] + &quot;\\n...&quot;)# 数据集包含了 3 个品种，也就是分类print(&quot;Target names: &#123;&#125;&quot;.format(iris_dataset[&#x27;target_names&#x27;]))# 数据集的特征列表print(&quot;Feature names: \\n&#123;&#125;&quot;.format(iris_dataset[&#x27;feature_names&#x27;]))# 样本数据在 data 里，是 numpy 的 array，其中 shape 记录了有多少样本，每行样本有几个属性print(&quot;Type of data: &#123;&#125;&quot;.format(type(iris_dataset[&#x27;data&#x27;])))print(&quot;Shape of data: &#123;&#125;&quot;.format(iris_dataset[&#x27;data&#x27;].shape))# 样本数据print(&quot;First five rows of data:\\n&#123;&#125;&quot;.format(iris_dataset[&#x27;data&#x27;][:5]))# 分类数据在 target 里，也是 numpy arrayprint(&quot;Shape of target: &#123;&#125;&quot;.format(iris_dataset[&#x27;target&#x27;].shape)) 划分数据 我们想要利用这些数据构建一个机器学习模型，用于预测新测量的鸢尾花的品种。但在将模型应用于新的测量数据之前，我们需要知道模型是否有效，也就是说，我们是否应该相信它的预测结果。 我们不能将用于构建模型的数据用于评估模型。因为我们的模型会一直记住整个训练集，所以对于训练集中的任何数据点总会预测正确的标签。这种“记忆”无法告诉我们模型的泛化（generalize）能力如何（换句话说，在新数据上能否正确预测）。 通常的做法是将收集好的带标签数据（此例中是 150 朵花的测量数据 分成两部分。一部分数据用于构建机器学习模型，叫作训练数据（training data）或训练集（training set）。其余的数据用来评估模型性能，叫作测试数据（test data）、测试集（test set）或留出集（hold-out set）。 12345678910111213# 切分样本from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitiris_dataset = load_iris()# train_test_split 函数利用伪随机数生成器将数据集打乱# 为保证多次运行输出一致，这里指定 random_state = 0（随机生成器种子）X_train, X_test, y_train, y_test = \\ train_test_split(iris_dataset[&#x27;data&#x27;], iris_dataset[&#x27;target&#x27;], random_state=0)# 75% 25%print(X_train.shape, y_train.shape)print(X_test.shape, y_test.shape) 观察数据 在构建机器学习模型之前，通常最好检查一下数据，看看如果不用机器学习能不能轻松完成任务，或者需要的信息有没有包含在数据中。此外，检查数据也是发现异常值和特殊值的好方法。 检查数据的最佳方法之一就是将其可视化。一种可视化方法是绘制散点图（scatter plot）。数据散点图将一个特征作为 x 轴，另一个特征作为 y 轴，将每一个数据点绘制为图上的一个点。用这种方法难以对多于 3 个特征的数据集作图。解决这个问题的一种方法是绘制散点图矩阵（pair plot），从而可以两两查看所有的特征。 1234567891011121314151617from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport pandas as pdimport mglearniris_dataset = load_iris()X_train, X_test, y_train, y_test = \\ train_test_split(iris_dataset[&#x27;data&#x27;], iris_dataset[&#x27;target&#x27;], random_state=0)# 利用 X_train 中的数据创建 DataFrame# 利用 iris_dataset.feature_names 中的字符串对数据列进行标记iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)# 利用 DataFrame 创建散点图矩阵，按 y_train 着色# mglearn 为自定义图像美化库grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), \\ marker=&#x27;o&#x27;, hist_kwds=&#123;&#x27;bins&#x27;: 20&#125;, s=60, alpha=.8, cmap=mglearn.cm3) 这个画散点矩阵图的函数说实话我看不太懂。 模型构建 好了，现在让我们开始构建真实的机器学习模型了。让我们先从一个比较容易理解的算法：KNN 分类器算法（k 近邻分类器）。 k 近邻算法中 k 的含义是，我们可以考虑训练集中与新数据点最近的任意 k 个邻居（比如说，距离最近的 3 个或 5 个邻居），而不是只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。 1234567891011121314151617181920212223242526272829from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifieriris_dataset = load_iris()# 切分 75% 的训练集和 25% 的测试集, 随机打乱数据的种子固定为 0X_train, X_test, y_train, y_test = train_test_split(iris_dataset[&#x27;data&#x27;], iris_dataset[&#x27;target&#x27;], random_state=0)# K 邻近分类器, 算法取最近 1 个点的标签作为分类 即 k=1knn = KNeighborsClassifier(n_neighbors=1)# 训练模型knn.fit(X_train, y_train)# 构造一个测试的数据, 需要作为一行放在矩阵里X_new = np.array([[5, 2.9, 1, 0.2]])# 预测分类y_new = knn.predict(X_new)# 分类是整形 0, 1, 2 分别对应相应的类型print(y_new)# 对应成分类的名字print(iris_dataset.target_names[y_new])# 在测试集做预测y_pred = knn.predict(X_test)# 计算预测的精度, 其中 y_pred==y_test 得到的是一个 bool 向量, np.mean 返回 True 的比例print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(np.mean(y_pred == y_test)))# 也可以直接用模型的 score 方法来完成精度计算print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(knn.score(X_test, y_test))) 小结 监督学习与非监督学习 可能的品种叫做分类，具体每个样本的品种就叫做标签 数据集需划分为训练集和测试集 数据集包括了 X 和 y，其中 X 是特征的二维数组，y 是标签的一维数组 本章用到了 scikit-learn 中任何机器学习算法的核心方法：fit、predict、score 监督学习 记住，每当想要根据给定输入预测某个结果，并且还有输入 / 输出对的示例时，都应该使用监督学习。 基本概念 分类与回归 简单来说，分类是预测标签，包括二分类与多分类。回归是预测连续值，比如预测收入、房价。 泛化、过拟合与欠拟合 随着模型算法逐渐复杂，其在训练集上的预测精度将提高，但在测试集上的预测精度将降低，因此模型的复杂度需要折衷。 模型过于复杂，将导致模型泛化能力差，即过拟合。 模型过于简单，将导致模型精度在训练集表现就很差，更不用说测试集的表现了，此时即欠拟合。 模型复杂度与数据集大小的关系 数据点的值变化范围越大，则可以应用更加复杂的模型，预测的表现也会越好。更多的训练数据往往伴随着更大范围的特征值变化，因此可以应用更复杂的模型算法。 但注意，如果是非常类似的数据点，无论数据集多大也是无济于事的。 算法 现在开始介绍最常用的机器学习算法，并解释这些算法如何从数据中学习以及如何预测。 许多算法都有分类和回归两种形式。 KNN 前面的鸢尾花的例子已经简单的介绍过 KNN 的原理，这里我们先用一张图来简单的复习一下，这张图表示的是 n_neighbors=3 的情况（3 个邻居）。 好了，我们将在现实世界的乳腺癌数据集上进行研究。先将数据集分成训练集和测试集，然后用不同的邻居个数对训练集和测试集的性能进行评估。 12345678910111213141516171819202122232425262728293031323334from sklearn.datasets import load_breast_cancerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_splitimport matplotlib.pyplot as plt# 加载数据cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=66)# 记录不同 n_neighbors 情况下，模型的训练集精度与测试集精度的变化training_accuracy = [] test_accuracy = []# n_neighbors 取值从 1 到 10 neighbors_settings = range(1, 11)for n_neighbors in neighbors_settings: # 模型对象 clf = KNeighborsClassifier(n_neighbors=n_neighbors) # 训练 clf.fit(X_train, y_train) # 记录训练集精度 training_accuracy.append(clf.score(X_train, y_train)) # 记录测试集精度 test_accuracy.append(clf.score(X_test, y_test))# 画出 2 条曲线，横坐标是邻居个数，纵坐标分别是训练集精度和测试集精度plt.plot(neighbors_settings, training_accuracy, label=&quot;training accuracy&quot;)plt.plot(neighbors_settings, test_accuracy, label=&quot;test accuracy&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.xlabel(&quot;n_neighbors&quot;)plt.legend() 通过前面的介绍，我们可以知道是否有乳腺癌是一个二分类的问题，这里的 KNN 属于分类算法形式。但其实 KNN 也可以用于回归算法形式。 我们先看看 wave 数据集在 n_neighbors = 3 的图示情况。在使用多个近邻时，预测结果为这些近邻的平均值。 12345678910111213141516from sklearn.neighbors import KNeighborsRegressor# 加载数据集 （自定义的 wave 数据集）X, y = mglearn.datasets.make_wave(n_samples=40)# 将 wave 数据集分为训练集和测试集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)# 模型实例化 并将邻居个数设为 3 这里用的是回归模型reg = KNeighborsRegressor(n_neighbors=3) # 利用训练数据和训练目标值来拟合模型 reg.fit(X_train, y_train)# predict 测试集print(&quot;Test set predictions:\\n&#123;&#125;&quot;.format(reg.predict(X_test)))# 评估模型print(&quot;Test set R^2: &#123;:.2f&#125;&quot;.format(reg.score(X_test, y_test))) 到这里，我们不难发现，KNN 在 n_neighbors 的值越大时，模型越简单，约适合简单的数据。在测试 KNN 分类形式时，我们做了 n_neighbors 从 1~10 的测试精确度与预测精确度的变化对比，从上面的图中可以很好的看出这一规律。其实也很好理解，毕竟判断所需要的邻居越多，那模型的鲁棒性肯定是越强的，那表现大概率也是趋于稳定的。 虽然 k 近邻算法很容易理解，但由于预测速度慢且不能处理具有很多特征的数据集，所以在实践中往往不会用到。接下来我们来看看实践中常用的模型。 回归线性模型 线性模型利用输入特征的线性函数（linear function）进行预测。 对于回归问题，线性模型预测的一般公式是：ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b。有许多种不同的线性回归模型，区别在于模型如何学习到参数 w 和 b，以及如何控制模型复杂度。 线性回归 线性回归，或者普通最小二乘法（ordinary least squares，OLS），是回归问题最简单也最经典的线性方法。线性回归寻找参数 w 和 b，使得对训练集的预测值与真实的回归目标值 y 之间的均方误差最小。 均方误差（mean squared error）是预测值与真实值之差的平方和除以样本数。线性回归没有参数，这是一个优点，但也因此无法控制模型的复杂度。 来看看 wave 数据集的线性回归模型预测的图示。 123456789101112131415161718192021222324from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitimport mglearn# 生成 60 个样本数据, 一维特征X, y = mglearn.datasets.make_wave(n_samples=60)# 切分数据集X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)# 训练线性回归模型lr = LinearRegression().fit(X_train, y_train)# coef_ 就是斜率 w 即每个特征对应一个权重print(&quot;lr.coef_: &#123;&#125;&quot;.format(lr.coef_))# intercept_ 是截距 bprint(&quot;lr.intercept_: &#123;&#125;&quot;.format(lr.intercept_))# 训练集精度print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(lr.score(X_train, y_train)))# 测试机精度print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(lr.score(X_test, y_test)))&#x27;&#x27;&#x27;Training set score: 0.67Test set score: 0.66&#x27;&#x27;&#x27; 你可能注意到了 coef_ 和 intercept_ 结尾处奇怪的下划线。sklearn 总是将从训练数据中得出的值保存在以下划线结尾的属性中。这是为了将其 与用户设置的参数区分开。 从这个模拟的小数据集 wave 的训练中得到的训练精确度和测试精确度非常接近，这表示可能存在欠拟合的问题。对于简单的数据（比如本例子中的一维数据集）来说，过拟合的风险很小，因为模型非常简单。 然而，对于更高维的数据集（即有大量特征的数据集），线性模型将变得更加强大，过拟合的可能性也会变大。接下来让我我们来看一下 LinearRegression 在更复杂的数据集上的表现。 1234567891011121314151617from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) lr = LinearRegression().fit(X_train, y_train)print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(lr.score(X_train, y_train)))print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(lr.score(X_test, y_test)))&#x27;&#x27;&#x27;Training set score: 0.95Test set score: 0.61&#x27;&#x27;&#x27; 在这个维度较高的数据集中，我们在训练集上的预测非常准确，但测试集上的精确度就要低很多。训练集和测试集之间的性能差异是过拟合的明显标志，因此我们应该试图找到一个可以控制复杂度的模型。标准线性回归最常用的替代方法之一就是岭回归（ridge regression），下面来看一下。 岭回归 采用线性回归同样的公式，但是模型约束学习得到的 w 系数尽可能的接近于 0，即每个特征对输出的影响尽可能小，从而避免过拟合。这个约束叫做正则化，岭回归用到的是 L2 正则化。 L1 正则化是指权值向量 w 中各个元素的绝对值之和，通常表示为 ||w||1。 L2 正则化是指权值向量 w 中各个元素的平方和然后再求平方根，通常表示为 ||w||2。 B 站视频详细讲解 123456789101112131415161718from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import Ridgeimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)# 默认值 alpha=1ridge = Ridge(alpha=1).fit(X_train, y_train)print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(ridge.score(X_train, y_train)))print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(ridge.score(X_test, y_test)))```Training set score: 0.89Test set score: 0.75``` Ridge 在训练集上的分数要低于 LinearRegression，但在测试集上的分数更高。 这和我们的预期一致。 岭回归泛化能力优于线性回归，带来的就是训练集精度下降，测试集精度上升。其基本原理是对影响力大的 w 项进行了惩罚。 该模型支持 alpha 参数，该参数默认为 1，调大 alpha 增大约束会进一步下降训练集精度，可能加强泛化能力；相反，调小 alpha 则减少了约束，训练集精度上升，可能降低泛化能力。 12345678910111213141516# 利用上面的数据尝试 alpha=10 的情况ridge10 = Ridge(alpha=10).fit(X_train, y_train)print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(ridge10.score(X_train, y_train)))print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(ridge10.score(X_test, y_test)))&#x27;&#x27;&#x27;Training set score: 0.79Test set score: 0.64&#x27;&#x27;&#x27;# alpha=0.1 的情况ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(ridge10.score(X_train, y_train)))print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(ridge10.score(X_test, y_test)))&#x27;&#x27;&#x27;Training set score: 0.93Test set score: 0.77&#x27;&#x27;&#x27; 岭回归会对模型的进行约束，所以在训练集较少的时候表现要优于线性回归。但需要注意的是，当数据量越来越多的时候，正则化变得不那么重要，并且岭回归和线性回归将具有相同的性能。 Lasso 除了 Ridge，还有一种正则化的线性回归是 Lasso，不同之处在于 Lasso 用的方法为 L1 正规化。L1 正则化的结果是，使用 Lasso 时某些系数刚好为 0。这说明某些特征被模型完全忽略。这可以看作是一种自动化的特征选择。某些系数刚好为 0，这样模型更容易解释，也可以呈现模型最重要的特征。 同样的的，我们将 Lasso 应用在波士顿房价数据集上。 1234567891011121314151617181920from sklearn.linear_model import Lassofrom sklearn.model_selection import train_test_splitimport numpy as npimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) lasso = Lasso().fit(X_train, y_train)print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(lasso.score(X_train, y_train)))print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(lasso.score(X_test, y_test)))# lasso.coef_ 是 w 斜率向量 数一下有几个特征的系数不为 0print(&quot;Number of features used: &#123;&#125;&quot;.format(np.sum(lasso.coef_ != 0)))&#x27;&#x27;&#x27;Training set score: 0.29Test set score: 0.21Number of features used: 4&#x27;&#x27;&#x27; 该模型只用到了 105 个特征中的 4 个，其他的 w 系数都是 0。从以上这个结果来看，该模型预测精度很差，属于欠拟合，需要减少模型的 alpha 参数，即放松正则化 L1。这么做的同时，我们还需要增加 max_iter 的值（运行迭代的最大次数）。 1234567891011121314151617181920from sklearn.linear_model import Lassofrom sklearn.model_selection import train_test_splitimport numpy as npimport mglearn# 波士顿 extended 数据集X, y = mglearn.datasets.load_extended_boston()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # 增大 max_iter 的值 否则模型会警告我们 说应该增大 max_iterlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train) print(&quot;Training set score: &#123;:.2f&#125;&quot;.format(lasso001.score(X_train, y_train))) print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(lasso001.score(X_test, y_test))) print(&quot;Number of features used: &#123;&#125;&quot;.format(np.sum(lasso001.coef_ != 0)))&#x27;&#x27;&#x27;Training set score: 0.90Test set score: 0.77Number of features used: 33&#x27;&#x27;&#x27; 在实践中，在两个模型中一般首选岭回归。但如果特征很多，你认为只有其中几个是重要的，那么选择 Lasso 可能更好。同样，如果你想要一个容易解释的模型，Lasso 可以给出更容易理解的模型，因为它只选择了一部分输入特征。scikit-learn 还提供了 ElasticNet 类，结合了 Lasso 和 Ridge 的惩罚项。在实践中，这种结合的效果最好，不过代价是要调节两个参数：一个用于 L1 正则化，一个用于 L2 正则化。 分类线性模型 线性模型也广泛应用于分类问题。我们首先来看二分类。这时可以利用下面的公式进行预测： ŷ = w[0] * x[0] + w[1] * x[1] + ...+ w[p] * x[p] + b &gt; 0 设置了阈值 0，y 小于 0 则预测为类别 -1，大于 0 则预测为类类别 +1。 不同的线性分类算法的区别包括 2 点： w 和 b 对训练集拟合好坏的度量方式（损失函数） 是否使用正则化以及使用哪种正则化 常见线性分类算法包括： LogisticRegression：Logistic 回归分类器（注意只是名字叫回归，但是分类算法） LinearSVC：线性支持向量机分类器 这两个分类算法默认应用的是 L2 正则化。 SVM=Support Vector Machine 支持向量机 SVC=Support Vector Classification 支持向量机用于分类 SVC=Support Vector Regression 支持向量机用于回归分析 LR &amp; SVC 让我们在构造的小数据集上尝试着对比这两种线性分类算法。 1234567891011121314151617181920212223242526from sklearn.linear_model import LogisticRegressionfrom sklearn.svm import LinearSVCimport mglearnimport matplotlib.pyplot as pltX, y = mglearn.datasets.make_forge()# subplots(m,n,figsize)函数 把 n 个图画在 m 行里，每个图片的长宽由 figsize 指定# 返回的第二个值是每个图的绘制位置fig, axes = plt.subplots(1, 2, figsize=(10, 3))# 利用 zip 组合：让 LinearSVC 画在第一个图片中，LogisticRegression 画在第二个图片中# zip() 函数用于将可迭代的对象作为参数# 将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。# 权衡参数默认 C=1for model, ax in zip([LinearSVC(C=1, penalty=&quot;l2&quot;), LogisticRegression(C=1, penalty=&quot;l2&quot;)], axes): # 训练模型 clf = model.fit(X, y) # 画出了这个线性 model 的图像 是一个斜线 mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=.7) # 特征一 特征二 输入的标签 图像位置 是一个离散点图 mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax) ax.set_title(&quot;&#123;&#125;&quot;.format(clf.__class__.__name__)) ax.set_xlabel(&quot;Feature 0&quot;) ax.set_ylabel(&quot;Feature 1&quot;)axes[0].legend() 两个模型得到了相似的决策边界。注意，两个模型中都有两个点的分类是错误的。两个模型都默认使用 L2 正则化，就像 Ridge 对回归所做的那样。 对于 LogisticRegression 和 LinearSVC，决定正则化强度的权衡参数叫作 C。C 值越大，对应的正则化越弱。换句话说，如果参数 C 值较大，那么 LogisticRegression 和 LinearSVC 将尽可能将训练集拟合到最好，而如果 C 值较小，那么模型更强调使系数向量（w）接近于 0。 参数 C 的作用还有另一个有趣之处。较小的 C 值可以让算法尽量适应大多数数据点，而较大的 C 值更强调每个数据点都分类正确的重要性。C 越大越有可能导致模型的过拟合。 与回归的情况类似，用于分类的线性模型在低维空间中看起来可能非常受限，决策边界只能是直线或平面。同样，在高维空间中，用于分类的线性模型变得非常强大，当考虑更多特征时，避免过拟合变得越来越重要。 然我们在乳腺癌数据集上详细分析逻辑回归。 1234567891011121314from sklearn.linear_model import LogisticRegressionfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitcancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=42)logreg = LogisticRegression().fit(X_train, y_train)print(&quot;Training set score: &#123;:.3f&#125;&quot;.format(logreg.score(X_train, y_train)))print(&quot;Test set score: &#123;:.3f&#125;&quot;.format(logreg.score(X_test, y_test)))&#x27;&#x27;&#x27;Training set score: 0.955Test set score: 0.958&#x27;&#x27;&#x27; 在训练集和测试集上的精度（性能）都很好，而且基本一样，这种情况可以尝试加强对训练集拟合看是否能带来进一步提升。将 C 调大以减弱正则化。 1234567logreg = LogisticRegression(C=100).fit(X_train, y_train)print(&quot;Training set score: &#123;:.3f&#125;&quot;.format(logreg.score(X_train, y_train)))print(&quot;Test set score: &#123;:.3f&#125;&quot;.format(logreg.score(X_test, y_test)))&#x27;&#x27;&#x27;Training set score: 0.974Test set score: 0.965&#x27;&#x27;&#x27; 从以上结果可以发现，模型的精度得到了进一步的提升。 如果想要一个可解释性更强的模型，使用 L1 正则化可能更好，因为它约束模型只使用少数几个特征 多分类 许多线性分类只适用于二分类问题，不能轻易推广到多类别问题（除了 LR）。将二分类算法推广到多分类算法的一种常见方法是一对其余 one-vs.-rest 方法。 one-vs.rest：对每个类别都学习一个二分类模型，将这个类别与所有其他类别尽量分开，这样就生成了与类别个数一样多的二分类模型。在测试点上运行所有二类分类器来进行预测。在对应类别上分数最高的分类器胜出，将这个类别标签返回作为预测结果。 我们用一个简单的自定义数据集来体验一下 one-vs.-rest 方法 1234567import mglearnfrom sklearn.datasets import make_blobsX, y = make_blobs(random_state=42)mglearn.discrete_scatter(X[:, 0], X[:, 1], y)plt.xlabel(&quot;Feature 0&quot;)plt.ylabel(&quot;Feature 1&quot;)plt.legend([&quot;Class 0&quot;, &quot;Class 1&quot;, &quot;Class 2&quot;]) 这个由高斯分布中采样得出的数据如下图所示。 现在，在这个数据集上训练一个 LinearSVC 分类器。 12345678from sklearn.svm import LinearSVClinear_svm = LinearSVC().fit(X, y)print(&quot;Coefficient shape: &quot;, linear_svm.coef_.shape)print(&quot;Intercept shape: &quot;, linear_svm.intercept_.shape)&#x27;&#x27;&#x27;Coefficient shape: (3, 2)Intercept shape: (3,)&#x27;&#x27;&#x27; coef_ 的形状是 (3, 2)，说明 coef_ 每行包含三个类别之一的系数向量，每列包含某个特征（这个数据集有 2 个特征）对应的系数值。现在 intercept_ 是一维数组，保存每个类别的截距。 接下来让我们将这 3 个二分类器所代表的直线进行可视化。 1234567891011mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)mglearn.discrete_scatter(X[:, 0], X[:, 1], y)# 生成数列line = np.linspace(-15, 15)for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, [&#x27;b&#x27;, &#x27;r&#x27;, &#x27;g&#x27;]): plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)plt.ylim(-10, 15)plt.xlim(-10, 8)plt.xlabel(&quot;Feature 0&quot;)plt.ylabel(&quot;Feature 1&quot;)plt.legend([&#x27;Class 0&#x27;, &#x27;Class 1&#x27;, &#x27;Class 2&#x27;, &#x27;Line class 0&#x27;, &#x27;Line class 1&#x27;, &#x27;Line class 2&#x27;], loc=(1.01, 0.3)) 线性模型总结 线性模型主要参数就是正则化参数，包括 L1 / L2，以及回归的 alpha 以及分类的 C 值。 alpha 越大或者 C 越小，则正则化越强，可以理解为 w 系数都很小，模型很简单，对训练集精度也会下降。 线性模型无论训练还是预测都很快，但是大数据集需要考虑 solver='sag' 加速训练。 L1 正则化因为会让很多 w 系数为 0，使模型更简单，更容易理解。 朴素贝叶斯分类器 与线性模型相似，但速度更快，泛化能力较弱的一种分类器。 朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。 一共有三种类型的模型。 GaussianNB: 特征可以是任意连续数据。 BernoulliNB：特征必须是 2 分类的数据。 MultinomialNB：特征是计数性质的数据。 GaussianNB 适合高维数据，后两者适合文本领域的稀疏数据。后两个模型支持 alpha 参数，调大该值可以略微提高精度。 决策树 让我们来看一个简单的 deep=2 的树的决策边界与相应的树的图示。 随着树的深度的增加，模型肯定会越来越复杂。那我们该如何控制决策树的复杂度呢？ 防止过拟合有两种常见的策略：一种是及早停止树的生长，也叫预剪枝（pre-pruning）；另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫后剪枝（post-pruning）或剪枝（pruning）。预剪枝的限制条件可能包括限制树的最大深度、限制叶结点的最大数目，或者规定一个结点中数据点的最小数目来防止继续划分。 sklearn 只实现了预剪枝，没有实现后剪枝。 让我们在乳腺癌数据集上更详细地看一下预剪枝的效果。 123456789101112131415161718192021from sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_breast_cancer# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, stratify=cancer.target, random_state=42)# 决策树分类tree = DecisionTreeClassifier(random_state=0)# 训练tree.fit(X_train, y_train)# 精度print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(tree.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(tree.score(X_test, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 1.000Accuracy on test set: 0.937&#x27;&#x27;&#x27; 我们用数据进行了建模，并没有进行预剪枝的处理，这个训练精度为 100% 的结果是可以预测到的，这是因为叶结点都是纯的，树的深度很大，足以完美地记住训练数据的所有标签。测试集精度比之前讲过的线性模型略低，线性模型的精度约为 95%。 如果我们不限制树的深度，其复杂度和深度将可以无限大。故未进行预剪枝的树很容易过拟合，对新数据的泛化能力不高。接下来，我们将树的深度控制在 4 来训练模型。 1234567891011# 决策树分类tree = DecisionTreeClassifier(max_depth=4, random_state=0)# 训练tree.fit(X_train, y_train)# 精度print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(tree.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(tree.score(X_test, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 0.988Accuracy on test set: 0.951&#x27;&#x27;&#x27; 对于决策树模型，我们可以利用 tree 模块的 export_graphviz 函数来将树可视化。这个函数会生成一个 .dot 格式的文件，这是一种用于保存图形的文本文件格式。 123456789# 导出from sklearn.tree import export_graphvizexport_graphviz(tree, out_file=&quot;tree.dot&quot;, class_names=[&quot;malignant&quot;,&quot;benign&quot;], feature_names=cancer.feature_names, impurity=False, filled=True)# 读入import graphvizwith open(&quot;tree.dot&quot;) as f: dot_graph = f.read()graphviz.Source(dot_graph) 查看整个树是非常费劲的，我们常用特征重要性（feature importance）来为每个特征对树的决策的重要性进行排序。对于每个特征来说，它都是一个介于 0 和 1 之间的数字，其中 0 表示根本没用到，1 表示完美预测目标值。特征重要性的求和始终为 1。 1234567print(&quot;Feature importances:\\n&#123;&#125;&quot;.format(tree.feature_importances_))&#x27;&#x27;&#x27;Feature importances:[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.01 0.048 0. 0. 0.002 0. 0. 0. 0. 0. 0.727 0.046 0. 0. 0.014 0. 0.018 0.122 0.012 0. ] &#x27;&#x27;&#x27; 回归的决策树与分类的决策树是很相似的，但在将基于树的模型用于回归时，都是不能外推（extrapolate）的，也不能在训练数据范围之外进行预测。 这是因为回归是为了预测一个连续值，但是树的叶节点只能保存训练集内出现过的目标值，因此对于训练集外的数据是无法进行目标预测的，只能得到一个训练集内出现过的结果。 决策树集成 集成（ensemble）是合并多个机器学习模型来构建更强大模型的方法。现已证明的对大量分类和回归的数据集都是有效的模型有：是随机森林（random forest）和梯度提升决策树（gradient boosted decision tree），二者都是以决策树为基础的。 随机森林 前面我们说过，决策树的一个主要缺点在于经常对训练数据过拟合。随机森林是解决这个问题的一种方法。随机森林本质上是许多决策树的集合，其中每棵树都和其他树略有不同。 随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。 随机森林的名字来自于将随机性添加到树的构造过程中，以确保每棵树都各不相同。随机森林中树的随机化方法有两种： 一种是通过选择用于构造树的数据点。 另一种是通过选择每次划分测试的特征。 想要构造一棵树，首先要对数据进行自助采样（bootstrap sample）。也就是说，从 n_samples 个数据点中有放回地（即同一样本可以被多次抽取）重复随机抽取一个样本，共抽取 n_samples 次。这样会创建一个与原数据集大小相同的数据集，但有些数据点会缺失（大约三分之一），有些会重复。 想要利用随机森林进行预测，算法首先对森林中的每棵树进行预测。对于回归问题，我们可以对这些结果取平均值作为最终预测。对于分类问题，则用到了软投票（soft voting）策略。也就是说，每个算法做出软预测，给出每个可能的输出标签的概率。对所有树的预测概率取平均值，然后将概率最大的类别作为预测结果。 让我们来看一个 5 个树的随机森林分类。 123456789101112from sklearn.ensemble import RandomForestClassifierfrom sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_splitX, y = make_moons(n_samples=100, noise=0.25, random_state=3)# 依据标签 y 按原数据 y 中各类比例 分配给 train 和 test# 使得 train 和 test 中各类数据的比例与原数据集一样X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)forest = RandomForestClassifier(n_estimators=5, random_state=2)forest.fit(X_train, y_train)print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(forest.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(forest.score(X_test, y_test))) 作为随机森林的一部分，树被保存在 estimator_ 属性中。 与决策树类似，随机森林也可以给出特征重要性，计算方法是将森林中所有树的特征重要性求和并取平均。一般来说，随机森林给出的特征重要性要比单棵树给出的更为可靠。是随机森林比单棵树更能从总体把握 数据的特征。 梯度提升回归树 梯度提升回归树是另一种集成方法，通过合并多个决策树来构建一个更为强大的模型。名字中含有回归，但这个模型既可以用于回归也可以用于分类。与随机森林方法不同，梯度提升采用连续的方式构造树，每棵树都试图纠正前一棵树的错误。默认情况下，梯度提升回归树中没有随机化，而是用到了强预剪枝。梯度提升树通常使用深度很小（1到 5 之间）的树，这样模型占用的内存更少，预测速度也更快。 梯度提升背后的主要思想是合并许多简单的模型（在这个语境中叫作弱学习器），比如深度较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越来越多，可以不断迭代提高性能。 与随机森林相比，它通常对参数设置更为敏感，但如果参数设置正确的话，模型精度更高。除了预剪枝与集成中树的数量之外，梯度提升的另一个重要参数是 learning_rate（学习率），用于控制每棵树纠正前一棵树的错误的强度。较高的学习率意味着每棵树都可以做出较强的修正，这样模型更为复杂。通过增大 n_estimators 来向集成中添加更多树，也可以增加模型复杂度，因为模型有更多机会纠正训练集上的错误。 让我们在乳腺癌数据集上应用 GradientBoostingClassifier 试试看。 12345678910111213141516171819202122232425262728293031323334353637from sklearn.ensemble import GradientBoostingClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitcancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)gbrt = GradientBoostingClassifier(random_state=0)gbrt.fit(X_train, y_train)print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(gbrt.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(gbrt.score(X_test, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 1.000Accuracy on test set: 0.958&#x27;&#x27;&#x27;# 训练精度太高 可能过拟合# 调小学习率从而降低迭代过程中的修正强度 默认 learning_rate=0.1# gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)gbrt.fit(X_train, y_train)print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(gbrt.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(gbrt.score(X_test, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 0.988Accuracy on test set: 0.965&#x27;&#x27;&#x27;# 可以看到泛化能力提高了# 我们也可以通过预剪枝来提升泛化能力 默认 max_depth=3gbrt = GradientBoostingClassifier(max_depth=1, random_state=0)gbrt.fit(X_train, y_train)print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(gbrt.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(gbrt.score(X_test, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 0.991Accuracy on test set: 0.972&#x27;&#x27;&#x27; 由于梯度提升和随机森林两种方法在类似的数据上表现得都很好，因此一种常用的方法就是先尝试随机森林，它的鲁棒性很好。如果随机森林效果很好，但预测时间太长，或者机器学习模型精度小数点后第二位的提高也很重要，那么切换成梯度提升通常会有用。 如果你想要将梯度提升应用在大规模问题上，可以研究一下 xgboost 包及其 Python 接口。这个库在许多数据集上的速度都比 scikit-learn 对梯度提升的实现要快。 核支持向量机 线性模型在特征少的情况下非常受限，比如二维特征情况下可能很难利用一条线区分 2 个分类。 为了继续使用之前讲过的线性模型，可以通过基于已有特征进行组合或者变换添加非线性特征（比如对某个特征求平方作为新特征），更高的维度可以解决线性模型的限制，达到不错的效果。 但问题是我们不知道对已有特征如何进行变换与组合对模型是有效的。 总之，能够将已有数据向更高维变换的话，模型就能够表现的更好。 核技巧 多项式核：在一 定阶数内计算原始特征所有可能的多项式（比如 feature1 ** 2 * feature2 ** 5）。 高斯核：对应无限维特征空间。 12345678910111213141516171819202122232425from sklearn.svm import SVC# 加载数据X, y = mglearn.tools.make_handcrafted_dataset()# 训练，用 RBF 核(高斯核)完成高维映射# gamma 控制宽度 决定点于点的靠近指多大距离 越小则核半径越大# C 参数是正则化参数 限制每个点的重要性# 默认情况下，C=1，gamma=1/n_featuressvm = SVC(kernel=&#x27;rbf&#x27;, C=10, gamma=0.1).fit(X, y) # 画分类的分界线mglearn.plots.plot_2d_separator(svm, X, eps=.5)# 画样本点mglearn.discrete_scatter(X[:, 0], X[:, 1], y)# 画出支持向量，支持向量的类别标签由 dual_coef_ 的正负号给出sv = svm.support_vectors_sv_labels = svm.dual_coef_.ravel() &gt; 0# 画支持向量点mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)plt.xlabel(&quot;Feature 0&quot;)plt.ylabel(&quot;Feature 1&quot;) 由图可以看出 SVM 是非线性的。上图中较大的点代表的是支持向量（位于类别之间边界上的那些点），想要对新样本点进行预测，需要测量它与每个支持向量之间的距离。分类决策是基于它与支持向量之间的距离以及在训练过程中学到的支持向量重要性（保存在 SVC 的 dual_coef_ 属性中）来做出的。 让我们通过图片来理解一下 SVC 的两个参数的作用。 左侧的图决策边界非常平滑，越向右的图决策边界更关注单个点。小的 gamma 值表示决策边界变化很慢，生成的是复杂度较低的模型，而大的 gamma 值则会生成更为复杂的模型。 左上角的图中，决策边界看起来几乎是线性的，误分类的点对边界几乎没有任何影响。再看左下角的图，增大 C 之后这些点对模型的影响变大，使得决策边界发生弯曲来将这些点正确分类。 虽然 SVM 的表现通常都很好，但它对参数的设定和数据的缩放非常敏感。特别地，它要求所有特征有相似的变化范围。所以在使用 SVM 时我们通常要进行数据预处理：每个特征进行缩放，使其大致都位于同一范围。核 SVM 常用的缩放方法就是将所有特征缩放到 0 和 1 之间。 （较复杂，待整理） 神经网络 一类被称为神经网络的算法最近以深度学习的名字再度流行。虽然深度学习在许多机器学习应用中都有巨大的潜力，但深度学习算法往往经过精确调整，只适用于特定的使用场景。 这里只讨论一些相对简单的方法，即用于分类和回归的多层感知机（multilayer perceptron，MLP），它可以作为研究更复杂的深度学习方法的起点。MLP 也被称为（普通）前馈神经网络，有时也简称为神经网络。 MLP 可以被视为广义的线性模型。输入特征经过多次线性变换得到输出。每一个隐层包含多个隐单元，每个隐单元是由前一层的特征经过线性计算后，应用一个非线性函数（叫做激活函数）得到的。计算出前一个隐层内的所有隐单元，作为下一个隐层的特征输入，如此往复。 深度学习入门 我们先用一个小的数据集来尝试一下神经网络。 12345678910111213141516171819202122232425262728from sklearn.neural_network import MLPClassifierfrom sklearn.datasets import make_moonsfrom sklearn.model_selection import train_test_splitimport mglearnimport matplotlib.pyplot as plt# 数据集X, y = make_moons(n_samples=100, noise=0.25, random_state=3)# 切分X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)# 训练神经网络 默认使用 100 个隐结点# solver 参数指定神经网络如何学习 w 系数# 默认 adam 对数据缩放敏感 lbfgs 对数据缩放不敏感 sgd 有大量参数需要调节mlp = MLPClassifier(solver=&#x27;lbfgs&#x27;, random_state=0, hidden_layer_sizes=[100]).fit(X_train, y_train)# 绘制模型分类边界mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)# 画样本点mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)plt.xlabel(&quot;Feature 0&quot;)plt.ylabel(&quot;Feature 1&quot;)# 精度print(mlp.score(X_train, y_train))print(mlp.score(X_test, y_test)) MLP 的可以利用 L2 惩罚使权重趋向于零，从而降低拟合与模型的复杂度。MLPClassifier 中调节 L2 惩罚的参数是 alpha（与线性回归模型中的相同），它的默认值很小（弱正则化）。 我们还是用图示的方式来理解这两个参数对 MLP 模型训练的影响。 从图中我们可以看出，参数 n_hidden 和 alpha 越大，模型越简单，曲线越平滑。 注意，神经网络对特征的范围也很敏感，也要求所有输入特征的变化范围相似，最理想的情况是均值为 0、方差为 1。 接下里我们用 cancer 这个比较大的数据集来对模型进行训练。首先用默认参数进行训练。 1234567891011121314151617181920from sklearn.neural_network import MLPClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)# 训练神经网络mlp = MLPClassifier(random_state=0).fit(X_train, y_train)# 精度print(&quot;Accuracy on training set: &#123;:.2f&#125;&quot;.format(mlp.score(X_train, y_train)))print(&quot;Accuracy on test set: &#123;:.2f&#125;&quot;.format(mlp.score(X_test, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 0.91Accuracy on test set: 0.91&#x27;&#x27;&#x27; 精度不错，但没达到预期（这里看起来也有点欠拟合了的样子），因为神经网络对输入特征要求范围相似，最理想情况是均值为 0，方差为 1，需要进行数据预处理，对数据进行缩放： 12345678910111213141516171819202122232425262728293031from sklearn.neural_network import MLPClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_split# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)# 计算训练集中每个特征的平均值 mean_on_train = X_train.mean(axis=0) # 计算训练集中每个特征的标准差 std_on_train = X_train.std(axis=0)# 减去平均值，然后乘以标准差的倒数 如此运算之后 mean=0 std=1X_train_scaled = (X_train - mean_on_train) / std_on_train # 对测试集做相同的变换(使用训练集的平均值和标准差) X_test_scaled = (X_test - mean_on_train) / std_on_train # 训练 这里我们提高一下最大迭代次数 否则模型会有警告mlp = MLPClassifier(random_state=0, max_iter=1000)mlp.fit(X_train_scaled, y_train)# 精度print(&quot;Accuracy on training set: &#123;:.3f&#125;&quot;.format(mlp.score(X_train_scaled, y_train)))print(&quot;Accuracy on test set: &#123;:.3f&#125;&quot;.format(mlp.score(X_test_scaled, y_test)))&#x27;&#x27;&#x27;Accuracy on training set: 0.995Accuracy on test set: 0.965&#x27;&#x27;&#x27; 分类器的不确定度估计 分类器能够给出预测的不确定度估计。一般来说，你感兴趣的不仅是分类器会预测一个测试点属于哪个类别，还包括它对这个预测的置信程度。 举个很经典的实际案例：不同类型的错误会在现实应用中导致非常不同的结果。想象一个用于测试癌症的医疗应用。假阳性预测可能只会让患者接受额外的测试，但假阴性预测却可能导致重病没有得到治疗。 sklearn 中有两个函数可用于获取分类器的不确定度估计：decision_function 和 predict_proba。大多数分类器（但不是全部）都至少有其中一个函数，很多分类器两个都有。 接下来我们来看这两个函数对一个模拟的二维数据的作用。 小结与展望 先从线性模型、朴素贝叶斯、最邻近等简单模型开始，对数据有了解后再使用随机森林、梯度提升决策树、SVM、神经网络。 K 邻近（KNN）：适用于小型数据集，是很好的基准模型，很容易解释。 线性模型：非常可靠的首选算法，适用于非常大的数据集，也适用于高维数据。 朴素贝叶斯：只适用于分类问题。比线性模型速度还快，适用于非常大的数据集和高维数据。精度通常要低于线性模型。 决策树：速度很快，不需要数据缩放，可以可视化，很容易解释。 随机森林：几乎总是比单棵决策树的表现要好，鲁棒性很好，非常强大。不需要数据缩放。不适用于高维稀疏数据。 梯度提升决策树：精度通常比随机森林略高。与随机森林相比，训练速度更慢，但预测速度更快，需要的内存也更少。比随机森林需要更多的参数调节。 支持向量机：对于特征含义相似的中等大小的数据集很强大。需要数据缩放，对参数敏感。 神经网络：可以构建非常复杂的模型，特别是对于大型数据集而言。对数据缩放敏感，对参数选取敏感。大型网络需要很长的训练时间。 无监督学习与预处理 训练算法时只有输入数据，没有已知输出。因为训练数据没有分类标签，所以很难用精度去评估模型有效。因此，无监督算法通常用于探索数据的规律，而不是自动化系统。 无监督学习类型 无监督变换（unsupervised transformation）：创建数据新的表示的算法，与数据的原始表示相比，新的表示可能更容易被人或其他机器学习算法所理解。 常用于降维（dimensionality reduction），它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新方法，用较少的特征就可以概括其重要特性。降维的一个常见应用是为了可视化将数据降为二维。 无监督变换的另一个应用是找到构成数据的各个组成部分。这方面的一个例子就是对文本文档集合进行主题提取。这里的任务是找到每个文档中讨论的未知主题，并学习每个文档中出现了哪些主题。这可以用于追踪社交媒体上的话题讨论，比如选举、枪支管制或 流行歌手等话题。 聚类算法（clustering algorithm）：将数据划分成不同的组，每组包含相似的物项。 思考向社交媒体网站上传照片的例子。为了方便你整理照片，网站可能想要将同一个人的照片分在一组。但网站并不知道每张照片是谁，也不知道你的照片集中出现了多少个 人。明智的做法是提取所有的人脸，并将看起来相似的人脸分在一组。但愿这些人脸对应同一个人，这样图片的分组也就完成了。 无监督学习的挑战 无监督学习的一个主要挑战就是评估算法是否学到了有用的东西。通常来说，评估无监督算法结果的唯一方法就是人工检查。 因此，如果数据科学家想要更好地理解数据，那么无监督算法通常可用于探索性的目的，而不是作为大型自动化系统的一部分。无监督算法的另一个常见应用是作为监督算法的预处理步骤。学习数据的一种新表示，有时可以提高监督算法的精度，或者可以减少内存占用和时间开销。 预处理与缩放 让我们先来看看对数据集缩放核预处理的各种方法的处理结果。 StandardScaler 确保每个特征的平均值为 0，方差为 1，使所有特征位于同一量级。 RobustScaler 使用的是中位数和四分位数 ，而不是平均值和方差。这样 RobustScaler 会忽略与其他点有很大不同的数据点（异常值，outlier）（比如测量误差）。 MinMaxScaler 移动数据，使所有特征都刚好位于 0 到 1 之间。对于二维数据集来说，所有的数据都包含在 x 轴 0 到 1 与 y 轴 0 到 1 组成的矩形中。 Normalizer 用到一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于 1。换句话说，它将一个数据点投射到半径为 1 的圆上（对于更高维度的情况，是球面）。这意味着每个数据点的缩放比例都不相同（乘以其长度的倒数）。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。 对训练数据和测试数据进行相同的缩放 注意！！！所有的缩放器总是对训练集核测试集应用完全相同的变换。也就是说，transform 方法总是减去训练集的最小值，然后除以训练集的范围，而这两个值可能与测试集的最小值和范围并不相同。为了让监督模型能够在测试集上运行，对训练集和测试集应用完全相同的变换是很重要的。 降维、特征提取与流形学习 利用无监督学习进行数据变换最简单也最常用的一种算法就是主成分分析（principal component analysis，PCA）。 主成分分析 主成分分析（principal component analysis，PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。 我们对 cancer 数据集利用 PCA 从 30 个维降到 2 个维度。 1234567891011121314151617181920212223242526from sklearn.decomposition import PCAfrom sklearn.datasets import load_breast_cancerfrom sklearn.preprocessing import StandardScaler# 数据集cancer = load_breast_cancer()# 标准化scaler = StandardScaler()scaler.fit(cancer.data)X_scaled = scaler.transform(cancer.data)# 降维到 2 的PCApca = PCA(n_components=2)# 训练 PCApca.fit(X_scaled)# 进行特征提取变换X_pca = pca.transform(X_scaled)# 降维前的特征个数print(&quot;Original shape: &#123;&#125;&quot;.format(str(X_scaled.shape))) # 降维后的特征个数print(&quot;Reduced shape: &#123;&#125;&quot;.format(str(X_pca.shape)))&#x27;&#x27;&#x27;Original shape: (569, 30)Reduced shape: (569, 2)&#x27;&#x27;&#x27; 因为 PCA 后只有 2 个维度，所以可以绘图做可视化分析。 重要的是要注意，PCA 是一种无监督方法，在寻找旋转方向时没有用到任何类别信息。它只是观察数据中的相关性。 PCA 的一个缺点在于，通常不容易对图中的两个轴做出解释。主成分对应于原始数据中的方向，所以它们是原始特征的组合，但这些组合往往非常复杂。 来看一个人脸识别的例子。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn.datasets import fetch_lfw_peopleimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_split# 加载数据 # 一共有 3023 张图像，每张大小为 87 像素 ×65 像素，分别属于 62 个不同的人people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)# 3023 张人脸照片作为样本输入print(&quot;people.images.shape: &#123;&#125;&quot;.format(people.images.shape))# 3023 张图片对应的人名print(&quot;Number of classes: &#123;&#125;&quot;.format(len(people.target_names)))&#x27;&#x27;&#x27;people.images.shape: (3023, 87, 65)Number of classes: 62&#x27;&#x27;&#x27;# 3023 列的 false 向量mask = np.zeros(people.target.shape, dtype=np.bool)# 遍历所有的分类(也就是人名，target 取值是 0~61)# 每个分类保留 50 个样本 防止某个 target 的数据集过多造成倾斜for target in np.unique(people.target): mask[np.where(people.target == target)[0][:50]] = TrueX_people = people.data[mask] # data 是已经把每张图片 n*m 的 2 维压成 1 维的数据格式y_people = people.target[mask]# 现在留下的样本，每个人不会超过50张# 特征缩放到 0~1 之间, 因为是RGB颜色X_people = X_people / 255# 切分数据# print(X_people, y_people)X_train, X_test, y_train, y_test = train_test_split( X_people, y_people, stratify=y_people, random_state=0)# KNN分类, 最近邻分类(只考虑最近的节点)knn = KNeighborsClassifier(n_neighbors=1)knn.fit(X_train, y_train)print(&quot;Test set score of 1-nn: &#123;:.2f&#125;&quot;.format(knn.score(X_test, y_test)))&#x27;&#x27;&#x27;Test set score of 1-nn: 0.27&#x27;&#x27;&#x27; 这个结果对于一个 62 分类问题来说也不算太大，但还是由改进的空间。 这里就可以用到 PCA。想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图像时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大不同，使用这种原始表示很难获取到面部特征。 例如，如果使用像素距离，那么将人脸向右移动一个像素将会发生巨大的变化，得到一个完全不同的表示。 我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启用 PCA 的白化（whitening）选项，它将主成分缩放到相同的尺度。 PCA 的白化（whitening）选项，它将主成分缩放到相同的尺度。变换后的结果与使用 StandardScaler 相同。 我们可以考虑利用 PCA 提取主成分，作为 100 个新特征输入到模型。对数据做了特征提取后再用 KNN 进行建模。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from sklearn.datasets import fetch_lfw_peopleimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.decomposition import PCA# 加载数据people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)# 3023 张人脸照片作为样本输入print(&quot;people.images.shape: &#123;&#125;&quot;.format(people.images.shape))# 3023 张图片对应的人名print(&quot;Number of classes: &#123;&#125;&quot;.format(len(people.target_names)))# 3023 列的 false 向量mask = np.zeros(people.target.shape, dtype=np.bool)# 遍历所有的分类(也就是人名，target 取值是 0~61)# 每个分类保留 50 个样本for target in np.unique(people.target): mask[np.where(people.target == target)[0][:50]] = TrueX_people = people.data[mask] # data 是已经把每张图片 n*m 的 2 维压成 1 维的数据格式y_people = people.target[mask]# 现在留下的样本，每个人不会超过50张# 特征缩放到 0~1 之间, 因为是 RGB 颜色X_people = X_people / 255# 切分数据# print(X_people, y_people)X_train, X_test, y_train, y_test = train_test_split( X_people, y_people, stratify=y_people, random_state=0)# PCA 提取 100 个主成分作为新的特征, 基于训练集 fit, 应用到训练集和测试集pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)print(&quot;X_train_pca.shape: &#123;&#125;&quot;.format(X_train_pca.shape))# KNN 分类, 最近邻分类(只考虑最近的节点)knn = KNeighborsClassifier(n_neighbors=1)knn.fit(X_train_pca, y_train)# 精度print(&quot;Test set accuracy: &#123;:.2f&#125;&quot;.format(knn.score(X_test_pca, y_test)))&#x27;&#x27;&#x27;Test set accuracy: 0.36&#x27;&#x27;&#x27; 在对数据进行 PCA 处理后，KNN 模型的精度确实得到了提升。 非负矩阵分解 非负矩阵分解（non-negative matrix factorization，NMF）是另一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于 PCA，也可以用于降维。与 PCA 相同，我们试图将每个数据点写成一些分量的加权求和。 但在 PCA 中，我们想要的是正交分量，并且能够解释尽可能多的数据方差；而在 NMF 中，我们希望分量和系数均为非负，也就是说，我们希望分量和系数都大于或等于 0。因此，这种方法只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。 （常用的降维方法为 PCA，NMF 待整理） 用 t-SNE 进行流形学习 虽然 PCA 通常是用于变换数据的首选方法，使你能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。有一类用于可视化的算法叫作流形学习算法（manifold learning algorithm），它允许进行更复杂的映射，通常也可以给出更好的可视化。其中特别有用的一个就是 t-SNE 算法。 新特征能够根据原始数据中数据点之间的远近程度将不同类比明确分开。在可视化用途比 PCA 更有效，但只能用于做可视化，无法像 PCA 一样应用到测试集上。 聚类 聚类（clustering）是将数据集划分成组的任务，这些组叫做簇。聚类算法给每个数据点分配一个数字，表示数据点属于哪个簇。 k 均值聚类 k 均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心（cluster center）。算法交替执行以下两个步骤：将每个数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化（收敛），那么算法结束。 123456789101112131415from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeans# 生成模拟的二维数据X, y = make_blobs(random_state=1)# 构建聚类模型, 指定 3 个簇中心kmeans = KMeans(n_clusters=3) kmeans.fit(X)# 打印每个数据所属的簇标签, 因为 n_clusters=3 所以就是 0~2print(kmeans.labels_)# 也支持 predict 方法来计算一个新数据点属于哪个簇标签print(kmeans.predict(X)) 簇标签没有先验意义，我们并不知道每一个簇代表什么，只能人为观察。 kmeans 类似于 PCA 可以进行特征变换，首先 kmeans 进行 fit 找到所有簇中心后，进而对训练集 / 测试集特征进行 transform，从而将原始数据点变换为到各个簇中心的距离，作为新的特征。 1234567891011121314151617181920212223242526from sklearn.datasets import make_blobsfrom sklearn.cluster import KMeansfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_moonsfrom sklearn.ensemble import GradientBoostingClassifier# 数据集X, y = make_moons(n_samples=200, noise=0.05, random_state=0)# 切分X_train, X_test, y_train, y_test = train_test_split(X, y)# 分成10簇kmeans = KMeans(n_clusters=10, random_state=0)kmeans.fit(X_train)# 转换原有特征为到簇中心的距离train_distance_features = kmeans.transform(X_train)test_distance_features = kmeans.transform(X_test)# 跑模型gbdt = GradientBoostingClassifier()gbdt.fit(train_distance_features, y_train)# 精度print(gbdt.score(test_distance_features, y_test)) kmeans 最初的簇中心是随机产生的，算法输出依赖于随机种子，sklearn 会默认跑 10 次选最好的 1 次。另外，kmeans 对簇形状有假设，人工确定簇个数是很难琢磨的。 凝聚聚类 算法首先声明每个点是自己的簇，然后不断合并相似的簇，直到簇数量达到目标。因为算法是不断合并簇的逻辑，所以它对新数据无法做出所属簇的预测。 scikit-learn 中实现了以下三种选项。 ward ：默认选项。ward 挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。 average：average 链接将簇中所有点之间平均距离最小的两个簇合并。 complete： complete 链接（也称为最大链接）将簇中点之间最大距离最小的两个簇合并。 ward 适用于大多数数据集。如果簇中的成员个数非常不同（比如其中一个比其他所有都大得多），那么 average 或 complete 可能效果更好。 1234567891011121314151617from sklearn.datasets import make_blobsfrom sklearn.cluster import AgglomerativeClusteringfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_moonsimport mglearn# 数据集X, y = make_blobs(random_state=1)# 凝聚聚类为 3 个簇agg = AgglomerativeClustering(n_clusters=3)assignment = agg.fit_predict(X)# 输出数据点分布以及所属簇mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)plt.xlabel(&quot;Feature 0&quot;)plt.ylabel(&quot;Feature 1&quot;) 凝聚聚类生成了所谓的层次聚类（hierarchical clustering）。聚类过程迭代进行，每个点都从一个单点簇变为属于最终的某个簇。每个中间步骤都提供了数据的一种聚类（簇的个数也不相同）。 可以将层次聚类可视化为树状图进行分析。但 sklearn 目前没有绘制树状图的功能，这里我们利用 Scipy 库。 DBSCAN 聚类 DBSCAN（density-based spatial clustering of applications with noise，即具有噪声的基于密度的空间聚类应用）的主要优点是它不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点，DBSCAN 也不允许对新的测试数据进行预测。 DBSCAN 有两个参数：min_samples 和 eps。如果在距一个给定数据点 eps 的距离内至少有 minsamples 个数据点，那么这个数据点就是核心样本。DBSCAN 将彼此距离小于 eps 的核心样本放到同一个簇中。 小结 无监督学习可以用于探索性的数据分析与预处理。 预处理以及分解方法在数据准备中具有重要作用。 通常来说，很难量化无监督算法的有用性，但这不应该妨碍你使用它们来深入理解数据。 数据表示与特征工程 到现在，我们已经知道了模型的形成是依赖数据的特征的，并且不同的算法对特征的敏感度不同。特征可以分为两大类型：分类特征与离散特征。 对于某个特定应用来说，如何找到最佳数据表示，这个问题被称为特征工程（feature engineering），它是数据科学家和机器学习从业者在尝试解决现实世界问题时的主要任务之一。用正确的方式表示数据，对监督模型性能的影响比所选择的精确参数还要大。 one-hot 编码（虚拟变量） 如果特征不是连续值，而是一些分类特征，来自一系列固定的可能取值（非数字），那么直接用于类似 Logistic 回归分类模型是没有意义。显然，在应用 Logistic 回归时，我们需要换一种方式来表示数据。下一节将会说明我们如何解决这一问题。 将 1 个特征的多种取值，改为多个特征的 0 / 1 取值，其中有一个特征为 1，其他为 0。 12345678910111213141516from sklearn.preprocessing import OneHotEncoder# fit 训练 one hot, 返回非稀疏矩阵# 当 transform 时遇到没见过的特征值则对应 one-hot 编码全部为 0enc = OneHotEncoder(sparse=False, handle_unknown=&#x27;ignore&#x27;)# 每一列代表一种特征的可能性# 第一列：[0, 1, 0, 1] 只有 0 和 1 两种情况 one-hot 占两列# 第二列：[0, 1, 2, 0] 有 0 1 2 三种情况 one-hot 占三列# 第三列：[3, 0, 1, 2] 有 0 1 2 3 四种情况 one-hot 占四列X = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]enc.fit(X) # one-hot 编码新数据X_one_hot = enc.transform([[1,4,2]]) print(X_one_hot)# OUT：[[0. 1. 0. 0. 0. 0. 0. 1. 0.]] 也可指定对哪些 one-hot，而对其他连续特征可以不做处理。 12345678910from sklearn.preprocessing import OneHotEncoder# 指定了只有前 2 个特征需要离散化enc = OneHotEncoder(categorical_features=[0,1], sparse=False, handle_unknown=&#x27;ignore&#x27;)X = [[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]enc.fit(X) # one-hot 编码新数据X_one_hot = enc.transform([[1,4,2]]) print(X_one_hot) 我们也可以利用 ColumnTransformer 来作为统一的特征处理方法，将字符串编码为整形。 1234567891011121314151617181920from sklearn.preprocessing import OrdinalEncoderfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.compose import ColumnTransformerimport numpy as npX = [[&#x27;male&#x27;, 0, 3], [&#x27;male&#x27;, 1, 0], [&#x27;female&#x27;, 2, 1], [&#x27;female&#x27;, 0, 2]]# 字符串编码为整形sex_enc = OrdinalEncoder(dtype = np.int)# 独热编码one_hot_enc = OneHotEncoder(sparse=False, handle_unknown=&#x27;ignore&#x27;, dtype=np.int)# 对第 0 列的字符串做整形转换, 然后对所有列做 one-hotcol_transformer = ColumnTransformer(transformers = [(&#x27;sex_enc&#x27;, sex_enc, [0]), (&#x27;one_hot_enc&#x27;, one_hot_enc, list(range(0,3)))])# 训练编码col_transformer.fit(X)X_trans = col_transformer.transform(X)print(X_trans) 分箱、离散化、线性模型与树 对于只有 1 个连续特征的线性模型，它的表现就是 y=w*x 的线，效果不好。这时候可以考虑将特征取值划分范围，这就是分箱，每个箱子的数值区间不同，提高了线性模型的表现力。 这样就把 1 个连续特征变成了离线特征，也就是在哪个区间里，可以继续通过 one-hot 编码成多个 0/1 特征。 分箱对线性模型有提升效果，对树模型效果不会很好可能还会下降。 交互特征与多项式特征 把原始特征之间进行组合和扩充，对线性模型有提升效果，比如：添加特征的平方或立方，或者把两两特征相乘。添加交互 / 多项式特征之后的线性模型，与没有交互特征的树模型 / 复杂模型的性能就比较相近了。 单变量非线性变换 对于简单模型（线性、朴素贝叶斯）来说，如果数据集的数据分布存在大量的小值以及个别非常大的值，会导致线性模型很难处理。这种情况出现在一些计数性质的特征上，比如点赞次数。 此时对该特征应用 log(x+1) 或者 exp （对数 / 指数）来调节特征值得比例，可以改进线性模、SVM、神经网络的效果。 123# 对跨度较大的特征做 logX_train_log = np.log(X_train + 1)X_test_log = np.log(X_test + 1) 对线性模型提升最明显，对树模型意义不大，对 SVM / KNN / 神经网络有可能受益。 自动化特征选择 前面提到的方法都是增加特征（线性 👉 非线性），现在是通过分析数据来减少特征，得到一个泛化更好，更简单的模型，也就是特征选择。 单变量统计 每次考虑一个特征，观察特征与目标值之间是否存在统计显著性，但是没法综合考虑多个特征。算法可以指定保留一定数量的重要特征。 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.datasets import load_breast_cancerfrom sklearn.feature_selection import SelectPercentilefrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegression# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0, test_size=.5)print(X_train.shape)# 自动选择 50% 的特征留下来select = SelectPercentile(percentile=50)select.fit(X_train, y_train)# 训练集提取特征X_train_selected = select.transform(X_train)print(X_train_selected.shape)# 测试集提取特征X_test_selected = select.transform(X_test)# 原始特征模型，看精度lr = LogisticRegression()lr.fit(X_train, y_train)print(&quot;Score with all features: &#123;:.3f&#125;&quot;.format(lr.score(X_test, y_test))) # 特征选择的模型, 看精度lr.fit(X_train_selected, y_train)print(&quot;Score with only selected features: &#123;:.3f&#125;&quot;.format(lr.score(X_test_selected, y_test)))&#x27;&#x27;&#x27;out：(284, 30)(284, 15)Score with all features: 0.954Score with only selected features: 0.954特征删了一半 但对模型的精度丝毫没有产生影响&#x27;&#x27;&#x27; 基于模型的特征选择 线性模型、决策树模型在训练的过程中自然的完成了对特征重要程度的学习。所以可以基于这些模型进行特征选择，再将选择后的特征作为另外一个模型的输入。 12345678910111213141516171819202122232425262728293031323334from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import RandomForestClassifier# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0, test_size=.5)# 从随机森林模型的训练的结果中进行特征选择, 取中位数偏上重要的特征，也就是保留一半最重要的特征# threshold：滤波器select = SelectFromModel( RandomForestClassifier(n_estimators=100, random_state=42), threshold=&quot;median&quot;)# 特征选择训练select.fit(X_train, y_train)# 留下选择后的特征X_train_l1 = select.transform(X_train)print(&quot;X_train.shape: &#123;&#125;&quot;.format(X_train.shape))print(&quot;X_train_l1.shape: &#123;&#125;&quot;.format(X_train_l1.shape))# 选择出来的特征完成训练lr = LogisticRegression()lr.fit(X_train_l1, y_train)# 测试集特征选择X_test_l1 = select.transform(X_test)print(&quot;Test score: &#123;:.3f&#125;&quot;.format(lr.score(X_test_l1, y_test))) 模型综合度量了所有特征的重要性，所以比单变量统计强大的多。 迭代特征选择 （RFE，Recursive Feature Elimination）利用模型进行多轮特征选择，每轮筛掉1个最不重要的特征。 下面用随机森林做特征选择，选择出来的特征用线性 LR 做训练，发现线性模型精度很好。 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.feature_selection import RFEfrom sklearn.ensemble import RandomForestClassifier# 数据集cancer = load_breast_cancer()# 切分X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0, test_size=.5)# 基于迭代的特征选择, 保留15个特征select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=15)# 训练特征选择select.fit(X_train, y_train)# 训练集特征选择X_train_rfe = select.transform(X_train)# 测试集特征选择X_test_rfe = select.transform(X_test)# 选择后的特征训练模型lr = LogisticRegression()lr.fit(X_train_rfe, y_train)# 测试集精度print(&quot;Test score: &#123;:.3f&#125;&quot;.format(lr.score(X_test_rfe, y_test) )) 模型评估与改进 到目前为止，为了评估我们的监督模型，我们使用 train_test_split 函数将数据集划分为 、训练集和测试集，在训练集上调用 fit 方法来构建模型，并且在测试集上用 score 方法来评估这个模型，对于分类问题而言，就是计算正确分类的样本所占的比例。 请记住，之所以将数据划分为训练集和测试集，是因为我们想要度量模型对前所未见的新数据的泛化性能。我们对模型在训练集上的拟合效果不感兴趣，而是想知道模型对于训练过程中没有见过的数据的预测能力。 交叉验证 单次划分数据集并不稳定和全面，因此我们需要对数据集进行多次划分，训练多个模型进行综合评估，这叫交叉验证。 K 折交叉 这是最常见的交叉验证方式，将数据均匀划分成 K 份，每次用 1 份做测试集，剩余做训练集。 12345678910111213141516171819from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import cross_val_score# 数据集iris = load_iris()# 模型logreg = LogisticRegression()# K折交叉验证scores = cross_val_score(logreg, iris.data, iris.target, cv=3)print(&quot;Cross-validation scores: &#123;&#125;&quot;.format(scores))print(&quot;Average cross-validation score: &#123;:.2f&#125;&quot;.format(scores.mean()))&#x27;&#x27;&#x27;Cross-validation scores: [0.96078431 0.92156863 0.95833333]Average cross-validation score: 0.95&#x27;&#x27;&#x27; 分层 K 折交叉 K 折交叉划分数据的方式是从头开始均分成 K 份，如果样本数据的分类分布不均匀，那么就会导致 K 折交叉策略失效。 在分层交叉验证中，我们划分数据，使每个折中类别之间的比例与整个数据集中的比例相同。sklearn 会根据模型是回归还是分类决定使用标准 K 折还是分层 K 折，不需我们关心，只需要了解。 留一法交叉验证 可以将留一法交叉验证看作是每折只包含单个样本的 k 折交叉验证。对于每次划分，你选择单个数据点作为测试集。这种方法可能非常耗时，特别是对于大型数据集来说，但在小型数据集上有时可以给出更好的估计结果。 打乱划分交叉验证 在打乱划分交叉验证中，每次划分为训练集取样 train_size 个点，为测试集取样 test_size 个 （不相交的）点。将这一划分方法重复 n_iter 次。 分组交叉验证 利用分组保证测试集与训练集的样本不同。（为了准确评估模型对新的人脸的泛化能力，我们必须确保训练集和测试集中包含不同人的图像。） 网格搜索 利用网格搜索，实现模型的自动化调参，找到最佳泛化性能的参数。 在尝试调参之前， 重要的是要理解参数的含义。找到一个模型的重要参数（提供最佳泛化性能的参数）的取值是一项棘手的任务，但对于几乎所有模型和数据集来说都是必要的。由于这项任务如此常见，所以 sklearn 中有一些标准方法可以帮你完成。最常用的方法就是网格搜索（grid search），它主要是指尝试我们关心的参数的所有可能组合。 带交叉验证的网格搜索 1234567891011121314151617181920212223242526272829303132from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, train_test_splitfrom sklearn.svm import SVC# 数据集iris = load_iris()# 切分数据X_train, X_test, y_train, y_test = train_test_split( iris.data, iris.target, random_state=0)# 2 种网格param_grid = [ # （高斯）径向基函数核（英語：Radial basis function kernel） # gamma 越大，支持向量越少，gamma 值越小，支持向量越多。支持向量的个数影响训练与预测的速度 # 第1个网格 &#123;&#x27;kernel&#x27;: [&#x27;rbf&#x27;], &#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100], &#x27;gamma&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;, # 第2个网格 &#123;&#x27;kernel&#x27;: [&#x27;linear&#x27;],&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;]# 在 2 个网格中, 找到 SVC 模型的最佳参数, 这里 cv=5 表示每一种参数组合进行 5 折交叉验证计算得分grid_search = GridSearchCV(SVC(), param_grid, cv=5)# fit 找到最佳泛化的参数grid_search.fit(X_train, y_train)# 查看精度print(&quot;泛化精度:&quot;, grid_search.score(X_test, y_test))# 打印最佳参数print(&quot;Best parameters: &#123;&#125;&quot;.format(grid_search.best_params_)) 交叉验证与网格搜索的嵌套 可以采用先交叉划分数据集，再进行 K 折网格搜索，这就是嵌套的意思。 1234567891011121314151617from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_splitfrom sklearn.svm import SVC# 数据集iris = load_iris()param_grid = [ &#123;&#x27;kernel&#x27;: [&#x27;rbf&#x27;], &#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100], &#x27;gamma&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;, &#123;&#x27;kernel&#x27;: [&#x27;linear&#x27;],&#x27;C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;]grid_search = GridSearchCV(SVC(), param_grid, cv=5)# 外层 K 折scores = cross_val_score(grid_search, iris.data, iris.target, cv=5)# 打印精度print(scores) 评估指标与评分 首先，需要牢记的一点，精度不是唯一目标！！！我们需要考虑商业指标，对商业的影响。 二分类指标 二分类一共有 2 种类型，一种称为正类（positive），一种称为反类（negative）。 根据样本分类与模型预测分类，可以产生 4 种组合： TP：预测是正类，样本是正类 FP：预测是正类，样本是反类 TN：预测是反类，样本是反类 FN：预测是反类，样本是正类 123456789101112131415161718192021222324252627282930313233343536373839404142434445from sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import confusion_matrix,f1_scorefrom sklearn.metrics import classification_report# 数据集digits = load_digits()# 转换成 2 分类, 即目标数字是否等于 9y = digits.target == 9# 切分数据集X_train, X_test, y_train, y_test = train_test_split( digits.data, y, random_state=0)# 模型lr = LogisticRegression()# 训练lr.fit(X_train, y_train)# 预测y_pred = lr.predict(X_test)# 混淆矩阵print(confusion_matrix(y_test, y_pred))# 打印 f-scoreprint(f1_score(y_test, y_pred))# 打印所有指标print(classification_report(y_test, y_pred))&#x27;&#x27;&#x27;Out:[[399 4] [ 7 40]]0.8791208791208791 precision recall f1-score support False 0.98 0.99 0.99 403 True 0.91 0.85 0.88 47 micro avg 0.98 0.98 0.98 450 macro avg 0.95 0.92 0.93 450weighted avg 0.98 0.98 0.98 450&#x27;&#x27;&#x27; 混淆矩阵的分布： 预测为反类 预测为正类 实际为反类 TN FP 实际为正类 FN TP 正确率 Accuracy：Accuracy = (TN+TP) / (TN+TP+FN+FP) 精确率 Precision：Precision = TP / (TP + FP) 精确率的商业目标就是限制假正例的数量，可能因为假正例会带来很严重的影响。 召回率 Recall：Recall = TP / (TP + FN) 精确率和召回率是矛盾的，如果模型预测所有都是正类，那么召回率就是100%；此时，就会出现很多假正类，精确率就很差。因此需要综合 2 个指标进行折衷，就是 f-score 或 f-measure。 F = 2 * precision * recall / (precision + recall) 阈值：阈值（默认为 50%）越高，意味着模型需要更加确信才能做出正类的判断。 准确率-召回率曲线 曲线越靠近右上角，则分类器越好。右上角的点表示对于同一个阈值，准确率和召回率都很高。曲线从左上角开始，这里对应于非常低的阈值，将所有样本都划为正类。提高阈值可以让曲线向准确率更高的方向移动，但同时召回率降低。继续增大阈值，大多数被划为正类的点都是真正例，此时准确率很高，但召回率更低。随着准确率的升高，模型越能够保持较高的召回率，则模型越好。 ROC 与 AUC 受试者工作特征曲线（receiver operating characteristics curve），简称为 ROC 曲线（ROC curve）。与准确率 - 召回率曲线类似，ROC 曲线考虑了给定分类器的所有可能的阈值，但它显示的是假正例率（false positive rate，FPR）和真正例率（true positive rate，TPR），而不是报告准确率和召回率。 与准确率 - 召回率曲线一样，我们通常希望使用一个数字来总结 ROC 曲线，即曲线下的面积［通常被称为 AUC（area under the curve），这里的曲线指的就是 ROC 曲线］。 多分类指标 用分类报告来观察各个分类的指标就很不错。 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import classification_report# 数据集digits = load_digits()# 切分X_train, X_test, y_train, y_test = train_test_split( digits.data, digits.target, random_state=0)# 训练lr = LogisticRegression().fit(X_train, y_train)# 预测pred = lr.predict(X_test)# 精度print(&quot;Accuracy: &#123;:.3f&#125;&quot;.format(lr.score(X_test, y_test)))# 精确度、召回率、f1 指标print(classification_report(pred, y_test))&#x27;&#x27;&#x27;Out:Accuracy: 0.953 precision recall f1-score support 0 1.00 1.00 1.00 37 1 0.91 0.89 0.90 44 2 0.93 0.95 0.94 43 3 0.96 0.90 0.92 48 4 1.00 0.97 0.99 39 5 0.98 0.98 0.98 48 6 1.00 0.96 0.98 54 7 0.94 1.00 0.97 45 8 0.90 0.93 0.91 46 9 0.94 0.96 0.95 46 micro avg 0.95 0.95 0.95 450 macro avg 0.95 0.95 0.95 450weighted avg 0.95 0.95 0.95 450&#x27;&#x27;&#x27; 宏（macro）平均：计算未加权的按类别 f- 分数。它对所有类别给出相同的权重，无论类别中的样本量大小。 加权（weighted）平均：以每个类别的支持作为权重来计算按类别 f- 分数的平均值。分类报告中给出的就是这个值。 微（micro）平均：计算所有类别中假正例、假反例和真正例的总数，然后利用这些计数来计算准确率、召回率和 f- 分数。 如果你对每个样本等同看待，那么推荐使用微平均 f1- 分数；如果你对每个类别等同看待，那么推荐使用宏平均 f1- 分数。 回归指标 对于回归问题来说，使用 score 方法评估即可，因为他没有分类的正反问题。score底层使用的是 R^2，它是评估回归模型的很好的指标。 模型指标选择 网格搜索评估和 K 折交叉验证最佳模型参数默认是基于精度评判的，我们可以指定基于其他指标（精确度、召回率、f1）。 12345678910111213141516171819202122232425262728293031323334353637383940from sklearn.datasets import load_digitsfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_splitfrom sklearn.svm import SVCfrom sklearn.metrics import classification_report# 数据集digits = load_digits()# 切分成 2 分类问题, 数字是否等于 9X_train, X_test, y_train, y_test = train_test_split( digits.data, digits.target == 9, random_state=0)# 2 种网格param_grid = &#123;&#x27;gamma&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;# 在 2 个网格中, 找到 SVC 模型的最佳参数, 每一组参数进行 3 折评估, 使用 f1-score 作为评估依据grid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring=&#x27;f1&#x27;)# 搜索最佳参数grid_search.fit(X_train, y_train)# 打印最佳参数print(grid_search.best_params_)# 打印最佳参数的 f1-scoreprint(grid_search.best_score_)# 打印在测试集上的各种指标print(classification_report(grid_search.predict(X_test), y_test))&#x27;&#x27;&#x27;Out:&#123;&#x27;gamma&#x27;: 0.001&#125;0.9771729298313027 precision recall f1-score support False 1.00 0.99 1.00 405 True 0.94 0.98 0.96 45 micro avg 0.99 0.99 0.99 450 macro avg 0.97 0.99 0.98 450weighted avg 0.99 0.99 0.99 450&#x27;&#x27;&#x27; 算法链和管道 大多数机器学习应用不仅需要应用单个算法，而且还需要将许多不同的处理步骤和机器学习模型链接在一起。我们一般使用 Pipeline 类来简化构建变换和模型链，Pipeline 类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型（比如分类器）链接在一起。 构建管道 12345from sklearn.pipeline import Pipelinepipe = Pipeline([(&quot;scaler&quot;, MinMaxScaler()), (&quot;svm&quot;, SVC())])pipe.fit(X_train, y_train)print(&quot;Test score: &#123;:.2f&#125;&quot;.format(pipe.score(X_test, y_test)))# Test score: 0.95 利用管道，我们减少了预处理 + 分类过程 所需要的代码量。但是，使用管道的主要优点在于，现在我们可以在 cross_val_score 或 GridSearchCV 中使用这个估计器。 网格搜索中使用管道 12345678910111213141516171819202122from sklearn.svm import SVCfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipeline# 加载并划分数据cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)# 先缩放再跑模型的pipelinepipe = Pipeline([(&quot;scaler&quot;, MinMaxScaler()), (&quot;svm&quot;, SVC())])# 5折网格搜索param_grid = &#123;&#x27;svm__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],&#x27;svm__gamma&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)print(&quot;Best cross-validation accuracy: &#123;:.2f&#125;&quot;.format(grid.best_score_))print(&quot;Test set score: &#123;:.2f&#125;&quot;.format(grid.score(X_test, y_test)))print(&quot;Best parameters: &#123;&#125;&quot;.format(grid.best_params_)) 这里采用 5 折网格参数搜索，但是在 pipeline 情况下，需要把搜索参数增加对应步骤的名字作为前缀，这样才会被 pipeline 中的某个步骤使用，因此，要想搜索 SVC 的 C 参数，必须使用 svm__C 作为参数网格字典的键，对 gamma 参数也是同理。 通用管道接口 123456789101112131415161718192021222324252627282930313233343536from sklearn.svm import SVCfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipeline# 加载并划分数据cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)# 先缩放再跑模型的 pipelinepipe = Pipeline([(&quot;scaler&quot;, MinMaxScaler()), (&quot;svm&quot;, SVC())])# 5 折网格搜索param_grid = &#123;&#x27;svm__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],&#x27;svm__gamma&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)# 最佳泛化的训练结果print(grid.best_estimator_)# 最佳结果中的 svm 步骤print(grid.best_estimator_.named_steps[&#x27;svm&#x27;])&#x27;&#x27;&#x27;Out:Pipeline(memory=None, steps=[(&#x27;scaler&#x27;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#x27;svm&#x27;, SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#x27;ovr&#x27;, degree=3, gamma=1, kernel=&#x27;rbf&#x27;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))])SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#x27;ovr&#x27;, degree=3, gamma=1, kernel=&#x27;rbf&#x27;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)&#x27;&#x27;&#x27; 可以看到具有最佳泛化的 pipeline，其 2 个步骤的训练结果也对应的保存了起来，我们可以取出其中的某一步骤的训练结果。 网格搜索&amp;预处理 添加交互多项式特征的预处理步骤。 12345678910111213141516171819202122232425262728from sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.datasets import load_bostonfrom sklearn.linear_model import Ridge# 加载并划分数据boston = load_boston()X_train, X_test, y_train, y_test = train_test_split( boston.data, boston.target, random_state=0)# 缩放数据 + 生成多项式特征 + 岭回归pipe = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;ploy&quot;, PolynomialFeatures()), (&#x27;ridge&#x27;, Ridge())])# 前缀指定各个步骤的搜索参数param_grid = &#123;&#x27;ploy__degree&#x27;: [1, 2, 3], &#x27;ridge__alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]&#125;# 网格搜索grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)grid.fit(X_train, y_train)# 精度print(grid.score(X_test, y_test))# 最佳泛化的一组参数print(grid.best_params_) 网格搜索&amp;模型选择 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import GridSearchCVfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.datasets import load_breast_cancerfrom sklearn.linear_model import Ridgefrom sklearn.ensemble import RandomForestClassifier# 加载并划分数据cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0)# 定义一下 pipeline 的 2 个步骤pipe = Pipeline([(&#x27;preprocessing&#x27;, StandardScaler()), (&#x27;classifier&#x27;, SVC())])# 对 SVC 模型进行 gamma 参数搜索、以及是否预处理的比较# 对随机森林分类器进行 max_features 参数搜索、并且不进行预处理param_grid = [ &#123;&#x27;classifier&#x27;: [SVC()], &#x27;preprocessing&#x27;: [StandardScaler(), None], &#x27;classifier__gamma&#x27;: [0.001, 0.01, 0.1, 1, 10, 100], &#x27;classifier__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100] &#125;, &#123;&#x27;classifier&#x27;: [RandomForestClassifier(n_estimators=100)], &#x27;preprocessing&#x27;: [None], &#x27;classifier__max_features&#x27;: [1, 2, 3] &#125;]# 网格搜索grid = GridSearchCV(pipe, param_grid, cv=5)grid.fit(X_train, y_train)# 精度print(grid.score(X_test, y_test))# 最佳泛化的一组参数print(grid.best_params_)&#x27;&#x27;&#x27;out:0.9790209790209791&#123;&#x27;classifier&#x27;: SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#x27;ovr&#x27;, degree=3, gamma=0.01, kernel=&#x27;rbf&#x27;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), &#x27;classifier__C&#x27;: 10, &#x27;classifier__gamma&#x27;: 0.01, &#x27;preprocessing&#x27;: StandardScaler(copy=True, with_mean=True, with_std=True)&#125;&#x27;&#x27;&#x27; 从结果可以看出，StandardScaler 缩放 + SVC 分类模型的效果要好于随机森林分类。 处理文本数据 在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。这些术语来自于信息检索（information retrieval，IR）和自然语言处理（natural language processing，NLP）的社区，它们主要针对文本数据。 词袋 下面利用 CountVectorizer 对原始本文输入进行词袋统计，从而转换成稀疏的特征向量作为模型输入。 123456789101112131415161718192021222324252627from sklearn.feature_extraction.text import CountVectorizer# 2 行文本数据bards_words =[&quot;The fool doth think he is wise,&quot;, &quot;but the wise man knows himself to be a fool&quot;]# 对每个文档分词, 生成样本中所有词的表, 但是忽略掉那些只出现在 1 个样本中的单词, 并且忽略掉停词vect = CountVectorizer(min_df=2, stop_words=&#x27;english&#x27;)vect.fit(bards_words)# 打印词表print(vect.vocabulary_)# 打印特征向量的构成print(vect.get_feature_names())# 文本数据转换成词袋特征向量bag_of_words = vect.transform(bards_words)# 转成稠密矩阵输出 2 行特征向量print(bag_of_words.toarray())&#x27;&#x27;&#x27;out:&#123;&#x27;fool&#x27;: 0, &#x27;wise&#x27;: 1&#125;[&#x27;fool&#x27;, &#x27;wise&#x27;][[1 1] [1 1]]&#x27;&#x27;&#x27; min_df 令词表仅保留了在不同文档中出现过至少 2 次的单词，另外 stop_words 指定忽略掉英文中的停词，其实上述过程就是舍弃我们认为不重要的单词。 tf-idf 缩放数据 （term frequency–inverse document frequency）即词频-逆向文档频率，如果一个单词在某个特定文档中经常出现，但在许多文档中却不常出现，那么这个单词可能是对文档的很好的描述。 123456789101112131415161718192021222324252627282930from sklearn.feature_extraction.text import TfidfVectorizer# 2 行文本数据bards_words =[&quot;The fool doth think he is wise,&quot;, &quot;but the wise man knows himself to be a fool&quot;]# 文本统计 tf-idfvect = TfidfVectorizer()vect.fit(bards_words)# 打印词表print(vect.vocabulary_)# 打印特征向量的构成print(vect.get_feature_names())# 将输入文本分词，并用每个分词的 tf-idf 作为特征值tfidf_X = vect.transform(bards_words)# 转成稠密矩阵输出print(tfidf_X.toarray())&#x27;&#x27;&#x27;&#123;&#x27;the&#x27;: 9, &#x27;fool&#x27;: 3, &#x27;doth&#x27;: 2, &#x27;think&#x27;: 10, &#x27;he&#x27;: 4, &#x27;is&#x27;: 6, &#x27;wise&#x27;: 12, &#x27;but&#x27;: 1, &#x27;man&#x27;: 8, &#x27;knows&#x27;: 7, &#x27;himself&#x27;: 5, &#x27;to&#x27;: 11, &#x27;be&#x27;: 0&#125;[&#x27;be&#x27;, &#x27;but&#x27;, &#x27;doth&#x27;, &#x27;fool&#x27;, &#x27;he&#x27;, &#x27;himself&#x27;, &#x27;is&#x27;, &#x27;knows&#x27;, &#x27;man&#x27;, &#x27;the&#x27;, &#x27;think&#x27;, &#x27;to&#x27;, &#x27;wise&#x27;][[0. 0. 0.42567716 0.30287281 0.42567716 0. 0.42567716 0. 0. 0.30287281 0.42567716 0. 0.30287281] [0.36469323 0.36469323 0. 0.25948224 0. 0.36469323 0. 0.36469323 0.36469323 0.25948224 0. 0.36469323 0.25948224]] &#x27;&#x27;&#x27;","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"深度学习入门","slug":"机器学习/深度学习入门","date":"2020-08-08T02:16:57.000Z","updated":"2023-09-20T07:35:51.890Z","comments":true,"path":"2020/08/08/机器学习/深度学习入门/","link":"","permalink":"https://wingowen.github.io/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/","excerpt":"深度学习入门：基于 Python 的理论与实现。","text":"深度学习入门：基于 Python 的理论与实现。 必备知识 在正式开始学习学习深度学习之前，我们先来了解一些必须要掌握的基本知识。（Python 的基础语法请参照本人的另一篇博客：父与子编程之旅） Anaconda Anaconda 可以统一管理开发环境，以及便捷的获取及管理包。 12345678# 创建一个名为 py37 的开发环境，并指定 python 版本conda create -n py37 python=3.7# 激活 py37 环境conda activate py37# 退出 py37 环境conda deactivate py37# 删除conda remove -n py37 --all NumPy 在深度学习的实现中，经常出现数组和矩阵的计算。NumPy 的数组类 numpy.array 中提供了很多便捷的方法，在实现深度学习时，我们将使用这些方法。本节我们来简单介绍一下后面会用到的 NumPy。 123456789101112131415## 导入 NumPy 库import numpy as np## 生成 NumPy 数组x = np.array([1.0, 2.0, 3.0])print(x) # [ 1. 2. 3.]type(x) # &lt;class &#x27;numpy.ndarray&#x27;&gt;## NumPy 数组的算术运算x = np.array([1.0, 2.0, 3.0])y = np.array([2.0, 4.0, 6.0])x + y # array([ 3., 6., 9.])x - y # array([ -1., -2., -3.])x * y # array([ 2., 8., 18.])x / y # array([ 0.5, 0.5, 0.5]) 注意，在上面的 NumPy 算数运算中，数组 x 和数组 y 的元素个数是相同的（对应元素计算），如果元素个数不同，程序就会报错，所以元素个数保持一致非常重要。 此外，NumPy 数组不仅可以进行 element-wise 运算，也可以和单一的数值（标量）组合器来进行运算。 此时，需要在 NumPy 数组的各个元素和标量之间进行运算。 这个功能也被称为广播。 12345678910111213141516171819202122232425262728293031x = np.array([1.0, 2.0, 3.0])x / 2.0 # array([ 0.5, 1. , 1.5])## NumPy 的 N 维数组A = np.array([[1, 2], [3, 4]])print(A)&#x27;&#x27;&#x27;[[1 2] [3 4]]&#x27;&#x27;&#x27;A.shape # (2, 2) 第一个维度有两个元素 第二维度有两个元素A.dtype # dtype(&#x27;int64&#x27;) 矩阵元素的数据类型## 矩阵的算术运算B = np.array([[3, 0],[0, 6]])A + B&#x27;&#x27;&#x27;array([[ 4, 2], [ 3, 10]])&#x27;&#x27;&#x27;A * B&#x27;&#x27;&#x27;array([[ 3, 0], [ 0, 24]])&#x27;&#x27;&#x27;# 与标量相乘的广播功能A * 10&#x27;&#x27;&#x27;array([[ 10, 20], [ 30, 40]])&#x27;&#x27;&#x27; NumPy 数组 np.array 可以生成 N 维数组，即可以生成一维数组、 二维数组、三维数组等任意维数的数组。数学上将一维数组称为向量， 将二维数组称为矩阵。另外，可以将一般化之后的向量或矩阵等统称为张量（tensor）。本书基本上将二维数组称为矩阵，将三维数组及三维以上的数组称为张量或多维数组。 1234567891011121314151617181920212223242526## 访问元素# 索引访问X = np.array([[51, 55], [14, 19], [0, 4]])print(X)&#x27;&#x27;&#x27;[[51 55] [14 19] [ 0 4]]&#x27;&#x27;&#x27;X[0] # 第 0 行 array([51, 55])X[0][1] # (0,1) 的元素 55# for 语句访问for row in X: print(row)&#x27;&#x27;&#x27;[51 55][14 19][ 0 4]&#x27;&#x27;&#x27;# 数组访问X = X.flatten() # 将 X 转换为一维数组print(X) # [51 55 14 19 0 4]X[np.array([0, 2, 4])] # 获取索引为 0、2、4 的元素 array([51, 14, 0])# 运用标记法，从 X 中抽出大于 15 的元素X &gt; 15 # array([ True, True, False, True, False, False], dtype=bool)X[X&gt;15] # array([51, 55, 19]) 取出 True 对应的元素 矩阵的运算。（注意！！！矩阵的点乘得遵循一定的法则 (A, B) · (B, A) = (A, A)） 12345678910111213A = np.array([[1,2,3], [4,5,6]])np.ndim(A) # 2 有两个维度A.shape # (2, 3)B = np.array([[1,2], [3,4], [5,6]])B.shape # (3, 2)np.dot(A, B) # 点乘，矩阵的乘法&#x27;&#x27;&#x27;array([[22, 28], [49, 64]])&#x27;&#x27;&#x27;B = np.array([[1,2], [3,4], [5,6]]np.ndim(B) # 2B.shape # (3, 2) Matplotlib 深度学习实验中，图形的绘制和数据的可视化非常重要。Matplotlib 是用于绘制图形的库，使用 Matplotlib 可以轻松地绘制图形和实现数据的可视化。这里，我们来介绍一下图形的绘制方法和图像的显示方法。 12345678910111213141516import numpy as npimport matplotlib.pyplot as plt## 绘制 sin 以及 cos 函数的图形# 生成数据x = np.arange(0, 6, 0.1) # 以 0.1 为步长单位，生成 0 到 6 的数据y1 = np.sin(x)y2 = np.cos(x)# 绘制图形plt.plot(x, y1, label=&quot;sin&quot;)plt.plot(x, y2, linestyle = &quot;--&quot;, label=&quot;cos&quot;) # 用虚线绘制plt.xlabel(&quot;x&quot;) # x 轴标签plt.ylabel(&quot;y&quot;) # y 轴标签plt.title(&#x27;sin &amp; cos&#x27;) # 标题plt.legend() # 自动对上诉添加的标签布局，复杂时可以自定义plt.show() pyplot 中还提供了用于显示图像的方法 imshow()。另外，可以使用 matplotlib.image 模块的 imread() 方法读入图像。 1234567## 显示图像import matplotlib.pyplot as pltfrom matplotlib.image import imreadimg = imread(&#x27;lena.png&#x27;) # 读入图像（设定合适的路径！）plt.imshow(img)plt.show() 感知机 感知机可以说是神经网络（深度学习）的起源的算法，因此， 学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。 感知机接收多个输入信号，输出一个信号。 简单逻辑电路 形式一：x1w1 + x2w2 ≤ θ 👉 y = 0 （θ 代表阈值、x 代表输入信号、w 代表权重） 形式二：x1w1 + x2w2 + b ≤ 0 （其中 b = -θ） 👉 y = 0 （b 代表偏置） 123456789101112131415161718192021222324252627282930313233343536373839404142## 与门# 简单实现 def AND(x1, x2): w1, w2, theta = 0.5, 0.5, 0.7 tmp = x1*w1 + x2*w2 if tmp &lt;= theta: return 0 elif tmp &gt; theta: return 1# 使用权重和偏置的实现def AND(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) b = -0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1## 非门：与门反过来即可def NAND(x1, x2): x = np.array([x1, x2]) w = np.array([-0.5, -0.5]) # 仅权重和偏置与 AND 不同！ b = 0.7 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1## 或门def OR(x1, x2): x = np.array([x1, x2]) w = np.array([0.5, 0.5]) # 仅权重和偏置与AND不同！ b = -0.2 tmp = np.sum(w*x) + b if tmp &lt;= 0: return 0 else: return 1## 异或门（无法用上面类似的形式进行实现） 为什么异或门无法用上述相似的形式（线性）进行实现呢？先来看看异或门的结果分部。 通过分析上图可以发现，无法用线性空间（直线）将 0 和 1 这两种情况进行区域的划分。但用非线性的空间（曲线）可以，那如何划分非线性空间呢？答案是：叠加感知机。即多层感知机。 多层感知机 通过组合与门、与非门、或门实现异或门。 123456# 异或门def XOR(x1, x2): s1 = NAND(x1, x2) s2 = OR(x1, x2) y = AND(s1, s2) return y 神经网络 在上一章中，我们使用感知机实现了简单的逻辑电路。从实现的逻辑中我们可以发现，即使是复杂的函数，感知机也隐含着能够代表它的可能性。但有一个很令人头疼的问题，那便是权重的设定。在实现简单逻辑电路等简单函数时，我们可以通过观察归纳得出符合条件的权重，但如果函数特别的复杂，那么光靠人脑去猜测其权重显然是不合适的。 神经网络的出现就是为了解决这个问题的。具体地讲，神经网络的一 个重要性质是它可以自动地从数据中学习到合适的权重参数。 激活函数 激活函数是连接感知机和神经网络的桥梁。 隐藏（中间层）的激活函数。 Sigmoid 函数 12def sigmoid(x): return 1 / (1 + np.exp(-x)) 阶跃函数 12345678910# 简单实现def step_function(x): if x &gt; 0: return 1 else: return 0# 支持 NumPy 数组实现def step_function(x): y = x &gt; 0 return y.astype(np.int) # 把数组 y 的元素类型从 boole 型转换为 int 型 ReLU 函数 12def relu(x): return np.maximum(0, x) 输出层的激活函数。 如何选择输出层的激活函数？ 一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数， 多元分类问题可以使用 softmax 函数。 机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题（寻找决策边界）。比如，区分图像中的人是男性还是女性的问题就是分类问题。而回归问题是根据某个输入预测一个（连续的） 数值的问题（找到最优拟合）。 恒等函数 12def identity_function(x): return x softmax 函数 exp(x) 是表示 ex 的指数函数（e 是纳皮尔常数2.7182 …），容易出现很大的值从而导致数值的溢出问题。在进行 softmax 的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果，为了防止溢出，一般会使用输入信号中的最大值。 如上所示，softmax 函数的输出是 0.0 到 1.0 之间的实数。并且，softmax 函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质。正因为有了这个性质，我们才可以把 softmax 函数的输出解释为概率。 123456789101112def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y# 优化def softmax(a): c = np.max(a) exp_a = np.exp(a - c) # 溢出对策 sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y 简单实现 了解了激活函数，让我们来看一个简单的神经网络。这个神经网络省略了偏置和激活函数，只有权重。 我们需要了解的重点是：神经网络的运算可以作为矩阵运算打包进行。 隐藏（中间）层的激活函数表示为：h() 输出层的激活函数表示为：σ() 1234567891011X = np.array([1, 2])X.shape # (2,)W = np.array([[1, 3, 5], [2, 4, 6]])print(W)&#x27;&#x27;&#x27;[[1 3 5] [2 4 6]]&#x27;&#x27;&#x27;W.shape # (2, 3)Y = np.dot(X, W)print(Y) # [ 5 11 17] 如上所示，使用 np.dot （多维数组的点积），可以一次性计算出 Y 的结果。 这意味着，即便 Y 的元素个数为 100 或 1000，也可以通过一次运算就计算出结果！如果不使用 np.dot，就必须单独计算 Y 的每一个元素（或者说必须使用 for 语句），非常麻烦。因此，通过矩阵的乘积一次性完成计算的技巧，在实现的层面上可以说是非常重要的。 现在我们将偏置还有激活函数考虑进来。 12345678910111213141516171819202122232425262728## 第一层处理# 设置信号、权重、偏置为某一任意值X = np.array([1.0, 0.5])W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])B1 = np.array([0.1, 0.2, 0.3])print(W1.shape) # (2, 3)print(X.shape) # (2,)print(B1.shape) # (3,)A1 = np.dot(X, W1) + B1 # 一次得出第一层的所有的 aZ1 = sigmoid(A1)print(A1) # [0.3, 0.7, 1.1] # 第一层的结果print(Z1) # [0.57444252, 0.66818777, 0.75026011] # sigmoid 函数进行处理## 第二层处理，与第一层类似W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])B2 = np.array([0.1, 0.2])print(Z1.shape) # (3,)print(W2.shape) # (3, 2)print(B2.shape) # (2,)A2 = np.dot(Z1, W2) + B2Z2 = sigmoid(A2)## 第二层到输出层的处理# 输出层的激活函数与之前隐藏层的有所不同def identity_function(x): return x # 恒等函数W3 = np.array([[0.1, 0.3], [0.2, 0.4]])B3 = np.array([0.1, 0.2])A3 = np.dot(Z2, W3) + B3Y = identity_function(A3) # 或者 Y = A3，此处按照规范使用一个返回自身的激活函数 最后我们将本章的代码流程进行整理，方便阅读。 12345678910111213141516171819202122232425## 权重及偏置的初始化def init_network(): network = &#123;&#125; # 保存所有初始化参数的字典 network[&#x27;W1&#x27;] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) network[&#x27;b1&#x27;] = np.array([0.1, 0.2, 0.3]) network[&#x27;W2&#x27;] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) network[&#x27;b2&#x27;] = np.array([0.1, 0.2]) network[&#x27;W3&#x27;] = np.array([[0.1, 0.3], [0.2, 0.4]]) network[&#x27;b3&#x27;] = np.array([0.1, 0.2]) return network## 封住了将输入信号转换为输出信号的处理过程def forward(network, x): W1, W2, W3 = network[&#x27;W1&#x27;], network[&#x27;W2&#x27;], network[&#x27;W3&#x27;] b1, b2, b3 = network[&#x27;b1&#x27;], network[&#x27;b2&#x27;], network[&#x27;b3&#x27;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = identity_function(a3) return ynetwork = init_network()x = np.array([1.0, 0.5])y = forward(network, x)print(y) # [ 0.31682708 0.69627909] 手写数字识别 介绍完神经网络的结构之后，现在我们来试着解决实际问题。 假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的推理处理。这个推理处理也称为神经网络的前向传播（forward propagation）。 求解机器学习问题的步骤可以分为学习和推理两个阶段。 首先，在学习阶段进行模型的学习， 然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。 一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。 并且，即便使用 softmax 函数，输出值最大的神经元的位置也不会变。因此， 神经网络在进行分类时，输出层的 softmax 函数可以省略。 在输出层使用 softmax 函数是因为它和神经网络的学习有关系。 1234567891011121314151617181920212223242526272829303132## 让我们先尝试着将数据集 MINST 进行导入import sys, ossys.path.append(os.pardir) # 为了导入父目录中的文件而进行的设定from dataset.mnist import load_mnist# 读入 MNIST 数据集（第一次调用会花费几分钟）# flatten 将形状压成一维# normalize 将输入图像正规化为 0.0～1.0 的值# (训练图像 ,训练标签)，(测试图像，测试标签)(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)# 输出各个数据的形状print(x_train.shape) # (60000, 784)print(t_train.shape) # (60000,)print(x_test.shape) # (10000, 784)print(t_test.shape) # (10000,)## 接下来试着使用 PIL（Python Image Library） 模块来显示图片import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom PIL import Imagedef img_show(img): # 定义显示图片的函数 pil_img = Image.fromarray(np.uint8(img)) pil_img.show()(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)img = x_train[0]label = t_train[0]print(label) # 5print(img.shape) # (784,)img = img.reshape(28, 28) # 把图像的形状变成原来的尺寸print(img.shape) # (28, 28)img_show(img) 好了，到这一步我们已经对这个数据集有一定的了解，接下来我们来思考一下对这个 MINST 数据集实现神经网络的推理处理。 神经网络的输入层有 784 个神经元，输出层有 10 个神经元。输入层的 784 这个数字来源于图像大小的 28 × 28 = 784，输出层的10 这个数字来源于 10 类别分类（数字 0 到 9，共 10 个类别）。此外，这个神经网络有 2 个隐藏层，第 1 个隐藏层有 50 个神经元，第 2 个隐藏层有 100 个神经元。这个 50 和 100 可以设置为任何值。 1234567891011121314151617181920212223242526272829303132333435## 获取测试数据（测试集与标签集）def get_data(): (x_train, t_train), (x_test, t_test) = \\ load_mnist(normalize=True, flatten=True, one_hot_label=False) return x_test, t_test## 获取权重（数据集自带的）def init_network(): # Binary Mode 返回 Bytes with open(&quot;sample_weight.pkl&quot;, &#x27;rb&#x27;) as f: network = pickle.load(f) return network## 预测过程（模型）def predict(network, x): W1, W2, W3 = network[&#x27;W1&#x27;], network[&#x27;W2&#x27;], network[&#x27;W3&#x27;] b1, b2, b3 = network[&#x27;b1&#x27;], network[&#x27;b2&#x27;], network[&#x27;b3&#x27;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = softmax(a3) return y## 评价识别精度x, t = get_data()network = init_network()accuracy_cnt = 0for i in range(len(x)): y = predict(network, x[i]) p = np.argmax(y) # 获取概率最高的元素的索引 if p == t[i]: accuracy_cnt += 1 # 记录正确的次数print(&quot;Accuracy:&quot; + str(float(accuracy_cnt) / len(x))) 把数据限定到某个范围内的处理称为正规化（normalization）。此外，对神经网络的输入数据进行某种既定的转换称为预处理（preprocessing）。 很多预处理都会考虑到数据的整体分布。比如，利用数据整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。除此之外，还有将数据整体的分布形状均匀化的方法，即数据白化（whitening）等。 批处理 在上面的预测中，我们一次只预测一个输入。其实还有一种更简单快捷的方式：批处理。 批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。 12345678910x, t = get_data()network = init_network()batch_size = 100 # 批数量accuracy_cnt = 0for i in range(0, len(x), batch_size): [0, 100, 200 ...] x_batch = x[i:i+batch_size] # 切片 [0, 100) [100, 200) y_batch = predict(network, x_batch) p = np.argmax(y_batch, axis=1) # 取得所有第一个维度上的最大值 accuracy_cnt += np.sum(p == t[i:i+batch_size]) # 批量判断是否准确并累加print(&quot;Accuracy:&quot; + str(float(accuracy_cnt) / len(x))) 神经网络的学习 上一章我们了解神经网络的推测过程，在推测的过程中我们使用到了权重与偏置参数，这些参数在上一章中并没有提及是如何得出的。 而神经网络的学习，其学习过程便是指从训练数据中自动获取最优权重参数的过程。 本章中，为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。为了找出尽可能小的损失函数的值，本章我们将介绍利用了函数斜率的梯度法。 从数据中学习 在实际的神经网络中，参数的数量可能是成千上万个，在层数更深的深度学习中，参数的数量甚至可能上亿，想要人工决定这些参数的值是不可能的。 所以，我们要完成一件非常了不起的事情：数据自动决定权重参数的值。 一般来讲，需要在数据中提取特征量，这里所说的特征量是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。特征的量的提取可以分为人工提取（机器学习）与机器提取（深度学习）两大类。 机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。 为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。 泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。顺便说一下，只对某个数据集过度拟合的状态称为过拟合（Over Fitting）。避免过拟合也是机器学习的一个重要课题。 损失函数 损失函数是表示神经网络性能的恶劣程度的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。 均方误差 12345678910def mean_squared_error(y, t): return 0.5 * np.sum((y-t)**2) # y 代表神经网络输出 t 代表监督数据# 设 2 为正确解t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]# 2 的概率最高的情况（0.6）y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]mean_squared_error(np.array(y), np.array(t)) # 0.097500000000000031# 7 的概率最高的情况（0.6）y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]mean_squared_error(np.array(y), np.array(t)) # 0.59750000000000003 交叉熵误差 1234567891011121314151617181920212223242526272829def cross_entropy_error(y, t): delta = 1e-7 # 微小值保护 np.log(0) 会变为负无限大的 -inf return -np.sum(t * np.log(y + delta))t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]cross_entropy_error(np.array(y), np.array(t))0.51082545709933802y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]cross_entropy_error(np.array(y), np.array(t))2.3025840929945458## mini-batch 版本实现def cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] # 用 batch 的个数进行正规化，计算单个数据的平均交叉熵误差 return -np.sum(t * np.log(y + 1e-7)) / batch_size# 监督数据是标签形式，即非 one-hotdef cross_entropy_error(y, t): if y.ndim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] # np.arange(batch_size) 生成从 0 到 batch_size-1 的数组 # y[np.arange(batch_size), t] 生成 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]] return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size 在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。 数值微分 梯度法的核心就是数值微分，在了解梯度法之前我们先来讨论一下数值微分中的导数和偏导数。 导数 只存在一个变量的情况。 123def numerical_diff(f, x): h = 1e-4 # 0.0001 return (f(x+h) - f(x-h)) / (2*h) # 中心差分 偏导数和梯度 存在多个变量。 由全部变量的偏导数汇总而成的向量称为梯度（gradient）。 1234567891011121314def numerical_gradient(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) # 生成和 x 形状相同的数组 for idx in range(x.size): # 每次只对一个变量进行中心差分求导 tmp_val = x[idx] # f(x+h)的计算 x[idx] = tmp_val + h fxh1 = f(x) # f(x-h)的计算 x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 还原值 return grad 实际上， 梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向。通过巧妙地使用梯度来寻找函数最小值 （或者尽可能小的值）的方法就是梯度法。 在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进， 如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进， 逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。 不断的前进的过程中，我们需要考虑一次前进多少？这就是神经网络的学习中的学习率。 1234567# 梯度下降法 学习率为 0.01 学习次数为 100 次def gradient_descent(f, init_x, lr=0.01, step_num=100): x = init_x for i in range(step_num): grad = numerical_gradient(f, x) x -= lr * grad return x 像学习率这样的由人工设定的参数称为超参数。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。 神经网络的梯度 神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度。 123456789101112131415161718192021222324252627282930313233343536373839import sys, ossys.path.append(os.pardir)import numpy as npfrom common.functions import softmax, cross_entropy_errorfrom common.gradient import numerical_gradientclass simpleNet: def __init__(self): self.W = np.random.randn(2,3) # 用高斯分布进行初始化 def predict(self, x): return np.dot(x, self.W) def loss(self, x, t): z = self.predict(x) y = softmax(z) loss = cross_entropy_error(y, t) # 交叉熵误差 return loss## 求这个简单神经网络的损失net = simpleNet()print(net.W) # 权重参数&#x27;&#x27;&#x27;[[ 0.47355232 0.9977393 0.84668094], [ 0.85557411 0.03563661 0.69422093]])&#x27;&#x27;&#x27;x = np.array([0.6, 0.9])p = net.predict(x)print(p) # [ 1.05414809 0.63071653 1.1328074]np.argmax(p) # 最大值的索引t = np.array([0, 0, 1]) # 正确解标签net.loss(x, t) # 0.92806853663411326## 求梯度f = lambda w: net.loss(x, t) # 此处传参 w 为伪参数，求梯度中会进行传参dW = numerical_gradient(f, net.W)print(dW)&#x27;&#x27;&#x27;[[ 0.21924763 0.14356247 -0.36281009] [ 0.32887144 0.2153437 -0.54421514]]&#x27;&#x27;&#x27; 到目前为止，我们已经得到了这个神经网络的损失函数所对应的梯度了，让我们来观察一下这个梯度矩阵。首先，形状上梯度矩阵与权重矩阵的形状相同，并且应该是对应的关系：w(2,3) 对应的梯度大约为 -0.5，这意味着这个方向上的权重每增加 h，损失函数的值将减小 0.5*h 求出神经网络的梯度后，接下来只需根据梯度法，更新权重参数即可。 学习算法的实现 步骤 1（mini-batch）： 从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小 mini-batch 的损失函数的值。 步骤 2（计算梯度）： 为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。 梯度表示损失函数的值减小最多的方向。 步骤 3（更新参数）： 将权重参数沿梯度方向进行微小更新。 步骤 4（重复）： 重复步骤1、步骤2、步骤3。 因为这里使用的数据是随机选择的 mini batch 数据，所以又称为随机梯度下降法 SGD（Stochastic Gradient Descent）。 下面，我们来实现手写数字识别的神经网络。这里以 2 层神经网络（隐藏层为 1 层的网络）为对象，使用 MNIST 数据集进行学习。 12345678910111213141516171819202122232425262728293031323334353637383940## 两层神经网络的类import sys, ossys.path.append(os.pardir)from common.functions import *from common.gradient import numerical_gradientclass TwoLayerNet: def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 初始化权重 self.params = &#123;&#125; self.params[&#x27;W1&#x27;] = weight_init_std * np.random.randn(input_size, hidden_size) self.params[&#x27;b1&#x27;] = np.zeros(hidden_size) self.params[&#x27;W2&#x27;] = weight_init_std * np.random.randn(hidden_size, output_size) self.params[&#x27;b2&#x27;] = np.zeros(output_size) def predict(self, x): W1, W2 = self.params[&#x27;W1&#x27;], self.params[&#x27;W2&#x27;] b1, b2 = self.params[&#x27;b1&#x27;], self.params[&#x27;b2&#x27;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 y = softmax(a2) return y # x:输入数据, t:监督数据 def loss(self, x, t): y = self.predict(x) return cross_entropy_error(y, t) def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis=1) t = np.argmax(t, axis=1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy # x:输入数据, t:监督数据 def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = &#123;&#125; grads[&#x27;W1&#x27;] = numerical_gradient(loss_W, self.params[&#x27;W1&#x27;]) grads[&#x27;b1&#x27;] = numerical_gradient(loss_W, self.params[&#x27;b1&#x27;]) grads[&#x27;W2&#x27;] = numerical_gradient(loss_W, self.params[&#x27;W2&#x27;]) grads[&#x27;b2&#x27;] = numerical_gradient(loss_W, self.params[&#x27;b2&#x27;]) return grads mini-batch 的实现 12345678910111213141516171819202122232425import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_laobel = True)train_loss_list = []# 超参数iters_num = 10000 # 循环次数train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)for i in range(iters_num): # 获取 mini-batch，随机获取 100 个 batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 计算梯度 grad = network.numerical_gradient(x_batch, t_batch) # grad = network.gradient(x_batch, t_batch) # 高速版! # 更新参数 for key in (&#x27;W1&#x27;, &#x27;b1&#x27;, &#x27;W2&#x27;, &#x27;b2&#x27;): network.params[key] -= learning_rate * grad[key] # 记录学习过程 loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) 这里给出的算法记录了每一次的损失，但其实这是没有必要的，我们其实只要对一次 epoch 做一次评价就可以。 epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被（宏观上）使用过一次时的更新次数。 1234567891011121314151617181920212223242526272829303132333435import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet(x_train, t_train), (x_test, t_test) = \\ load_mnist(normalize=True, one_hot_ laobel = True)train_loss_list = []train_acc_list = []test_acc_list = []# 平均每个 epoch 的重复次数iter_per_epoch = max(train_size / batch_size, 1)# 超参数iters_num = 10000batch_size = 100learning_rate = 0.1network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)for i in range(iters_num): # 获取 mini-batch batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 计算梯度 grad = network.numerical_gradient(x_batch, t_batch) # grad = network.gradient(x_batch, t_batch) # 高速版! # 更新参数 for key in (&#x27;W1&#x27;, &#x27;b1&#x27;, &#x27;W2&#x27;, &#x27;b2&#x27;): network.params[key] -= learning_rate * grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) # 计算每个 epoch 的识别精度 if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(&quot;train acc, test acc | &quot; + str(train_acc) + &quot;, &quot; + str(test_acc)) 误差反向传播 在前面的章节中，我们用数值微分计算了神经网络的权重参数的梯度。数值微分简单易实现，但在计算上是比较耗费时间的。本章我们将学习一个高效计算权重参数的梯度方法：误差反向传播。 首先我们通过图片来理解这一方法。 上图表示的是基于反向传播的导数传递。 让我们来看看图中的各数值代表什么。在这个例子中，反向传播从右向左传递导数的值（1 👉 1.1 👉2.2）。这表示支付金额关于苹果价格的导数的值是 2.2，即苹果每上涨 ð 元，最终的支付金额会增加 2.2ð 元。 通过观察我们可以发现一个更为通用的法则，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数（链式法则）。 反向传播的节点除了 x 还有 +。加法节点的变化在传播的过程中不对上一层的链式传播数值产生影响。 来看一个比较复杂的例子。 苹果个数对最终价格的影响为：110。 推导过程：从右到左 715 = 650 x 1.1 这里得到影响因子 1.1 👉 加法节点直接继承上一层的影响因子 1.1 👉 200 = 2 x 100 这里得到影响因子 1.1 x 100 = 110。 代码实现 在这里我们用 Python 实现上述的例子。乘法节点为乘法层（MulLayer），加法节点称为加法层（AddLayer）。 乘法层 123456789101112131415class MulLayer: def __init__(self): self.x = None self.y = None # 正向传播 def forward(self, x, y): self.x = x self.y = y out = x * y return out # 反向传播 dout 为上游传来的导数 def backward(self, dout): dx = dout * self.y # 翻转 x 和 y dy = dout * self.x return dx, dy 那我们买苹果的计算图的正向传播可以用以下代码表示。 12345678910apple = 100apple_num = 2tax = 1.1# layermul_apple_layer = MulLayer()mul_tax_layer = MulLayer()# forwardapple_price = mul_apple_layer.forward(apple, apple_num)price = mul_tax_layer.forward(apple_price, tax)print(price) # 220 关于各个变量的导数可由 backward() 求出 12345# backwarddprice = 1dapple_price, dtax = mul_tax_layer.backward(dprice)dapple, dapple_num = mul_apple_layer.backward(dapple_price)print(dapple, dapple_num, dtax) # 2.2 110 200 加法层 12345678910class AddLayer: def __init__(self): pass def forward(self, x, y): out = x + y return out def backward(self, dout): dx = dout * 1 dy = dout * 1 return dx, dy 我们来用 Python 代码实现一下买苹果和橘子的计算图。 1234567891011121314151617181920212223apple = 100apple_num = 2orange = 150orange_num = 3tax = 1.1# layermul_apple_layer = MulLayer()mul_orange_layer = MulLayer()add_apple_orange_layer = AddLayer()mul_tax_layer = MulLayer()# forwardapple_price = mul_apple_layer.forward(apple, apple_num) # (1)orange_price = mul_orange_layer.forward(orange, orange_num) # (2)all_price = add_apple_orange_layer.forward(apple_price, orange_price) # (3)price = mul_tax_layer.forward(all_price, tax) #( 4)# backwarddprice = 1dall_price, dtax = mul_tax_layer.backward(dprice) # (4)dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) # (3)dorange, dorange_num = mul_orange_layer.backward(dorange_price) # (2)dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (1)print(price) # 715print(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650 激活函数层的实现 现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 ReLU 层和 Sigmoid 层。 ReLU 层 如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。 123456789101112class Relu: def __init__(self): self.mask = None def forward(self, x): self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self, dout): dout[self.mask] = 0 dx = dout return dx ReLU 类有实例变量 mask。这个变量 mask 是由 True / False 构成的 NumPy 数 组，它会把正向传播时的输入 x 的元素中小于等于 0 的地方保存为 True，其他地方（大于 0 的元素）保存为 False。 如果正向传播时的输入值小于等于 0，则反向传播的值为 0。 因此，反向传播中会使用正向传播时保存的 mask，将从上游传来的 dout 的 mask 中的元素为 True 的地方设为 0。 Sigmoid 层 这里 / 用倒数的 x 计算。 对结果进行进一步的整理。 让我们将推导的过程隐蔽，我们可以得到一个 Sigmoid 节点。 12345678910class Sigmoid: def __init__(self): self.out = None def forward(self, x): out = 1 / (1 + np.exp(-x)) self.out = out return out def backward(self, dout): dx = dout * (1.0 - self.out) * self.out return dx Affine / Softmax 层的实现 在很前面的章节中（忘了赶紧回去给我复习：神经网络 / 简单实现]]），我们利用公式 Y = np.dot(X,W) + B 计算出神经元的加权。然后 Y 经过激活函数转换后，传递给下一层，这就是神经网络正向传播过程。 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为仿射变换。因此，这里将进行仿射变换的处理实现为 Affine 层。 让我们来看看将这个过程转换为计算图是什么样子的。 正向传播时，偏置会被加到每一个数据上。因此，反向传播时，各个数据的反向传播的值需要汇总为偏置的元素。 Affine 层：神经网络的正向传播中，进行的矩阵的乘积运算，在几何学领域被称为仿射变换。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。 12345678910111213141516class Affine: def __init__(self, W, b): self.W = W self.b = b self.x = None self.dW = None self.db = None def forward(self, x): self.x = x out = np.dot(x, self.W) + self.b return out def backward(self, dout): dx = np.dot(dout, self.W.T) self.dW = np.dot(self.x.T, dout) self.db = np.sum(dout, axis=0) # 公式 3 把 b 当成 return dx Softmax-with-Loss 层 在神经网络的学习中，我们一般需要将输出正规化后输入损失函数进行处理，一般会使用 Softmax 层；当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。 Softmax-with-Loss：这里的 Loss 指交交叉熵误差层 Cross Entropy Error。 这图看到人就傻了，让我们来简化一下。 让我们来看看各个字母所代表的含义。（y1, y2, y3） 是 Softmax 层的输出，（t1, t2, t3） 是监督数据，所以 （y1 − t1, y2 − t2, y3 − t3） 是 Softmax 层的输出和教师标签的差分。所以 （y1 − t1, y2 − t2, y3 − t3） 是 Softmax 层的输出和教师标签的差分（即输出与标签的误差）。 使用交叉熵误差作为 softmax 函数的损失函数后，反向传播得到 （y1 − t1, y2 − t2, y3 − t3） 这样漂亮的结果。实际上，这样漂亮的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。 回归问题中输出层使用恒等函数，损失函数使用平方和误差，也是出于同样的理由（忘了就给我翻回去复习！！！）。也就是说，使用平方和误差作为恒等函数的损失函数，反向传播才能得到 （y1 − t1, y2 − t2, y3 − t3） 这样漂亮的结果。 1234567891011121314class SoftmaxWithLoss: def __init__(self): self.loss = None # 损失 self.y = None # softmax 的输出 self.t = None # 监督数据（one-hot vector） def forward(self, x, t): self.t = t self.y = softmax(x) self.loss = cross_entropy_error(self.y, self.t) return self.loss def backward(self, dout=1): batch_size = self.t.shape[0] dx = (self.y - self.t) / batch_size return dx 总结 现在来进行神经网络的实现。这里我们要把 2 层神经网络实现为 TwoLayerNet。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import sys, ossys.path.append(os.pardir)import numpy as npfrom common.layers import *from common.gradient import numerical_gradientfrom collections import OrderedDictclass TwoLayerNet: # 输入层的神经元数、隐藏层的神经元数、输出层的神经元数、初始化权重时的高斯分布的规模 def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 初始化权重 params 为参数字典 self.params = &#123;&#125; self.params[&#x27;W1&#x27;] = weight_init_std * np.random.randn(input_size, hidden_size) self.params[&#x27;b1&#x27;] = np.zeros(hidden_size) self.params[&#x27;W2&#x27;] = weight_init_std * np.random.randn(hidden_size, output_size) self.params[&#x27;b2&#x27;] = np.zeros(output_size) # 生成层 layers 有序字典型变量 保存神经网络的层 self.layers = OrderedDict() self.layers[&#x27;Affine1&#x27;] = Affine(self.params[&#x27;W1&#x27;], self.params[&#x27;b1&#x27;]) self.layers[&#x27;Relu1&#x27;] = Relu() self.layers[&#x27;Affine2&#x27;] = Affine(self.params[&#x27;W2&#x27;], self.params[&#x27;b2&#x27;]) # 神经网络的最后一层 self.lastLayer = SoftmaxWithLoss() def predict(self, x): for layer in self.layers.values(): x = layer.forward(x) return x # x:输入数据, t:监督数据 def loss(self, x, t): y = self.predict(x) return self.lastLayer.forward(y, t) # 计算识别精度 def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis=1) if t.ndim != 1 : t = np.argmax(t, axis=1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy # x:输入数据, t:监督数据 通过数值微分计算关于权重的梯度 def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = &#123;&#125; grads[&#x27;W1&#x27;] = numerical_gradient(loss_W, self.params[&#x27;W1&#x27;]) grads[&#x27;b1&#x27;] = numerical_gradient(loss_W, self.params[&#x27;b1&#x27;]) grads[&#x27;W2&#x27;] = numerical_gradient(loss_W, self.params[&#x27;W2&#x27;]) grads[&#x27;b2&#x27;] = numerical_gradient(loss_W, self.params[&#x27;b2&#x27;]) return grads # 通过误差反向传播计算关于权重参数的梯度 def gradient(self, x, t): # forward self.loss(x, t) # backward dout = 1 dout = self.lastLayer.backward(dout) layers = list(self.layers.values()) layers.reverse() for layer in layers: dout = layer.backward(dout) # 设定 grads = &#123;&#125; grads[&#x27;W1&#x27;] = self.layers[&#x27;Affine1&#x27;].dW grads[&#x27;b1&#x27;] = self.layers[&#x27;Affine1&#x27;].db grads[&#x27;W2&#x27;] = self.layers[&#x27;Affine2&#x27;].dW grads[&#x27;b2&#x27;] = self.layers[&#x27;Affine2&#x27;].db return grads 注意，将神经网络的层保存在 OrderedDict 这一点非常重要（正向与反向都需要按顺序调用各层）。 可能在座的各位会有一个疑问：既然我们有了计算较为简单的误差反向传播，为什么还要由数值微分的方法呢？误差反向传播的计算虽然简单，但是实现较为复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。 1234567891011121314151617# 梯度确认import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet# 读入数据(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)x_batch = x_train[:3]t_batch = t_train[:3]grad_numerical = network.numerical_gradient(x_batch, t_batch)grad_backprop = network.gradient(x_batch, t_batch)# 求各个权重的绝对误差的平均值for key in grad_numerical.keys(): diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) ) print(key + &quot;:&quot; + str(diff)) 最后，我们利用反向传播误差来实现一下神经网络的学习（就是把前面的数值微分替换成反向误差传播）。 12345678910111213141516171819202122232425262728293031323334import sys, ossys.path.append(os.pardir)import numpy as npfrom dataset.mnist import load_mnistfrom two_layer_net import TwoLayerNet# 读入数据(x_train, t_train), (x_test, t_test) = \\load_mnist(normalize=True, one_hot_label=True)network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)iters_num = 10000train_size = x_train.shape[0]batch_size = 100learning_rate = 0.1train_loss_list = []train_acc_list = []test_acc_list = []iter_per_epoch = max(train_size / batch_size, 1)for i in range(iters_num): batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 通过误差反向传播法求梯度 grad = network.gradient(x_batch, t_batch) # 更新 for key in (&#x27;W1&#x27;, &#x27;b1&#x27;, &#x27;W2&#x27;, &#x27;b2&#x27;): network.params[key] -= learning_rate * grad[key] loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(train_acc, test_acc) 与学习相关的技巧 本章将介绍一下神经网络学习中的一些重要观点，让我们可以更高效地进行神经网络地学习，提高识别精度。 参数更新 我们已经知道，神经网络的学习目的就是找到使损失函数的尽可能小的参数，这个寻找最优参数的问题称为最优化。随机梯度下降法（stochastic gradient descent）， 简称 SGD 就是一个简单最优化的方法。但是，根据不同的问题，也存在比 SGD 更加聪明的方法。接下来我们会介绍这些方法。 SGD 先来看看 SGD 吧。 SGD 可以简单的用上面的图表示（右边的式子更新左边的式子）。 1234567891011class SGD: def __init__(self, lr=0.01): self.lr = lr def update(self, params, grads): for key in params.keys(): params[key] -= self.lr * grads[key]# 优化器optimizer = SGD()# ...optimizer.update(params, grads) Momentum Momentum 是动量的意思，和物理有关。 v 对应物理上的速度。αv 这一项表示在物体不受任何力时，该项承担使物体逐渐减速的任务（α 设定为 0.9 之类的值），对应物理上的地面摩擦或空气阻力。 对这两个公式的大致理解：小的叠加，大的抵消（减少震荡）。 AdaGrad 前面提到的学习过程，其学习率是固定的，有些优化方式是动态的调节学习率，比如：学习率衰减。逐渐减小学习率的想法，相当于将全体参数的学习率值一起降低。 而 AdaGrad 进一步发展了这个想法，针对一个一个的参数，赋予其定制的值。 AdaGrad 会为参数的每个元素适当地调整学习率，与此同时进行学习。 h 保存了以前所有梯度的平方和，我们用这个参数来动态的调整梯度的变化程度。 123456789101112class AdaGrad: def __init__(self, lr=0.01): self.lr = lr self.h = None def update(self, params, grads): if self.h is None: self.h = &#123;&#125; for key, val in params.items(): self.h[key] = np.zeros_like(val) for key in params.keys(): self.h[key] += grads[key] * grads[key] params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) 权重初始值 权重的初始值也决定了这个神经网络的学习是否可以成功，让我们来看看权重初始值的一些推荐值。 然我们先来考虑一下权值应该越大越好？还是越小越好？若权值越大，这个模型的泛化能力就差，所以权值应该是一个较小的数，在这之前的权重初始值都是像 0.01 * np.random.randn(10, 100) 这样，使用由高斯分布生成的值乘以 0.01 后得到的值（标准差为0.01的高斯分布）。 那初始权值可以设为 0 吗？答案是不可的，若初始权重都为 0，这使得神经网络拥有许多不同的权重的意义丧失了。为了防止权重均一化（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。 各层的激活值的分布都要求有适当的广度。通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者表现力受限的问题，导致学习可能无法顺利进行。 现在，在一般的深度学习框架中，Xavier 初始值已被作为标准使用。Xavier 初始值是以激活函数是线性函数为前提而推导出来的。因为 sigmoid 函数和 tanh 函数左右对称，且中央附近可以视作线性函数，所以适合使用 Xavier 初始值。 其实，权重初始值的设定与激活函数也有一定的关系。当激活函数使用 ReLU 时，一般推荐使用 ReLU 专用的初始值 He 初始值。 Batch Normalization 前面讲到我们需要在各层拥有适当的广度，这样才可以使学习顺利进行。那么，为了使各层拥有适当的广度，强制性地调整激活值的分布会怎样呢？Batch Normalization 就是基于这种想法产生的。 正则化 过拟合 学习后生成的模型泛化能力弱：对样本数据识别良好，对测试数据识别很差。 权值衰退 权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。 Dropout 作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的 L2 范数的权值衰减方法。该方法可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情 况下，我们经常会使用 Dropout 方法。 Dropout 是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。 可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习，进而达到集成学习的效果。 超参数的验证 神经网络中，除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。超参数的决定过程中常常伴随着很多试错。 因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。我们使用这个验证数据来评估超参数的好坏。 卷积神经网络 卷积神经网络（Convolutional Neural Network，CNN）常被用于图像识别、语音识别等场合，在图像识别的比赛中，基于深度学习的方法几乎都以 CNN 为基础。本章就让我们来一睹 CNN 的 风采。 整体结构 先让我们大致了解一下 CNN 的框架，其实它和神经网络一样都是通过组装层来构建的。不过，在 CNN 中还出现了新的层：卷积层（Convolution 层）和池化层（Pooling 层）。 CNN 中出现了一些特有的术语，比如填充、步幅等。此外，各层中传递的数据是有形状的数据（比如，3 维数据），这与之前的全连接网络不同，因此刚开始学习 CNN 时可能会感到难以理解。 卷积层 前面的神经网络中我们使用了全连接层（Affine 层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。但是，全连接层牺牲掉了数据的形状。。比如，输 入数据是图像时，图像通常是高、长、通道方向上的 3 维形状。但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。 3 维的形状，其中可能包含由重要的空间信息。比如，空间上邻近的像素为相似的值、RBG 的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3 维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。 而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以 3 维数据的形式接收输入数据，并同样以 3 维数据的形式输出至下一层。因此， 在 CNN 中，可以（有可能）正确理解图像等具有形状的数据。 另外，CNN 中，有时将卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。 卷积运算 卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的滤波器运算。 将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）。 填充 在卷积运算中，我们经常会向输入数据的周围填入固定的数据（比如 0 等），这成为填充。填充通常用于保持数据在卷积运算后仍然保持空间大小不变的将数据传给下一层。 步幅 应用滤波器的位置间隔称为步幅（stride）。之前的例子中步幅都是 1，如果将步幅设为 2，则情况如下图所示。 综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。这里，假设输入大小为 (H, W)，滤波器大小为 (FH, FW)，输出大小为 (OH, OW)，填充为 P，步幅为 S。可以得到如下关系。 三维数据的卷积计算 3 维数据和 2 维数据时相比，可以发现纵深方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。 需要注意的是，在 3 维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。在这个例子中，输入数据和滤波器的通道数一致，均为 3。 滤波器大小可以设定为任意值（不过，每个通道的滤波器大小要全部相同）。 通过应用 FN 个滤波器，输出特征图也生成了 FN 个。如果将这 FN 个特征图汇集在一起，就得到了形状为 (FN, OH, OW) 的方块。将这个方块传给下一层，就是 CNN 的处理流。 这里需要注意的是，作为 4 维数据，滤波器的权重数据要按 (output_channel, input_ channel, height, width) 的顺序书写。比如，通道数为 3、大小为 5 × 5 的滤波器有 20 个时，可以写成 (20, 3, 5, 5)。 3 维的卷积运算也存在偏置项，让我们来看看添加偏置的情况。 批处理 让我们来看看 3 维卷积计算的批处理示意图。 池化层 池化是缩小高、长方向上的空间的运算。比如：Max 池化如下图所示。 除了 Max 池化之外，还有 Average 池化等。相对于 Max 池化是从目标区域中取出最大值，Average 池化则是计算目标区域的平均值。在图像识别领域，主要使用 Max 池化（默认）。 代码实现 在实现具体的代码之前，让我们来看看计算的过程的转化图示。 im2col 会考虑滤波器大小、步幅、填充，将输入数据展开为 2 维数组。 123456789101112131415import sys, ossys.path.append(os.pardir)from common.util import im2colx1 = np.random.rand(1, 3, 7, 7)## im2col (input_data, filter_h, filter_w, stride=1, pad=0)# input_data 由（数据量，通道，高，长）的 4 维数组构成的输入数据# filter_h 滤波器的高 filter_w 滤波器的长# stride 步幅 pad 填充col1 = im2col(x1, 5, 5, stride=1, pad=0)# batch = 1print(col1.shape) # (9, 75)x2 = np.random.rand(10, 3, 7, 7)# batch = 10col2 = im2col(x2, 5, 5, stride=1, pad=0)print(col2.shape) # (90, 75) 了解完 im2col，让我们利用它来实现卷积层。 123456789101112131415161718class Convolution: def __init__(self, W, b, stride=1, pad=0): self.W = W self.b = b self.stride = stride self.pad = pad def forward(self, x): FN, C, FH, FW = self.W.shape N, C, H, W = x.shape out_h = int(1 + (H + 2*self.pad - FH) / self.stride) out_w = int(1 + (W + 2*self.pad - FW) / self.stride) col = im2col(x, FH, FW, self.stride, self.pad) col_W = self.W.reshape(FN, -1).T # 滤波器的展开 (10, 3, 5, 5) 👉 (10, 75) out = np.dot(col, col_W) + self.b out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) return out","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"Spring 实战学习笔记","slug":"后台技术/Spring 实战学习笔记","date":"2020-05-20T07:12:26.000Z","updated":"2023-09-20T07:12:50.416Z","comments":true,"path":"2020/05/20/后台技术/Spring 实战学习笔记/","link":"","permalink":"https://wingowen.github.io/2020/05/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20%E5%AE%9E%E6%88%98%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"《Spring 实战》阅读笔记。","text":"《Spring 实战》阅读笔记。 bean 的装配 在 XML 中进行显式配置； 在 Java 中进行显式配置； 隐式的 bean 发现机制和自动装配。 自动化装配 bean 组件扫描 component scanningSpring 会自动发现应用上下文中所创建的 bean； 自动装配 autowiringSpring 自动满足 bean 之间的依赖。 @Component @Named 注解表明该类会作为组件类并告知 Spring 要为这个类创建 bean。 @ComponentSacn(basePackages=&#123;...&#125;) &lt;context:component-scan base-package=&quot;...&quot; /&gt; 注解启动了组件扫描。 @Configuration 注解表明这个类是一个配置类，该类应该包含如何在 Spring 应用上下文中创建 bean 的细节。 1234@Configuration@ComponentScan // 默认扫描同包类，可指定 public class MyConfig &#123;&#125; @Autowired(required=&quot;T/F&quot;) @Inject 注解实现自动装配。（在 Spring 应用上下文中寻找匹配某个 bean 需求的其他 bean） 可以用在类的任何方法上，不管是构造器、Setter 方法还是其他的方法 Spring 都会尝试满足方法参数上所声明的依赖。假如有且只有一个 bean 匹配依赖需求的话那么这个 bean 将会被装配进来。 如果没有匹配的 bean 那么在应用上下文创建的时候 Spring 会抛出一个异常。为了避免异常的出现你可以将@Autowired的required属性设置为false。 如果有多个bean都能满足依赖关系的话Spring将会抛出一个异常表明没有明确指定要选择哪个bean进行自动装配。 @Bean 注解会告诉 Spring 这个方法将会返回一个对象该对象要注册为 Spring 应用上下文中的 bean。方法体中包含了最终产生 bean 实例的逻辑。 XML 装配 bean 对强依赖使用构造器注入而对可选性的依赖使用属性注入。 12345678910111213141516171819202122232425262728293031&lt;bean id=&quot;&quot; class=&quot;&quot;&gt; &lt;!-- 构造器注入 --&gt; &lt;constructor-arg ref=&quot;...&quot; /&gt; &lt;!-- 构造器参数名 --&gt; &lt;c:[name]-ref=&quot;...&quot; /&gt; &lt;!-- 参数索引 --&gt; &lt;c:_0-ref=&quot;...&quot; /&gt; &lt;!-- 只有一个索引的省略形式 --&gt; &lt;c:_-ref=&quot;...&quot; /&gt; &lt;!-- 字面量装配 --&gt; &lt;constructor-arg value=&quot;...&quot; /&gt; &lt;c:_[name]=&quot;...&quot; /&gt; &lt;c:_0=&quot;...&quot; /&gt; &lt;c:_=&quot;...&quot; /&gt; &lt;!-- list / set --&gt; &lt;constructor-arg&gt; &lt;list&gt; &lt;ref bean=&quot;&quot; /&gt; &lt;value&gt;&quot;...&quot;&lt;/value&gt; &lt;/list&gt; &lt;/constructor-arg&gt; &lt;!-- 属性注入（用法同上） --&gt; &lt;property name=&quot;...&quot; ref=&quot;...&quot; /&gt; &lt;!-- 属性名 --&gt; &lt;p:[name]-ref=&quot;...&quot; /&gt;&lt;/bean&gt;&lt;!-- util 命名空间 --&gt;&lt;util:list id=&quot;...&quot;&gt; &lt;value&gt;&quot;...&quot;&lt;/value&gt;&lt;/util:list&gt; 配置的引用 @ComponentScan &lt;context:component-scan&gt; @Import(&#123;MyConfig.class, ...&#125;) &lt;bean class=&quot;MyConfig.class&quot; /&gt; @ImportResource(“classpath:my-config.xml”) &lt;import resource=&quot;my-config.xml&quot; /&gt; 处理歧义性 将可选 bean 中的某一个设为首选 primary 的 bean 或者使用限定符 qualifier 来帮助 Spring 将可选的 bean 的范围缩小到只有一个 bean。 @Primary &lt;bean id=&quot;...&quot; class=&quot;...&quot; primary=&quot;true&quot; /&gt; 注解声明为首选的 bean。 @Qualifier(&quot;[Name]&quot;) 注解与@Autowired和@Inject协同使用在注入的时候指定想要注入进去的是哪个 bean。 注解与@Component和@Bean协同使用创建自定义限定符。 bean 的 id 发生改变则装配失败？创建自定义限定符可以解决此问题。 可以创建自定义的限定符注解，解决 Java 不允许在同一个条目上重复出现相同类型的多个注解的规则。 1234@Target(&#123;ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifierpublic @interface Wingo &#123;&#125; bean 的作用域 在默认情况下 Spring 应用上下文中所有 bean 都是作为以单例 singleton 的形式创建的。也就是说不管给定的一个 bean 被注入到其他 bean 多少次每次所注入的都是同一个实例。 单例 Singleton 在整个应用中只创建 bean 的一个实例； 原型 Prototype 每次注入或者通过 Spring 应用上下文获取的时候都会创建一个新的 bean 实例； 会话 Session 在 Web 应用中为每个会话创建一个 bean 实例； 请求 Rquest 在 Web 应用中为每个请求创建一个 bean 实例。 @Scope(ConfigurableBeanFactory.SCOPE_PROTOTYPE) &lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;prototype&quot; /&gt; 在典型的电子商务应用中可能会有一个 bean 代表用户的购物车。如果购物车是单例的话那么将会导致所有的用户都会向同一个购物车中添加商品。另一方面如果购物车是原型作用域的那么在应用中某一个地方往购物车中添加商品在应用的另外一个地方可能就不可用了因为在这里注入的是另外一个原型作用域的购物车。 就购物车 bean 来说会话作用域是最为合适的因为它与给定的用户关联性最大。要指定会话作用域我们可以使用@Scope注解它的使用方式与指定原型作用域是相同的。 12345@Component@Scope&#123;value=WebApplication.SCOPE_SESSION, proxyMode=ScopedProxyMode.INTERFACES&#125;public ShoppingCart cart()&#123; // ??? 这里有点不理解，咋就把一个方法申明成一个 Component 了，然后代理还是用的 INTERFACE&#125; 12345&lt;bean id=&quot;...&quot; class=&quot;...&quot; scope=&quot;prototype&quot; &gt; &lt;!-- 默认情况下它会使用 CGLib 创建目标类的代理 --&gt; &lt;!-- 设置为 false 则生成基于接口的代理 --&gt; &lt;aop:scope-proxy proxy-target-class=&quot;false &quot;/&gt;&lt;/bean&gt; 要注意的是@Scope同时还有一个proxyMode属性它被设置成了ScopedProxyMode.INTERFACES。这个属性解决了将会话或请求作用域的bean注入到单例bean中所遇到的问题。 因为StoreService是一个单例的bean会在Spring应用上下文加载的时候创建。当它创建的时候Spring会试图将ShoppingCart bean注入到setShoppingCart()方法中。但是ShoppingCartbean是会话作用域的此时并不存在。直到某个用户进入系统创建了会话之后才会出现ShoppingCart实例。 另外系统中将会有多个ShoppingCart实例每个用户一个。我们并不想让 Spring 注入某个固定的ShoppingCart实例到StoreService中。我们希望的是当StoreService处理购物车功能时它所使用的ShoppingCart实例恰好是当前会话所对应的那一个。 请求作用域的 bean 会面临相同的装配问题。因此请求作用域的 bean 应该也以作用域代理的方式进行注入。 Spring 并不会将实际的ShoppingCart bean 注入到 StoreService 中，Spring 会注入一个到ShoppingCart bean 的代理。这个代理会暴露与ShoppingCart相同的方法所以StoreService会认为它就是一个购物车。但是当StoreService调用ShoppingCart的方法时代理会对其进行懒解析并将调用委托给会话作用域内真正的ShoppingCart bean。 如果ShoppingCart是接口而不是类的话这是可以的也是最为理想的代理模式。但如果ShoppingCart是一个具体的类的话 Spring 就没有办法创建基于接口的代理了。此时它必须使用 CGLib 来生成基于类的代理。所以如果 bean 类型是具体类的话我们必须要将proxyMode属性设置为ScopedProxyMode.TARGET_CLASS以此来表明要以生成目标类扩展的方式创建代理。 运行时值注入 属性占位符 Property placeholder； Spring 表达式语言 SpEL。 @PropertySource(&quot;classpath:app.properties&quot;) 注解声明属性源并通过 Spring 的Environment来检索属性。 @Value(&quot;$&#123;xxx.xxx&#125;&quot;) 注解配合解析属性占位符使用。 为了使用占位符我们必须要配置一个PropertySourcesPlaceholderConfigurer bean。 1234@Beanpublic static PropertysourcesPlaceholderConfigurer placeholderConfigurer() &#123; return new PropertySourcesPlaceholderConfigurer();&#125; 123&lt;beans&gt; &lt;context:property-placeholder /&gt;&lt;/beans&gt; 解析外部属性能够将值的处理推迟到运行时但是它的关注点在于根据名称解析来自于 Spring Environment和属性源的属性。而 Spring 表达式语言提供了一种更通用的方式在运行时计算所要注入的值。 Spring 3 引入了 Spring 表达式语言 Spring Expression Language，SpEL 它能够以一种强大和简洁的方式将值装配到 bean 属性和构造器参数中，在这个过程中所使用的表达式会在运行时计算得到值。 在 XML 配置中你可以将 SpEL 表达式传入&lt;property&gt;或```的 value 属性中或者将其作为 p- 命名空间或 c- 命名空间条目的值。 #&#123;&#125; @Value(&quot;#&#123;ststemProperties['xxx.xxx']&#125;&quot;) 使用 bean 的 ID 来引用 bean； 调用方法和访问对象的属性； 对值进行算术、关系和逻辑运算； 正则表达式匹配； 集合操作。 环境与 Profile 配置 @profile(&quot;dev&quot;) 12345678&lt;beans&gt; &lt;beans profile=&quot;dev&quot;&gt; &lt;/beans&gt; &lt;beans profile=&quot;prod&quot;&gt; &lt;/beans&gt;&lt;/beans&gt; 激活 @ActiveProfiles 123456789101112131415161718&lt;!-- web.xml --&gt;&lt;!-- 作为 Web 应用的上下文参数 --&gt;&lt;context-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 作为 DispatcherServlet 的初始化参数 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;spring.profiles.default&lt;/param-name&gt; &lt;param-value&gt;dev&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 条件化 Spring 4 引入了一个新的@Conditional注解它可以用到带有@Bean注解的方法上。如果给定的条件计算结果为true就会创建这个 bean 否则的话这个 bean 会被忽略。 123456789101112131415public class MyConfig &#123; @Bean @Conditional(MyCondition.class) // 条件化地创建 bean public ConditionBean conditionBean() &#123; return new ConditonBean(); &#125; &#125;public class Mycondition implements Condition &#123; public boolean matches (ConditionContext ctxt, AnnotatedTypeMetadata metadata)&#123; // 具体判断逻辑 &#125;&#125; 面向切面 面向切面编程的基本原理 通过 POJO 创建切面 使用@AspectJ注解 为 AspectJ 切面注入依赖 Spring 只支持方法级别的连接点，如果需要方法拦截之外的连接点拦截功能那么我们可以利用 Aspect 来补充 Spring AOP 的功能。 描述切面的常用术语有通知 advice、切点 pointcut 和连接点 join point。 引入 Introdution 允许向现有的类添加新方法或属性。 织入 Weaving 织入是把切面应用到目标对象并创建新的代理对象的过程。 编译期切面在目标类编译时被织入。这种方式需要特殊的编译器。AspectJ 的织入编译器就是以这种方式织入切面的。 类加载期切面在目标类加载到 JVM 时被织入。这种方式需要特殊的类加载器ClassLoader它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ 5 的加载时织入 load-time weavingLTW 就支持以这种方式织入切面。 运行期切面在应用运行的某个时刻被织入。一般情况下在织入切面时 AOP 容器会为目标对象动态地创建一个代理对象。Spring AOP 就是以这种方式织入切面的。 Spring 切面可以应用 5 种类型的通知： 前置通知 Before 在目标方法被调用之前调用通知功能； 后置通知 After 在目标方法完成之后调用通知此时不会关心方法的输出是什么； 返回通知 After-returning 在目标方法成功执行之后调用通知； 异常通知 After-throwing 在目标方法抛出异常后调用通知； 环绕通知Around通知包裹了被通知的方法在被通知的方法调用之前和调用之后执行自定义的行为。 代理类封装了目标类并拦截被通知方法的调用再把调用转发给真正的目标 bean。当代理拦截到方法调用时在调用目标 bean 方法之前会执行切面逻辑。 定义切面 Spring 借助 AspectJ 的切点表达式语言来定义 Spring 切面。 AspectJ 指示器 描 述 arg() 限制连接点匹配参数为指定类型的执行方法 @args() 限制连接点匹配参数由指定注解标注的执行方法 execution() 用于匹配是连接点的执行方法 this() 限制连接点匹配 AOP 代理的 bean 引用为指定类型的类 target 限制连接点匹配目标对象为指定类型的类 @target() 限制连接点匹配特定的执行对象这些对象对应的类要具有指定类型的注解 within() 限制连接点匹配指定的类型 @within() 限制连接点匹配指定注解所标注的类型当使用 Spring AOP 时方法定义在由指定的注解所标注的类里 @annotation 限定匹配带有指定注解的连接点 XML 中的切面声明。 AOP配置元素 用 途 &lt;aop:advisor&gt; 定义 AOP 通知器 &lt;aop:after&gt; 定义 AOP 后置通知不管被通知的方法是否执行成功 &lt;aop:after-returning&gt; 定义 AOP 返回通知 &lt;aop:after-throwing&gt; 定义 AOP 异常通知 &lt;aop:around&gt; 定义 AOP 环绕通知 &lt;aop:aspect&gt; 定义一个切面 &lt;aop:aspectj-autoproxy&gt; 启用 @AspectJ注解驱动的切面 &lt;aop:before&gt; 定义一个 AOP 前置通知 &lt;aop:config&gt; 顶层的 AOP 配置元素。大多数的 &lt;aop:*&gt;元素必须包含在&lt;aop:config&gt;元素内 &lt;aop:declare-parents&gt; 以透明的方式为被通知的对象引入额外的接口 &lt;aop:pointcut&gt; 定义一个切点 为了阐述 Spring 中的切面我们需要有个主题来定义切面的切点。为此我们定义一个Performance接口。 1234package concert;public interface Performance &#123; public void perform();&#125; 通知 Spring 使用 AspectJ 注解来声明通知方法。 注 解 通 知 @After 通知方法会在目标方法返回或抛出异常后调用 @AfterReturning 通知方法会在目标方法返回后调用 @AfterThrowing 通知方法会在目标方法抛出异常后调用 @Around 通知方法会将目标方法封装起来 @Before 通知方法会在目标方法调用之前执行 1234567891011@Aspectpublic class Audience &#123; @Pointcut(&quot;execution(* concert.Performance.perform(..))&quot;) public void performance()&#123;&#125; @Before(&quot;performance()&quot;) public void silenceCellPhones() &#123; System.out.println(&quot;Silencing cell phones&quot;) &#125;&#125; 12345678910@Configuration@EnableAspectJAutoProxy // 启动 AspectJ 自动代理@ComponentScanpublic class ConcertConfig &#123; @Bean public Audience audience() &#123; // 声明 Audience bean return new Audience(); &#125;&#125; 123456789101112&lt;beans&gt; &lt;context:component-scan base-package=&quot;concert&quot; /&gt; &lt;aop:aspectj-autoproxy /&gt; &lt;bean id=&quot;audience&quot; class=&quot;concert.Audiance&quot; /&gt; &lt;aop:config&gt; &lt;aop:aspect ref=&quot;audience&quot;&gt; &lt;aop:pointcut id=&quot;performance&quot; expression=&quot;execution(* concert.Performance.perform(..))&quot; /&gt; &lt;aop:before pointcut-ref=&quot;performance&quot; method=&quot;silenceCellPhones&quot; /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 环绕通知 环绕通知是最为强大的通知类型。它能够让你所编写的逻辑将被通知的目标方法完全包装起来。实际上就像在一个通知方法中同时编写前置通知和后置通知。 通知方法中可以做任何的事情，当要将控制权交给被通知的方法时它需要调用ProceedingJoinPoint的proceed()方法。 1234567891011@Around(&quot;performance()&quot;)public void watchPerformance(ProceedingJoinPoint jp) try &#123; System.out.println(&quot;Silencing cell phones&quot;); System.out.println(&quot;Taking seats&quot;); jp.proceed(); // 不调这个方法的话那么你的通知实际上会阻塞对被通知方法的调用 System.out.println(&quot;CLAP CLAP CLAP!!!&quot;); &#125; catch (Throwable e) &#123; System.out.println(&quot;Demanding a refund&quot;); &#125;&#125; 123456&lt;aop:config&gt; &lt;aop:aspect ref=&quot;audience&quot;&gt; &lt;aop:pointcut id=&quot;performance&quot; expression=&quot;execution(* concert.Performance.perform(..))&quot; /&gt; &lt;aop:around pointcut-ref=&quot;performance&quot; method=&quot;watchPerformance&quot; /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 参数化通知，使用参数化的通知来记录磁道播放的次数。 引入 如果切面能够为现有的方法增加额外的功能，为什么不能为一个对象增加新的方法呢？实际上利用被称为引入的 AOP 概念切面可以为 Spring bean 添加新方法。 Web 中的 Spring Spring 通常用来开发 Web 应用。 构建 Spring Web 映射请求到 Spring 控制器 透明地绑定表单参数 校验表单提交 使用 Spring MVC 所经历的所有站点。 请求旅程的第一站是 Spring 的DispatcherServlet。与大多数基于Java 的 Web 框架一样 Spring MVC 所有的请求都会通过一个前端控制器 front controllerServlet。前端控制器是常用的 Web 应用程序模式在这里一个单实例的 Servlet 将请求委托给应用程序的其他组件来执行实际的处理。在 Spring MVC 中DispatcherServlet就是前端控制器。 DispatcherServlet的任务是将请求发送给 Spring MVC 控制器 controller。控制器是一个用于处理请求的 Spring 组件。在典型的应用程序中可能会有多个控制器DispatcherServlet需要知道应该将请求发送给哪个控制器。所以DispatcherServlet会查询一个或多个处理器映射 handler mapping 来确定请求的下一站在哪里。处理器映射会根据请求所携带的 URL 信息来进行决策。 一旦选择了合适的控制器DispatcherServlet会将请求发送给选中的控制器。到了控制器请求会卸下其负载用户提交的信息并耐心等待控制器处理这些信息。实际上设计良好的控制器本身只处理很少甚至不处理工作而是将业务逻辑委托给一个或多个服务对象进行处理。 控制器在完成逻辑处理后通常会产生一些信息这些信息需要返回给用户并在浏览器上显示。这些信息被称为模型 model。不过仅仅给用户返回原始的信息是不够的——这些信息需要以用户友好的方式进行格式化一般会是 HTML。所以信息需要发送给一个视图 view 通常会是 JSP。 控制器所做的最后一件事就是将模型数据打包并且标示出用于渲染输出的视图名。它接下来会将请求连同模型和视图名发送回DispatcherServlet。 这样控制器就不会与特定的视图相耦合传递给DispatcherServlet的视图名并不直接表示某个特定的 JSP。实际上它甚至并不能确定视图就是JSP。相反它仅仅传递了一个逻辑名称，这个名字将会用来查找产生结果的真正视图。DispatcherServlet将会使用视图解析器 view resolver 来将逻辑视图名匹配为一个特定的视图实现，它可能是也可能不是 JSP。 既然DispatcherServlet已经知道由哪个视图渲染结果，那请求的任务基本上也就完成了。它的最后一站是视图的实现，可能是 JSP，在这里它交付模型数据。请求的任务就完成了。视图将使用模型数据渲染输出，这个输出会通过响应对象传递给客户端。 DispatcherServlet 此 Servlet 为 Spring MVC 的核心类。 按照传统的方式像DispatcherServlet这样的 Servlet 会配置在 web.xml 文件中这个文件会放到应用的 WAR 包里面。当然这是配置DispatcherServlet的方法之一。但是借助于 Servlet 3 规范和 Spring 3.1 的功能增强可以使用 Java 将DispatcherServlet配置在 Servlet 容器中而不会再使用 web.xml 文件。 12345678910111213141516171819// 配置 DispatcherServletpackage spittr.config;import org.springframework.web.servlet.support.AbstractAnnotationConfigDispatcherServletInitializer;public class SpittrWebAppInitializer extends AbstractAnnotationConfigDispactcherServletInitializer &#123; @Override protected String[] getServletMappings() &#123; // 将 DispatcherServlet 映射到 “/” return new String[] &#123;&quot;/&quot;&#125;; &#125; // 定义拦截器 ContextLoaderListener 应用上下文的 beans @Override protected class&lt;?&gt;[] getRootConfigClasses() &#123; return new class&lt;?&gt;[] &#123;RootConfig.class&#125;; &#125; // 定义 DispatcherServlet 应用上下文的 beans @Override protected class&lt;?&gt;[] getServletConfigClasses() &#123; // 指定配置类 return new class&lt;?&gt;[] &#123;WebConfig.class&#125;; &#125;&#125; 扩展AbstractAnnotation-ConfigDispatcherServletInitializer的任意类都会自动地配置Dispatcher-Servlet和 Spring 应用上下文，Spring 的应用上下文会位于应用程序的 Servlet 上下文之中。 最小但可用的 Spring MVC 配置。 12345678910111213141516171819@Configuration@EnableWebMvc // 启用 Spring MVC@ComponentScan(&quot;spittr.web&quot;) // 开启组件扫描 @Controllerpublic class WebConfig extends WebMvcConfigurerAdapter &#123; @Bean public ViewResolver viewResolver() &#123; // 配置 JSP 视图解析器 InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(&quot;/WEB-INF/views/&quot;); resolver.setSuffix(&quot;.jsp&quot;); resolver.setExposeContextBeansAsAttributes(true); return resolver; &#125; @Override public void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer) &#123; configurer.enable(); // 配置静态资源的处理 &#125;&#125; 12345678@Configuration@ComponentScan(basePackages=&#123;&quot;spitter&quot;&#125;, excludeFilter=&#123; @Filter&#123;type=FilterType.ANNOTATION, value=EnbaleWebMVC.class&#125; &#125;)public class RootConfig &#123;&#125; 添加其它组件 除了DispatcherServlet以外，项目可能还需要额外的 Servlet 和 Filter ，并且可能还需要对DispatcherServlet本身做一些额外的配置。 在AbstractAnnotation-ConfigDispatcherServletInitializer将DispatcherServlet注册到Servlet容器中之后，就会调用customizeRegistration()并将Servlet注册后得到的Registration.Dynamic传递进来。通过重载customizeRegistration()方法我们可以对DispatcherServlet进行额外的配置。 如果我们想往 Web 容器中注册其他组件的话，只需创建一个新的初始化器就可以了。最简单的方式就是实现 Spring 的WebApplicationInitializer接口。 12345678910111213public class MyServletInitializer extends WebApplicationInitializer &#123; @Override public void onStartup(ServletContext servletContext) throws ServletException &#123; // 添加自定义 servlet Dynamic myServlet = servlectContext.addServlet(&quot;myServlet&quot;, MyServlet.class); myServlect.addMapping(&quot;/custom/**&quot;); // 添加自定义 Filter javax.servlet.FilterRegistration.Dynamic filter = servletContext.addFilter(&quot;myFilter&quot;, MyFilter.class); filter.addMappingForUrlPatterns(null, false, &quot;/custom/**&quot;); &#125;&#125; 源码片段： 1234// 寻找任何继承了 WebApplicationInitializer 接口的类并用其来配置 servlet 容器for (WebApplicationInitializer initializer : initializers) &#123; initializer.onStartup(servletContext);&#125; 编写基本控制器 @Controller 注解辅助实现组件扫描，与@Component注解所实现的效果是一样的，但是在表意性上可能会差一些。 @RequestMapping(value=&quot;/&quot;, method=GET) 注解绑定请求：value属性指定了这个方法所要处理的请求路径，method属性细化了它所处理的 HTTP 方法。 控制器方法的 Model 参数实际上就是一个 Map，也就是 key-value 对的集合，它会传递给视图，这样数据就能渲染到客户端了。 12(Model model)(Map mode) 传参 Spring MVC 允许以多种方式将客户端中的数据传送到控制器的处理器方法中。 查询参数 Query Parameter； 表单参数 Form Parameter； 路径变量 Path Variable。 @RequrearParam(&quot;ParamName&quot;) [URL]?ParamName=xxx 注解用于参数。 @RequestMapping(value=&quot;/&#123;ParamName&#125;&quot;, method=GET) @PathVariable&#123;&quot;ParamName&quot;&#125; [URL]/xxx 表单 POST 请求的控制器方法参数可直接使用对象，Spring MVC 将会使用请求中同名的参数进行填充。 校验 从 Spring 3.0 开始在 Spring MVC 中提供了对 Java 校验 API 的支持。 Java 校验 API 所提供的校验注解。 注 解 描 述 @AssertFalse 所注解的元素必须是Boolean类型并且值为 false @AssertTrue 所注解的元素必须是Boolean类型并且值为 true @DecimalMax 所注解的元素必须是数字并且它的值要小于或等于给定的 BigDecimalString值 @DecimalMin 所注解的元素必须是数字并且它的值要大于或等于给定的 BigDecimalString值 @Digits 所注解的元素必须是数字并且它的值必须有指定的位数 @Future 所注解的元素的值必须是一个将来的日期 @Max 所注解的元素必须是数字并且它的值要小于或等于给定的值 @Min 所注解的元素必须是数字并且它的值要大于或等于给定的值 @NotNull 所注解元素的值必须不能为 null @Null 所注解元素的值必须为 null @Past 所注解的元素的值必须是一个已过去的日期 @Pattern 所注解的元素的值必须匹配给定的正则表达式 @Size 所注解的元素的值必须是 String、集合或数组并且它的长度要符合给定的范围 @valid 注解用于参数，添加了@Valid注解这会告知Spring需要确保这个对象满足校验限制。 属性上添加校验限制并不能阻止表单提交，如果有校验出现错误的话那么这些错误可以通过Errors对象进行访问，Errors对象可作为控制器方法的一个参数。首次调用Errors.hasErrors()来检查是否有错误，之后再进行业务逻辑处理。 渲染 Web 视图 将模型数据渲染为 HTML 使用 JSP 视图 通过 tiles 定义视图布局 使用 Thymeleaf 视图 Spring 自带了 13 个视图解析器能够将逻辑视图名转换为物理实现。 视图解析器 描 述 BeanNameViewResolver 将视图解析为 Spring 应用上下文中的 bean 其中 bean 的 ID 与视图的名字相同 ContentNegotiatingViewResolver 通过考虑客户端需要的内容类型来解析视图委托给另外一个能够产生对应内容类 型的视图解析器 FreeMarkerViewResolver 将视图解析为 FreeMarker 模板 InternalResourceViewResolver 将视图解析为 Web 应用的内部资源一般为 JSP JasperReportsViewResolver 将视图解析为 JasperReports 定义 ResourceBundleViewResolver 将视图解析为资源 bundle 一般为属性文件 TilesViewResolver 将视图解析为 Apache Tile 定义其中 tile ID 与视图名称相同。注意有两个不同的 TilesViewResolver实现分别对应于 Tiles 2.0 和 Tiles 3.0 UrlBasedViewResolver 直接根据视图的名称解析视图视图的名称会匹配一个物理视图的定义 VelocityLayoutViewResolver 将视图解析为 Velocity 布局从不同的 Velocity 模板中组合页面 VelocityViewResolver 将视图解析为 Velocity 模板 XmlViewResolver 将视图解析为特定 XML 文件中的 bean 定义。类似于BeanName-ViewResolver XsltViewResolver 将视图解析为 XSLT 转换后的结果 配置适用于 JSP 的视图解析器InternalResourceViewResolver。它遵循一种约定会在视图名上添加前缀和后缀进而确定一个 Web 应用中视图资源的物理路径。 12345678@Beanpublic ViewResolver viewResolver() &#123; // 配置 JSP 视图解析器 InternalResourceViewResolver resolver = new InternalResourceViewResolver(); resolver.setPrefix(&quot;/WEB-INF/views/&quot;); resolver.setSuffix(&quot;.jsp&quot;); resolver.serViewClass(InternalResourceViewResolver) // 解析 JSTL 视图 return resolver;&#125; 1234&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot; p:prefix=&quot;/WEB-INF/views&quot; p:suffix=&quot;.jsp&quot; p:viewClass=&quot;InternalResourceViewResolver&quot; /&gt; 解析 JSTL 视图，使用 Spring 的 JSP 库。 借助 Spring 表单绑定标签库中所包含的标签我们能够将模型对象绑定到渲染后的 HTML 表单中。 12&lt;!-- 为了使用表单绑定库需要在JSP页面中对其进行声明 --&gt;&lt;%@taglib prefix=&quot;sf&quot; uri=&quot;http://www.springframework.org/tags/form&quot; %&gt; JSP标签 描 述 &lt;sf:checkbox&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为checkbox &lt;sf:checkboxes&gt; 渲染成多个 HTML &lt;input&gt;标签其中type属性设置为checkbox &lt;sf:errors&gt; 在一个 HTML &lt;span&gt;中渲染输入域的错误 &lt;sf:form&gt; 渲染成一个 HTML &lt;form&gt;标签并为其内部标签暴露绑定路径用于数据绑定 &lt;sf:hidden&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为hidden &lt;sf:input&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为text &lt;sf:label&gt; 渲染成一个 HTML &lt;label&gt;标签 &lt;sf:option&gt; 渲染成一个 HTML &lt;option&gt;标签其selected属性根据所绑定的值进行设置 &lt;sf:options&gt; 按照绑定的集合、数组或 Map 渲染成一个HTML &lt;option&gt;标签的列表 &lt;sf:password&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为password &lt;sf:radiobutton&gt; 渲染成一个 HTML &lt;input&gt;标签其中type属性设置为radio &lt;sf:radiobuttons&gt; 渲染成多个 HTML &lt;input&gt;标签其中type属性设置为radio &lt;sf:select&gt; 渲染为一个 HTML &lt;select&gt;标签 &lt;sf:textarea&gt; 渲染为一个 HTML &lt;textarea&gt;标签 ThymeLeaf Thymeleaf 模板是原生的,不依赖于标签库。它能在接受原始 HTML 的地方进行编辑和渲染。因为它没有与 Servlet 规范耦合，因此 Thymeleaf 模板能够进入 JSP 所无法涉足的领域。 ThymeleafViewResolver将逻辑视图名称解析为 Thymeleaf 模板视图； SpringTemplateEngine处理模板并渲染结果； TemplateResolver加载 Thymeleaf 模板。 12345678910111213141516171819202122232425// thymeleaf 视图解析器@Beanpublic ViewResolver viewResolver(TemplateEngine templateEngine)&#123; ThymeleafViewResolver viewResolver = new ThymeleafViewResolver(); viewResolver.setTemplateEngine(templateEngine); return viewResolver;&#125;// 模板引擎@Beanpublic TemplateEngine templateEngine(TemplateResolver templateResolver)&#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); return templateEngine;&#125;// 模板解析器@Beanpublic ITemplateResolver templateResolver()&#123; SpringResourceTemplateResolver templateResolver = new SpringResourceTemplateResolver(); templateResolver.setPrefix(&quot;/WEB-INF/templates/&quot;); templateResolver.setSuffix(&quot;.html&quot;); templateResolver.setTemplateMode(&quot;HTML5&quot;) return templateResolver;&#125; Thymeleaf 教程 文件上传 一般表单提交所形成的请求结果是很简单的，就是以&amp;符分割的多个 name-value 对。 尽管这种编码形式很简单，并且对于典型的基于文本的表单提交也足够满足要求，但是对于传送二进制数据，如上传图片，就显得力不从心了。与之不同的是，multipart 格式的数据会将一个表单拆分为多个部分（part），每个部分对应一个输入域。在一般的表单输入域中，它所对应的部分中会放置文本型数据，但是如果上传文件的话，它所对应的部分可以是二进制。 在编写控制器方法处理文件上传之前我们必须要配置一个 multipart 解析器通过它来告诉DispatcherServlet该如何读取 multipart 请求。 1234@Beanpublic MultipartResolver multipartResolver() throws IOException &#123; return new StandardServletMultipartResolver();&#125; 在 Servlet registration 上调用setMultipartConfig()方法传入一个MultipartConfig-Element实例。 12345DispatcherServlet ds = new DispatchServlet();Dynamic registration = context.addServlet(&quot;appServlet&quot;, ds);registration.addMapping(&quot;/&quot;);// 将临时路径设置为 /tmp/spittr/uploadsregistration.setMultipartConfig(new MultipartConfigElement(&quot;/tmp/spittr/uploads&quot;)); 如果我们配置DispatcherServlet的Servlet初始化类继承了Abstract AnnotationConfigDispatcherServletInitializer或AbstractDispatcher-ServletInitializer的话那么我们不会直接创建DispatcherServlet实例并将其注册到 Servlet 上下文中。这样的话将不会有对Dynamic Servlet registration 的引用供我们使用了。但是我们可以通过重载customizeRegistration()方法它会得到一个Dynamic作为参数来配置 multipart 的具体细节。 123456@Overrideprotected void customizeRegistration(Dynamic registration) &#123; registration.setMultipartConfig( new MultipartConfigElement(&quot;/tmp/spittr/uploads&quot;, 2097152, 4194304, 0); );&#125; 123456789101112&lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatchServlet &lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;multipart-config&gt; &lt;location&gt;/tmp/spittr/upload&lt;/location&gt; &lt;max-file-size&gt;2097152&lt;/max-file-size&gt; &lt;max-request-size&gt;4194304&lt;/max-request-size&gt; &lt;/multipart-config&gt;&lt;/servlet&gt; 对于非 Servlet 3.0 环境，Spring 内置了 CommonsMultipartResolver，可以作为 StandardServletMultipartResolver 的替代方案。 @RequestPart 注解指定用于接收请求中对应 part 的数组。 1234&lt;form method=&quot;POST&quot; th:object=&quot;$&#123;spitter&#125;&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;label&gt;Profile Picture&lt;/label&gt;: &lt;input type=&quot;file&quot; name=&quot;profilePicture&quot; accept=&quot;image/jpeg,image/png,image/gif&quot; /&gt;&lt;br/&gt;&lt;/form&gt; 1234567@RequestMapping(value=&quot;/register&quot;, method=POST)public String processRegistration( @RequestPart(&quot;profilePicture&quot;) byte[] profilePicture, @Valid Spittr spittr, Errors errors) &#123; // 将文件保存到某个位置&#125; 使用上传文件的原始byte比较简单但是功能有限。因此 Spring 还提供了MultipartFile接口它为处理 multipart 数据提供了内容更为丰富的对象。 12345678910111213141516package org.springframework.web.multipart;import java.io.File;import java.io.IOException;import java.io.InputStream;public interface MultipartFile &#123; String getName(); String getOriginalFilename(); String getContentType(); boolean isEmpty(); long getSize(); byte[] getBytes() throws IOException; InputStream getInputStream() throws IOException; void transferTo(File dest) throws IOException;&#125; 12// 将上传的文件写入到文件系统中的便捷方法profilePicture.tranferTo(new File(&quot;/data/spittr/&quot; + profilePicture.getOriginalFilename())); 如将应用部署到 Servlet 3.0 的容器中，那么会有MultipartFile的一个替代方案。Spring MVC 也能接受javax.servlet.http.Part作为控制器方法的参数。 Part 接口与MultipartFile并没有太大的差别，Part 接口中的一些方法其实是与MultipartFile相对应的。 异常处理 特定的 Spring 异常将会自动映射为指定的 HTTP 状态码； 异常上可以添加@ResponseStatus注解从而将其映射为某一个 HTTP 状态码； 在方法上可以添加@ExceptionHandler注解使其用来处理异常。 Spring 的一些异常会默认映射为 HTTP 状态码。 Spring异常 HTTP状态码 BindException 400 - Bad Request ConversionNotSupportedException 500 - Internal Server Error HttpMediaTypeNotAcceptableException 406 - Not Acceptable HttpMediaTypeNotSupportedException 415 - Unsupported Media Type HttpMessageNotReadableException 400 - Bad Request HttpMessageNotWritableException 500 - Internal Server Error HttpRequestMethodNotSupportedException 405 - Method Not Allowed MethodArgumentNotValidException 400 - Bad Request MissingServletRequestParameterException 400 - Bad Request MissingServletRequestPartException 400 - Bad Request NoSuchRequestHandlingMethodException 404 - Not Found TypeMismatchException 400 - Bad Request 1234567891011@RequestMapping(value=&quot;/&#123;spittleId&#125;&quot;, method=RequestMethod.GET)public String spittle( @PathVariable(&quot;spittleId&quot;) long spittleId, Model model) &#123; Spittle spittle = spittleRepository.findOne(spittleId); if (spittle == null) &#123; // 若未找到此对象则抛出异常 throw new SpittleNotFoundException(); &#125; model.addAttribute(spittle); return &quot;spittle&quot;;&#125; 12345678package spittr.web;import org.springframework.http.HttpStatus;import org.springframework.web.bind.annotation.ResponseStatus;@ResponseStatus(value=HttpStatus.NOT_FOUND, reason=&quot;Spittle Not Found&quot;)public class SpittleNotFoundException extends RuntimeException &#123;&#125; 在很多的场景下，将异常映射为状态码是很简单的方案，并且就功能来说也足够了。但是如果想在响应中不仅要包括状态码，还要包含所产生的错误，那该怎么办呢？此时的话，我们就不能将异常视为 HTTP 错误了，而是要按照处理请求的方式来处理异常了。 123456789101112// 在处理请求的方法中直接处理异常@RequestMapping(method=RequestMethod.POST)public String saveSpittle(SpittleForm form, Model model) &#123; try &#123; spittleRepository.save( new Spittle(null, form.getMessage(), new Date(), form.getLongitude(), form.getLatitude()) ); return &quot;redirect:/spittles&quot;; &#125; catch (DuplicateSpittleException e) &#123; return &quot;error/duplicate&quot;; &#125;&#125; 它运行起来没什么问题，但是这个方法有些复杂。该方法可以有两个路径，每个路径会有不同的输出。如果能让saveSpittle()方法只关注正确的路径，而让其他方法处理异常的话，那么它就能简单一些。 123456789101112@RequestMapping(method=RequestMethod.POST)public String saveSpittle(SpittleForm form, Model model) &#123; spittleRepository.save( new Spittle(null, form.getMessage(), new Date(), form.getLongitude(), form.getLatitude()) ); return &quot;redirect:/spittles&quot;;&#125;@ExceptionHandler(DuplicateSpittleException.class)public String handleDuplicateSpittle() &#123; return &quot;error/duplicate&quot;;&#125; 如果要在多个控制器中处理异常那@ExceptionHandler注解所标注的方法是很有用的。不过如果多个控制器类中都会抛出某个特定的异常那么你可能会发现要在所有的控制器方法中重复相同的@ExceptionHandler方法。或者为了避免重复我们会创建一个基础的控制器类所有控制器类要扩展这个类从而继承通用的@ExceptionHandler方法。 Spring 3.2 为这类问题引入了一个新的解决方案控制器通知。控制器通知 controller advice 是任意带有@ControllerAdvice注解的类这个类会包含一个或多个如下类型的方法 @ExceptionHandler注解标注的方法； @InitBinder注解标注的方法； @ModelAttribute注解标注的方法。 1234567@ControllerAdvicepublic class AppWideExceptionHandler &#123; @ExceptionHandler(DuplicateSpittleException.class) public String handleNotFound() &#123; return &quot;error/duplicate&quot;; &#125;&#125; 重定向 在处理完 POST 请求后通常来讲一个最佳实践就是执行一下重定向。除了其他的一些因素外，这样做能够防止用户点击浏览器的刷新按钮或后退箭头时客户端重新执行危险的 POST 请求。 当控制器方法返回的String值以“redirect:”开头的话那么这个String不是用来查找视图的而是用来指导浏览器进行重定向的路径。 1return &quot;redirect:/spitter/&quot; + spitter.getUsername(); 模型的属性是以请求属性的形式存放在请求中的在重定向后无法存活。 使用 URL 模板以路径变量和 / 或查询参数的形式传递数据； 通过 flash 属性发送数据。 123456@RequestMapping(value=&quot;/register&quot;, method=POST);public String processRegistration(Spitter spitter, Model model) &#123; spitterRepository.save(spitter); model.addAttribute(&quot;username&quot;, spitter.getUsername()); return &quot;redirect:/spitter/&#123;username&#125;&quot;;&#125; 除此之外，模型中所有其他的原始类型值都可以添加到 URL 中作为查询参数。 1234567@RequestMapping(value=&quot;/register&quot;, method=POST)public String processRegistration(Spitter spitter, Model model) &#123; spitterRepository.save(spitter); model.addAttribute(&quot;username&quot;, spitter.getUsername()); model.addAttribute(&quot;spitterId&quot;, spitter.getId()); // 会以查询参数的形式进行重定向 return &quot;redirect:/spitter/&#123;username&#125;&quot;;&#125; 如果 username 属性的值是 habuma 并且 spitterId 属性的值是 42，那么结果得到的重定向 URL 路径将会是 /spitter/habuma?spitterId=42。 在 URL 中，并没有办法发送更为复杂的值，但这正是flash属性能够提供帮助的领域。 Spring 提供了通过RedirectAttributes设置 flash 属性的方法，这是 Spring 3.1 引入的Model的一个子接口。RedirectAttributes提供了Model的所有功能除此之外还有几个方法是用来设置 flash 属性的。 1234567@RequestMapping(value=&quot;/register&quot;, method=POST)public String processRegistration(Spitter spitter, RedirectAttribute model) &#123; spitterRespository.save(spitter); model.addAttribute(&quot;username&quot;, spitter.getUsername()); model.addFlashAttribute(&quot;spitter&quot;, spitter); // flash 属性 return &quot;redirect:/spitter/&#123;username&#125;&quot;;&#125; 安全 Spring Security 介绍； 使用 Servlet 规范中的 Filter 保护Web应用； 基于数据库和 LDAP 进行认证。 Spring Security 是为基于 Spring 的应用程序提供声明式安全保护的安全性框架。Spring Security 提供了完整的安全性解决方案，能够在 Web 请求级别和方法调用级别处理身份认证和授权。因为基于 Spring 框架所以 Spring Security 充分利用了依赖注入 dependency injectionDI 和面向切面的技术。 Spring Security 从两个角度来解决安全性问题。它使用 Servlet 规范中的 Filter 保护 Web 请求并限制 URL 级别的访问。Spring Security 还能够使用 Spring AOP 保护方法调用——借助于对象代理和使用通知能够确保只有具备适当权限的用户才能访问安全保护的方法。 Spring Security 被分成了11个模块。 模 块 描 述 ACL 支持通过访问控制列表 access control list 为域对象提供安全性 Aspects 当使用 Spring Security 注解时，会使用基于 AspectJ 的切面，而不是使用标准的 Spring AOP CAS Client 提供与 Jasig 的中心认证服务 Central Authentication Service 进行集成的功能 Configuration 包含通过 XML 和 Java 配置 Spring Security 的功能支持 Core 提供 Spring Security 基本库 Cryptography 提供了加密和密码编码的功能 LDAP 支持基于 LDAP 进行认证 OpenID 支持使用 OpenID 进行集中式认证 Remoting 提供了对 Spring Remoting 的支持 Tag Library Spring Security 的 JSP 标签库 Web 提供了 Spring Security 基于 Filter 的 Web 安全性支持 Spring Security 借助一系列 Servlet Filter 来提供各种安全性功能。这是否意味着我们需要在 web.xml 或WebApplicationInitializer中配置多个 Filter 呢？实际上借助于 Spring 的小技巧我们只需配置一个 Filter 就可以了。 DelegatingFilterProxy是一个特殊的 Servlet Filter 它本身所做的工作并不多。只是将工作委托给一个javax.servlet.Filter实现类这个实现类作为一个&lt;bean&gt;注册在 Spring 应用的上下文。 不管我们通过 web.xml 还是通过AbstractSecurityWebApplicationInitializer的子类来配置DelegatingFilterProxy它都会拦截发往应用中的请求并将请求委托给ID为springSecurityFilterChain bean。 springSecurityFilterChain本身是另一个特殊的 Filter，它也被称为FilterChainProxy。它可以链接任意一个或多个其他的 Filter。Spring Security 依赖一系列 Servlet Filter 来提供不同的安全特性。不需要知道这些细节因为开发中不需要显式声明springSecurityFilterChain以及它所链接在一起的其他 Filter。当我们启用 Web 安全性的时候会自动创建这些Filter。 简单的安全性配置 方 法 描 述 configure(WebSecurity) 通过重载配置 Spring Security 的 Filter 链 configure(HttpSecurity) 通过重载配置如何通过拦截器保护请求 configure(AuthenticationManagerBuilder) 通过重载配置 user-detail 服务 @EnableWebSecurity注解将会启用 Web 安全功能。但它本身并没有什么用处 Spring Security 必须配置在一个实现了WebSecurityConfigurer的 bean 中或者简单起见扩展WebSecurityConfigurerAdapter。 @EnableWebSecurity可以启用任意 Web 应用的安全性功能，不过如果是使用 Spring MVC 开发的那么就应该考虑使用@EnableWebMvcSecurity`替代它。 12345678910111213@Configuration@EnableWebMvcSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .inMemoryAuthentication() .withUser(&quot;user&quot;).password(&quot;password&quot;).roles(&quot;USER&quot;) .and .withUser(&quot;user&quot;).password(&quot;password&quot;).authorities(&quot;ROLE_USER&quot;); // 等价 &#125;&#125; AuthenticationManagerBuilder有多个方法用来配置 Spring Security 对认证的支持。通过inMemoryAuthentication()方法我们可以启用、配置并任意填充基于内存的用户存储。 withUser()方法返回的是UserDetailsManagerConfigurer.UserDetailsBuilder，这个对象提供了多个进一步配置用户的方法包括设置用户密码的password()方法以及为给定用户授予一个或多个角色权限的roles()方法。 配置用户详细信息的方法。 方 法 描 述 accountExpired(boolean) 定义账号是否已经过期 accountLocked(boolean) 定义账号是否已经锁定 and() 用来连接配置 authorities(GrantedAuthority...) 授予某个用户一项或多项权限 authorities(List) 授予某个用户一项或多项权限 authorities(String...) 授予某个用户一项或多项权限 credentialsExpired(boolean) 定义凭证是否已经过期 disabled(boolean) 定义账号是否已被禁用 password(String) 定义用户的密码 roles(String...) 授予某个用户一项或多项角色 基于数据库表认证 用户数据通常会存储在关系型数据库中并通过 JDBC 进行访问。为了配置 Spring Security 使用以 JDBC 为支撑的用户存储，可以使用jdbcAuthentication()方法。 123456789101112@Overrideprotected void configure(AuthenticationManagerBuilder auth) throws Exception &#123; auth .jdbcAuthentication() .dataSource(dataSource) .usersByUsernameQuery( &quot;select username, password, true &quot; + &quot;from Spitter where username=?&quot;) .authoritiesByUsernameQuery( &quot;select username, &#x27;ROLE_USER&#x27; from Spitter where username=?&quot;) .passwordEncoder(new StandardPasswordEnconder(&quot;123456&quot;));&#125; passwordEncoder()方法可以接受 Spring Security 中PasswordEncoder接口的任意实现。Spring Security 的加密模块包括了三个这样的实现BCryptPasswordEncoder、NoOpPasswordEncoder和StandardPasswordEncoder。 基于 LDAP认证 LDAP（Light Directory Access Portocol），它是基于 X.500 标准的轻量级目录访问协议。 拦截请求 适量地应用安全性。 在任何应用中并不是所有的请求都需要同等程度地保护。有些请求需要认证而另一些可能并不需要。有些请求可能只有具备特定权限的用户才能访问没有这些权限的用户会无法访问。 对每个请求进行细粒度安全性控制的关键在于重载configure(HttpSecurity)方法。 123456789@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(&quot;/spitters/me&quot;).hasAuthority(&quot;ROLE_SPITTER&quot;) // 路径可使用 Ant 风格的通配符 .antMatchers(HttpMethoed.POST, &quot;/spitters&quot;).hasAuthority(&quot;ROLE_SPITTER&quot;) // .antMatchers(HttpMethoed.POST, &quot;/spitters&quot;).hasRole(&quot;SPITTER&quot;) 同上 .anyRequest().permitAll();&#125; 用来定义如何保护路径的配置方法。 方 法 能够做什么 access(String) 如果给定的SpEL表达式计算结果为true就允许访问 anonymous() 允许匿名用户访问 authenticated() 允许认证过的用户访问 denyAll() 无条件拒绝所有访问 fullyAuthenticated() 如果用户是完整认证的话不是通过 Remember-me 功能认证的就允许访问 hasAnyAuthority(String...) 如果用户具备给定权限中的某一个的话就允许访问 hasAnyRole(String...) 如果用户具备给定角色中的某一个的话就允许访问 hasAuthority(String) 如果用户具备给定权限的话就允许访问 hasIpAddress(String) 如果请求来自给定 IP 地址的话就允许访问 hasRole(String) 如果用户具备给定角色的话就允许访问 not() 对其他访问方法的结果求反 permitAll() 无条件允许访问 rememberMe() 如果用户是通过Remember-me功能认证的就允许访问 上表中的大多数方法都是一维的，例如使用hasRole()限制某个特定的角色的同时不能在相同的路径上同时通过hasIpAddress()限制特定的 IP 地址。 借助access()方法我们也可以将 SpEL 作为声明访问限制的一种方式。 安全表达式 计 算 结 果 authentication 用户的认证对象 denyAll 结果始终为 false hasAnyRole(list of roles) 如果用户被授予了列表中任意的指定角色结果为 true hasRole(role) 如果用户被授予了指定的角色结果为 true hasIpAddress(IP Address) 如果请求来自指定IP的话结果为 true isAnonymous() 如果当前用户为匿名用户结果为 true isAuthenticated() 如果当前用户进行了认证的话结果为 true isFullyAuthenticated() 如果当前用户进行了完整认证的话，即非 Remember-me 功能进行的认证结果为 true isRememberMe() 如果当前用户是通过 Remember-me 自动认证的结果为 true permitAll 结果始终为 true principal 用户的 principal 对象 12.antMatchers(&quot;/spitter/me&quot;).access(&quot;hasRole(&#x27;ROLE_SPITTER&#x27;) and hasIpAddress(&#x27;192.168.1.2&#x27;)&quot;); // 二维认证 强制通道的安全性 对于敏感的信息，为了保证注册表单的数据通过 HTTPS 传送，可以在配置中添加requiresChannel()方法。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .authorizeRequests() .antMatchers(&quot;/spitter/me&quot;).hasRole(&quot;SPITTER&quot;) .antMatchers(HttpMethod.POST, &quot;/spittles&quot;).hasRole(&quot;SPITTER&quot;) .anyRequest().permitAll() .and() .requeresChannel() .antMatchers(&quot;/spitter/form&quot;).requiresSecure(); // 需要 HTTPS&#125; 不论何时只要是对 “/spitter/form” 的请求，Spring Security 都视为需要安全通道通过调用requiresChannel()确定的并自动将请求重定向到 HTTPS 上。 与之相反有些页面并不需要通过 HTTPS 传送，可以使用requiresInsecure()代替requiresSecure()方法将首页声明为始终通过 HTTP 传送。 防止跨站请求伪造 如果一个站点欺骗用户提交请求到其他服务器的话就会发生 cross-site request forgery CSRF 攻击。 Spring Security 通过一个同步 token 的方式来实现 CSRF 防护的功能。它将会拦截状态变化的请求，例如非GET、HEAD、OPTIONS和TRACE的请求并检查 CSRF token。如果请求中不包含 CSRF token 的话或者 token 不能与服务器端的 token 相匹配请求将会失败并抛出CsrfException异常。 这意味着在你的应用中所有的表单必须在一个 “_csrf” 域中提交 token 而且这个 token 必须要与服务器端计算并存储的 token 一致这样的话当表单提交的时候才能进行匹配。 12345&lt;!-- Thymeleaf 只要 &lt;form&gt; 标签的 action 属性添加了 Thymeleaf 命名空间前缀则会自动生成 “_csrf” 隐藏域 --&gt;&lt;form method=&quot;POST&quot; th:action=&quot;@&#123;/spittles&#125;&quot;&gt;&lt;/form&gt;&lt;!-- JSP --&gt;&lt;input type=&quot;hidden&quot; name=&quot;$&#123;_csrf.parameterName&#125;&quot; value=&quot;$&#123;_csrf.token&#125;&quot; /&gt; 12345678// 手动禁用，不推荐@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http // ... .csrf() .disable();&#125; 认证用户页面 实际上在重写configure(HttpSecurity)之前我们都能使用一个简单却功能完备的登录页。但是一旦重写了configure(HttpSecurity)方法就失去了这个简单的登录页面。 12345678910111213@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() // 指定自定义的登录页面的访问路径 .and() .authorizeRequests() .antMatchers(&quot;/spitter/me&quot;).hasRole(&quot;SPITTER&quot;) .antMatchers(HttpMethod.POST, &quot;/spittles&quot;).hasRole(&quot;SPITTER&quot;) .anyRequest().permitAll() .and() .requeresChannel() .antMatchers(&quot;/spitter/form&quot;).requiresSecure();&#125; HTTP Basic 认证（HTTP Basic Authentication）会直接通过 HTTP 请求本身，对要访问应用程序的用户进行认证。你可能在以前见过 HTTP Basic 认证。当在 Web 浏览器中使用时，它将向用户弹出一个简单的模态对话框。 但这只是 Web 浏览器的显示方式。本质上，这是一个 HTTP 401 响应， 表明必须要在请求中包含一个用户名和密码。在 REST 客户端向它使用的服务进行认证的场景中，这种方式比较适合。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(&quot;/login&quot;) .and() .httpBasic() .realmName(&quot;Spittr&quot;) .and() // ...&#125; Spring Security 使得为应用添加 Remember-me 功能变得非常容易。为了启用这项功能只需在configure()方法所传入的HttpSecurity对象上调用rememberMe()即可。这个功能是通过在 cookie 中存储一个 token 完成的。 存储在 cookie 中的 token 包含用户名、密码、过期时间和一个私钥 —— 在写入 cookie 前都进行了 MD5 哈希。默认情况下，私钥的名为 SpringSecured，但在这里我们将其设置为 spitterKey，使它专门用于 Spittr应用。 1234567891011@Overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(&quot;/login&quot;) .and() .rememberMe() .tokenValiditySeconds(2419200) // token 四周内有效 .key(&quot;spittrKey&quot;) // ...&#125; 退出功能是通过 Servlet 容器中的 Filter 实现的（默认情况下），这个 Filter 会拦截针对 “/logout” 的请求。 1&lt;a th:href=&quot;@&#123;/logout&#125;&quot;&gt;Logout&lt;/a&gt; 当用户点击这个链接的时候，会发起对 “/logout” 的请求，这个请求会被 Spring Security 的 LogoutFilter 所处理。用户会退出应用，所有的 Remember-me token 都会被清除掉。在退出完成后，用户浏览器将会 重定向到 “/login?logout”，从而允许用户进行再次登录。 如果你希望用户被重定向到其他的页面，如应用的首页，那么可以在 configure() 中进行配置。 1234567891011@overrideprotected void configure(HttpSecurity http) throws Exception &#123; http .formLogin() .loginPage(&quot;/login&quot;) .and() .logout() .logoutSuccessUrl(&quot;/&quot;) // 指定退出成功后的跳转页面 .logout(&quot;signout&quot;) // 重写的默认的 LogoutFilter 拦截路径 // ...&#125; 保护视图 使用 Spring Security 的 JSP 标签库。 JSP 标签 作 用 &lt;security:accesscontrollist&gt; 如果用户通过访问控制列表授予了指定的权限那么渲染该标签体中的内容 &lt;security:authentication&gt; 渲染当前用户认证对象的详细信息 &lt;security:authorize&gt; 如果用户被授予了特定的权限或者 SpEL 表达式的计算结果为 true 那么渲染该标签 体中的内容 使用&lt;security:authentication&gt; JSP 标签来访问用户的认证详情。 认 证 属 性 描 述 authorities 一组用于表示用户所授予权限的 GrantedAuthority 对象 Credentials 用于核实用户的凭证通常这会是用户的密码 details 认证的附加信息IP地址、证件序列号、会话 ID 等 principal 用户的基本信息对象 1234567891011121314&lt;sec:authorize access=&quot;hasRole(&#x27;ROLE_SPITTER&#x27;)&quot;&gt; &lt;s:url value=&quot;/spittles&quot; var=&quot;spittle_url&quot; /&gt; &lt;sf:form modelAttribute=&quot;spittle&quot; action=&quot;$&#123;spittle_url&#125;&quot;&gt; &lt;sf:label path=&quot;text&quot;&gt; &lt;s:message code=&quot;label.spittle&quot; text=&quot;Enter spittle:&quot; /&gt; &lt;/sf:label&gt; &lt;sf:textarea path=&quot;text&quot; row=&quot;2&quot; cols=&quot;40&quot; /&gt; &lt;sf:errors path=&quot;text&quot; /&gt; &lt;br/&gt; &lt;div class=&quot;spitItSubmit&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;Spit it !&quot; class=&quot;status-btn round-btn disabled&quot; /&gt; &lt;/div&gt; &lt;/sf:form&gt;&lt;/sec:authorize&gt; Thymeleaf 的安全方言提供了条件化渲染和显示认证细节的能力。 属 性 作 用 sec:authentication 渲染认证对象的属性 sec:authorize 基于表达式的计算结果条件性的渲染内容 sec:authorize-acl 基于表达式的计算结果条件性的渲染内容 sec:authorize-expr sec:authorize属性的别名 sec:authorize-url 基于给定URL路径相关的安全规则条件性的渲染内容 1234567@Beanpublic SpringTemplateEngine templateEngine(TemplateResolver templateResolver) &#123; SpringTemplateEngine templateEngine = new SpringTemplateEngine(); templateEngine.setTemplateResolver(templateResolver); templateEngine.addDialect(new SpringSecurityDialect()); // 注册安全方言 return templateEngine;&#125; 123&lt;div sec:authorize=&quot;isAuthenticated&quot;&gt; Hello &lt;span sec:authentication=&quot;name&quot;&gt;someone&lt;/span&gt;&lt;/div&gt; 后端中的 Spring 关注 Spring 如何帮助我们在后端处理数据。 Spring 与 JDBC 定义 Spring 对数据访问的支持； 配置数据库资源； 使用 Spring 的 JDBC 模版。 为了避免持久化的逻辑分散到应用的各个组件中最好将数据访问的功能放到一个或多个专注于此项任务的组件中。这样的组件通常称为数据访问对象 data access object DAO 或 Repository。 为了避免应用与特定的数据访问策略耦合在一起编写良好的 Repository 应该以接口的方式暴露功能。 服务对象本身并不会处理数据访问而是将数据访问委托给 Repository，Repository 接口确保其与服务对象的松耦合。 Spring 将数据访问过程中固定的和可变的部分明确划分为两个不同的类模板 template 和回调 callback。模板管理过程中固定的部分而回调处理自定义的数据访问代码。 配置数据源 通过 JDBC 驱动程序定义的数据源； 通过 JNDI 查找的数据源； 连接池的数据源。 JNDI（Java Naming and Directory Interface ），类似于在一个中心注册一个东西，以后要用的时候，只需要根据名字去注册中心查找，注册中心返回你要的东西。在 web 程序中可以将一些东西（比如数据库相关的）交给服务器软件去配置和管理（有全局配置和单个 web 程序的配置），在程序代码中只要通过名称查找就能得到注册的东西，而且如果注册的东西有变，比如更换了数据库，我们只需要修改注册信息，名称不改，因此代码也不需要修改。 12345678@Beanpublic JndiObjectFactoryBean dataSource() &#123; JndiObjectFactoryBean jndiObjectFB = new JndiObjectFactoryBean(); jndiObjectFB.setJndiName(&quot;jdbc/SpittrDS&quot;); jndiObjectFB.setResourceRef(true); jndiObjectFB.setProxyInterface(javax.sql.DataSource.class); return jndiObjectFB;&#125; 使用数据源连接池。（推荐） 1234567891011@Beanpublic BasicDataSource dataSource() &#123; BasicDataSource ds = new BasicDataSource(); ds.setDriverClassName(&quot;org.h2.Driver&quot;); ds.setUrl(&quot;jdbc:h2:tcp://localhost/~/spitter&quot;); ds.setUsername(&quot;sa&quot;); ds.setPassword(&quot;&quot;); ds.setInitialSize(5); ds.setMaxActive(10); return ds;&#125; 在Spring中通过 JDBC 驱动定义数据源是最简单的配置方式。Spring 提供了三个这样的数据源类均位于org.springframework.jdbc.datasource包中供选择。 DriverManagerDataSource在每个连接请求时都会返回一个新建的连接。与 DBCP 的BasicDataSource不同由DriverManagerDataSource提供的连接并没有进行池化管理 SimpleDriverDataSource与DriverManagerDataSource的工作方式类似但是它直接使用 JDBC 驱动来解决在特定环境下的类加载问题这样的环境包括OSGi容器 SingleConnectionDataSource在每个连接请求时都会返回同一个的连接。尽管SingleConnectionDataSource不是严格意义上的连接池数据源但是你可以将其视为只有一个连接的池。 123456789@Beanpublic DataSource dataSource() &#123; DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(&quot;org.h2.Driver&quot;); ds.setUrl(&quot;jdbc:h2:tcp://localhost/~/spitter&quot;); ds.setUsername(&quot;sa&quot;); ds.setPassword(&quot;&quot;); return ds;&#125; 与具备池功能的数据源相比，唯一的区别在于这些数据源 bean 都没有提供连接池功能，所以没有可配置的池相关的属性。 Spring 的 JDBC 命名空间能够简化嵌入式数据库的配置，可以使用EmbeddedDatabaseBuilder来构建DataSource。 12345678@Beanpublic DataSource dataSource() &#123; return new EmbeddedDatabaseBuilder() .setType(EmbeddedDatabaseType.H2) .setScript(&quot;classpath:schema.sql&quot;) .setScript(&quot;classpath:text-data.sql&quot;) .build();&#125; 使用 @profile 选择数据源。 JDBC 模板 JdbcTemplate最基本的 Spring JDBC 模板，这个模板支持简单的 JDBC 数据库访问功能以及基于索引参数的查询； NamedParameterJdbcTemplate使用该模板类执行查询时可以将值以命名参数的形式绑定到 SQL 中而不是使用简单的索引参数； SimpleJdbcTemplate该模板类利用 Java 5 的一些特性如自动装箱、泛型以及可变参数列表来简化 JDBC 模板的使用。 为了让JdbcTemplate正常工作，只需要为其设置DataSource就可以了。之后将jdbcTemplate装配到 Repository 中并使用它来访问数据库就可以了。 1234@Beanpublic JdbcTemplate jdbcTemplate(DataSource dataSource) &#123; return new JdbcTemplate(dataSource);&#125; 1234567891011@Respositorypublic class JdbcSpitterRepository implements SpitterRepository &#123; // 通过注入非具体的 JdbcTemplate 达到松耦合 private JdbcOperations jdbcOperations; @Inject public JdbcSpitterRepository(JdbcOperations jdbcOperations) &#123; this.jdbcOperations= jdbcOperations; &#125; // ...&#125; ORM 持久化数据 使用 Spring 和 Hibernate； 借助上下文 Session 编写不依赖于 Spring 的 Repository； 通过 Spring 使用 JPA； 借助 Spring Data 实现自动化的 JPA Repository。 HIbernate 使用 Hibernate 所需的主要接口是org.hibernate.Session。Session接口提供了基本的数据访问功能如保存、更新、删除以及从数据库加载对象的功能。通过 Hibernate 的Session接口应用程序的 Repository 能够满足所有的持久化需求。 获取 Hibernate Session 对象的标准方式是借助于 Hibernate SessionFactory接口的实现类。除了一些其他的任务SessionFactory主要负责 Hibernate Session的打开、关闭以及管理。 JPA 在 Spring 中使用 JPA 的第一步是要在 Spring 应用上下文中将实体管理器工厂（entity manager factory）按照 bean 的形式来进行配置。 基于 JPA 的应用程序需要使用 EntityManagerFactory 的实现类来获取 EntityManager 实例。 容器管理的 JPA 采取了一个不同的方式。当运行在容器中时，可以使用容器（在我们的场景下是 Spring）提供的信息来生成 EntityManagerFactory。 12345678910111213141516171819@Beanpublic LocalContainerEntityManagerFactoryBean entityManagerFactory( DataSource dataSource, JpaVendorAdapter jpaVendorAdapter) &#123; LocalContainerEntityManagerFactoryBean emfb = new LocalContainerEntityManagerFactoryBean(); emfb.setDataSource(dataSource); emfb.setJpaVendorAdapter(jpaVendorAdapter); return emfb;&#125;@Beanpublic JpaVendorAdapter jpaVendorAdapter() &#123; // Hibernate 厂商提供的适配器 HibernateJpaVendorAdapter adapter = new HibernateJpaVendorAdapter(); adapter.setDatabase(&quot;HSQL&quot;); adapter.setShowSql(true); adapter.setGenerateDdl(false); adapter.setDatabasePlatform(&quot;org.hibernet.dialect.HSQLDialect&quot;); return adapter;&#125; 1234567public interface SpitterRepository extends JpaRepository&lt;Spitter, Long&gt; &#123;&#125;@Configuration@EnableJpaRepositories(basePackages=&quot;com.habuma.spittr.db&quot;)public class JpaConfiguration &#123;&#125; Repository 方法的命名遵循一种模式，有助于 Spring Data 生成针对数据库的查询。（主题可以省略） 12// 方法签名List&lt;Spitter&gt; readByFirstnameIgnoringCaseOrLastnameIgnoringCase(String first, String last); 声明自定义查询。 12@Query(&quot;select s from Spitter s where s.email like &#x27;%gmail.com&#x27;&quot;)List&lt;Spitter&gt; findAllGmailSpitters(); 混合自定义查询，即嵌套查询。 NoSQL 为 MongoDB 和 Neo4j 编写 Repository； 为多种数据存储形式持久化数据； 组合使用 Spring 和 Redis。 MongoDB MongoDB 是最为流行的开源文档数据库之一。Spring Data MongoDB 提供了三种方式在 Spring 应用中使用 MongoDB。 通过注解实现对象-文档映射； 使用MongoTemplate实现基于模板的数据库访问； 自动化的运行时 Repository 生成功能。 杂项 基于LDAP进行认证使用 Apache Common Lang 包来实现equals()和hashCode()方法。 12345678@Overridepublic boolean equals(Object that) &#123; return EqualsBuilder.reflectionEquals(this, that, &quot;id&quot;, &quot;time&quot;);&#125;@Overridepublic int hashCode() &#123; return HashCodeBuilder.reflectionHashCode(this, &quot;id&quot;, &quot;time&quot;);&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"}]},{"title":"Flume","slug":"大数据/Flume","date":"2020-05-08T06:27:33.000Z","updated":"2023-09-20T07:35:51.825Z","comments":true,"path":"2020/05/08/大数据/Flume/","link":"","permalink":"https://wingowen.github.io/2020/05/08/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flume/","excerpt":"Flume 基本介绍及简单使用。","text":"Flume 基本介绍及简单使用。 基本概念 Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume 基于流式架构，灵活简单。 Flume 最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写入到 HDFS。 基础框架 Flume 支持将事件流向一个或者多个目的地，这种模式可以将相同数据复制到多个 channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。 Agent Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。 Agent 主要有 3 个部分组成：Source、Channel、Sink。 Source Source 是负责接收数据到 Flume Agent 的组件。Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。 Sink Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。 Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。 Channel Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个 Sink 的读取操作。 Flume 自带两种 Channel：Memory Channel 和 File Channel。 Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适 用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。 File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。 Event 传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。 Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构， Body 用来存放该条数据，形式为字节数组。 快速入门 12345tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/mv apache-flume-1.7.0-bin flumemv flume-env.sh.template flume-env.shvi flume-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144 使用 Flume 监听一个端口，收集该端口数据（使用 nectcat 工具向该端口发送数据），并打印到控制台。 1234567891011121314151617181920212223242526272829303132# 安装 netcat 工具sudo yum install -y nc# 判断 44444 端口是否被占用 sudo netstat -tunlp | grep 44444# 创建 Flume Agent 配置文件 flume-netcat-logger.conf # 在 flume 目录下创建 job 文件夹并进入 job 文件夹。mkdir jobcd job/# 在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf。# vim flume-netcat-logger.conf# 在 flume-netcat-logger.conf 文件中添加如下内容。（参考官方文档配置） # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat # 数据来源 netcat 工具 a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = logger # 数据输出目的地为 logger 控制台类型 # Use a channel which buffers events in memory a1.channels.c1.type = memory # 通道的存储为内存型 a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # 100 条 even 后才提交事务 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1# 开启 flume 监听端口 控制台日志打印级别设置为 INFO 级别bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console# 使用 netcat 工具向本机的 44444 端口发送内容 nc localhost 44444 实际案例 Exec 实时监控 Hive 日志，并上传到 HDFS 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243# Flume 要想将数据输出到 HDFS，须持有 Hadoop 相关 jar 包# 创建 flume-file-hdfs.conf 文件 vim flume-file-hdfs.conf # Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 # Describe/configure the source a2.sources.r2.type = exec a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log a2.sources.r2.shell = /bin/bash -c # Describe the sink a2.sinks.k2.type = hdfs a2.sinks.k2.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%H # 上传文件的前缀 a2.sinks.k2.hdfs.filePrefix = logs- # 是否按照时间滚动文件夹 a2.sinks.k2.hdfs.round = true # 多少时间单位创建一个新的文件夹 a2.sinks.k2.hdfs.roundValue = 1 # 重新定义时间单位 a2.sinks.k2.hdfs.roundUnit = hour # 是否使用本地时间戳 a2.sinks.k2.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k2.hdfs.batchSize = 1000 # 设置文件类型，可支持压缩 a2.sinks.k2.hdfs.fileType = DataStream # 多久生成一个新的文件 a2.sinks.k2.hdfs.rollInterval = 30 # 设置每个文件的滚动大小 a2.sinks.k2.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a2.sinks.k2.hdfs.rollCount = 0 # Use a channel which buffers events in memory a2.channels.c2.type = memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2bin/flume-ng agent --conf conf/ --name a2 --conf-file job/flume-file-hdfs.conf# 开启 Hadoop 和 Hive 并操作 Hive 产生日志 Spooldir 使用 Flume 监听整个目录的文件，并上传至 HDFS。 123456789101112131415161718192021222324252627282930313233343536373839404142vim flume-dir-hdfs.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source a3.sources.r3.type = spooldir a3.sources.r3.spoolDir = /opt/module/flume/upload a3.sources.r3.fileSuffix = .COMPLETED # 定义文件上传完的后缀 a3.sources.r3.fileHeader = true # 忽略所有以.tmp 结尾的文件，不上传 a3.sources.r3.ignorePattern = ([^ ]*\\.tmp) # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H # 上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- # 是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true # 多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 # 重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour # 是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 # 设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream # 多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 # 设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf Exec source 适用于监控一个实时追加的文件，但不能保证数据不丢失；Spooldir Source 能够保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控；而 Taildir Source 既能够实现断点续传，又可以保证数据不丢失，还能够进行实时监控。 Taildir 使用 Flume 监听整个目录的实时追加文件，并上传至 HDFS。 12345678910111213141516171819202122232425262728293031323334353637383940414243vim flume-taildir-hdfs.conf a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source a3.sources.r3.type = TAILDIR # 维护了一个 json 格式的 position File # 其会定期的往 position File 中更新每个文件读取到的最新的位置，因此能够实现断点续传 a3.sources.r3.positionFile = /opt/module/flume/tail_dir.json a3.sources.r3.filegroups = f1 a3.sources.r3.filegroups.f1 = /opt/module/flume/files/file.* # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H # 上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- # 是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true # 多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 # 重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour # 是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 # 设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream # 多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 # 设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-taildir-hdfs.conf 进阶特性 事务 内部原理 ChannelSelector ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 Replicating（复制）和 Multiplexing（多路复用）。 ReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。 SinkProcessor SinkProcessor 共有三种类型，分别是 DefaultSinkProcessor、LoadBalancingSinkProcessor 和 FailoverSinkProcessor。 DefaultSinkProcessor 对应的是单个的 Sink ， LoadBalancingSinkProcessor 和 FailoverSinkProcessor 对应的是 Sink Group，LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以实现故障转移的功能。 拓扑结构 简单串联 将多个 flume 顺序连接起来了，从最初的 source 开始到最终 sink 传送的目的存储系统。此模式不建议桥接过多的 flume 数量，flume 数量过多不仅会影响传输速率， 而且一旦传输过程中某个节点 flume 宕机，会影响整个传输系统。 复制和多路复用 Flume 支持将事件流向一个或者多个目的地。这种模式可以将相同数据复制到多个 channel 中，或者将不同数据分发到不同的 channel 中，sink 可以选择传送到不同的目的地。 负载均衡和故障转移 Flume支持使用将多个 sink 逻辑上分到一个 sink 组，sink 组配合不同的 SinkProcessor 可以实现负载均衡和错误恢复的功能。 聚合 这种模式是我们最常见的，也非常实用，日常 web 应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用 flume 的这种组合方式能很好的解决这一问题，每台服务器部署一个 flume 采集日志，传送到一个集中收集日志的 flume，再由此 flume 上传到 hdfs、hive、hbase 等，进行日志分析。 企业开发 复制和复用 使用 Flume-1 监控文件变动，Flume-1 将变动内容传递给 Flume-2，Flume-2 负责存储到 HDFS。同时 Flume-1 将变动内容传递给 Flume-3，Flume-3 负责输出到 Local FileSystem。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899cd group1/mkdir flume3# 监控文件变动vim flume-file-flume.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 # 将数据流复制给所有 channel a1.sources.r1.selector.type = replicating # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log a1.sources.r1.shell = /bin/bash -c # Describe the sink # sink 端的 avro 是一个数据发送者 a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop102 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = hadoop102 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2# 负责存储到 HDFSvim flume-flume-hdfs.conf # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source # source 端的 avro 是一个数据接收服务 a2.sources.r1.type = avro a2.sources.r1.bind = hadoop102 a2.sources.r1.port = 4141 # Describe the sink a2.sinks.k1.type = hdfs a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H # 上传文件的前缀 a2.sinks.k1.hdfs.filePrefix = flume2- # 是否按照时间滚动文件夹 a2.sinks.k1.hdfs.round = true # 多少时间单位创建一个新的文件夹 a2.sinks.k1.hdfs.roundValue = 1 # 重新定义时间单位 a2.sinks.k1.hdfs.roundUnit = hour # 是否使用本地时间戳 a2.sinks.k1.hdfs.useLocalTimeStamp = true # 积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k1.hdfs.batchSize = 100 # 设置文件类型，可支持压缩 a2.sinks.k1.hdfs.fileType = DataStream # 多久生成一个新的文件 a2.sinks.k1.hdfs.rollInterval = 600 # 设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollSize = 134217700 # 文件的滚动与 Event 数量无关 a2.sinks.k1.hdfs.rollCount = 0 # Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1# 保存到本地目录vim flume-flume-dir.conf # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop102 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = file_roll # 输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录。 a3.sinks.k1.sink.directory = /opt/module/data/flume3 # Describe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.confbin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.confbin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf# 启动 Hadoop 和 Hive，查看结果 均衡和转移 使用 Flume1 监控一个端口，其 sink 组中的 sink 分别对接 Flume2 和 Flume3，采用 FailoverSinkProcessor，实现故障转移的功能。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980cd group2/# 监听 nectcat 发送的数据vim flume-netcat-flume.conf # Name the components on this agent a1.sources = r1 a1.channels = c1 a1.sinkgroups = g1 a1.sinks = k1 k2 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # 失败重试 a1.sinkgroups.g1.processor.type = failover a1.sinkgroups.g1.processor.priority.k1 = 5 a1.sinkgroups.g1.processor.priority.k2 = 10 a1.sinkgroups.g1.processor.maxpenalty = 10000 # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop102 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = hadoop102 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 # 成组负载平衡或故障转移策略 a1.sinkgroups.g1.sinks = k1 k2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c1# 控制台输出 1 号vim flume-flume-console1.conf # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = avro a2.sources.r1.bind = hadoop102 a2.sources.r1.port = 4141 # Describe the sink a2.sinks.k1.type = logger # Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1# 控制台输出 2 号vim flume-flume-console2.conf # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop102 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = logger # Describe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume-netcat-flume.conf# 使用 netcat 工具向本机的 44444 端口发送内容nc localhost 44444# 查看 Flume2 及 Flume3 的控制台打印日志 # 将 Flume2 kill，观察 Flume3 的控制台打印情况# 使用 jps -ml 查看 Flume 进程。 聚合 hadoop102 上的 Flume-1 监控文件 /opt/module/data/group.log，hadoop103 上的 Flume-2 监控某一个端口的数据流， Flume-1 与 Flume-2 将数据发送给 hadoop104 上的 Flume-3，Flume-3 将最终数据打印到控制台。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 分发程序xsync flume# 各节点上创建配置文件目录mkdir group3# 配置 Source 用于监控 hive.log 文件，配置 Sink 输出数据到下一级 Flume。# 在 hadoop102 上编辑配置文件vim flume1-logger-flume.conf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /opt/module/group.log a1.sources.r1.shell = /bin/bash -c # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = hadoop104 a1.sinks.k1.port = 4141 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1# 配置 Source 监控端口 44444 数据流，配置 Sink 数据到下一级 Flume：# 在 hadoop103 上编辑配置文件vim flume2-netcat-flume.conf # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = netcat a2.sources.r1.bind = hadoop103 a2.sources.r1.port = 44444 # Describe the sink a2.sinks.k1.type = avro a2.sinks.k1.hostname = hadoop104 a2.sinks.k1.port = 4141 # Use a channel which buffers events in memory a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1# 配置 source 用于接收 flume1 与 flume2 发送过来的数据流，最终合并后 sink 到控制台。# 在 hadoop104 上编辑配置文件touch flume3-flume-logger.confvim flume3-flume-logger.conf # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c1 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = hadoop104 a3.sources.r1.port = 4141 # Describe the sink a3.sinks.k1.type = logger # Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 a3.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c1 a3.sinks.k1.channel = c1bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group3/flume3-flume-logger.conf -Dflume.root.logger=INFO,consolebin/flume-ng agent --conf conf/ --name a2 --conf-file job/group3/flume1-logger-flume.confbin/flume-ng agent --conf conf/ --name a1 --conf-file job/group3/flume2-netcat-flume.conf# 在 hadoop103 上向/opt/module 目录下的 group.log 追加内容 echo &#x27;hello&#x27; &gt; group.log# 在 hadoop102 上向 44444 端口发送数据 telnet hadoop102 44444# 检查 hadoop104 上数据 Interceptor 在实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing 结构。 Multiplexing 的原理是根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel 中，所以我们需要自定义一个 Interceptor，为不同类型的 event 的 Header 中的 key 赋予不同的值。 在该案例中，我们以端口数据模拟日志，以数字（单个）和字母（单个）模拟不同类型的日志，我们需要自定义 interceptor 区分数字和字母，将其分别发往不同的分析系统 （Channel）。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536// 打成 jar 包放在 lib 目录下public class CustomInterceptor implements Interceptor &#123; @Override public void initialize() &#123; &#125; @Override public Event intercept(Event event) &#123; byte[] body = event.getBody(); if (body[0] &lt; &#x27;z&#x27; &amp;&amp; body[0] &gt; &#x27;a&#x27;) &#123; // 头部添加 K-V 用于识别分发 event.getHeaders().put(&quot;type&quot;, &quot;letter&quot;); &#125; else if (body[0] &gt; &#x27;0&#x27; &amp;&amp; body[0] &lt; &#x27;9&#x27;) &#123; event.getHeaders().put(&quot;type&quot;, &quot;number&quot;); &#125; return event; &#125; @Override public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123; for (Event event : events) &#123; intercept(event); &#125; return events; &#125; @Override public void close() &#123; &#125; public static class Builder implements Interceptor.Builder &#123; @Override public Interceptor build() &#123; return new CustomInterceptor(); &#125; @Override public void configure(Context context) &#123; &#125; &#125;&#125; 为 hadoop102 上的 Flume1 配置 1 个 netcat source，1 个 sink group（2 个 avro sink）， 并配置相应的 ChannelSelector 和 interceptor。 123456789101112131415161718192021222324252627282930313233343536373839# Name the components on this agenta1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444a1.sources.r1.interceptors = i1# 全路径，并且用”$“符号分割加上自定义 Builder 的类名a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.CustomInterceptor$Buildera1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = type# 根据头部 value 进行通道的选择a1.sources.r1.selector.mapping.letter = c1a1.sources.r1.selector.mapping.number = c2# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop103a1.sinks.k1.port = 4141a1.sinks.k2.type=avroa1.sinks.k2.hostname = hadoop104a1.sinks.k2.port = 4242# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Use a channel which buffers events in memorya1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2# 为 hadoop103/104 上的 Flume3 配置一个 avro source 和一个 logger sink# 分别在 hadoop102，hadoop103，hadoop104 上启动 flume 进程，注意先后顺序# 在 hadoop102 使用 netcat 向 localhost:44444 发送字母和数字# 观察 hadoop103 和 hadoop104 打印的日志 Source 使用 flume 接收数据，并给每条数据添加前缀，输出到控制台。前缀可从 flume 配置文件中配置。 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243// 打成 jar 包放在 lib 目录下public class MySource extends AbstractSource implements Configurable, PollableSource &#123; // 定义配置文件将来要读取的字段 private Long delay; private String field; // 初始化配置信息 @Override public void configure(Context context) &#123; delay = context.getLong(&quot;delay&quot;); field = context.getString(&quot;field&quot;, &quot;Hello!&quot;); &#125; @Override public Status process() throws EventDeliveryException &#123; try &#123; // 创建事件头信息 HashMap&lt;String, String&gt; hearderMap = new HashMap&lt;&gt;(); // 创建事件 SimpleEvent event = new SimpleEvent(); // 循环封装事件 for (int i = 0; i &lt; 5; i++) &#123; // 给事件设置头信息 event.setHeaders(hearderMap); // 给事件设置内容 event.setBody((field + i).getBytes()); // 将事件写入 channel getChannelProcessor().processEvent(event); Thread.sleep(delay); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return Status.BACKOFF; &#125; return Status.READY; &#125; @Override public long getBackOffSleepIncrement() &#123; return 0; &#125; @Override public long getMaxBackOffSleepInterval() &#123; return 0; &#125;&#125; 123456789101112131415161718# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = com.wingo.MySource# 自定义配置a1.sources.r1.delay = 1000#a1.sources.r1.field = wingo# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 Sink Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个 Flume Agent。 Sink 是完全事务性的。在从 Channel 批量删除数据之前，每个 Sink 用 Channel 启动一个事务。批量事件一旦成功写出到存储系统或下一个 Flume Agent，Sink 就利用 Channel 提交事务。事务一旦被提交，该 Channel 从自己的内部缓冲区删除事件。 使用 flume 接收数据，并在 Sink 端给每条数据添加前缀和后缀，输出到控制台。前后缀可在 flume 任务配置文件中配置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 打成 jar 包放在 lib 目录下public class MySink extends AbstractSink implements Configurable &#123; // 创建 Logger 对象 private static final Logger LOG = LoggerFactory.getLogger(AbstractSink.class); private String prefix; private String suffix; @Override public Status process() throws EventDeliveryException &#123; // 声明返回值状态信息 Status status; // 获取当前 Sink 绑定的 Channel Channel ch = getChannel(); // 获取事务 Transaction txn = ch.getTransaction(); // 声明事件 Event event; // 开启事务 txn.begin(); // 读取 Channel 中的事件，直到读取到事件结束循环 while (true) &#123; event = ch.take(); if (event != null) &#123; break; &#125; &#125; try &#123; // 处理事件（打印） LOG.info(prefix + new String(event.getBody()) + suffix); // 事务提交 txn.commit(); status = Status.READY; &#125; catch (Exception e) &#123; // 遇到异常，事务回滚 txn.rollback(); status = Status.BACKOFF; &#125; finally &#123; // 关闭事务 txn.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; // 读取配置文件内容，有默认值 prefix = context.getString(&quot;prefix&quot;, &quot;hello:&quot;); // 读取配置文件内容，无默认值 suffix = context.getString(&quot;suffix&quot;); &#125;&#125; 123456789101112131415161718# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = com.wingo.MySinka1.sinks.k1.suffix = :wingo# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 Ganglia 监控 大公司一般用平台组开发的监控服务，中小型公司用的比较多的是这个，了解即可。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://wingowen.github.io/tags/Flume/"}]},{"title":"Kafka","slug":"大数据/Kafka","date":"2020-05-06T04:05:51.000Z","updated":"2023-09-20T07:35:51.890Z","comments":true,"path":"2020/05/06/大数据/Kafka/","link":"","permalink":"https://wingowen.github.io/2020/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE/Kafka/","excerpt":"Kafka 基本介绍及简单使用。","text":"Kafka 基本介绍及简单使用。 基本概念 Kafka 是一个分布式的基于发布 / 订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 发布 / 订阅模式（一对多，消费者消费数据之后不会清除消息） 基础架构 Producer：消息生产者，就是向 Kafka Broker 发消息的客户端。 Consumer：消息消费者，向 Kafka Broker 取消息的客户端。 Consumer Group：消费者组，由多个 Consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 Broker：一台 Kafka 服务器就是一个 Broker。一个集群由多个 Broker 组成。一个 Broker 可以容纳多个 Topic。 Topic：可以理解为一个队列，生产者和消费者面向的都是一个 Topic。 Partition：为了实现扩展性，一个非常大的 Topic 可以分布到多个 Broker（即服务器）上，一个 Topic 可以分为多个 Partition，每个 Partition 是一个有序的队列。 Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 Partition 数据不丢失，且 Kafka 仍然能够继续工作，Kafka 提供了副本机制，一个 Topic 的每个分区都有若干个副本，一个 Leader 和若干个 Follower。 Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。 Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和 Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Follower。 安装部署 123456789101112131415161718192021222324252627282930313233343536tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/mv kafka_2.11-0.11.0.0/ kafkamkdir logscd config/vi server.properties #broker 的全局唯一编号，不能重复 broker.id=0 #删除 topic 功能使能 delete.topic.enable=true #处理网络请求的线程数量 num.network.threads=3 #用来处理磁盘 IO 的现成数量 num.io.threads=8 #发送套接字的缓冲区大小 socket.send.buffer.bytes=102400 #接收套接字的缓冲区大小 socket.receive.buffer.bytes=102400 #请求套接字的缓冲区大小 socket.request.max.bytes=104857600 #kafka 运行日志存放的路径 log.dirs=/opt/module/kafka/logs #topic 在当前 broker 上的分区个数 num.partitions=1 #用来恢复和清理 data 下数据的线程数量 num.recovery.threads.per.data.dir=1 #segment 文件保留的最长时间，超时将被删除 log.retention.hours=168 #配置连接 Zookeeper 集群地址 zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181sudo vi /etc/profile #KAFKA_HOME export KAFKA_HOME=/opt/module/kafka export PATH=$PATH:$KAFKA_HOME/binsource /etc/profile# 将配置分发到其它 Broker，并且记得修改其它 Broker 的环境变量# 其它 Broker 的 broker.id 记得修改，不得重复 123456# Kafka 群起脚本for i in hadoop102 hadoop103 hadoop104 do echo &quot;========== $i ==========&quot; ssh $i &#x27;/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties&#x27; done Shell 操作 12345678910111213141516171819# 集群规划：# hadoop102 位 kafka # 查看当前服务器中的所有 topicbin/kafka-topics.sh --zookeeper hadoop102:2181 --list# 创建 topicbin/kafka-topics.sh --zookeeper hadoop102:2181 --create --replication-factor 3 --partitions 1 --topic first# 删除 topicbin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic first# 需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除。# 发送消息bin/kafka-console-producer.sh --brokerlist hadoop102:9092 --topic first# 消费消息bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first# --from-beginning：会把主题中以往所有的数据都读取出来bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first# 查看某个 Topic 的详情bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic first# 修改分区数bin/kafka-topics.sh --zookeeper hadoop102:2181 --alter --topic first --partitions 6 架构深入 Kafka 中消息是以 topic 进行分类的，生产者生产消息，消费者消费消息，都是面向 topic 的。 topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文 件，该 log 文件中存储的就是 producer 生产的数据。Producer 生产的数据会被不断追加到该 log 文件末端，且每条数据都有自己的 offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。 由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位 效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment。每个 segment 对应两个文件：“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。 例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0,first-1,first-2。 index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log 文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中 message 的物理偏移地址。 生产者 分区策略 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic 又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发，因为可以以 Partition 为单位读写了。 将 producer 发送的数据封装成一个 ProducerRecord 对象。 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 可靠性 为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到 producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果 producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1 个副本，而 Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 ISR 采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据， 但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去， 直到它完成同步，才能发送 ack。这个问题怎么解决呢？ Leader 维护了一个动态的 in-sync replica set (ISR)，意为和 leader 保持同步的 follower 集 合。当 ISR 中的 follower 完成数据的同步之后，follower 就会给 fleader 发送 ack。如果 follower 长时间未向 leader 同步数据 ， 则该 follower 将被踢出 ISR ， 该时间阈值由 replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 应答机制 对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失， 所以没必要等 ISR 中的 follower 全部接收成功。 所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡， 选择以下的配置。 acks 为 0： producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； acks 为 1：producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower 同步成功之前 leader 故障，那么将会丢失数据； acks 为 -1（all）：producer 等待 broker 的 ack，partition 的 leader 和 follower 全部落盘成功后才 返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会造成数据重复。 故障处理 HW：指的是消费者能见到的最大的 offset，ISR 队列中最小的 LEO。 follower 故障 follower 发生故障后会被临时踢出 ISR，待该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。 等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 leader 故障 leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 将服务器的 ACK 级别设置为 -1，可以保证 Producer 到 Server 之间不会丢失数据，但不能保证数据不重复，即 At Least Once 语义。 将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，但不能保证数据不丢失，即 At Most Once 语义。 Exactly Once 0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论 向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语 义，就构成了 Kafka 的 Exactly Once 语义。 要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka 的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在 初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而 Broker 端会对做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。 但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。 消费者 consumer 采用 pull（拉）模式从 broker 中读取数据。 push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。 它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。 pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费，consumer 会等待一段时间之后再消费，这段时长即为 timeout。 分区分配策略 一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 Kafka 有两种分配策略，一是 RoundRobin，一是 Range。 offset 维护 由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始， consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为 __consumer_offsets。 事务 Kafka 事务 Kafka 从 0.11 版本开始引入了事务支持。事务可以保证 Kafka 在 Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 Producer 事务 为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer 获得的 PID 和 Transaction ID 绑定。这样当 Producer 重启后就可以通过正在进行的 Transaction ID 获得原来的 PID。 为了管理 Transaction，Kafka 引入了一个新的组件 Transaction Coordinator。Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于 事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 Producer API 消息发送流程 Kafka 的 Producer 发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程：main 线程和 Sender 线程，以及一个线程共享变量：RecordAccumulator。 main 线程将消息发送给 RecordAccumulator，Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka broker。 相关参数 batch.size：只有数据积累到 batch.size 之后，sender 才会发送数据。 linger.ms：如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。 异步发送 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt; 常用类 KafkaProducer：需要创建一个生产者对象，用来发送数据； ProducerConfig：获取所需的一系列配置参数； ProducerRecord：每条数据都要封装成一个 ProducerRecord 对象。 1234567891011121314151617181920212223242526272829// 不带回调函数的 APIpublic class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // kafka 集群，broker-list props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;); // 重试次数 props.put(&quot;retries&quot;, 1); // 批次大小 props.put(&quot;batch.size&quot;, 16384); // 等待时间 props.put(&quot;linger.ms&quot;, 1); // RecordAccumulator 缓冲区大小 props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; producer.send( new ProducerRecord&lt;String, String&gt;( &quot;first&quot;, Integer.toString(i), Integer.toString(i) // 组 key value ) ); &#125; producer.close(); &#125;&#125; 回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是 RecordMetadata 和 Exception，如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。 消息发送失败会自动重试，不需要我们在回调函数中手动重试。 1234567891011121314151617181920212223242526// 带回调函数的 APIpublic class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // 常规配置 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; producer.send( new ProducerRecord&lt;String, String&gt;( &quot;first&quot;, Integer.toString(i), Integer.toString(i) ), new Callback() &#123; // 回调函数，该方法会在 Producer 收到 ack 时调用，为异步调用 @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception == null) &#123; System.out.println(&quot;success-&gt;&quot; + metadata.offset()); &#125; else &#123; exception.printStackTrace(); &#125; &#125; &#125; ); &#125; producer.close(); &#125;&#125; 同步发送 同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回 ack。 由于 send 方法返回的是一个 Future 对象，根据 Futrue 对象的特点也可以实现同步发送的效果，只需在调用 Future 对象的 get 方发即可。 12345678910111213141516public class CustomProducer &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; Properties props = new Properties(); // 常规配置 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; producer.send( new ProducerRecord&lt;String, String&gt;( &quot;first&quot;, Integer.toString(i), Integer.toString(i) ) ).get(); // 阻塞等待获取返回值 &#125; producer.close(); &#125;&#125; Consumer API consumer 消费数据时的可靠性是很容易保证的，因为数据在 Kafka 中是持久化的，故不用担心数据丢失问题。 由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 所以 offset 的维护是 consumer 消费数据是必须考虑的问题。 为了使我们能够专注于自己的业务逻辑，Kafka 提供了自动提交 offset 的功能。 自动提交 offset 的相关参数： enable.auto.commit：是否开启自动提交 offset 功能； auto.commit.interval.ms：自动提交 offset 的时间间隔。 自动提交 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122public class CustomConsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); props.put(&quot;group.id&quot;, &quot;test&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;first&quot;)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( &quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value() ); &#125; &#125; &#125;&#125; 手动提交 自动提交是基于时间提交的，开发人员难以把握 offset 提交的时机。因此 Kafka 还提供了手动提交 offset 的 API。 12345678910111213141516171819202122232425262728// 同步提交public class CustomComsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // Kafka 集群 props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(&quot;group.id&quot;, &quot;test&quot;); props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); // 关闭自动提交 offset props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(&quot;first&quot;)); // 消费者订阅主题 while (true) &#123; // 消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( &quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value() ); &#125; // 同步提交，当前线程会阻塞直到 offset 提交成功 consumer.commitSync(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728// 异步提交public class CustomComsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); // ... 常规配置 consumer.subscribe(Arrays.asList(&quot;first&quot;)); // 消费者订阅主题 while (true) &#123; // 消费者拉取数据 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( &quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value() ); &#125; //异步提交 consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete( Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.err.println(&quot;Commit failed for&quot; + offsets); &#125; &#125; &#125;); &#125; &#125;&#125; 自定义存储 Kafka 0.9 版本之前，offset 存储在 zookeeper，0.9 版本及之后，默认将 offset 存储在 Kafka 的一个内置的 topic 中。除此之外，Kafka 还可以选择自定义存储 offset。 offset 的维护是相当繁琐的，因为需要考虑到消费者的 Rebalance。 当有新的消费者加入消费者组、已有的消费者推出消费者组或者所订阅的主题的分区发生变化，就会触发到分区的重新分配，重新分配的过程叫做 Rebalance。 消费者发生 Rebalance 之后，每个消费者消费的分区就会发生变化。因此消费者要首先获取到自己被重新分配到的分区，并且定位到每个分区最近提交的 offset 位置继续消费。 要实现自定义存储 offset，需要借助 ConsumerRebalanceListener，以下为示例代码，其中提交和获取 offset 的方法，需要根据所选的 offset 存储系统自行实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class CustomConsumer &#123; // 自定义存储 offset 的容器 private static Map&lt;TopicPartition, Long&gt; currentOffset = new HashMap&lt;&gt;(); public static void main(String[] args) &#123; // 创建配置信息 Properties props = new Properties(); // Kafka 集群 props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); // 消费者组，只要 group.id 相同，就属于同一个消费者组 props.put(&quot;group.id&quot;, &quot;test&quot;); // 关闭自动提交 offset props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); // Key 和 Value 的反序列化类 props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); // 创建一个消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); //消费者订阅主题 consumer.subscribe( Arrays.asList(&quot;first&quot;), new ConsumerRebalanceListener() &#123; // 该方法会在 Rebalance 之前调用 @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; commitOffset(currentOffset); &#125; // 该方法会在 Rebalance 之后调用 @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; currentOffset.clear(); for (TopicPartition partition : partitions) &#123; // 定位到最近提交的 offset 位置继续消费 consumer.seek(partition, getOffset(partition)); &#125; &#125; &#125;); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); //消费者拉取数据 for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf( &quot;offset = %d, key = %s, value = %s%n&quot;, record.offset(), record.key(), record.value() ); currentOffset.put( new TopicPartition(record.topic(), record.partition()), record.offset() ); &#125; commitOffset(currentOffset); // 异步提交 &#125; &#125; // 获取某分区的最新 offset private static long getOffset(TopicPartition partition) &#123; return 0; &#125; // 提交该消费者所有分区的 offset private static void commitOffset(Map&lt;TopicPartition, Long&gt; currentOffset) &#123; &#125;&#125; 拦截器 producer 拦截器（interceptor）是在 Kafka 0.10 版本被引入的，主要用于实现 clients 端的定制化控制逻辑。 对于 producer 而言，interceptor 使得用户在消息发送前以及 producer 回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。 同时，producer 允许用户指定多个 interceptor 按序作用于同一条消息从而形成一个拦截链（interceptor chain）。 123456789101112131415161718192021// 增加时间戳拦截器public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; &#123; // 获取配置信息和初始化数据时调用 @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; // 对消息进行自定义操作 @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; // 创建一个新的 record，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + &quot;,&quot; + record.value().toString()); &#125; @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; &#125; @Override public void close() &#123; &#125;&#125; 1234567891011121314151617181920212223242526272829// 统计发送消息成功和发送失败消息数，并在 producer 关闭时打印这两个计数器public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;&#123; private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125; @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) &#123; return record; &#125; // 消息发送成功或者失败后都会调用此方法 @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123; // 统计成功和失败的次数 if (exception == null) &#123; successCounter++; &#125; else &#123; errorCounter++; &#125; &#125; @Override public void close() &#123; // 保存结果 System.out.println(&quot;Successful sent: &quot; + successCounter); System.out.println(&quot;Failed sent: &quot; + errorCounter); &#125;&#125; 1234567891011121314151617181920212223242526272829// producer public class InterceptorProducer &#123; public static void main(String[] args) throws Exception &#123; // 设置配置信息 Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;hadoop102:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;retries&quot;, 3); props.put(&quot;batch.size&quot;, 16384); props.put(&quot;linger.ms&quot;, 1); props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); // 构建拦截链 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add(&quot;com.wingo.kafka.interceptor.TimeInterceptor&quot;); interceptors.add(&quot;com.wingo.kafka.interceptor.CounterInterceptor&quot;); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = &quot;first&quot;; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 发送消息 for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, &quot;message&quot; + i); producer.send(record); &#125; // 一定要关闭 producer，这样才会调用 interceptor 的 close 方法 producer.close(); &#125;&#125; Kafka Eagle 基于 Web 的 Kafka 监控。 杂项 高效读写数据 Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端， 为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这 磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 零拷贝。 Zookeeper 的作用 Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。 Controller 的管理工作都是依赖于 Zookeeper 的。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://wingowen.github.io/tags/Kafka/"}]},{"title":"Hbase","slug":"大数据/HBase","date":"2020-05-03T00:41:00.000Z","updated":"2023-09-20T07:35:51.828Z","comments":true,"path":"2020/05/03/大数据/HBase/","link":"","permalink":"https://wingowen.github.io/2020/05/03/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/","excerpt":"HBase 的简本介绍及简使用。","text":"HBase 的简本介绍及简使用。 简介 HBase 是一种分布式、可扩展、支持海量数据存储的 NoSQL 数据库。 逻辑上，HBase 的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。 但从 HBase 的底层物理存储结构（K-V）来看，HBase 更像是一个 multi-dimensional map。 数据模型 Name Space 命名空间，类似于关系型数据库的 DatabBase 概念，每个命名空间下有多个表。HBase 有两个自带的命名空间，分别是 hbase 和 default，hbase 中存放的是 HBase 内置的表， default 表是用户默认使用的命名空间。 Region 类似于关系型数据库的表概念。不同的是，HBase 定义表时只需要声明列族即可，不需 要声明具体的列。这意味着，往 HBase 写入数据时，字段可以动态、按需指定。因此，和关 系型数据库相比，HBase 能够轻松应对字段变更的场景。 Row HBase 表中的每行数据都由一个 RowKey 和多个 Column（列）组成，数据是按照 RowKey 的字典顺序存储的，并且查询数据时只能根据 RowKey 进行检索，所以 RowKey 的设计十分重 要。 Column HBase 中的每个列都由 (Column Family) 列族 和 (Column Qualifier )列限定符进行限 定，例如 info：name，info：age。建表时，只需指明列族，而列限定符无需预先定义。 Time Stamp 用于标识数据的不同版本（version），每条数据写入时，如果不指定时间戳，系统会自动为其加上该字段，其值为写入 HBase 的时间。 Cell 由 {RowKey, Column Family：Column Qualifier, Time Stamp} 唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 基本架构 Region Server Region Server 为 Region 的管理者，其实现类为 HRegionServer，主要作用如下：对于数据的操作：get, put, delete； 对于 Region 的操作：splitRegion、compactRegion。 Master Master 是所有 Region Server 的管理者，其实现类为 HMaster，主要作用如下： 对于表的操作：create, delete, alter；对于 RegionServer的操作：分配 regions 到每个RegionServer，监控每个 RegionServer 的状态，负载均衡和故障转移。 Zookeeper HBase 通过 Zookeeper 来做 Master 的高可用、RegionServer 的监控、元数据的入口以及集群配置的维护等工作。 HDFS HDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高可用的支持。 安装部署 首先要保证 Zookeeper 集群和 Hadoop 集群正常部署并启动。 1234tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/modulevi hbase-env.sh export JAVA_HOME=/opt/module/jdk1.6.0_144 export HBASE_MANAGES_ZK=false # 关闭自带的 Zookeeper 12345678910111213141516171819202122232425&lt;!-- 修改 hbase-site.xml 配置文件 --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000/HBase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 0.98 后的新变动，之前版本没有.port,默认端口为 60000 --&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- region servers --&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/zookeeper-3.4.10/zkData&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12345678# 软连接 hadoop 配置文件到 HBaseln -s /opt/module/hadoop2.7.2/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xmlln -s /opt/module/hadoop2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml# 利用脚本发送到其它集群 xsync hbase/ # 时间同步，否则会导致 regionserver 无法启动，抛出 ClockOutOfSyncException 异常 bin/start-hbase.sh # 可通过 16010 端口访问 HBase 管理页面 Shell 基本操作 123456# 进入 HBase 客户端命令行bin/hbase shell# 查看帮助命令hbase(main):001:0&gt; help# 查看当前数据库中有哪些表 hbase(main):002:0&gt; list 表操作 1234567891011121314151617181920212223242526272829303132333435# 创建表，只需要初始化一个列族信息即可hbase(main):002:0&gt; create &#x27;student&#x27;,&#x27;info&#x27;# 插入数据到表 hbase(main):003:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:sex&#x27;,&#x27;male&#x27;hbase(main):004:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;18&#x27;hbase(main):005:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:name&#x27;,&#x27;Janna&#x27;hbase(main):006:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;,&#x27;female&#x27;hbase(main):007:0&gt; put &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:age&#x27;,&#x27;20&#x27;# 扫描查看表数据hbase(main):008:0&gt; scan &#x27;student&#x27;hbase(main):009:0&gt; scan &#x27;student&#x27;,&#123;STARTROW=&gt;&#x27;1001&#x27;, STOPROW=&gt;&#x27;1001&#x27;&#125;hbase(main):010:0&gt; scan &#x27;student&#x27;,&#123;STARTROW=&gt;&#x27;1001&#x27;&#125;# 查看表结构hbase(main):011:0&gt; describe ‘student’# 更新指定字段的数据hbase(main):012:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;,&#x27;Nick&#x27;hbase(main):013:0&gt; put &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:age&#x27;,&#x27;100&#x27;hbase(main):014:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;hbase(main):015:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#x27;info:name&#x27;# 统计表数据行数hbase(main):021:0&gt; count &#x27;student&#x27;# 删除数据# 删除某 rowkey 的全部数据hbase(main):016:0&gt; deleteall &#x27;student&#x27;,&#x27;1001&#x27;# 删除某 rowkey 的某一列数据hbase(main):017:0&gt; delete &#x27;student&#x27;,&#x27;1002&#x27;,&#x27;info:sex&#x27;# 清空表数据hbase(main):018:0&gt; truncate &#x27;student&#x27;# 删除表 需要先让该表为 disable 状态hbase(main):019:0&gt; disable &#x27;student&#x27;# 然后才能 drop 这个表hbase(main):020:0&gt; drop &#x27;student&#x27;# 变更表信息 将 info 列族中的数据存放 3 个版本hbase(main):022:0&gt; alter &#x27;student&#x27;,&#123;NAME=&gt;&#x27;info&#x27;,VERSIONS=&gt;3&#125;hbase(main):022:0&gt; get &#x27;student&#x27;,&#x27;1001&#x27;,&#123;COLUMN=&gt;&#x27;info:name&#x27;,VERSIONS=&gt;3&#125; 进阶 结构原理 StoreFile 保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有 一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 MemStore 写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile。 WAL Write-Ahead Logging 由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件 中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 写流程 Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server； 访问对应的 Region Server，获取 hbase:meta 表，根据写请求的 namespace:table/rowkey， 查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以 及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问； 与目标 Region Server 进行通讯； 将数据顺序写入（追加）到 WAL； 将数据写入对应的 MemStore，数据会在 MemStore 进行排序； 向客户端发送 ack； 等达到 MemStore 的刷写时机后，将数据刷写到 HFile。 MemStore Flush 刷写时机 当某个 memstroe 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M）， 其所在 region 的所有 memstore 都会刷写。 当 memstore 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M） * hbase.hregion.memstore.block.multiplier（默认值 4） 时，会阻止继续往该 memstore 写数据。 当 region server 中 memstore 的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值 0.4）* hbase.regionserver.global.memstore.size.lower.limit（默认值 0.95）， region 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server 中所有 memstore 的总大小减小到上述值以下。 当 region server 中 memstore 的总大小达到 java_heapsize * hbase.regionserver.global.memstore.size（默认值 0.4） 时，会阻止继续往所有的 memstore 写数据。 到达自动刷写的时间，也会触发 memstore flush。自动刷新的时间间隔由该属性进行配置，官方建议关闭此配置，hbase.regionserver.optionalcacheflushinterval（默认 1 小时）。 当 WAL 文件的数量超过 hbase.regionserver.max.logs，region 会按照时间顺序依次进行刷写，直到 WAL 文件数量减小到 hbase.regionserver.max.log 以下（该属性名已经废弃， 现无需手动设置，最大值为 32）。 读流程 Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server； 访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey， 查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以 及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问； 与目标 Region Server 进行通讯； 分别在 Block Cache（读缓存），MemStore 和 Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put / Delete）； 将从文件中查询到的数据块（Block，HFile 数据存储单元，默认大小为 64KB）缓存到 Block Cache； 将合并后的最终结果返回给客户端。 StoreFile Compaction 由于 memstore 每次刷写都会生成一个新的 HFile，且同一个字段的不同版本（timestamp） 和不同类型（Put / Delete）有可能会分布在不同的 HFile 中，因此查询时需要遍历所有的 HFile。为了减少 HFile 的个数，以及清理掉过期和删除的数据，会进行 StoreFile Compaction。 Compaction 分为两种，分别是 Minor Compaction 和 Major Compaction。 Minor Compaction 会将临近的若干个较小的 HFile 合并成一个较大的 HFile，但不会清理过期和删除的数据。 Major Compaction 会将一个 Store 下的所有的 HFile 合并成一个大 HFile，并且会清理掉过期 和删除的数据。 Region Split 默认情况下，每个 Table 起初只有一个 Region，随着数据的不断写入，Region 会自动进行拆分。刚拆分时，两个子 Region 都位于当前的 Region Server，但处于负载均衡的考虑， HMaster 有可能会将某个 Region 转移给其他的 Region Server。 Region Split 时机： .当1个region中的某个 Store 下所有 StoreFile 的总大小超过 hbase.hregion.max.filesize， 该 Region 就会进行拆分（0.94 版本之前）。 当 1 个 region 中 的 某 个 Store 下所有 StoreFile 的 总 大 小 超 过 Min(R^2 * “hbase.hregion.memstore.flush.size”,hbase.hregion.max.filesize&quot;)，该 Region 就会进行拆分，其 中 R 为当前 Region Server 中属于该 Table 的个数（0.94 版本之后）。 API 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 123456789// 获取 Configuration 对象public static Configuration conf;static&#123; // 使用 HBaseConfiguration 的单例方法实例化 conf = HBaseConfiguration.create(); // 只需要配置 Zookeeper 的信息，因为 mate 都存在 Zookeeper 中 conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.166.9.102&quot;); conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);&#125; 表操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119// 判断表石是否存在public static boolean isTableExist(String tableName) throws MasterNotRunningException,ZooKeeperConnectionException, IOException&#123; // 在 HBase 中管理、访问表需要先创建 HBaseAdmin 对象 ConnectionFactory.createConnection(conf); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName);&#125;// 创建表public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); // 判断表是否存在 if(isTableExist(tableName))&#123; System.out.println(&quot;表&quot; + tableName + &quot;已存在&quot;); &#125;else&#123; // 创建表属性对象，表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); // 创建多个列族 for(String cf : columnFamily)&#123; descriptor.addFamily(new HColumnDescriptor(cf)); &#125; // 根据对表的配置，创建表 admin.createTable(descriptor); System.out.println(&quot;表&quot; + tableName + &quot;创建成功！&quot;); &#125;&#125;// 删除表public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException&#123; HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName))&#123; admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println(&quot;表&quot; + tableName + &quot;删除成功！&quot;); &#125;else&#123; System.out.println(&quot;表&quot; + tableName + &quot;不存在！&quot;); &#125;&#125;// 向表中插入数据public static void addRowData( String tableName, String rowKey, String columnFamily, String column, String value) throws IOException&#123; // 创建 HTable 对象 HTable hTable = new HTable(conf, tableName); // 向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); // 向 Put 对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println(&quot;插入数据成功&quot;);&#125;// 删除多行数据public static void deleteMultiRow(String tableName, String... rows) throws IOException&#123; HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows)&#123; Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); &#125; hTable.delete(deleteList); hTable.close();&#125;// 获取所有数据public static void getAllRows(String tableName) throws IOException&#123; HTable hTable = new HTable(conf, tableName); // 得到用于扫描 region 的对象 Scan scan = new Scan(); // 使用 HTable 得到 resultcanner 实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner)&#123; // 获取单元 Cell[] cells = result.rawCells(); for(Cell cell : cells)&#123; //得到 rowkey System.out.println(&quot; 行 键 :&quot; + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println(&quot; 列 族 &quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot; 列 :&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot; 值 :&quot; + Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125;&#125;// 获取某一行数据public static void getRow(String tableName, String rowKey) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(&quot; 行 键 :&quot; + Bytes.toString(result.getRow())); System.out.println(&quot; 列 族 &quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot; 列 :&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot; 值 :&quot; + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println(&quot;时间戳:&quot; + cell.getTimestamp()); &#125;&#125;// 获取某一行指定(列族:列)的数据public static void getRowQualifier( String tableName, String rowKey, String family, String qualifier) throws IOException&#123; HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family),Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells())&#123; System.out.println(&quot; 行 键 :&quot; + Bytes.toString(result.getRow())); System.out.println(&quot; 列 族 &quot; + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println(&quot; 列 :&quot; + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println(&quot; 值 :&quot; + Bytes.toString(CellUtil.cloneValue(cell))); &#125;&#125; MapReduce 通过 HBase 的相关 Java API，我们可以实现伴随 HBase 操作的 MapReduce 过程，比如使用 MapReduce 将数据从本地文件系统导入到 HBase 的表中，比如我们从 HBase 中读取一些原 始数据后使用 MapReduce 做数据分析。 官方案例 12345678910111213141516171819202122vi /etc/profile # export HBASE_HOME=/opt/module/hbase # export HADOOP_HOME=/opt/module/hadoop-2.7.2vi hadoop-env.sh # 注意：在 for 循环之后配 # export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*# 使用 MapReduce 将本地数据导入到 HBasevi fruit.tsv # 1001 Apple Red # 1002 Pear Yellow # 1003 Pineapple Yellow# 创建 Hbase 表Hbase(main):001:0&gt; create &#x27;fruit&#x27;,&#x27;info&#x27;# 在 HDFS 中创建 input_fruit 文件夹并上传 fruit.tsv 文件/opt/module/hadoop-2.7.2/bin/hdfs dfs -mkdir /input_fruit//opt/module/hadoop-2.7.2/bin/hdfs dfs -put fruit.tsv /input_fruit/# 执行 MapReduce 到 HBase 的 fruit 表中/opt/module/hadoop-2.7.2/bin/yarn jar lib/hbase-server-1.3.1.jarimporttsv \\-Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit \\hdfs://hadoop102:9000/input_fruit# 使用 scan 命令查看导入后的结果Hbase(main):001:0&gt; scan ‘fruit’ 自定义 将 fruit 表中的一部分数据，通过 MR 迁入到 fruit_mr 表中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// 继承的都是 HBase 提供的 Mapper 和 Reducerpublic class ReadFruitMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; // 将 fruit 的 name 和 color 提取出来，相当于将每一行数据读取出来放入到 Put 对象中 Put put = new Put(key.get()); // 遍历添加 column 行 for(Cell cell: value.rawCells())&#123; // 添加/克隆列族:info if(&quot;info&quot;.equals(Bytes.toString(CellUtil.cloneFamily(cell))))&#123; // 添加/克隆列：name if(&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; // 将该列 cell 加入到 put 对象中 put.add(cell); // 添加/克隆列:color &#125;else if(&quot;color&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; // 向该列 cell 加入到 put 对象中 put.add(cell); &#125; &#125; &#125; // 将从 fruit 读取到的每行数据写入到 context 中作为 map 的输出 context.write(key, put); &#125;&#125;public class WriteFruitMRReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; // 读出来的每一行数据写入到 fruit_mr 表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125;public class Fruit2FruitMRRunner extends Configured implements Tool&#123; // 组装 Job public int run(String[] args) throws Exception &#123; // 得到 Configuration Configuration conf = this.getConf(); // 创建 Job 任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Fruit2FruitMRRunner.class); // 配置 Job Scan scan = new Scan(); scan.setCacheBlocks(false); scan.setCaching(500); // 设置 Mapper，注意导入的是 mapreduce 包下的，不是 mapred 包下的，后者是老版本 TableMapReduceUtil.initTableMapperJob( &quot;fruit&quot;, // 数据源的表名 scan, // scan 扫描控制器 ReadFruitMapper.class, // 设置 Mapper 类 ImmutableBytesWritable.class, // 设置 Mapper 输出 key 类型 Put.class, // 设置 Mapper 输出 value 值类型 job // 设置给哪个 JOB ); // 设置 Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRReducer.class, job); // 设置 Reduce 数量，最少 1 个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(&quot;Job running with error&quot;); &#125; return isSuccess ? 0 : 1; &#125;&#125;public static void main( String[] args ) throws Exception&#123; Configuration conf = HbaseConfiguration.create(); int status = ToolRunner.run(conf, new Fruit2FruitMRRunner(), args); System.exit(status);&#125; 实现将 HDFS 中的数据写入到 Hbase 表中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class ReadFruitFromHDFSMapper extends Mapper&lt;LongWritable, Text, ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 从 HDFS 中读取的数据 String lineValue = value.toString(); // 读取出来的每行数据使用 \\t 进行分割，存于 String 数组 String[] values = lineValue.split(&quot;\\t&quot;); // 根据数据中值的含义取值 String rowKey = values[0]; String name = values[1]; String color = values[2]; // 初始化 rowKey ImmutableBytesWritable rowKeyWritable = new ImmutableBytesWritable(Bytes.toBytes(rowKey)); // 初始化 put 对象 Put put = new Put(Bytes.toBytes(rowKey)); // 参数分别 列族、列、值 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(name)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;color&quot;),Bytes.toBytes(color)); context.write(rowKeyWritable, put); &#125;&#125;public class WriteFruitMRFromTxtReducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException&#123; // 读出来的每一行数据写入到 fruit_hdfs 表中 for(Put put: values)&#123; context.write(NullWritable.get(), put); &#125; &#125;&#125;public class Txt2FruitRunner extends Configured implements Tool&#123; public int run(String[] args) throws Exception &#123; // 得到 Configuration Configuration conf = this.getConf(); // 创建 Job 任务 Job job = Job.getInstance(conf, this.getClass().getSimpleName()); job.setJarByClass(Txt2FruitRunner.class); Path inPath = new Path(&quot;hdfs://hadoop102:9000/input_fruit/fruit.tsv&quot;); FileInputFormat.addInputPath(job, inPath); // 设置 Mapper job.setMapperClass(ReadFruitFromHDFSMapper.class); job.setMapOutputKeyClass(ImmutableBytesWritable.class); job.setMapOutputValueClass(Put.class); // 设置 Reducer TableMapReduceUtil.initTableReducerJob(&quot;fruit_mr&quot;, WriteFruitMRFromTxtReducer.class, job); // 设置 Reduce 数量，最少 1 个 job.setNumReduceTasks(1); boolean isSuccess = job.waitForCompletion(true); if(!isSuccess)&#123; throw new IOException(&quot;Job running with error&quot;); &#125; return isSuccess ? 0 : 1; &#125;&#125; 集成 Hive 操作 Hive 的同时对 HBase 也会产生影响，所以 Hive 需要持有操作 HBase 的 Jar。 1234567891011# 这里使用软链接的方式引包export HBASE_HOME=/opt/module/hbaseexport HIVE_HOME=/opt/module/hiveln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar $HIVE_HOME/lib/hbase-common-1.3.1.jarln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbaseserver-1.3.1.jarln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jarln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jarln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it1.3.1.jarln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jarln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jarln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar 1234567891011121314&lt;!-- 修改 hive-site.xml --&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop102,hadoop103,hadoop104&lt;/value&gt; &lt;description&gt;The list of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.zookeeper.client.port&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;description&gt;The port of ZooKeeper servers to talk to. This is only needed for read/write locks.&lt;/description&gt;&lt;/property&gt; 1234567891011121314151617181920212223242526272829303132333435# 在 Hive 中创建表同时关联 HBaseCREATE TABLE hive_hbase_emp_table( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key, info:ename, info:job, info:mgr,i nfo:hiredate,i nfo:sal, info:comm, info:deptno&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);# 完成之后，可以分别进入 Hive 和 HBase 查看，都生成了对应的表# 不能将数据直接 load 进 Hive 所关联 HBase 的那张表中CREATE TABLE emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)row format delimited fields terminated by &#x27;\\t&#x27;;# 向 Hive 中间表中 load 数据load data local inpath &#x27;/home/admin/softwares/data/emp.txt&#x27; into table emp;# 通过 insert 命令将中间表中的数据导入到 Hive 关联 Hbase 的那张表中insert into table hive_hbase_emp_table select * from emp;# 查看 Hive 以及关联的 HBase 表中是否已经成功的同步插入了数据select * from hive_hbase_emp_table;scan ‘hbase_emp_table’ 在 HBase 中已经存储了某一张表 hbase_emp_table，然后在 Hive 中创建一个外部表来关联 HBase 中的 hbase_emp_table 这张表，使之可以借助 Hive 来分析 HBase 这张表中的数据。 12345678910111213141516CREATE EXTERNAL TABLE relevance_hbase_emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)STORED BY &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; =&quot;:key, info:ename, info:job, info:mgr, info:hiredate, info:sal, info:comm, info:deptno&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;hbase_emp_table&quot;);# 关联后就可以使用 Hive 函数进行一些分析操作了 select * from relevance_hbase_emp; 优化 高可用 在 HBase 中 HMaster 负责监控 HRegionServer 的生命周期，均衡 RegionServer 的负载， 如果 HMaster 挂掉了，那么整个 HBase 集群将陷入不健康的状态，并且此时的工作状态并 不会维持太久。所以 HBase 支持对 HMaster 的高可用配置。 123456789# 关闭 HBase 集群 bin/stop-hbase.sh# 在 conf 目录下创建 backup-masters 文件touch conf/backup-masters# 在 backup-masters 文件中配置高可用 HMaster 节点echo hadoop103 &gt; conf/backup-master# 将整个 conf 目录 scp 到其他节点scp -r conf/ hadoop103:/opt/module/hbase/scp -r conf/ hadoop104:/opt/module/hbase/ 预分区 每一个 region 维护着 StartRow 与 EndRow，如果加入的数据符合某个 Region 维护的 RowKey 范围，则该数据交给这个 Region 维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高 HBase 性能。 123456# 手动设定预分区create &#x27;staff1&#x27;,&#x27;info&#x27;,&#x27;partition1&#x27;,SPLITS =&gt; [&#x27;1000&#x27;,&#x27;2000&#x27;,&#x27;3000&#x27;,&#x27;4000&#x27;]# 生成 16 进制序列预分区create &#x27;staff2&#x27;,&#x27;info&#x27;,&#x27;partition2&#x27;,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &#x27;HexStringSplit&#x27;&#125;# 按照文件中设置的规则预分区create &#x27;staff3&#x27;,&#x27;partition3&#x27;,SPLITS_FILE =&gt; &#x27;splits.txt&#x27; 123456789// Java API// 自定义算法，产生一系列 hash 散列值存储在二维数组中byte[][] splitKeys = 某个散列值函数// 创建 HbaseAdmin 实例HBaseAdmin hAdmin = new HBaseAdmin(HbaseConfiguration.create());// 创建 HTableDescriptor 实例HTableDescriptor tableDesc = new HTableDescriptor(tableName);// 通过 HTableDescriptor 实例和散列值二维数组创建带有预分区的 Hbase 表hAdmin.createTable(tableDesc, splitKeys); RowKey 一条数据的唯一标识就是 RowKey，那么这条数据存储于哪个分区，取决于 RowKey 处于哪个一个预分区的区间内，设计 RowKey 的主要目的 ，就是让数据均匀的分布于所有的 region 中，在一定程度上防止数据倾斜。 HASH，在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的 rowKey 来 Hash 后作为每个分区的临界值。 内存优化 HBase 操作过程中需要大量的内存开销，毕竟 Table 是可以缓存在内存中的，一般会分配整个可用内存的 70%给 HBase 的 Java 堆。但是不建议分配非常大的堆内存，因为 GC 程持续太久会导致 RegionServer 处于长期不可用状态，一般 16~48G 内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。 属性：dfs.support.append 解释：开启 HDFS 追加同步，可以优秀的配合 HBase 的数据同步和持久化。 默认值为 true。 属性：dfs.datanode.max.transfer.threads 解释：HBase 一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作， 设置为 4096 或者更高。 默认值：4096 属性：dfs.image.transfer.timeout 解释：如果对于某一次数据操作来讲，延迟非常高，socket 需要等待更长的时间，建议把该值设置为更大的值（默认 60000 毫秒），以确保 socket 不会被 timeout 掉。 属性：mapreduce.map.output.compress mapreduce.map.output.compress.codec 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为 true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec 或者其他压缩方式。 属性：Hbase.regionserver.handler.count 解释：默认值为 30，用于指定 RPC 监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 属性：hbase.hregion.max.filesize 解释：默认值 10737418240（10GB），如果需要运行 HBase 的 MR 任务，可以减小此值， 因为一个 region 对应一个 map 任务，如果单个 region 过大，会导致 map 任务执行时间过长。该值的意思就是，如果 HFile 的大小达到这个数值，则这个 region 会被切分为两个 Hfile。 属性：hbase.client.write.buffer 解释：用于指定 Hbase 客户端缓存，增大该值可以减少 RPC 调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少 RPC 次数的目的。 属性：hbase.client.scanner.caching 解释：用于指定 scan.next 方法获取的默认行数，值越大，消耗内存越大。 flush、compact、split 机制 当 MemStore 达到阈值，将 Memstore 中的数据 Flush 进 Storefile；compact 机制则是把 flush 出来的小文件合并成大的 Storefile 文件。split 则是当 Region 达到阈值，会把过大的 Region 一分为二。 hbase.hregion.memstore.flush.size：134217728 这个参数的作用是当单个 HRegion 内所有的 Memstore 大小总和超过指定值时，flush 该 HRegion 的所有 memstore。RegionServer 的 flush 是通过将请求添加一个队列，模拟生 产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求 时，可能会导致内存陡增，最坏的情况是触发 OOM hbase.regionserver.global.memstore.upperLimit：0.4 hbase.regionserver.global.memstore.lowerLimit：0.38 当 MemStore 使用内存总量达到hbase.regionserver.global.memstore.upperLimit 指定值时，将会有多个 MemStores flush 到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到 MemStore 使用内存略小于 lowerLimit 项目实战 微博内容的浏览，数据库表设计； 用户社交体现：关注用户，取关用户； 拉取关注的人的微博内容。 表的创建 123456789101112131415161718192021222324252627282930313233343536373839// 创建命名空间以及表名的定义// 获取配置 confprivate Configuration conf = HbaseConfiguration.create();// 微博内容表的表名private static final byte[] TABLE_CONTENT = Bytes.toBytes(&quot;weibo:content&quot;);// 用户关系表的表名private static final byte[] TABLE_RELATIONS = Bytes.toBytes(&quot;weibo:relations&quot;);// 微博收件箱表的表名private static final byte[] TABLE_RECEIVE_CONTENT_EMAIL = Bytes.toBytes(&quot;weibo:receive_content_email&quot;);public void initNamespace()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); // 命名空间类似于关系型数据库中的 schema，可以想象成文件夹 NamespaceDescriptor weibo = NamespaceDescriptor .create(&quot;weibo&quot;) .addConfiguration(&quot;creator&quot;, &quot;wingo&quot;) .addConfiguration(&quot;create_time&quot;, System.currentTimeMillis() + &quot;&quot;) .build(); admin.createNamespace(weibo); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建微博内容表 12345678910111213141516171819202122232425262728293031323334353637383940414243/** 表结构Table Name weibo:contentRowKey 用户 ID_时间戳ColumnFamily infoColumnLabel 内容Version 1 个版本*/public void createTableContent()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); // 创建表表述 HTableDescriptor content = new HTableDescriptor(TableName.valueOf(TABLE_CONTENT)); // 创建列族描述 HColumnDescriptor info = new HColumnDescriptor(Bytes.toBytes(&quot;info&quot;)); // 设置块缓存 info.setBlockCacheEnabled(true); // 设置块缓存大小 info.setBlocksize(2097152); // 设置压缩方式 // info.setCompressionType(Algorithm.SNAPPY); // 设置版本数量 info.setMaxVersions(1); info.setMinVersions(1); content.addFamily(info); admin.createTable(content); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建用户关系表 12345678910111213141516171819202122232425262728293031323334353637383940414243/** 表结构Table Name weibo:relationsRowKey 用户 IDColumnFamily attends、fansColumnLabel 关注用户 ID，粉丝用户 IDColumnValue 用户 IDVersion 1 个版本*/public void createTableRelations()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); HTableDescriptor relations = new HTableDescriptor(TableName.valueOf(TABLE_RELATIONS)); HColumnDescriptor attends = new HColumnDescriptor(Bytes.toBytes(&quot;attends&quot;)); attends.setBlockCacheEnabled(true); attends.setBlocksize(2097152); attends.setMaxVersions(1); attends.setMinVersions(1); HColumnDescriptor fans = new HColumnDescriptor(Bytes.toBytes(&quot;fans&quot;)); fans.setBlockCacheEnabled(true); fans.setBlocksize(2097152); fans.setMaxVersions(1); fans.setMinVersions(1); relations.addFamily(attends); relations.addFamily(fans); admin.createTable(relations); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建微博收件箱表 1234567891011121314151617181920212223242526272829303132333435363738/** 表结构Table Name weibo:receive_content_emailRowKey 用户 IDColumnFamily infoColumnLabel 用户 IDColumnValue 取微博内容的 RowKeyVersion 1000*/public void createTableReceiveContentEmail()&#123; HbaseAdmin admin = null; try &#123; admin = new HbaseAdmin(conf); HTableDescriptor receive_content_email = new HTableDescriptor(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); HColumnDescriptor info = new HColumnDescriptor(Bytes.toBytes(&quot;info&quot;)); info.setBlockCacheEnabled(true); info.setBlocksize(2097152); info.setMaxVersions(1000); info.setMinVersions(1000); receive_content_email.addFamily(info);; admin.createTable(receive_content_email); &#125; catch (MasterNotRunningException e) &#123; e.printStackTrace(); &#125; catch (ZooKeeperConnectionException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != admin)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 功能实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/**微博内容表中添加 1 条数据微博收件箱表对所有粉丝用户添加数据：微博内容的 RowKey*/public class Message &#123; private String uid; private String timestamp; private String content; public String getUid() &#123; return uid; &#125; public void setUid(String uid) &#123; this.uid = uid; &#125; public String getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(String timestamp) &#123; this.timestamp = timestamp; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125; @Override public String toString() &#123; return &quot;Message [uid=&quot; + uid + &quot;, timestamp=&quot; + timestamp + &quot;, content=&quot; + content + &quot;]&quot;; &#125;&#125;public void publishContent(String uid, String content)&#123; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); // 微博内容表中添加 1 条数据，首先获取微博内容表描述 HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT)); // 组装 Rowkey long timestamp = System.currentTimeMillis(); String rowKey = uid + &quot;_&quot; + timestamp; Put put = new Put(Bytes.toBytes(rowKey)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;content&quot;), timestamp, Bytes.toBytes(content)); contentTBL.put(put); // 向微博收件箱表中加入发布的 Rowkey // 查询用户关系表，得到当前用户有哪些粉丝 HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS)); // 取出目标数据 Get get = new Get(Bytes.toBytes(uid)); // 指定取出的目标数据的值 get.addFamily(Bytes.toBytes(&quot;fans&quot;)); Result result = relationsTBL.get(get); // 初始化一个二进制数组用于存放数据 List&lt;byte[]&gt; fans = new ArrayList&lt;byte[]&gt;(); // 遍历取出当前发布微博的用户的所有粉丝数据 for(Cell cell : result.rawCells())&#123; fans.add(CellUtil.cloneQualifier(cell)); &#125; // 如果该用户没有粉丝，则直接 return if(fans.size() &lt;= 0) return; // 开始操作收件箱表 HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); for(byte[] fan : fans)&#123; Put fanPut = new Put(fan); fanPut.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(uid), timestamp, Bytes.toBytes(rowKey)); puts.add(fanPut); &#125; recTBL.put(puts); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != connection)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 添加关注用户 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/**在微博用户关系表中，对当前主动操作的用户添加新关注的好友在微博用户关系表中，对被关注的用户添加新的粉丝微博收件箱表中添加所关注的用户发布的微博*/public void addAttends(String uid, String... attends)&#123; // 参数过滤 if(attends == null || attends.length &lt;= 0 || uid == null || uid.length() &lt;= 0)&#123; return; &#125; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); // 用户关系表操作对象（连接到用户关系表） HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS)); List&lt;Put&gt; puts = new ArrayList&lt;Put&gt;(); // 在微博用户关系表中，添加新关注的好友 Put attendPut = new Put(Bytes.toBytes(uid)); for(String attend : attends)&#123; // 为当前用户添加关注的人 attendPut.add(Bytes.toBytes(&quot;attends&quot;), Bytes.toBytes(attend), Bytes.toBytes(attend)); // 为被关注的人，添加粉丝 Put fansPut = new Put(Bytes.toBytes(attend)); fansPut.add(Bytes.toBytes(&quot;fans&quot;), Bytes.toBytes(uid), Bytes.toBytes(uid)); // 将所有关注的人一个一个的添加到 puts（List）集合中 puts.add(fansPut); &#125; puts.add(attendPut); relationsTBL.put(puts); // 微博收件箱添加关注的用户发布的微博内容（content）的 rowkey HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT)); Scan scan = new Scan(); // 用于存放取出来的关注的人所发布的微博的 rowkey List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;(); for(String attend : attends)&#123; // 过滤扫描 rowkey，即：前置位匹配被关注的人的 uid_ RowFilter filter = new RowFilter (CompareFilter.CompareOp.EQUAL, new SubstringComparator(attend + &quot;_&quot;)); // 为扫描对象指定过滤规则 scan.setFilter(filter); // 通过扫描对象得到 scanner ResultScanner result = contentTBL.getScanner(scan); // 迭代器遍历扫描出来的结果集 Iterator&lt;Result&gt; iterator = result.iterator(); while(iterator.hasNext())&#123; // 取出每一个符合扫描结果的那一行数据 Result r = iterator.next(); for(Cell cell : r.rawCells())&#123; // 将得到的 rowkey 放置于集合容器中 rowkeys.add(CellUtil.cloneRow(cell)); &#125; &#125; &#125; // 将取出的微博 rowkey 放置于当前操作用户的收件箱中 if(rowkeys.size() &lt;= 0) return; // 得到微博收件箱表的操作对象 HTableInterface recTBL =connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); // 用于存放多个关注的用户的发布的多条微博 rowkey 信息 List&lt;Put&gt; recPuts = new ArrayList&lt;Put&gt;(); for(byte[] rk : rowkeys)&#123; Put put = new Put(Bytes.toBytes(uid)); // uid_timestamp String rowKey = Bytes.toString(rk); // 截取 uid String attendUID = rowKey.substring(0, rowKey.indexOf(&quot;_&quot;)); long timestamp = Long.parseLong(rowKey.substring(rowKey.indexOf(&quot;_&quot;) + 1)); // 将微博 rowkey 添加到指定单元格中 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(attendUID), timestamp, rk); recPuts.add(put); &#125; recTBL.put(recPuts); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; if(null != connection)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 取关用户 12345678910111213141516171819202122232425262728293031323334// 即关注用户的反向操作public void removeAttends(String uid, String... attends)&#123; // 过滤数据 if(uid == null || uid.length() &lt;= 0 || attends == null || attends.length &lt;= 0) return; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); // 在微博用户关系表中，删除已关注的好友 HTableInterface relationsTBL = connection.getTable(TableName.valueOf(TABLE_RELATIONS)); // 待删除的用户关系表中的所有数据 List&lt;Delete&gt; deletes = new ArrayList&lt;Delete&gt;(); // 当前取关操作者的 uid 对应的 Delete 对象 Delete attendDelete = new Delete(Bytes.toBytes(uid)); // 遍历取关，同时每次取关都要将被取关的人的粉丝 -1 for(String attend : attends)&#123; attendDelete.deleteColumn(Bytes.toBytes(&quot;attends&quot;), Bytes.toBytes(attend)); Delete fansDelete = new Delete(Bytes.toBytes(attend)); fansDelete.deleteColumn(Bytes.toBytes(&quot;fans&quot;), Bytes.toBytes(uid)); deletes.add(fansDelete); &#125; deletes.add(attendDelete); relationsTBL.delete(deletes); // 收件箱表中删除取关的人的微博 rowkey HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); Delete recDelete = new Delete(Bytes.toBytes(uid)); for(String attend : attends)&#123; recDelete.deleteColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(attend)); &#125; recTBL.delete(recDelete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 获取关注的人的微博内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 即通过微博收件箱中的 RowKey 获取对应的微博内容public List&lt;Message&gt; getAttendsContent(String uid)&#123; HConnection connection = null; try &#123; connection = HConnectionManager.createConnection(conf); HTableInterface recTBL = connection.getTable(TableName.valueOf(TABLE_RECEIVE_CONTENT_EMAIL)); // 从收件箱中取得微博 rowKey Get get = new Get(Bytes.toBytes(uid)); // 设置最大版本号 get.setMaxVersions(5); List&lt;byte[]&gt; rowkeys = new ArrayList&lt;byte[]&gt;(); Result result = recTBL.get(get); for(Cell cell : result.rawCells())&#123; rowkeys.add(CellUtil.cloneValue(cell)); &#125; // 根据取出的所有 rowkey 去微博内容表中检索数据 HTableInterface contentTBL = connection.getTable(TableName.valueOf(TABLE_CONTENT)); List&lt;Get&gt; gets = new ArrayList&lt;Get&gt;(); // 根据 rowkey 取出对应微博的具体内容 for(byte[] rk : rowkeys)&#123; Get g = new Get(rk); gets.add(g); &#125; // 得到所有的微博内容的 result 对象 Result[] results = contentTBL.get(gets); List&lt;Message&gt; messages = new ArrayList&lt;Message&gt;(); for(Result res : results)&#123; for(Cell cell : res.rawCells())&#123; Message message = new Message(); String rowKey = Bytes.toString(CellUtil.cloneRow(cell)); String userid = rowKey.substring(0, rowKey.indexOf(&quot;_&quot;)); String timestamp = rowKey.substring(rowKey.indexOf(&quot;_&quot;) + 1); String content = Bytes.toString(CellUtil.cloneValue(cell)); message.setContent(content); message.setTimestamp(timestamp); message.setUid(userid); messages.add(message); &#125; &#125; return messages; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return null;&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://wingowen.github.io/tags/HBase/"}]},{"title":"Hive","slug":"大数据/Hive","date":"2020-04-28T03:27:12.000Z","updated":"2023-09-20T07:35:51.828Z","comments":true,"path":"2020/04/28/大数据/Hive/","link":"","permalink":"https://wingowen.github.io/2020/04/28/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hive/","excerpt":"Hive 基本介绍及简单使用。","text":"Hive 基本介绍及简单使用。 基本概念 Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。 本质上是将 HQL 转化为 MapReduce 程序。 Hive 处理的数据存储在 HDFS； Hive 分析数据底层的实现是 MapReduce； 执行程序运行在 Yarn 上 HQL VS SQL Hive RDBMS 查询语言 HQL SQL 数据存储 HDFS LOCAL FS 执行 MapReduce Executor 执行延迟 高 低 处理数据规模 大 小 索引 位图索引 复杂索引 用户接口 Client： CLI（hive shell）、JDBC / ODBC（java 访问 hive）、WEBUI（浏览器访问 hive）。 元数据 Metastore： 元数据包括表名、表所属的数据（默认是 default）、表的拥有者、列 / 分区字段、表的类型（是否是外部表）、表的数据所在目录等；默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore。 Hadoop 使用 HDFS 进行存储，使用 MapReduce 进行计算。 驱动器 Driver 解析器 SQL Parser：将 SQL 字符串转换成抽象语法树 AST； 编译器 Physical Plan： 将 AST 编译生成逻辑执行计划； 优化器 Query Optimizer：对逻辑执行计划进行优化； 执行器 Execution：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来 说，就是 MR / Spark。 Hive 通过给用户提供的一系列交互接口，接收到用户的指令（SQL），使用自己的 Driver， 结合元数据（MetaStore），将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。 安装配置 1234567891011121314# hive 安装tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/mv apache-hive-1.2.1-bin/ hivemv hive-env.sh.template hive-env.shvi hive-env.sh # export HADOOP_HOME=/opt/module/hadoop-2.7.2 # export HIVE_CONF_DIR=/opt/module/hive/conf# 启动 hive 前必须启动集群sbin/start-dfs.shsbin/start-yarn.sh# 创建目录bin/hadoop fs -mkdir -p /user/hive/warehousebin/hadoop fs -chmod g+w /user/hive/warehouse# 或者直接在配置文件中关闭权限检查 123456&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.permissions.enable&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 基本操作 1234567891011bin/hive# 进入 hive shellhive&gt; show databases;hive&gt; use default;hive&gt; show tables;hive&gt; create table student(id int, name string);hive&gt; show tables;hive&gt; desc student;hive&gt; insert into student values(1000,&quot;ss&quot;);hive&gt; select * from student;hive&gt; quit; 文件导入 1234# 将 /opt/module/datas/student.txt 文件导入 hive 的 student(id int, name string) 表中hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\t&#x27;;hive&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table student; 再打开一个客户端窗口启动 hive，会产生 java.sql.SQLException 异常。原因是，Metastore 默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore。 MySQL 安装 1234567891011121314151617181920212223242526272829303132# 检查环境rpm -qa|grep mysqlrpm -e --nodepsunzip mysql-libs.zip # MySQL-client-5.6.24-1.el6.x86_64.rpm # mysql-connector-java-5.1.27.tar.gz 驱动包 # MySQL-server-5.6.24-1.el6.x86_64.rpm# 安装服务rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm# 获取随机密码cat /root/.mysql_secret# 检查服务状态service mysql statusservice mysql start# 安装客户端rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm# 利用好随机密码进行登录mysql -uroot -pOEXaQuS8IWkG19Xs# 修改密码mysql&gt;SET PASSWORD=PASSWORD(&#x27;000000&#x27;);# 修改 user 表中的主机配置mysql&gt;show databases;mysql&gt;use mysql;mysql&gt;show tables;mysql&gt;desc user;mysql&gt;select User, Host, Password from user;mysql&gt;update user set host=&#x27;%&#x27; where host=&#x27;localhost&#x27;;mysql&gt;delete from user where Host=&#x27;127.0.0.1&#x27;;mysql&gt;delete from user where Host=&#x27;::1&#x27;;# 配置生效mysql&gt;flush privileges;mysql&gt;quit; 元数据配置 12# 拷贝所需驱动cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/ 123456789101112131415161718192021222324252627&lt;!-- 在 /opt/module/hive/conf 目录下创建一个 hive-site.xml --&gt;&lt;!-- 拷贝官方文档的配置参数 --&gt;&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://[ip_address]:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 查看 MySQL 数据库，显示增加了 metastore 数据库。 交互命令 1234567891011121314# 查看帮助bin/hive -help# 不进入 hive 的交互窗口执行 sql 语句bin/hive -e &quot;select id from student;&quot;# 执行脚本中 sql 语句bin/hive -f /opt/module/datas/hivef.sql# 执行脚本，并写出结果bin/hive -f /opt/module/datas/hivef.sql &gt; /opt/module/datas/hive_result.txt# 在 hive cli 命令窗口中查看 hdfs 文件系统hive(default)&gt;dfs -ls /;# 在 hive cli 命令窗口中查看本地文件系统hive(default)&gt;! ls /opt/module/datas;# 查看在 hive 中输入的所有历史命令，用户家目录下cat .hivehistory 属性配置 仓库路径 Default 数据仓库的最原始位置是在 hdfs 上的：/user/hive/warehouse 路径下； 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。 改 default 数据仓库原始位置（将 hive-default.xml.template 如下配置信息拷贝到 hive-site.xml 文件中） 12345678910111213141516&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;!-- 显示数据表头信息 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 显示当前数据库名称信息 --&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 12# 配置同组用户有执行权限bin/hdfs dfs -chmod g+w /user/hive/warehouse 日志信息 12345678910# 方式一mv hive-log4j.properties.template hive-log4j.propertiesvi hive-log4j.properties # hive.log.dir=/opt/module/hive/logs# 方式二，仅对本次 hive 启动有效bin/hive -hiveconf hive.log.dir=/opt/module/hive/logs# 方式三，仅对本次 hive 启动有效hive (default)&gt; set hive.log.dir=/opt/module/hive/logs;# 查看所有参数配置hive (default)&gt; set [某一参数] 数据类型 简单类型 类型 描述 示例 boolean true / false TRUE tinyint 1 字节的有符号整数 -128~127 1Y smallint 2 个字节的有符号整数，-32768~32767 1S int 4 个字节的带符号整数 1 bigint 8 字节带符号整数 1L float 4 字节单精度浮点数 1.0 double 8 字节双精度浮点数 1.0 deicimal 任意精度的带符号小数 1.0 String 字符串，变长 “a”,’b’ varchar 变长字符串 “a”,’b’ char 固定长度字符串 “a”,’b’ binary 字节数组 无法表示 timestamp 时间戳，纳秒精度 122327493795 date 日期 ‘2018-04-07’ 复杂类型 类型 描述 示例 array 有序的的同类型的集合 array(1,2) map key-value，key 必须为原始类型，value 可以任意类型 map(‘a’,1,’b’,2) struct 字段集合,类型可以不同 struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0) 实例操作 123456789101112&#123; &quot;name&quot;: &quot;songsong&quot;, &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , // 列表 Array &quot;children&quot;: &#123; // 键值 Map &quot;xiao song&quot;: 18 , &quot;xiaoxiao song&quot;: 19 &#125; &quot;address&quot;: &#123; // 结构 Struct, &quot;street&quot;: &quot;hui long guan&quot; , &quot;city&quot;: &quot;beijing&quot;&#125;&#125; 基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing // 第一条 JSONyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing // 第二条 JSON MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用 “_”。 1234567891011create table test( name string, friends array&lt;string&gt;, children map&lt;string, int&gt;, address struct&lt;street:string, city:string&gt;)row format delimitedfields terminated by &#x27;,&#x27;collection items terminated by &#x27;_&#x27;map keys terminated by &#x27;:&#x27;lines terminated by &#x27;\\n&#x27;; 12345678# 导入数据hive (default)&gt; load data local inpath ‘/opt/module/datas/test.txt’ into table test# 访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式select friends[1],children[&#x27;xiao song&#x27;],address.city from test where name=&quot;songsong&quot;;OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s) 类型转化 Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式 使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如， 某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST 操作。 隐式类型转换规则： 任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT； 所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE； TINYINT、SMALLINT、INT 都可以转换为 FLOAT； BOOLEAN 类型不可以转换为任何其它的类型。 可以使用 CAST 操作显示进行数据类型转换 例如 CAST(‘1’ AS INT) 将把字符串 ‘1’ 转换成整数 1；如果强制类型转换失败，如执行 CAST(‘X’ AS INT)，表达式返回空值 NULL。 DDL Data Definition Language 数据定义。 库操作 创建 创建一个数据库，数据库在 HDFS 上的默认存储路径是 /user/hive/warehouse/*.db 123create database if not exists db_hive;# 指定库存储路径create database db_hive location &#x27;/db_hive.db&#x27;; 显示 123456# 显示数据库信息desc database db_hive;# 显示数据库详细信息 extendeddesc database extended db_hive;# 切换当前数据库use db_hive; 修改 用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。 数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。修改当前正在使用的数据库，要先退出使用. 1alter database db_hive set dbproperties(&#x27;createtime&#x27;=&#x27;20170830&#x27;); 删除 1234# 采用 if exists 判断数据库是否存在drop database if exists [db_name];# 如果数据库不为空，可以采用 cascade 命令强制删除drop database [db_name] cascade; 表操作 表类型 123456789101112CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path]# 重命名ALTER TABLE table_name RENAME TO new_table_name EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）。 Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。 在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 PARTITIONED 表示根据某一个 key （不在 create table 里面）对数据进行分区，体现在 HDFS 上就是 table 目录下有 n 个不同的分区文件夹（country=China,country=USA）。 CLUSTERED 对于每一个表（table）或者分区， Hive 可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive 也是针对某一列进行桶的组织。Hive 采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。 使取样（sampling）更高效。 ROW FORMAT 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。SerDe 是 Serialize / Deserilize 的简称，目的是用于序列化和反序列化。 STORED AS 指定存储文件类型。常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、 RCFILE（列式存储格式文件） 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩， 使用 STORED AS SEQUENCEFILE。 LOCATION 指定表在 HDFS 上的存储位置。 LIKE 允许用户复制现有的表结构，但是不复制数据。 内部表 默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项 hive.metastore.warehouse.dir（例如，/user/hive/warehouse）所定义的目录的子目录下。当删除一个管理表时，Hive 也会删除这个表中数据。内部表不适合和其他工具共享数据。 12345678910111213# 创建内部表create table if not exists student(id int, name string)row format delimited fields terminated by &#x27;\\t&#x27;stored as textfilelocation &#x27;/user/hive/warehouse/student&#x27;;# 根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student01 as select id, name from student;# 根据已经存在的表结构创建表create table if not exists student02 like student;# 查询表的类型desc formatted student01 外部表 因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这 份数据，不过描述表的元数据信息会被删除掉。 每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础 上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT 进入内部表。 12345678910111213141516171819202122232425262728293031323334# 创建外部表部门表create external table if not exists default.dept( deptno int, dname string, loc int)row format delimited fields terminated by &#x27;\\t&#x27;;# 创建外部表员工表create external table if not exists default.emp( empno int, ename string, job string, mgr int, hiredate string, sal double, comm double, deptno int)row format delimited fields terminated by &#x27;\\t&#x27;;# 查询表结构hive (default)&gt; show tables;OKtab_namedeptemp# 向外部表中导入数据hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept;hive (default)&gt; load data local inpath &#x27;/opt/module/datas/emp.txt&#x27; into table default.emp;# 查询结果hive (default)&gt; select * from emp;hive (default)&gt; select * from dept;# 查看表的类型hive (default)&gt; desc formatted dept;Table Type: EXTERNAL_TABLE 表转换 只能用单引号，严格区分大小写，如果不是完全符合，那么只会添加 K V 而不生效。 1234567891011# 查询表的类型hive (default)&gt; desc formatted student;Table Type: MANAGED_TABLE# 修改内部表 student 为外部表alter table student set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;TRUE&#x27;);hive (default)&gt; desc formatted student;Table Type: EXTERNAL_TABLE# 修改外部表 student 为内部表alter table student set tblproperties(&#x27;EXTERNAL&#x27;=&#x27;FALSE&#x27;);hive (default)&gt; desc formatted student;Table Type: MANAGED_TABLE 分区表 分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 一级分区 12345678910111213141516171819202122232425262728293031323334353637383940# 引入分区表（需要根据日期对日志进行管理）/user/hive/warehouse/log_partition/20170702/20170702.log/user/hive/warehouse/log_partition/20170703/20170703.log/user/hive/warehouse/log_partition/20170704/20170704.log# 创建分区表语法hive (default)&gt; create table dept_partition( deptno int, dname string, loc string)partitioned by (month string)row format delimited fields terminated by &#x27;\\t&#x27;;# 分区域导入数据hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into tabledefault.dept_partition partition(month=&#x27;201709&#x27;);hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into tabledefault.dept_partition partition(month=&#x27;201708&#x27;);hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into tabledefault.dept_partition partition(month=&#x27;201707’);# 单分区查询hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;;# 多分区联合查询 union（排序） or in 三种方式hive (default)&gt; select * from dept_partition where month=&#x27;201709&#x27;unionselect * from dept_partition where month=&#x27;201708&#x27;unionselect * from dept_partition where month=&#x27;201707&#x27;;# 增加分区hive (default)&gt; alter table dept_partition add partition(month=&#x27;201706&#x27;) ;# 同时创建多个分区 用空格分开hive (default)&gt; alter table dept_partition add partition(month=&#x27;201705&#x27;) partition(month=&#x27;201704&#x27;);# 删除单个分区hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201704&#x27;);# 同时删除多个分区 用逗号分开hive (default)&gt; alter table dept_partition drop partition (month=&#x27;201705&#x27;), partition (month=&#x27;201706&#x27;);# 查看分区表有多少分区hive&gt; show partitions dept_partition;# 查看分区表结构hive&gt; desc formatted dept_partition; # Partition Information # col_name data_type # comment month string 二级分区 1234567891011121314151617181920212223242526272829303132333435# 创建二级分区表hive (default)&gt; create table dept_partition2( deptno int, dname string, loc string)partitioned by (month string, day string)row format delimited fields terminated by &#x27;\\t&#x27;;# 加载数据到二级分区表中hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table default.dept_partition2 partition(month=&#x27;201709&#x27;, day=&#x27;13&#x27;);hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;13&#x27;;# 把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式# 方式一：上传数据后修复# 上传数据hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; # 查询数据（查询不到刚上传的数据）hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;; # 执行修复命令hive&gt; msck repair table dept_partition2; # 再次查询数据hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;12&#x27;;# 方式二：上传数据后添加分区# 上传数据hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=11;hive (default)&gt; dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=11; # 执行添加分区hive (default)&gt; alter table dept_partition2 add partition(month=&#x27;201709&#x27;,day=&#x27;11&#x27;); # 查询数据hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;11&#x27;;# 方式三：上传数据后 load 数据到分区# 创建目录hive (default)&gt; dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=10; # 上传数据hive (default)&gt; load data local inpath &#x27;/opt/module/datas/dept.txt&#x27; into table dept_partition2 partition(month=&#x27;201709&#x27;,day=&#x27;10&#x27;);# 查询数据hive (default)&gt; select * from dept_partition2 where month=&#x27;201709&#x27; and day=&#x27;10&#x27;; 列信息 12345# 更新列ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)# ADD 是代表新增一字段，字段位置在所有列后面（partition 列前），REPLACE 则是表示替换表中所有字段 删除表 1hive (default)&gt; drop table [table_name]; DML 数据导入 123456789# 创建一张表hive (default)&gt; create table student(id string, name string) row format delimited fields terminated by &#x27;\\t&#x27;;# 加载本地文件到 hivehive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table default.student;# 加载 HDFS 文件到 hive 中# 上传文件到 HDFShive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;# 加载 HDFS 上的数据hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; into table default.student; 加载数据覆盖表中已有的数据。 1234# 上传文件到 HDFShive (default)&gt; dfs -put /opt/module/datas/student.txt /user/atguigu/hive;# 加载数据覆盖表中已有的数据hive (default)&gt; load data inpath &#x27;/user/atguigu/hive/student.txt&#x27; overwrite into table default.student; 通过查询语句向表中插入数据（Insert）。 12345678910111213# 创建一张分区表hive (default)&gt; create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by &#x27;\\t&#x27;;# 基本插入数据hive (default)&gt; insert into table student partition(month=&#x27;201709&#x27;) values(1,&#x27;wangwu&#x27;);# 基本模式插入（根据单张表查询结果）hive (default)&gt; insert overwrite table student partition(month=&#x27;201708&#x27;) select id, name from student where month=&#x27;201709&#x27;;# 多插入模式（根据多张表查询结果）hive (default)&gt; from student insert overwrite table student partition(month=&#x27;201707&#x27;) select id, name where month=&#x27;201709&#x27; insert overwrite table student partition(month=&#x27;201706&#x27;) select id, name where month=&#x27;201709&#x27;; 查询语句中创建表并加载数据（As Select）。 12# 根据查询结果创建表（查询的结果会添加到新创建的表中）create table if not exists student_new as select id, name from student; 创建表时通过 Location 指定加载数据路径. 12345678910# 创建表，并指定在 hdfs 上的位置hive (default)&gt; create table if not exists student( id int, name string)row format delimited fields terminated by &#x27;\\t&#x27;location &#x27;/user/hive/warehouse/student&#x27;;# 上传数据到 hdfs 上hive (default)&gt; dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student;# 查询数据hive (default)&gt; select * from student; Import 数据（export 到处的数据）到指定 Hive 表中。 1hive (default)&gt; import table student partition(month=&#x27;201709&#x27;) from &#x27;/user/hive/warehouse/export/student&#x27;; 数据导出 Insert 导出。 123456# 将查询的结果导出到本地hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27; select * from student;# 将查询的结果格式化导出到本地hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/export/student&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\t&#x27; select * from student;# 将查询的结果导出到 HDFS 上（没有 local）hive (default)&gt; insert overwrite directory &#x27;/user/wingo/student&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\t&#x27; select * from student; Hadoop 命令导出到本地。 1hive (default)&gt; dfs -get /user/hive/warehouse/student/month=201709/000000_0 /opt/module/datas/export/student.txt; Hive Shell 命令导出。 1bin/hive -e &#x27;select * from default.student;&#x27; &gt; /opt/module/datas/export/student.txt; Export 导出到 HDFS 上。 1hive (default) &gt;export table default.student to &#x27;/user/hive/warehouse/export/student&#x27;; 清除表 Truncate 只能删除管理表，不能删除外部表中数据。 1hive (default)&gt; truncate table student; 查询 基本查询 123456# 全表查询hive (default)&gt; select * from emp;# 选择特定列查询hive (default)&gt; select empno, ename from emp;# 列别名hive (default)&gt; select ename AS name, deptno dn from emp; 运算符查询 12# 算术运算符：查询出所有员工的薪水后加 1 显示hive (default)&gt; select sal+1 from emp; 函数查询 12345678910# 求总行数（count）hive (default)&gt; select count(*) cnt from emp;# 求工资的最大值（max）hive (default)&gt; select max(sal) max_sal from emp;# 求工资的最小值（min）hive (default)&gt; select min(sal) min_sal from emp;# 求工资的总和（sum）hive (default)&gt; select sum(sal) sum_sal from emp;# 求工资的平均值（avg）hive (default)&gt; select avg(sal) avg_sal from emp; Limit 语句 典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。 1hive (default)&gt; select * from emp limit 5; Where 12# 查询出薪水大于 1000 的所有员工hive (default)&gt; select * from emp where sal &gt;1000; 比较运算符（Between/In/ Is Null），这些操作符同样可以用于 JOIN…ON 和 HAVING 语句中。 12345678# 查询出薪水等于 5000 的所有员工hive (default)&gt; select * from emp where sal =5000;# 查询工资在 500 到 1000 的员工信息hive (default)&gt; select * from emp where sal between 500 and 1000;# 查询 comm 为空的所有员工信息hive (default)&gt; select * from emp where comm is null;# 查询工资是 1500 和 5000 的员工信息hive (default)&gt; select * from emp where sal IN (1500, 5000); Like 和 RLike % 代表零个或多个字符（任意个字符）；_ 代表一个字符 RLIKE 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件。 123456# 查找以 2 开头薪水的员工信息hive (default)&gt; select * from emp where sal LIKE &#x27;2%&#x27;;# 查找第二个数值为 2 的薪水的员工信息hive (default)&gt; select * from emp where sal LIKE &#x27;_2%&#x27;;# 查找薪水中含有 2 的员工信息hive (default)&gt; select * from emp where sal RLIKE &#x27;[2]&#x27;; 逻辑运算符（And/Or/Not） 123456# 查询薪水大于 1000，部门是 30hive (default)&gt; select * from emp where sal&gt;1000 and deptno=30;# 查询薪水大于 1000，或者部门是 30hive (default)&gt; select * from emp where sal&gt;1000 or deptno=30;# 查询除了 20 部门和 30 部门以外的员工信息hive (default)&gt; select * from emp where deptno not IN(30, 20); 分组 GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。 1234# 计算 emp 表每个部门的平均工资hive (default)&gt; select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;# 计算 emp 每个部门中每个岗位的最高薪水hive (default)&gt; select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job; Having 语句 where 针对表中的列发挥作用，查询数据；Having 针对查询结果中的列发挥作用， 筛选数据。 12# 求每个部门的平均薪水大于 2000 的部门hive (default)&gt; select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt; 2000; Join 是只支持等值连接，不支持非等值连接 123456789101112# 根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门编号；hive (default)&gt; select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno;# 合并员工表和部门表hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;# 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;# 左外连接：JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;# 右外连接：JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;# 满外连接：将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。hive (default)&gt; select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno; 多表连接，大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。Hive 总是按照从左到右的 顺序执行的。 排序 全局排序 Order By：全局排序，一个 MapReduce。 ASC（ascend）: 升序（默认）；DESC（descend）: 降序。 12345678# 查询员工信息按工资升序排列hive (default)&gt; select * from emp order by sal;# 查询员工信息按工资降序排列hive (default)&gt; select * from emp order by sal desc;# 按照别名排序，按照员工薪水的 2 倍排序hive (default)&gt; select ename, sal*2 twosal from emp order by twosal;# 按照部门和工资升序排序hive (default)&gt; select ename, deptno, sal from emp order by deptno, sal ; 内部排序 每个 MapReduce 内部排序（Sort By）。 Sort By：每个 MapReduce 内部进行排序，对全局结果集来说不是排序。 1234567# 设置 reduce 个数hive (default)&gt; set mapreduce.job.reduces=3;# 查看设置 reduce 个数hive (default)&gt; set mapreduce.job.reduces;# 根据部门编号降序查看员工信息hive (default)&gt; select * from emp sort by empno desc;hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/sortby-result&#x27; select * from emp sort by deptno desc; 分区排序 Distribute By：类似 MR 中 partition，进行分区，结合 sort by 使用。 Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前 1234# 先按照部门编号分区，再按照员工编号降序排序。hive (default)&gt; set mapreduce.job.reduces=3;hive (default)&gt; insert overwrite local directory &#x27;/opt/module/datas/distribute-result&#x27; select * from emp distribute by deptno sort by empno desc; Cluster By 除了具有 Distribute By 的功能外还兼具 Sort By 的功能。但是排序只能是倒序排序，不能指定排序规则为 ASC 或者 DESC。 123hive (default)&gt; select * from emp cluster by deptno;# 等价于hive (default)&gt; select * from emp distribute by deptno sort by deptno; 桶 分区针对的是数据的存储路径；分桶针对的是数据文件。 123456789101112131415161718192021222324252627282930# 第一次尝试# 创建分桶表，按 id 取模create table stu_buck(id int, name string) clustered by(id) into 4 buckets row format delimited fields terminated by &#x27;\\t&#x27;;# 查看表结构hive (default)&gt; desc formatted stu_buck; # Num Buckets: 4# 导入数据到分桶表中，直接 load 不会进行分桶，还是一整个文件，那么通过 MR 导入呢？hive (default)&gt; load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table stu_buck;# 第二次尝试，创建分桶表时，数据通过子查询的方式导入# 创建用于子查询的表create table stu(id int, name string) row format delimited fields terminated by &#x27;\\t&#x27;;# 向普通的 stu 表中导入数据load data local inpath &#x27;/opt/module/datas/student.txt&#x27; into table stu;# 清空 stu_buck 表中数据truncate table stu_buck;select * from stu_buck;# 导入数据到分桶表，通过子查询的方式insert into table stu_buck select id, name from stu;# 发现还是只有一个分桶# 需要设置一个属性hive (default)&gt; set hive.enforce.bucketing=true;hive (default)&gt; set mapreduce.job.reduces=-1;hive (default)&gt; insert into table stu_buck select id, name from stu;# 查询分桶的数据，分桶成功hive (default)&gt; select * from stu_buck; 分桶抽样查询 对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结 果。Hive 可以通过对表进行抽样来满足这个需求。 12# 查询表 stu_buck 中的数据hive (default)&gt; select * from stu_buck tablesample (bucket 1 out of 4 on id); TABLESAMPLE(BUCKET x OUT OF y) y 必须是 table 总 bucket 数的倍数或者因子。hive 根据 y 的大小，决定抽样的比例。 table 总 bucket 数为 4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2 个 bucket 的数据，抽取第 1(x) 个和第 4(x+y) 个 bucket 的数据。 其它 空字段赋值 12# 如果员工的 comm 为 NULL，则用-1 代替hive (default)&gt; select nvl(comm,-1) from emp; CASE WHEN 12345678910111213141516171819202122232425262728# 求出不同部门男女各多少人vi emp_sex.txt # 悟空 A 男 # 大海 A 男 # 宋宋 B 男 # 凤姐 A 女 # 婷姐 B 女 # 婷婷 B 女# 创建 hive 表并导入数据create table emp_sex( name string, dept_id int, sex string)row format delimited fields terminated by &quot;\\t&quot;;load data local inpath &#x27;/opt/module/datas/emp_sex.txt&#x27; into table emp_sex;# 按需求查询数据select dept_id, sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count, sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_countfrom emp_sexgroup by dept_id;# 结果 # A 2 1 # B 1 2 行转列 CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入 字符串； CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数为参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL， 返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间； COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 array 类型字段。 1234567891011121314151617181920212223242526272829vi constellation.txt # 孙悟空 白羊座 A # 大海 射手座 A # 宋宋 白羊座 B # 猪八戒 白羊座 A # 凤姐 射手座 A# 按数据格式创建表create table person_info( name string, constellation string, blood_type string)row format delimited fields terminated by &quot;\\t&quot;;load data local inpath “/opt/module/datas/person_info.txt” into table person_info;# 按需求查询数据select t1.base, concat_ws(&#x27;|&#x27;, collect_set(t1.name)) namefrom (select name,concat(constellation, &quot;,&quot;, blood_type) base # 星座,血型 from person_info ) t1group by t1.base;# 结果 # 射手座,A 大海|凤姐 # 白羊座,A 孙悟空|猪八戒 # 白羊座,B 宋宋 列转行 EXPLODE(col)：将 hive 一列中复杂的 array 或者 map 结构拆分成多行。 LATERAL VIEW 用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias 解释：与 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。 1234567891011121314151617181920212223242526272829303132# 创建本地 movie.txt，导入数据vi movie.txt # 《疑犯追踪》 悬疑,动作,科幻,剧情 # 《Lie to me》 悬疑,警匪,动作,心理,剧情 # 《战狼 2》 战争,动作,灾难create table movie_info( movie string, category array&lt;string&gt;)row format delimited fields terminated by &quot;\\t&quot;collection items terminated by &quot;,&quot;;load data local inpath &quot;/opt/module/datas/movie.txt&quot; into table movie_info;# 按需求查询数据select movie,category_namefrom movie_info lateral view explode(category) table_tmp as category_name;# 结果 # 《疑犯追踪》 悬疑 # 《疑犯追踪》 动作 # 《疑犯追踪》 科幻 # 《疑犯追踪》 剧情 # 《Lie to me》 悬疑 # 《Lie to me》 警匪 # 《Lie to me》 动作 # 《Lie to me》 心理 # 《Lie to me》 剧情 # 《战狼 2》 战争 # 《战狼 2》 动作 # 《战狼 2》 灾难 窗口函数 OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化； CURRENT ROW：当前行； n PRECEDING：往前 n 行数据； n FOLLOWING：往后 n 行数据； UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点，UNBOUNDED FOLLOWING 表示到后面的终点； LAG(col,n)：往前第 n 行数据 LEAD(col,n)：往后第 n 行数据 NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从 1 开始， 对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为 int 类型。 12345678910111213141516# 数据准备name,orderdate,costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 1234567891011121314151617181920212223242526272829303132333435363738394041# 创建本地 business.txt，导入数据vi business.txt# 创建 hive 表并导入数据create table business( name string, orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;load data local inpath &quot;/opt/module/datas/business.txt&quot; into table business;# 查询在 2017 年 4 月份购买过的顾客及总人数select name,count(*) over() # group by 统计每一次条件下的数据；over() 开窗把整个数据即开给你用from businesswhere substring(orderdate,1,7) = &#x27;2017-04&#x27;group by name;# 查询顾客的购买明细及月购买总额select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;# 上述的场景,要将 cost 按照日期进行累加select name,orderdate,cost,sum(cost) over() as sample1,-- 所有行相加sum(cost) over(partition by name) as sample2,-- 按 name 分组，组内数据相加sum(cost) over(partition by name order by orderdate) as sample3,-- 按 name 分组，组内数据累加sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDINGand current row ) as sample4 ,-- 和 sample3 一样,由起点到当前行的聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and currentrow) as sample5, -- 当前行和前面一行做聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1FOLLOWING ) as sample6,-- 当前行和前边一行及后面一行sum(cost) over(partition by name order by orderdate rows between current row andUNBOUNDED FOLLOWING ) as sample7 -- 当前行及后面所有行from business;# 查看顾客上次的购买时间select name,orderdate,cost,lag(orderdate,1,&#x27;1900-01-01&#x27;) over(partition by name order by orderdate ) as time1,lag(orderdate,2) over (partition by name order by orderdate) as time2from business;# 查询前 20% 时间的订单信息select * from (select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business) twhere sorted = 1; Rank RANK() 排序相同时会重复，总数不会变； DENSE_RANK()排序相同时会重复，总数会减少； ROW_NUMBER() 会根据顺序计算。 12345678910111213select name, subject, score,rank() over(partition by subject order by score desc) rp,dense_rank() over(partition by subject order by score desc) drp,row_number() over(partition by subject order by score desc) rmpfrom score;# 结果name subject score rp drp rmp宋宋 英语 84 1 1 1大海 英语 84 1 1 2婷婷 英语 78 3 2 3 函数 123456# 查看系统自带的函数hive&gt; show functions;# 显示自带的函数的用法hive&gt; desc function upper;# 详细显示自带的函数的用法hive&gt; desc function extended upper; 自定义函数 官方文档地址","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://wingowen.github.io/tags/Hive/"}]},{"title":"Hadoop","slug":"大数据/Hadoop","date":"2020-04-25T02:09:27.000Z","updated":"2023-09-20T07:35:51.828Z","comments":true,"path":"2020/04/25/大数据/Hadoop/","link":"","permalink":"https://wingowen.github.io/2020/04/25/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/","excerpt":"大数据技术 Hadoop 的整体入门介绍。","text":"大数据技术 Hadoop 的整体入门介绍。 基本概念 主要解决：海量数据的存储和海量数据的分析计算问题。 一般形式的大数据部门组成结构。 Hadoop 三大发行版本： Apache 版本最原始（最基础）的版本，对于入门学习最好； Cloudera 在大型互联网企业中用的较多； Hortonworks 文档较好。 Hadoop 优势： 高可靠性：Hadoop 底层维护多个数据副本，所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失； 高扩展性：在集群间分配仼务数据，可方便的扩展数以干计的节点； 高效性：在 MapReduce 的思想下,，Hadoop 是并行工作的，以加快任务处理速度； 高容错性：能够自动将失败的任务重新分配。 1.x VS 2.x HDFS NameNode（nn）：存储文件的元数据，如文件名、文件目录结构、文件属性（生成时间、副本数 文件权限），以及每个文件的块列表和块所在的 DataNode 等； DataNode（dn）：在本地文件系统存储文件块数据,以及块数据的校验和。 Secondary NameNode（2nn）：用来监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。 YARN ResourceManager 由两个关键组件 Scheduler 和 ApplicationsManager 组成。 Scheduler 在容量和队列限制范围内负责为运行的容器分配资源。Scheduler 是一个纯调度器（pure scheduler），只负责调度，它不会监视或跟踪应用程序的状态，也不负责重启失败任务，这些全都交给 ApplicationMaster 完成。Scheduler 根据各个应用程序的资源需求进行资源分配。 ApplicationManager 负责接收作业的提交，然后启动一个 ApplicationMaster 容器负责该应用。它也会在ApplicationMaster 容器失败时，重新启动 ApplicationMaster 容器。 Hadoop 2.X 集群中的每个 DataNode 都会运行一个NodeManager 来执行 Yarn 的功能。每个节点上的 NodeManager 执行以下功能： 定时向 ResourceManager 汇报本节点资源使用情况和各个 Container 的运行状况 监督应用程序容器的生命周期 （监控资源）监控、管理和提供容器消耗的有关资源（CPU / 内存）的信息 （监控资源使用情况） （监控容器）监控容器的资源使用情况，杀死失去控制的程序 （启动/停止容器）接受并处理来自 ApplicationMaster 的 Container 启动/停止等各种请求。 提交到 YARN 上的每一个应用都有一个专用的 ApplicationMaster（注意，ApplicationMaster 需要和 ApplicationManager 区分）。ApplicationMaster 运行在应用程序启动时的第一个容器内。ApplicationMaster 会与 ResourceManager 协商获取容器来执行应用中的 mappers 和 reducers，之后会将 ResourceManager 分配的容器资源呈现给运行在每个 DataNode 上的 NodeManager。ApplicationMaster 请求的资源是具体的。包括： 处理作业需要的文件块 为应用程序创建的以容器为单位的资源 容器大小（例如，1GB 内存和一个虚拟核心） 资源在何处分配，这个依据从 NameNode 获取的块存储位置信息（如机器1的节点10上分配4个容器，机器2的节点20上分配8个容器） 资源请求的优先级 ApplicationMaster 是一个特定的框架。例如，MapReduce 程序是 MRAppMaster，spark 是 SparkAppMaster。 Container 是对于资源的抽象, 它封装了某个节点上的多维度资源，如内存、CPU 等。 MapReduce Map 阶段并行处理输入数据； Reduce 阶段对 Map 结果进行汇总。 生态体系 Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive 与传统的数据库（MySql）间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。 Kafka 是一种高吞吐量的分布式发布订阅消息系统。 Storm 用于连续计算，对数据流做连续查询，在计算时就将结果以流的形式输出给用户。 Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数据进行计算。 Oozie 是一个管理 Hdoop 作业（job）的工作流程调度管理系统。 HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。 Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行。 其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。 R 语言是用于统计分析、绘图的语言和操作环境。R 是属于 GNU 系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。 Apache Mahout 是个可扩展的机器学习和数据挖掘库。 Zookeeper 是 Google 的 Chubby 一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 环境搭建 软件安装 123456789101112131415161718192021222324252627282930313233# 在 /opt 目录下创建 module、software 文件夹mkdir modulemkdir software# 修改 module、software 文件夹的所有者chown [group]:[user] module/ software/# 安装 JDK# 检查是否安装 Java 软件rpm -qa | grep javarpm -e [software]# 查看 JDK 安装路径which java# 解压 JDK 到 /opt/module 目录下tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/# 配置 JDK 环境变量pwd /opt/module/jdk1.8.0_144vi /etc/profile # 文件末尾添加 JDK 路径 # export JAVA_HOME=/opt/module/jdk1.8.0_144 # export PATH=$PATH:$JAVA_HOME/bin# 配置生效source /etc/profile# 测试环境java -version# 安装 Hadooptar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/# 添加 Hadoop 环境变量pwd /opt/module/hadoop-2.7.2vi /etc/profile # export HADOOP_HOME=/opt/module/hadoop-2.7.2 # export PATH=$PATH:$HADOOP_HOME/bin # export PATH=$PATH:$HADOOP_HOME/sbinsource /etc/profilehadoop version Hadoop 目录结构 bin 目录：存放对 Hadoop 相关服务（HDFS，YARN）进行操作的脚本； etc 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件； lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能）； sbin 目录：存放启动或停止 Hadoop 相关服务的脚本； share 目录：存放 Hadoop 的依赖 jar 包、文档、和官方案例。 配置文件说明 Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。 要获取的默认文件 文件存放在 Hadoop 的 jar 包中的位置 [core-default.xml] hadoop-common-2.7.2.jar/ core-default.xml [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml [mapred-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml 自定义配置文件： ​ core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml 四个配置文件存放在 $HADOOP_HOME/etc/hadoop 这个路径上，用户可以根据项目需求重新进行修改配置。 运行模式 本地模式 12345678910# hadoop-2.7.2 目录下mkdir wcinputcd wcinputtouch wc.input# 编写分析文本vi wc.input# hadoop-2.7.2 目录下执行hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount wcinput wcoutput# 查看结果cat wcoutput/part-r-00000 伪分布式 123# 配置集群# 配置 hadoop-env.sh，修改 JAVA_HOME 路径 # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置 core-site.xml --&gt;&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop101:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 Hadoop 运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 1234567&lt;!-- 配置 hdfs-site.xml --&gt;&lt;!-- 指定 HDFS 副本的数量 --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; 1234567891011# 启动集群# 格式化 NameNode（第一次启动时需要格式化）bin/hdfs namenode -format# 启动 NameNodesbin/hadoop-daemon.sh start namenode# 启动 DataNodesbin/hadoop-daemon.sh start datanode# 查看集群是否启动成功jps# 查看产生的 Log 日志cd /opt/module/hadoop-2.7.2/logs 格式化 NameNode 后会产生新的集群 id，导致 NameNode 和 DataNode 的集群 id 不一致，导致集群找不到之前数据。所以，格式 NameNode 前一定要先删除 data 数据和 log 日志，然后才能格式化 NameNode。 启动 YARN 12# 配置 yarn-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置yarn-site.xml --&gt;&lt;!-- Reducer 获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop101&lt;/value&gt;&lt;/property&gt; 1234# 配置 mapred-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144# 配置 mapred-site.xmlmv mapred-site.xml.template mapred-site.xml 12345&lt;!-- 指定 MR 运行在 YARN 上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 1234# 启动集群，启动前必须保证 NameNode 和 DataNode 已经启动sbin/yarn-daemon.sh start resourcemanagersbin/yarn-daemon.sh start nodemanager# 8088 端口查看 web 页面 历史服务器 用于查看程序的历史运行情况。 123456789101112&lt;!-- 配置 mapred-site.xml --&gt;&lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop101:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器 web 端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop101:19888&lt;/value&gt;&lt;/property&gt; 123# 启动历史服务器sbin/mr-jobhistory-daemon.sh start historyserver# 19888 端口查看 web 页面 日志聚集 应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上，方便的查看到程序运行详情和开发调试。 12345678910111213&lt;!-- yarn-site.xml --&gt;&lt;!-- 开启日志聚集功能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 1234567# 重启后日志聚集才生效sbin/yarn-daemon.sh stop resourcemanagersbin/yarn-daemon.sh stop nodemanagersbin/mr-jobhistory-daemon.sh stop historyserversbin/yarn-daemon.sh start resourcemanagersbin/yarn-daemon.sh start nodemanagersbin/mr-jobhistory-daemon.sh start historyserver 完全分布式 三台虚拟机之间的相互通讯，协调工作。 集群分发 scp（secure copy）可以实现服务器与服务器之间的数据拷贝。 1234567# 在 hadoop101 上将 hadoop101 中 /opt/module 目录下的软件拷贝到 hadoop102 上# -r 表示递归，即全部都拷贝过去scp -r /opt/module root@hadoop102:/opt/module# 将 hadoop101 中 /etc/profile 文件拷贝到 hadoop102 的 /etc/profile 上scp /etc/profile root@hadoop102:/etc/profile# 配置生效source /etc/profile rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去。 选项 功能 -r 递归 -v 显示复制过程 -l 拷贝符号连接 12# 把 hadoop101 机器上的 /opt/software 目录同步到 hadoop102 服务器的 root 用户下的 /opt/ 目录rsync -rvl /opt/software/ root@hadoop102:/opt/software 12345# 编写集群分发脚本# /usr/local/bin 目录下的脚本全局可用# /home/wingo/bin 这个目录下存放的脚本，wingo 用户可以在系统任何地方直接执行touch xsyncvi xsync 12345678910111213141516171819202122232425#!/bin/bash# 获取输入参数个数，如果没有参数，直接退出pcount=$#if((pcount==0)); thenecho no args;exit;fi# 获取文件名称，basename 获取路劲最后那个名称p1=$1fname=`basename $p1`echo fname=$fname# 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir# 获取当前用户名称user=`whoami`# 循环，这里根据需求修改循环for((host=103; host&lt;105; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 集群配置 先在 hadoop102 中进行配置，然后将配置分发到其余的节点。 hadoop102 hadoop103 hadoop104 HDFS NameNode、DataNode DataNode SecondaryNameNode、DataNode YARN NodeManager ResourceManager、NodeManager NodeManager 12345678910111213&lt;!-- 配置 core-site.xml --&gt;&lt;!-- 指定 HDFS 中 NameNode 的地址 --&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop102:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 Hadoop 运行时产生文件的存储目录 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt; 123# 配置集群# 配置 hadoop-env.sh，修改 JAVA_HOME 路径 # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置 hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;!-- 分片数必须要小于节点数 --&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 Hadoop 辅助名称节点主机配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop104:50090&lt;/value&gt;&lt;/property&gt; 12# 配置 yarn-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144 12345678910111213&lt;!-- 配置yarn-site.xml --&gt;&lt;!-- Reducer 获取数据的方式 --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定 YARN 的 ResourceManager 的地址 --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop103&lt;/value&gt;&lt;/property&gt; 1234# 配置 mapred-env.sh # export JAVA_HOME=/opt/module/jdk1.8.0_144# 配置 mapred-site.xmlmv mapred-site.xml.template mapred-site.xml 12345&lt;!-- 指定 MR 运行在 YARN 上 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; 12# 在集群上分发配置好的Hadoop配置文件xsync /opt/module/hadoop-2.7.2/ 单点启动 12345# NameNode 第一次启动初始化hadoop namenode -formathadoop-daemon.sh start namenode# 分别在三个节点上启动 DataNodehadoop-daemon.sh start datanode 集群启动 1234567891011vi /opt/module/hadoop-2.7.2/etc/hadoop/slaves # hadoop102 # hadoop103 # hadoop104# 同步配置xsync slaves# NameNode 节点bin/hdfs namenode -formatsbin/start-dfs.sh# ResourceManage 节点sbin/start-yarn.sh SSH 配置 1234567# ~/.ssh 生成密钥ssh-keygen -t rsa# 每个节点都需要进行此操作# 将公钥拷贝到要免密登录的目标机器上ssh-copy-id hadoop102ssh-copy-id hadoop103ssh-copy-id hadoop104 文件名 功能 known_hosts 记录ssh访问过计算机的公钥(public key) id_rsa 生成的私钥 id_rsa.pub 生成的公钥 authorized_keys 存放授权过得无密登录服务器公钥 时间同步 NPT（Network Time Protocol） 时间同步的方式：找一个节点作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 12345678910111213141516171819202122232425262728# 检查安装rpm -qa | grep ntpvi /etc/ntp.confyum install -y ntp# 修改配置vi /etc/ntp.conf # 授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间 # restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap # 集群在局域网中，不使用其他互联网上的时间 # #server 0.centos.pool.ntp.org iburst # #server 1.centos.pool.ntp.org iburst # #server 2.centos.pool.ntp.org iburst # #server 3.centos.pool.ntp.org iburst # 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步 # server 127.127.1.0 # fudge 127.127.1.0 stratum 10vim /etc/sysconfig/ntpd # SYNC_HWCLOCK=yesservice ntpd restart# 开机启动此服务chkconfig ntpd on# 其它节点配置crontab -e # */10 * * * * /usr/sbin/ntpdate hadoop102# 修改时间date -s &quot;2017-9-11 11:11:11&quot;# 一段时间后查看是否同步date 操作集群 123456789101112131415161718# 在 HDFS 文件系统上创建一个 input 文件夹bin/hdfs dfs -mkdir -p /user/wingo/input# 将测试文件内容上传到文件系统上bin/hdfs dfs -put wcinput/wc.input /user/wingo/input/# 查看上传的文件bin/hdfs dfs -ls /user/wingo/input/bin/hdfs dfs -cat /user/wingo/input/# 运行 MapReduce 程序bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /user/wingo/input/ /user/wingo/output# 查看输出结果，可在 50070 端口 Web 查看bin/hdfs dfs -cat /user/wingo/output/*# 下载文件到本地bin/hdfs dfs -get /user/wingo/output/part-r-00000 ./wcoutput/# 删除输出结果bin/hdfs dfs -rm -r /user/wingo/output 启动停止操作 123456789# 分别启动 / 停止 HDFS 组件hadoop-daemon.sh start / stop namenode / datanode / secondarynamenode# 启动 / 停止 YARNyarn-daemon.sh start / stop resourcemanager / nodemanager# 配置了 ssh 后可一次性启动集群# 整体启动 / 停止 HDFSstart-dfs.sh / stop-dfs.sh# 整体启动/停止 YARNstart-yarn.sh / stop-yarn.sh HDFS HDFS( Hadoop Distributed File System）它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 具有高容错性，处理大数据，可构建在廉价机器上等优点； 具有延迟较高，无法高效存储大量小数据，不支持并发写入及文件随机修改等缺陷。 架构 NameNode 就是 Master，它是一个主管、管理者。 管理HDFS的名称空间； 配置副本策略; 管理数据块( Block）映射信息; 处理客户端读写请求。 DataNode：就是 Slave。 NameNode 下达命令, DataNode 执行实际的操作。 存储实际的数据块; 执行数据块的读写操作。 Client：客户端。 文件切分。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block 然后进行上传； 与 NameNode 交互，获取文件的位置信息； 与 DataNode 交互，读取或者写入数据； Client 提供一些命令来管理 HDFS,比如 NameNode 格式化； Client 可以通过一些命令来访问 HDFS，比如对 HDFS 增删查改操作。 Secondary NameNode：并非 NameNode 的热备。 辅助 NameNode，分担其工作量，比如定期合并 Silage 和 Edits，并推送给 NameNode； 在紧急情况下,可辅助恢复 NameNode。 文件快大小 HDFS 中的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfs.blocksize）来规定,默认大小在 Hadoop2x 版本中是 128M，老版本中是 64M。 HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置； 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。 总结：HDFS 块的大小设置主要取决于磁盘传输速率。（寻址时间为传输时间的 1% 时最合适） Shell 操作 hfs 是 fs 的具体实现类。 1234567891011121314151617181920212223242526272829303132333435363738# 命令大全bin/hadoop fs# 输出这个命令参数hadoop fs -help rm# 显示目录信息hadoop fs -ls /# 在 HDFS 上创建目录hadoop fs -mkdir -p /win/go# 从本地剪切粘贴到 HDFShadoop fs -moveFromLocal ./win.txt /win/go# 追加一个文件到已经存在的文件末尾hadoop fs -appendToFile go.txt /win/go/win.txt# 显示文件内容hadoop fs -cat /win/go/win.txt# 修改文件所属权限，HDFS 操作的用户及用户组必须是系统中存在的hadoop fs -chmod 666 /win/go/win.txthadoop fs -chown win:go /win/go/win.txt# 从 HDFS 拷贝到本地hadoop fs -copyToLocal /win/go/win.txt ./# 从 HDFS 的一个路径拷贝到 HDFS 的另一个路径hadoop fs -cp /win/go/win.txt /other.txt# 在 HDFS 目录中移动文件hadoop fs -mv /other.txt /win/go/# 从 HDFS 下载文件到本地hadoop fs -get /win/go/win.txt ./# 合并下载多个 HDFS 文件hadoop fs -getmerge /user/wingo/test/* ./together.txt# 等同于 copyFromLocalhadoop fs -put ./local.txt /user/wingo/test/# 显示一个文件的末尾hadoop fs -tail /win/go/win.txt# 删除文件或文件夹hadoop fs -rm /user/wingo/test/delete.txthadoop fs -rmdir /test# 统计文件夹的大小信息hadoop fs -du -h /user/wingo/test# 设置 HDFS 中文件的副本数量：若条件允许的话才会达到hadoop fs -setrep 10 /win/go/win.txt 写流程 向 HDFS 写数据时，客户端与 NameNode、DataNode 通信过程。 网络拓扑 在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。 节点距离：两个节点到达最近的共同祖先的距离总和。 机架感知 副本节点选择。 读流程 在 HDFS 读数据时，客户端与 NameNode、DataNode 通信过程。 2NN 如果存储在 NameNode 节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的 FsImage。 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新 FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦 NameNode 节点断电，就会产生数据丢失。因此，引入 Edits 文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中。这样，一旦 NameNode 节点断电，可以通过 FsImage 和 Edits 的合并，合成元数据。 但是，如果长时间添加数据到 Edits 中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行 FsImage 和 Edits 的合并，如果这个操作由 NameNode 节点完成，又会效率过低。因此，引入一个新的节点 SecondaryNamenode，专门用于 FsImage 和 Edits 的合并。 Fsimage 和 Edits 12345# oiv 查看 Fsimage 文件pwd # /opt/module/hadoop-2.7.2/data/tmp/dfs/name/currenthdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-2.7.2/fsimage.xmlcat /opt/module/hadoop-2.7.2/fsimage.xml 12345678910&lt;!-- 记录了 NameNode 的信息，DataNode 信息定时同步到 NameNode --&gt;&lt;inode&gt; &lt;id&gt;16386&lt;/id&gt; &lt;type&gt;DIRECTORY&lt;/type&gt; &lt;name&gt;user&lt;/name&gt; &lt;mtime&gt;1512722284477&lt;/mtime&gt; &lt;permission&gt;wingo:supergroup:rwxr-xr-x&lt;/permission&gt; &lt;nsquota&gt;-1&lt;/nsquota&gt; &lt;dsquota&gt;-1&lt;/dsquota&gt;&lt;/inode&gt; 123# oev 查看 Edits 文件hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-2.7.2/edits.xmlcat /opt/module/hadoop-2.7.2/edits.xml 12345678910&lt;!-- 记录了操作信息 --&gt;&lt;EDITS&gt; &lt;EDITS_VERSION&gt;-63&lt;/EDITS_VERSION&gt; &lt;RECORD&gt; &lt;OPCODE&gt;OP_START_LOG_SEGMENT&lt;/OPCODE&gt; &lt;DATA&gt; &lt;TXID&gt;129&lt;/TXID&gt; &lt;/DATA&gt; &lt;/RECORD&gt;&lt;/EDITS &gt; CheckPoint 12345678910111213141516171819&lt;!-- 默认配置 hdfs-default.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;!-- 一小时执行一次 --&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt; &lt;description&gt;操作次数达一百万次&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt; &lt;description&gt;一分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; 故障处理 NameNode 故障后，可以采用如下两种方法恢复数据。 123456789# 方法一 将 SecondaryNameNode 中数据拷贝到 NameNode 存储数据的目录# 杀掉 NameNode 相关进程kill -9 [pid]# 删除 NameNode 存储的数据rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*# 拷贝 SecondaryNameNode 中数据到原 NameNode 存储数据目录scp -r root@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/# 重新启动 NameNodesbin/hadoop-daemon.sh start namenode 1# 方法二 使用 -importCheckpoint 选项启动 NameNode 守护进程，从而将 Secondary NameNode 中数据拷贝到 NameNode 目录中。 1234567891011&lt;!-- 修改hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;120&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp/dfs/name&lt;/value&gt;&lt;/property&gt; 123456789101112# 杀掉 NameNode 相关进程kill -9 [pid]# 删除 NameNode 存储的数据rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*# 如果 SecondaryNameNode 不和 NameNode 在一个主机节点上，# 需要将 SecondaryNameNode 存储数据的目录拷贝到 NameNode 存储数据的平级目录，并删除 in_use.lock 文件scp -r root@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./rm -rf in_use.lock# 导入检查点数据（等待一会 ctrl+c 结束掉）bin/hdfs namenode -importCheckpoint# 启动 NameNodesbin/hadoop-daemon.sh start namenode 安全模式 NameNode 启动时，首先将镜像文件(（Fsimage）载入內存，并执行编辑日志(（Edits）中的各项操作，一旦在内存中成功建立文件系统元数据的映像，则创建一个新的 Silage文件和一个空的编辑日志。此时，NameNode 开始监听 DataNode 请求。这个过程期间，Namenode 一直运行在安全模式，即 NameNode 的文件系统对于客户端来说是只读的。 系统中的数据块的位置并不是由 NameNode 维护的，而是以块列表的形式存储在 DataNode 中。在系统的正常操作期间, NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个 Datanode 会向 NameNode 发送最新的块列表信息,，NameNode 了解到足够多的块位置信息之后，即可高效运行文件系统。 如果满足“最小副本条件”,，NameNode 会在 30 秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中 99.9% 的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的 HDFS 集群时，因为系统中还没有任何块，所以 NameNode 不会进入安全模式。 12345bin/hdfs dfsadmin -safemode get # 查看安全模式状态bin/hdfs dfsadmin -safemode enter # 功能描述：进入安全模式状态bin/hdfs dfsadmin -safemode leave # 离开安全模式状态bin/hdfs dfsadmin -safemode wait # 等待安全模式结束状态# 可以编写脚本，在安全模式下堵塞等待安全模式结束运行接下来的命令 多目录 NameNode 的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性。 1234567891011&lt;!-- hdfs-site.xml 配置 --&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/data/tmp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; DataNode 也可以配置成多个目录，每个目录存储的数据不一样。即：数据不是副本。 123456&lt;!-- hdfs-site.xml 配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/data1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/data2&lt;/value&gt;&lt;/property&gt; DataNode DataNode 掉线时限参数设置 12345678910111213&lt;!-- Timeout = 2*dfs.namenode.heartbeat.recheck-interval + 10*dfs.heartbeat.interval --&gt;&lt;!-- hdfs-default.xml 默认配置 --&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;!-- 单位 ms --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;!-- 单位 s --&gt;&lt;/property&gt; 数据完整性 检测 DataNode 节点上的数据是否完整？ 当 DataNode 读取 Block 的时候，它会计算 CheckSum，如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。 Client 读取其他 DataNode 上的 Block，DataNode 在其文件创建后周期验证 CheckSum。 黑白名单 添加到白名单的主机节点，都允许访问 NameNode，不在白名单的主机节点，都会被退出。 1234# 白名单# 在 NameNode 的 /opt/module/hadoop-2.7.2/etc/hadoop 目录下创建 dfs.hosts 文件vi dfs.hosts # 添加允许的主机名 123456&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 1234567# 分发配置文件xsync hdfs-site.xml# 更新节点信息hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes# 如果数据不均衡，可以用命令实现集群的再平衡./start-balancer.sh 在黑名单上面的主机都会被强制退出。 123# 黑名单vi dfs.hosts.exclude # 添加不允许的主机名 123456&lt;!-- hdfs-site.xml --&gt;&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 1234567# 更新节点信息hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes# 如果数据不均衡，可以用命令实现集群的再平衡./start-balancer.sh# 检查 Web 浏览器，退役节点的状态为 decommission in progress（退役中），说明数据节点正在复制块到其他节点# 等待退役节点状态为 decommissioned（所有块已经复制完成），停止该节点及节点资源管理器 回收站 开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。 1234# 0 表禁用，其他值表示设置文件的存活时间# fs.trash.interval=0# 检查回收站的间隔时间。如果该值为 0，则该值设置和 fs.trash.interval 的参数值相等。# fs.trash.checkpoint.interval=0 12345678910111213&lt;!-- core-site.xml --&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;!-- min --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;wingo&lt;/value&gt; &lt;!-- 进入回收站的名称 --&gt;&lt;/property&gt; 123456789# 回收站在集群中的路径/user/wingo/.Trash/...# 通过程序删除的文件不会经过回收站，需要调用 moveToTrash() 才进入回收站# Trash trash = New Trash(conf);# trash.moveToTrash(path);# 回复回收站数据hadoop fs -mv /user/wingo/.Trash/Current/user/wingo/input /user/wingo/input# 清空回收站hadoop fs -expunge 快照管理 12345678hdfs dfsadmin -allowSnapshot [路径] # 开启指定目录的快照功能hdfs dfsadmin -disallowSnapshot [路径] # 禁用指定目录的快照功能，默认是禁用hdfs dts -createSnapshot [路径] # 对目录创建快照hdfs dts -createSnapshot [路径] [名称] # 指定名称创建快照hdts dfs -renameSnapshot [路径] [旧名称] [新名称] # 重命名快照hdfs lsSnapshottableDir # 列出当前用户所有可快照目录hdfs snapshotDiff [路径] [路径] # 比较两个快照目录的不同之处hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotname&gt; # 删除快照 MapReduce MapReduce 是一个分布式运算程序的编程框架,是用户开发“基于 Hadoop 的数据分析应用”的核心框架。 MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。 具有易于编程、扩展性良好、高容错性、PB 级别海量数据处理能力等优点； 具有不擅长实时计算、不擅长流式计算、不擅长 DAG （有向图）计算，即需要迭代的计算等缺陷。 实例进程 MrAppMaster：负责整个程序的过程调度及状态协调； MapTask：负责 Map 阶段的整个数据处理流程； ReduceTask：负责 Reduce 阶段的整个数据处理流程。 序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable 编程规范 用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。 Mapper 阶段 用户自定义的 Mapper 要继承指定的父类； Mapper 的输入数据是 K V 对的开式（K V 的类型可自定义）； Mapper 中的业务逻辑写在 map() 方法中； Mapper 的输出数据是 K Ⅴ 对的形式（K V 的类型可自定义）； map() 方法( MapTask 进程）对每一个 K V&gt;调用一次。 123456789101112131415161718192021222324// Key 偏移量 输入数据 Value 类型 输出数据类型的 K V 👉 对应 Reduce 的输入public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; Text k = new Text(); IntWritable v = new IntWritable(1); // 此方法每次只获取一行 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 切割 String[] words = line.split(&quot; &quot;); // 输出 for (String word : words) &#123; k.set(word); // (word, 1) context.write(k, v); &#125; &#125;&#125; Reduce 阶段 用户自定义的 Reducer 要继承自己的父类； Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 K V 对的形式 Reducer 的业务逻辑写在 reduce() 方法中 ReduceTask 进程对每一组相同 K 的 K V 组调用一次 reduce() 方法 12345678910111213141516171819public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; int sum; IntWritable v = new IntWritable(); // 这里直接穿入一个迭代器了，直接循环取值即可 @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 累加求和 sum = 0; for (IntWritable count : values) &#123; sum += count.get(); &#125; // 输出 v.set(sum); context.write(key,v); &#125;&#125; Driver 阶段 相当于 YARN 集群的客户端，用于提交我们整个程序到 YARN 集群,提交的是封装了 MapReduce 程序相关运行参数的 job 对象。 123456789101112131415161718192021222324252627282930313233public class WordcountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 设置 jar 加载路径 job.setJarByClass(WordcountDriver.class); // 设置 map 和 reduce 类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); // 设置 map 输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设置最终输出 K V 类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 设置输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; Shuffle Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle，包含了两端的 Combiner 和 Partition。 Partition 要求将统计结果按照条件输出到不同文件中（分区）。 1234567// 默认 Partition 分区机制，用户无法控制 Key 的存储分区public class HashPartironer&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V vlaue, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numberReduceTasks; &#125;&#125; 自定义 12345678910111213141516171819202122232425262728293031323334// 需求：将统计结果按照手机归属地不同省份输出到不同文件中（分区）public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125;// Driver// 指定自定义数据分区job.setPartitionerClass(ProvincePartitioner.class);// 同时指定相应数量的 reduce taskjob.setNumReduceTasks(5);// job.setNumReduceTasks(1) 会正常运行,只不过会产生一个输出文件// job.setNumReduceTasks(2) 会报错// job.setNumReduceTasks(6) 大于 5 程序正常运行，会产生空文件 排序 MapTask 和 ReduceTask 均会对数据按照 key 进行排序，该操作属 Hadoop 的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上否需要。 默认排序是按照字曲顺序排序，且实现该排序的方法是快速排序。 对于 MapTask，它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。 对于 ReduceTask，它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后, ReduceTask 统一对内存和磁盘上的所有数据进行一次归并排序。 自定义 123456789101112131415161718// bean 对象做为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法，就可以实现排序@Overridepublic int compareTo(FlowBean o) &#123; int result; // 按照总流量大小，倒序排列 if (sumFlow &gt; bean.getSumFlow()) &#123; result = -1; &#125;else if (sumFlow &lt; bean.getSumFlow()) &#123; result = 1; &#125;else &#123; result = 0; &#125; return result;&#125; 合并 Combiner 组件的父类就是 Reducer。 Combiner 是在每一个 MapTask 所在的节点运行，进行局部汇总，减少网络传输量；Reducer 是接收全局所有 Mapper 的输出结果。 123456789101112131415161718192021// 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法// Combiner K V 对应 Reducer K Vpublic class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text , IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; // 汇总操作 int count = 0; for(IntWritable v : values)&#123; count += v.get(); &#125; // 写出 context.write(key, new IntWritable(count)); &#125;&#125;// Driverjob.setCombinerClass(WordcountCombiner.class); 分组 对 Reduce 阶段的数据根据某一个或几个字段进行分组。 自定义类继承 WritableComparator； 重写 compare() 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// OrderBeanpublic class OrderBean implements WritableComparable&lt;OrderBean&gt; &#123; private int order_id; // 订单id号 private double price; // 价格 public OrderBean() &#123; super(); &#125; public OrderBean(int order_id, double price) &#123; super(); this.order_id = order_id; this.price = price; &#125; // 序列化 @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeDouble(price); &#125; // 反序列化 @Override public void readFields(DataInput in) throws IOException &#123; order_id = in.readInt(); price = in.readDouble(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + price; &#125; // Getter / Setter // 二次排序，即 map 结束之前的一次排序，排序后输出到 reduce @Override public int compareTo(OrderBean o) &#123; int result; if (order_id &gt; o.getOrder_id()) &#123; result = 1; &#125; else if (order_id &lt; o.getOrder_id()) &#123; result = -1; &#125; else &#123; // 价格倒序排序 result = price &gt; o.getPrice() ? -1 : 1; &#125; return result; &#125;&#125; 123456789101112131415161718192021222324// 编写 OrderSortMapper 类// 0000001 Pdt_01 222.8public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt; &#123; OrderBean k = new OrderBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 截取 String[] fields = line.split(&quot;\\t&quot;); // 封装对象 k.setOrder_id(Integer.parseInt(fields[0])); k.setPrice(Double.parseDouble(fields[2])); // 写出 context.write(k, NullWritable.get()); &#125;&#125; 12345678910111213141516171819202122232425262728// 编写 OrderSortGroupingComparator 类// 在 reduce 前通过此排序进行分组public class OrderGroupingComparator extends WritableComparator &#123; // 创建一个构造将比较对象的类传给父类 protected OrderGroupingComparator() &#123; super(OrderBean.class, true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; OrderBean aBean = (OrderBean) a; OrderBean bBean = (OrderBean) b; int result; if (aBean.getOrder_id() &gt; bBean.getOrder_id()) &#123; result = 1; &#125; else if (aBean.getOrder_id() &lt; bBean.getOrder_id()) &#123; result = -1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; 123456789101112131415// 编写 OrderSortReducer 类public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt; &#123; @Override protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 只能通过 NullWritable.get() 获取空值 context.write(key, NullWritable.get()); &#125;&#125;// Driver// 设置 reduce 端的分组job.setGroupingComparatorClass(OrderGroupingComparator.class); MapTask Read 阶段：MapTask 通过用户编写的 RecordReader，从输入 InputSplit 中解析出一个个 key / value； Map 阶段：该阶段主要是将解析出的 key / value 交给用户编写 map() 函数处理，并产生一系列新的 key / value； Collect 收集阶段：在用户编写 map() 函数中，当数据处理完成后，一般会调用 OutputCollector.collect() 输出结果。在该函数内部，它会将生成的 key / value 分区（调用 Partitioner），并写入一个环形内存缓冲区中； Spill 阶段：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。 溢写阶段详情： 利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照 key 有序。 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output/spillN.out（N表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。 将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过 1MB，则将内存索引写到文件output/spillN.out.index 中。 Combine阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output/file.out 中，同时生成相应的索引文件 output/file.out.index。 在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并 io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 ReduceTask Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。 Merge 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。 Sort 阶段：按照 MapReduce 语义，用户编写 reduce() 函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。 Reduce 阶段：reduce() 函数将计算结果写到 HDFS上。 ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置。 12// 默认值是 1，手动设置为 4job.setNumReduceTasks(4); 工作流程 Hadoop 序列化 序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。反序列化就是序列化的逆过程。 Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后会附带很多额外的信息（各种校验信息、Header、继承体系等）不便于在网络中高传输。 所以 Hadoop 自己开发了一套序列化机制（Writable）。 紧凑：高效使用存储空间； 快速：读写数据的额外开销小； 可扩展：随着通信协议的升级而可升级； 互操作：支持多语言的交互。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 自定义 bean 对象实现序列化接口// 实现 writable 接口public class FlowBean implements Writable&#123; private long upFlow; private long downFlow; private long sumFlow; // 反序列化时，需要反射调用空参构造函数，所以必须有 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; // 写序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; // 反序列化方法 // 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 编写toString方法，方便后续打印到文本 @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + downFlow + &quot;\\t&quot; + sumFlow; &#125; // Getter / Setter&#125; InputFormat MapTask 并行度决定机制： 数据块：Block 是 HDFS 物理上把数据分成一块一块。 数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。切片时不考虑数据集整体，而是逐个针对每一个文件单独切片。 Block 的大小：local 32 1.x 64 2.x 128 FileInputFormat TextInputFormat 是默认的 FilelnputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量，Long writable类型。值是这行的内容，不包括任何行终止符（换行符和回车符）Text 类型。 123456789101112131415161718192021222324252627282930313233343536// 源码 Debug 主要流程waitForCompletion()submit(); // 建立连接 connect(); // 创建提交 Job 的代理 new Cluster(getConfiguration()); // 判断是 LocalJobRunner 还是 YarnRunner initialize(jobTrackAddr, conf); // 提交 job submitter.submitJobInternal(Job.this, cluster) // 创建给集群提交数据的 Stag 路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 获取 jobid ，并创建 Job 路径 JobID jobId = submitClient.getNewJobID(); // 拷贝 jar 包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 计算切片，生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job); // 向 Stag 路径写 XML 配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 提交 Job，返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 12345// 获取切片信息 API// 获取切片文件名称String name = inputSplit.getPath().getName();// 根据文件类型获取切片信息FileSplit inputSpli = (FileSplit)context. getInputSplit() CombineText 框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。 CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。 12345// 如果不设置 InputFormat，它默认用的是 TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);// 虚拟存储切片最大值设置CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时将文件均分成 2 个虚拟存储块（防止出现太小切片）。 KeyValueText KeyValueTextInputFormat 每一行均为一条记录被分隔符分割为 K V。可以通过在驱动类中设定分隔符。默认分隔符是 tab（\\t）。 1conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR,&quot;\\t&quot;) 1234567891011121314// 需求：统计输入文件中每一行的第一个单词相同的行数// Mapper 编写public class KVTextMapper extends Mapper&lt;Text, Text, Text, LongWritable&gt;&#123; // 设置 value LongWritable v = new LongWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; // 写出 (第一个单词, 1) context.write(key, v); &#125;&#125; 123456789101112131415161718192021public class KVTextReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt;&#123; LongWritable v = new LongWritable(); @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; long sum = 0L; // 汇总统计 for (LongWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); // 输出 context.write(key, v); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536public class KVTextDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); // 设置切割符 conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot; &quot;); // 获取 job 对象 Job job = Job.getInstance(conf); // 设置 jar 包位置，关联 mapper 和 reducer job.setJarByClass(KVTextDriver.class); job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); // 设置 map 输出 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(LongWritable.class); // 设置最终输出 k v 类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(LongWritable.class); // 设置输入数据路径 FileInputFormat.setInputPaths(job, new Path(args[0])); // 设置输入格式 job.setInputFormatClass(KeyValueTextInputFormat.class); // 设置输出数据路径 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交 job job.waitForCompletion(true); &#125;&#125; NLine NLineInputFormat 代表每个 map 进程处理的 InputSplit 不再按 Block块去划分,而是按 NLineInputFormat 指定的行数来划分。即输入文件的总行数 N = 切片数，如果不整除则切片数 = 商 + 1。 1234567// 对每个单词进行个数统计,要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中// 设置每个切片 InputSplit 中划分三条记录NLineInputFormat.setNumLinesPerSplit(job, 3);// 使用 NLineInputFormat 处理记录数 job.setInputFormatClass(NLineInputFormat.class); 自定义 无论 HDFS 还是 MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义 InputFormat 实现小文件的合并。 123456789101112131415161718192021222324252627/** * 将多个小文件合并成一个 SequenceFile 文件 * SequenceFile 文件是 Hadoop 用来存储二进制形式的 key-value 对的文件格式 * SequenceFile 里面存储着多个文件，存储的形式为（文件路径 + 名称）key，文件内容 value*/// 自定义 InputFormat// 定义类继承 FileInputFormatpublic class WholeFileInputformat extends FileInputFormat&lt;Text, BytesWritable&gt;&#123; // 定义是否可切割，此程序不可切片，最终把所有文件封装到了 value 中 @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; // 实现一次读取一个完整文件封装为 K V @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader( InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(split, context); return recordReader; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// 自定义 RecordReader 类public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt;&#123; private Configuration configuration; private FileSplit split; private boolean isProgress = true; private BytesWritable value = new BytesWritable(); private Text k = new Text(); @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; // 初始化数据以及配置 this.split = (FileSplit)split; configuration = context.getConfiguration(); &#125; // 核心业务逻辑，即拼接自定义的 K @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (isProgress) &#123; // 定义缓存区，获取切片长度 byte[] contents = new byte[(int)split.getLength()]; FileSystem fs = null; FSDataInputStream fis = null; try &#123; // 获取文件系统，通过路径获取文件系统，一波反向操作 Path path = split.getPath(); fs = path.getFileSystem(configuration); // 读取数据 fis = fs.open(path); // 读取文件内容 IOUtils.readFully(fis, contents, 0, contents.length); // 输出文件内容 value.set(contents, 0, contents.length); // 获取文件路径及名称 String name = split.getPath().toString(); // 设置输出的 key 值 k.set(name); &#125; catch (Exception e) &#123; &#125;finally &#123; IOUtils.closeStream(fis); &#125; // 本切片已处理完，清标记位 isProgress = false; // 进行其它切片的处理 return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 编写 SequenceFileMapper 类处理流程public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt;&#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125;// 编写 SequenceFileReducer 类处理流程public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; context.write(key, values.iterator().next()); &#125;&#125;// 编写SequenceFileDriver类处理流程public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[] &#123; &quot;e:/input/inputinputformat&quot;, &quot;e:/output1&quot; &#125;; // 获取 job 对象 Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 设置 jar 包存储位置、关联自定义的 mapper 和 reducer job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 设置输入的 inputFormat job.setInputFormatClass(WholeFileInputformat.class); // 设置输出的 outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); // 设置 map 输出端的 K V 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(BytesWritable.class); // 设置最终输出端的 K V 类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); // 设置输入输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交 job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; OutputFormat 默认的输出格式是 TextOutputFormat ，它把每条记录写为文本行，它的键和值可以是任意类型，因为 TextOutputFormat调用 toString() 方法把它们转换为字符串。 将 SequenceFileOutputFormat 输出作为后续 MapReduce 任务的输入，因为其格式紧凑，很容易被压缩。 自定义 为了实现控制最终文件的输岀路径和输出格式，可进行自定义。 自定义一个类继承 FileOutputFormat； 改写 RecordWriter，具体改写输出数据的方法 write()。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 继承 FileOutputFormatpublic class FilterOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException &#123; // 创建一个RecordWriter return new FilterRecordWriter(job); &#125;&#125;// 重写 write()public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream wingoOut = null; FSDataOutputStream otherOut = null; public FilterRecordWriter(TaskAttemptContext job) &#123; FileSystem fs; try &#123; // 获取文件系统 fs = FileSystem.get(job.getConfiguration()); // 创建输出文件路径 Path wingoPath = new Path(&quot;e:/wingo.log&quot;); Path otherPath = new Path(&quot;e:/other.log&quot;); // 创建输出流 atguiguOut = fs.create(wingoPath); otherOut = fs.create(otherPath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; // 判断是否包含 wingo 输出到不同文件 if (key.toString().contains(&quot;wingo&quot;)) &#123; wingoOut.write(key.toString().getBytes()); &#125; else &#123; otherOut.write(key.toString().getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; // 关闭资源 IOUtils.closeStream(wingoOut); IOUtils.closeStream(otherOut); &#125;&#125;// 将自定义的输出格式组件设置到 job 中job.setOutputFormatClass(FilterOutputFormat.class);// 虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat// 而 fileoutputformat 要输出一个 _SUCCESS 文件，所以，在这还得指定一个输出目录 Reduce Join order(1001 01 1) join pd(01 小米) 👉 result(001 小米 1) Bean 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 合并后的 Bean 类public class TableBean implements Writable &#123; private String order_id; // 订单 id private String p_id; // 产品 id private int amount; // 产品数量 private String pname; // 产品名称 private String flag; // 表的标记 public TableBean() &#123; super(); &#125; public TableBean(String order_id, String p_id, int amount, String pname, String flag) &#123; super(); this.order_id = order_id; this.p_id = p_id; this.amount = amount; this.pname = pname; this.flag = flag; &#125; // Getter / Setter @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(order_id); out.writeUTF(p_id); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.order_id = in.readUTF(); this.p_id = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; @Override public String toString() &#123; return order_id + &quot;\\t&quot; + pname + &quot;\\t&quot; + amount + &quot;\\t&quot; ; &#125;&#125; Mapper 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt;&#123; String name; TableBean bean = new TableBean(); Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取输入文件切片 FileSplit split = (FileSplit) context.getInputSplit(); // 获取输入文件名称 name = split.getPath().getName(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取输入数据 String line = value.toString(); // 不同文件分别处理 if (name.startsWith(&quot;order&quot;)) &#123;// 订单表处理 // 切割 String[] fields = line.split(&quot;\\t&quot;); // 封装bean对象 bean.setOrder_id(fields[0]); bean.setP_id(fields[1]); bean.setAmount(Integer.parseInt(fields[2])); bean.setPname(&quot;&quot;); bean.setFlag(&quot;order&quot;); k.set(fields[1]); &#125;else &#123; // 产品表处理 // 切割 String[] fields = line.split(&quot;\\t&quot;); // 封装bean对象 bean.setP_id(fields[0]); bean.setPname(fields[1]); bean.setFlag(&quot;pd&quot;); bean.setAmount(0); bean.setOrder_id(&quot;&quot;); k.set(fields[0]); &#125; // 写出 context.write(k, bean); &#125;&#125; Reduce 处理压力大，并且极易产生数据倾斜。 12345678910111213141516171819202122232425262728293031323334353637383940public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; // 准备存储订单的集合 ArrayList&lt;TableBean&gt; orderBeans = new ArrayList&lt;&gt;(); // 准备 bean 对象 TableBean pdBean = new TableBean(); // 便利 key = 1 的集合 for (TableBean bean : values) &#123; if (&quot;order&quot;.equals(bean.getFlag())) &#123; // 订单表 // 拷贝传递过来的每条订单数据到集合中 TableBean orderBean = new TableBean(); try &#123; BeanUtils.copyProperties(orderBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; orderBeans.add(orderBean); &#125; else &#123; // 产品表 try &#123; // 此处已在 Map 做好按 p_id 排序工作，最后一定是一个 pd 表信息 // 拷贝传递过来的产品表到内存中 BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 表的拼接 for(TableBean bean : orderBeans)&#123; bean.setPname (pdBean.getPname()); // 数据写出去 context.write(bean, NullWritable.get()); &#125; &#125;&#125; Driver 1234567891011121314151617181920212223242526272829303132333435public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 路径 args = new String[]&#123;&quot;e:/input/inputtable&quot;,&quot;e:/output1&quot;&#125;; // 获取配置信息，或者 job 对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 指定本程序的 jar 包所在的本地路径 job.setJarByClass(TableDriver.class); // 指定本业务 job 要使用的 Mapper / Reducer 业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 指定 Mapper 输出数据的 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 指定最终输出的数据的 k v 类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 指定 job 的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; Map Join 适用于一张表十分小、一张表很大的场景。 采用 Distributed cache：在 Mapper 的 setup 阶段，将文件读取到缓存集合中。 job.addCacheFile(new URI(“file://e:/cache/pd.txt”)); Driver 添加缓存文件。 1234567891011121314151617181920212223242526272829303132333435363738394041public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 路径 args = new String[]&#123;&quot;e:/input/inputtable&quot;,&quot;e:/output1&quot;&#125;; // 获取配置信息，或者 job 对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 指定本程序的 jar 包所在的本地路径 job.setJarByClass(TableDriver.class); // 指定本业务 job 要使用的 Mapper / Reducer 业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 指定 Mapper 输出数据的 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 指定最终输出的数据的 k v 类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 指定 job 的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 加载缓存数据 job.addCacheFile(new URI(&quot;file:///e:/input/inputcache/pd.txt&quot;)); // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0 job.setNumReduceTasks(0); // 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; Map 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Map&lt;String, String&gt; pdMap = new HashMap&lt;&gt;(); Text k = new Text(); @Override protected void setup(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; // 获取缓存的文件 URI[] cacheFiles = context.getCacheFiles(); String path = cacheFiles[0].getPath().toString(); BufferedReader reader = new BufferedReader (new InputStreamReader(new FileInputStream(path), &quot;UTF-8&quot;)); String line; while(StringUtils.isNotEmpty(line = reader.readLine()))&#123; // 切割 String[] fields = line.split(&quot;\\t&quot;); // 缓存数据到集合 (01 小米) pdMap.put(fields[0], fields[1]); &#125; // 关流 reader.close(); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 截取 String[] fields = line.split(&quot;\\t&quot;); // 获取产品 id String pId = fields[1]; // 获取商品名称 String pdName = pdMap.get(pId); // 拼接 k.set(line + &quot;\\t&quot;+ pdName); // 写出 context.write(k, NullWritable.get()); &#125;&#125; 数据清洗 ETL Hadoop 为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。 Map 12345678910111213141516171819202122232425262728293031323334353637383940414243public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取 1 行数据 String line = value.toString(); // 解析日志，即此条日志是否有意义 boolean result = parseLog(line,context); // 日志不合法退出 if (!result) &#123; return; &#125; // 设置 key k.set(line); // 写出数据 context.write(k, NullWritable.get()); &#125; // 解析日志 private boolean parseLog(String line, Context context) &#123; // 截取 String[] fields = line.split(&quot; &quot;); // 日志长度大于 11 的为合法 if (fields.length &gt; 11) &#123; // 系统计数器，记录一条合法的日志 context.getCounter(&quot;map&quot;, &quot;true&quot;).increment(1); return true; &#125;else &#123; // 记录一条不合法的日志 context.getCounter(&quot;map&quot;, &quot;false&quot;).increment(1); return false; &#125; &#125;&#125; Driver 1234567891011121314151617181920212223242526272829303132333435363738394041public class TableDriver &#123; public static void main(String[] args) throws Exception &#123; // 路径 args = new String[]&#123;&quot;e:/input/inputlog&quot;, &quot;e:/output1&quot;&#125;; // 获取配置信息，或者 job 对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 指定本程序的 jar 包所在的本地路径 job.setJarByClass(TableDriver.class); // 指定本业务 job 要使用的 Mapper / Reducer 业务类 job.setMapperClass(TableMapper.class); job.setReducerClass(TableReducer.class); // 指定 Mapper 输出数据的 k v 类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); // 指定最终输出的数据的 k v 类型 job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); // 指定 job 的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 加载缓存数据 job.addCacheFile(new URI(&quot;file:///e:/input/inputcache/pd.txt&quot;)); // Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0 job.setNumReduceTasks(0); // 将 job 中配置的相关参数，以及 job 所用的 java 类所在的 jar 包， 提交给 yarn 去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 数据压缩 适当压缩是提高 Hadoop 运行效率的一种优化策略。 压缩格式 hadoop自带？ 算法 文件扩展名 是否可切分 换成压缩格式后，原来的程序是否需要修改 DEFLATE 是，直接使用 DEFLATE .deflate 否 和文本处理一样，不需要修改 Gzip 是，直接使用 DEFLATE .gz 否 和文本处理一样，不需要修改 bzip2 是，直接使用 bzip2 .bz2 是 和文本处理一样，不需要修改 LZO 否，需要安装 LZO .lzo 是 需要建索引，还需要指定输入格式 Snappy 否，需要安装 Snappy .snappy 否 和文本处理一样，不需要修改 为了支持多种压缩 / 解压缩算法，Hadoop 引入了编码 / 解码器 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 启动压缩功能的参数配置。 参数 阶段 建议 io.compression.codecs 输入压缩 文件扩展名判断是否支持某种编解码器 mapreduce.map.output.compress mapper 输出 这个参数设为 true 启用压缩 mapreduce.map.output.compress.codec mapper 输出 企业多使用 LZO 或 Snappy 编解码器在此阶段压缩数据 mapreduce.output.fileoutputformat.compress reducer 输出 这个参数设为 true 启用压缩 mapreduce.output.fileoutputformat.compress.codec reducer 输出 使用标准工具或者编解码器，如 gzip 和 bzip2 mapreduce.output.fileoutputformat.compress.type reducer 输出 SequenceFile 压缩类型： NONE 和 BLOCK Driver 定义压缩 12345678910111213141516171819202122// Map 输出端采用压缩public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration configuration = new Configuration(); // 开启 map 端输出压缩 configuration.setBoolean(&quot;mapreduce.map.output.compress&quot;, true); // 设置 map 端输出压缩方式 configuration.setClass(&quot;mapreduce.map.output.compress.codec&quot;, BZip2Codec.class, CompressionCodec.class); // ... &#125;&#125;// Reduce 输出端采用压缩// 设置reduce端输出压缩开启FileOutputFormat.setCompressOutput(job, true);// 设置压缩的方式FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); YARN 资源调度器 Hadoop 作业调度器主要有三种： FIFO 先到先服务、Capacity Scheduler 和 Fair Schedule。 1234567&lt;!-- yarn-default.xml --&gt;&lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler&lt;/value&gt;&lt;/property&gt; Capacity Scheduler，容量调度器。 支持多队列，每个队列配置一定的资源量，每个队列采用 FIFO 调度策略； 为防止同一用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定； 运行任务数 / 计算资源 👉 往最闲的队列里交任务。 Fair Scheduler，公平调度器。 支持多队列多用户，每个队列中的资源量可以配置，同队列中的作业公平共享队列中所有资源； 每个队列中的 job 按照优先级分配资源，优先级越高分配的资源越多，但是每个 job 都会分配到资源以确保公平；（差额：job 需要的资源 - job 实际获取的资源） 在同一个队列中，job 旳资源缺额越大，越先获得资源优先执行。 推测执行 某些任务过慢，则为其启动多个备份任务，同时运行，谁先运行完成用谁的结果。 每个 Task 只能有一个备份任务； 当前 Job 已完成的 Task 必须不小于0.05（5%）； 开启推测执行参数设置。mapred-site.xml 文件中默认是打开的。","categories":[{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wingowen.github.io/tags/Hadoop/"}]},{"title":"sklearn","slug":"机器学习/sklearn","date":"2020-04-23T03:07:44.000Z","updated":"2023-09-20T07:35:51.890Z","comments":true,"path":"2020/04/23/机器学习/sklearn/","link":"","permalink":"https://wingowen.github.io/2020/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn/","excerpt":"简单了解机器学习的基本概念以及常规流程，并简单介绍 Python 的机器学习库 sklearn 的使用。","text":"简单了解机器学习的基本概念以及常规流程，并简单介绍 Python 的机器学习库 sklearn 的使用。 Jupyter 安装配置 123456pip3 install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple/ # 国内镜像下载 jupyterjupyter notebook --generate-config # 生成默认的配置文件## Writing default config to: C:\\Users\\wingo\\.jupyter\\jupyter_notebook_config.py## C.NotebookApp.notebook_dir=&#x27;&#x27; 修改此项配置jupyter notebook # 启动 Jupyter notebookpip3 install -U scikit-learn -i https://pypi.tuna.tsinghua.edu.cn/simple/ # 下载库 机器学习概述 数据 👉 模型 👉 预测，即从历史数据中获得规律。 数据集的构成：特征值 + 目标值。 根据是否有目标值，可以分为无监督学习（无目标值）、监督学习（有目标值）。 根据数据的形式，可以分为分类问题（类别）、回归问题（连续性数据）。 机器学习的普遍开发流程： 1）获取数据； 2）数据处理； 3）特征工程； 4）模型训练； 5）模型评估； 6）实际应用。 数据获取 sklearn 自带了许多数据，供开发者进行测试使用。 123456789# 导入满满的数据库from sklearn import datasets# 分割数据的模块，把数据集分为训练集和测试集from sklearn.model_selection import train_test_split# 载入数据，经典鸢尾花数据iris = datasets.load_iris()# random_state 为了保证程序每次运行都分割一样的训练集合测试集。否则，同样的算法模型在不同的训练集和测试集上的效果不一样。x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=0) 查看此数据集的详细描述： 数据集的划分： 训练数据，用于训练，构建模型； 测试数据，在模型检验时使用，用于评估模型是否有效。 特征工程 Feature Engineering：特征工程就是对特征值进行处理，选择有意义的特征输入机器学习的算法和模型进行训练。 字典特征提取 12345678910111213141516171819202122232425262728from sklearn.feature_extraction import DictVectorizer# list:[dict&#123;&#125;, dict&#123;&#125;, dict&#123;&#125;]data = [&#123;&#x27;city&#x27;: &#x27;北京&#x27;,&#x27;temperature&#x27;:100&#125;, &#123;&#x27;city&#x27;: &#x27;上海&#x27;,&#x27;temperature&#x27;:60&#125;, &#123;&#x27;city&#x27;: &#x27;深圳&#x27;,&#x27;temperature&#x27;:30&#125;]# 实例化一个转换器类，此处 sparse=True 设置结果为稀疏矩阵方便查看transfer = DictVectorizer(sparse=True)# 调用 fit_transform() 进行特征提取data_new = transfer.fit_transform(data)# 打印特征提取结果print(&quot;data_new_sparse:\\n&quot;, data_new, &quot;\\n&quot;, type(data_new))print(&quot;data_new:\\n&quot;, data_new.toarray())print(&quot;特征名字:\\n&quot;, transfer.get_feature_names())&quot;&quot;&quot; 打印结果data_new_sparse: (0, 1) 1.0 (0, 3) 100.0 (1, 0) 1.0 (1, 3) 60.0 (2, 2) 1.0 (2, 3) 30.0 &lt;class &#x27;scipy.sparse.csr.csr_matrix&#x27;&gt;data_new: [[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 30.]]特征名字: [&#x27;city=上海&#x27;, &#x27;city=北京&#x27;, &#x27;city=深圳&#x27;, &#x27;temperature&#x27;]&quot;&quot;&quot; 由上面的例子可以看出，经过 sklearn 的字典特征转换后，其类型转换为 matrix，即一个矩阵类型。 原始数据中的特征值只有 2 个：city、temperature；但转换出来的特征矩阵却有 4 列，这是为了将特征值变量转换为机器学习算法易于利用的一种形式，此转换过程称为 one hot 编码。 文本特征提取 CountVectorizer 123456789101112131415from sklearn.feature_extraction.text import CountVectorizerdata = [&quot;How to shift your mindset and choose your future&quot;, &quot;I like python, Do you like python?&quot;]# 实例化一个转换器类，可以屏蔽一些没有意义的词语transfer = CountVectorizer(stop_words=[&quot;and&quot;, &quot;to&quot;])# 调用 fit_transformdata_new = transfer.fit_transform(data)print(&quot;data_new:\\n&quot;, data_new.toarray())print(&quot;特征名字：\\n&quot;, transfer.get_feature_names())&quot;&quot;&quot; 打印结果data_new: [[1 0 1 1 0 1 0 1 0 2] [0 1 0 0 2 0 2 0 1 0]]特征名字： [&#x27;choose&#x27;, &#x27;do&#x27;, &#x27;future&#x27;, &#x27;how&#x27;, &#x27;like&#x27;, &#x27;mindset&#x27;, &#x27;python&#x27;, &#x27;shift&#x27;, &#x27;you&#x27;, &#x27;your&#x27;]&quot;&quot;&quot; 思考：若是对一句中文文本进行文本特征提取，那会出现什么结果？ 对于中文文本，因为词语之间不存在空格，所以需要分词库先进行分词：&quot; &quot;.join(list(jieba.cut(text))) TfidfVectorizer Term Frequency - Inverse Document Frequency，即词频 - 逆文本频率，由 TF 和 IDF 组成。 TF，即普通的词频统计，这个比较好理解， IDF，用于反应一个词的重要性，进而修正仅仅用词频表示的词特征值。比如，“to” 这个词的词频非常高，但它在每一篇文章中都出现了，那么它这个高词频就没有什么太大的意义。 1234567891011121314151617181920212223242526272829303132333435from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerimport jiebadata = [&quot;今天是星期一，有点快乐&quot;, &quot;今天是星期二，无敌快乐&quot;, &quot;今天是星期三，超级快乐&quot;]data_new = []for sent in data: data_new.append(&quot; &quot;.join(list(jieba.cut(sent))))&quot;&quot;&quot;data_new[&#x27;今天 是 星期一 ， 有点 快乐&#x27;, &#x27;今天 是 星期二 ， 无敌 快乐&#x27;, &#x27;今天 是 星期三 ， 超级 快乐&#x27;]&quot;&quot;&quot;# 转换器transfer = TfidfVectorizer()transfer_normal = CountVectorizer()# 特征提取data_final = transfer.fit_transform(data_new)data_normal = transfer_normal.fit_transform(data_new)print(&quot;data_new:\\n&quot;, data_final.toarray())print(&quot;data_new_normal:\\n&quot;, data_normal.toarray())print(&quot;特征名字：\\n&quot;, transfer.get_feature_names())&quot;&quot;&quot;打印结果data_new: [[0.35959372 0.35959372 0. 0.6088451 0. 0. 0.6088451 0. ] [0.35959372 0.35959372 0.6088451 0. 0. 0.6088451 0. 0. ] [0.35959372 0.35959372 0. 0. 0.6088451 0. 0. 0.6088451 ]]data_new_normal: [[1 1 0 1 0 0 1 0] [1 1 1 0 0 1 0 0] [1 1 0 0 1 0 0 1]]特征名字： [&#x27;今天&#x27;, &#x27;快乐&#x27;, &#x27;无敌&#x27;, &#x27;星期一&#x27;, &#x27;星期三&#x27;, &#x27;星期二&#x27;, &#x27;有点&#x27;, &#x27;超级&#x27;]&quot;&quot;&quot; 分析上述结果，普通的词频分析得出的结果是词频相同，而 tf-idf 分析，对于每个句子中都出现的词的重要性会有所降低。 特征预处理 无量纲化：进行无量纲化后表征不同属性（单位不同）的各特征之间才有可比性，如 1cm 与 0.1kg 如何比较？ 归一化 无法处理异常值（最大值，最小值）； 123456789from sklearn.preprocessing import MinMaxScalerx_mm = [[1,-1,2],[2,0,0],[0,1,-1]]x_mm_new = MinMaxScaler().fit_transform(x_mm)&quot;&quot;&quot;x_mm_newarray([[0.5 , 0. , 1. ], [1. , 0.5 , 0.33333333], [0. , 1. , 0. ]])&quot;&quot;&quot; 标准化 标准差（描述样本集中程度，异常值对总样本的集中程度的影响不大），在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。 12345678from sklearn.preprocessing import StandardScalerx = [[1,2,3],[4,5,6],[1,2,1]]x_new = StandardScaler().fit_transform(x)&quot;&quot;&quot;x_newarray([[-0.70710678, -0.70710678, -0.16222142], [ 1.41421356, 1.41421356, 1.29777137], [-0.70710678, -0.70710678, -1.13554995]])&quot;&quot;&quot; 特征降维 这里的降维的维度，不是指矩阵、向量、标量等，而是指降低特征的个数，让最终得到的数据中的每个特征之间互不相关。 思考：特征之间相关度过高会有什么问题？？？若 m、n 都可以得出 r，那么去掉 m 、n 中的任意一个特征对结果都不会影响，在一个庞大的数据中，去掉某些相关性特别高的特征可以节省很多算力与内存空间，模型更加高效。 Filter 方差选择法：低方差特征过滤，即过滤差别不大的特征。 12345678910111213141516171819202122232425262728293031323334from sklearn.feature_selection import VarianceThresholdimport pandas as pdfrom scipy.stats import pearsonr# 获取数据data = pd.read_csv(&quot;factor_returns.csv&quot;)# 保留每一行，保留从[第二列到倒数第二列)data = data.iloc[:, 1:-2]data_v = data.valuesprint(&quot;data:\\n&quot;, data_v.shape)# 实例化一个转换器类transfer = VarianceThreshold(threshold=10)# 调用 fit_transformdata_new = transfer.fit_transform(data)print(&quot;data_new:\\n&quot;,data_new.shape)# 计算某两个变量之间的相关系数r1 = pearsonr(data[&quot;pe_ratio&quot;], data[&quot;pb_ratio&quot;])print(&quot;pe_ratio 与 pb_ratio 之间的相关性：\\n&quot;, r1)r2 = pearsonr(data[&#x27;revenue&#x27;], data[&#x27;total_expense&#x27;])print(&quot;revenue 与 total_expense 之间的相关性：\\n&quot;, r2)&quot;&quot;&quot;打印结果data: (2318, 9)data_new: (2318, 7)pe_ratio 与 pb_ratio 之间的相关性： (-0.0043893227799362685, 0.8327205496564927)revenue 与 total_expense 之间的相关性： (0.9958450413136116, 0.0)&quot;&quot;&quot; pearsonr：皮尔逊相关系数，取值范围：–1≤ r ≤+1。 若特征与特征之间相关性很高： 选取其中一个； 加权求和； 主成分分析； 主成分分析 PCA，Principal components analysis。 1transfer = PCA(n_components=0.95) # 进行 PCA 降维，并保存 95% 的信息。 案例分析：购物车 预测某个 user_id 下次会购买 aisle。 123456789101112131415161718192021222324252627282930313233343536import pandas as pd# 获取数据# 订单与商品信息order_products = pd.read_csv(&quot;./instacart/order_products__prior.csv&quot;)# 商品详细信息products = pd.read_csv(&quot;./instacart/products.csv&quot;)# 订单详细信息orders = pd.read_csv(&quot;./instacart/orders.csv&quot;)# 商品所属类别信息aisles = pd.read_csv(&quot;./instacart/aisles.csv&quot;)# 打印特征值找规律，将所有的特征值合并到一张表中然后进行 PCA 降维print(order_products.columns.values.tolist(),&quot;\\n&quot;, products.columns.values.tolist(), &quot;\\n&quot;, orders.columns.values.tolist(),&quot;\\n&quot;, aisles.columns.values.tolist())&quot;&quot;&quot;打印结果[&#x27;order_id&#x27;, &#x27;product_id&#x27;, &#x27;add_to_cart_order&#x27;, &#x27;reordered&#x27;] [&#x27;product_id&#x27;, &#x27;product_name&#x27;, &#x27;aisle_id&#x27;, &#x27;department_id&#x27;] [&#x27;order_id&#x27;, &#x27;user_id&#x27;, &#x27;eval_set&#x27;, &#x27;order_number&#x27;, &#x27;order_dow&#x27;, &#x27;order_hour_of_day&#x27;, &#x27;days_since_prior_order&#x27;] [&#x27;aisle_id&#x27;, &#x27;aisle&#x27;]&quot;&quot;&quot;tab1 = pd.merge(aisles, products, on=[&quot;aisle_id&quot;, &quot;aisle_id&quot;])tab2 = pd.merge(tab1, order_products, on=[&quot;product_id&quot;, &quot;product_id&quot;])tab3 = pd.merge(tab2, orders, on=[&quot;order_id&quot;, &quot;order_id&quot;])tab3.columns.values.tolist()&quot;&quot;&quot;打印结果[&#x27;aisle_id&#x27;, &#x27;aisle&#x27;, &#x27;product_id&#x27;, &#x27;product_name&#x27;, &#x27;department_id&#x27;, &#x27;order_id&#x27;, &#x27;add_to_cart_order&#x27;, &#x27;reordered&#x27;, &#x27;user_id&#x27;, &#x27;eval_set&#x27;, &#x27;order_number&#x27;, &#x27;order_dow&#x27;, &#x27;order_hour_of_day&#x27;, &#x27;days_since_prior_order&#x27;]&quot;&quot;&quot;# 虽然所有的数据特征都在一个表中，但表的形式不是需要的 user_id 与 aisle 的关系# 利用交叉表反映的 user_id 和 aisle 之间的关系table = pd.crosstab(tab3[&quot;user_id&quot;], tab3[&quot;aisle&quot;]) 123456789101112# 有大量的特征值，对数据进行降维处理from sklearn.decomposition import PCAdata = table[:10000]data.shape# (10000, 134)from sklearn.decomposition import PCA# 实例化一个转换器类transfer = PCA(n_components=0.95)# 调用 fit_transformdata_new = transfer.fit_transform(data)data_new.shape# (10000, 42) 在上述案例中，通过 PCA 降维，将 134 个特征值降到了 42 个，并且还保留了原本数据 95% 的信息，可见 PCA 降维还是挺好用的。 分类算法 sklearn 的转换器和估计器。 转换器（transformer），特征工程的父类； fit_transform() 其实是两个过程：fit() 计算每一列的平均值、标准差；transform() 实现最终的转换。 估计器（estimator），机器学习算法的实现。 1234estimator.fit(x_train, y_train) # 计算生成模型y_predict = estimator.predict(x_test) # 对测试数据进行测试y_test == y_predict # 测试结果与原有结果进行对比accuracy = estimator.score(x_test, y_test) # 计算准确率 K - 近邻算法 即，根据邻近的类型来推测出自身类型。邻近指的是距离： 欧氏距离；曼哈顿距离（绝对值距离），明可夫斯基距离。 鸢尾花分类案例 12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifier# 获取数据iris = load_iris()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)# 特征工程transfer = StandardScaler()# 训练集和测试集都要做一样的处理x_train = transfer.fit_transform(x_train)# 测试基使用测试机的 fit() 进行计算x_test = transfer.transform(x_test)# KNN 算法预估器，自定义距离（不断尝试出一个准确率较高的距离）estimator = KNeighborsClassifier(n_neighbors=3)estimator.fit(x_train, y_train)# 模型评估y_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;直接比对真实值和预测值:\\n&quot;, y_test == y_predict)&quot;&quot;&quot;结果打印y_predict: [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]直接比对真实值和预测值: [ True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True False]&quot;&quot;&quot;# 计算准确率score = estimator.score(x_test, y_test)print(&quot;准确率为：\\n&quot;, score)&quot;&quot;&quot;结果打印准确率为： 0.9736842105263158&quot;&quot;&quot; 优点：简单，易于理解，易于实现，无需训练; 缺点：必须指定 K 值，K 值选择不当则分类精度不能保证；懒惰算法，对测试样本分类时的计算量大，内存开销大； 使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试。 模型选择与调优 有时候得到的模型的准确率不尽人意，这时就需要使用某些算法对模型进行选择和调优了。 交叉验证 验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成 3、5 和 10 份。 网格搜索 又称为超参数搜索。对 KNN 的 K 值进行参数调优，把一组 K 值传入网格搜索中，找出最优值。 鸢尾花 K 值调优 1234567891011121314151617181920212223estimator = KNeighborsClassifier()# 加入网格搜索与交叉验证# 参数准备param_dict = &#123;&quot;n_neighbors&quot;: [1, 3, 5, 7, 9, 11]&#125;# cv=10，进行10折交叉验证estimator = GridSearchCV(estimator, param_grid=param_dict, cv=10)estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;直接比对真实值和预测值:\\n&quot;, y_test == y_predict)score = estimator.score(x_test, y_test)print(&quot;准确率为：\\n&quot;, score)# 最佳参数：best_params_print(&quot;最佳参数：\\n&quot;, estimator.best_params_)# 最佳结果：best_score_print(&quot;最佳结果：\\n&quot;, estimator.best_score_)# 最佳估计器：best_estimator_print(&quot;最佳估计器:\\n&quot;, estimator.best_estimator_)# 交叉验证结果：cv_results_print(&quot;交叉验证结果:\\n&quot;, estimator.cv_results_) 朴素贝叶斯 Naive Bayesian。朴素？即，假设特征与特征之间是相互独立。 应用场景：文本分类，单词作为特征。 决策树 哪个特征值的信息增益大，那就应该先用这个特征值做过滤。 缺点：容易产生过拟合，即一个 overfitted 模型记住太多 training data 的细节从而降低了 generalization 的能力。 欠拟合：光看书不做题觉得自己会了，上了考场啥都不会； 过拟合: 做课后题全都能做对，上了考场还是啥都不会。 123456789101112131415161718192021222324252627282930313233343536373839from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.tree import DecisionTreeClassifier, export_graphviz# 获取数据集iris = load_iris()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)# 决策树预估器estimator = DecisionTreeClassifier(criterion=&quot;entropy&quot;)estimator.fit(x_train, y_train)# 模型评估# 直接比对真实值和预测值y_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;直接比对真实值和预测值:\\n&quot;, y_test == y_predict)# 计算准确率score = estimator.score(x_test, y_test)print(&quot;准确率为：\\n&quot;, score)&quot;&quot;&quot;结果打印y_predict: [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 1 0 0 1 1 1 0 0 0]直接比对真实值和预测值: [ True True True True True True True False True True True True True True True True True True False True True True True True True True True True True False True True True True True True True True]准确率为： 0.9210526315789473&quot;&quot;&quot;# 可视化决策树export_graphviz(estimator, out_file=&quot;iris_tree.dot&quot;, feature_names=iris.feature_names) 在线可视化 随机森林 森林：包含多个决策树的分类器。 12345678# 数据同决策树from sklearn.ensemble import RandomForestClassifier# 使用随机森林estimator = RandomForestClassifier()estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print(estimator.score(x_test, y_test)) 案例分析：签到预测 根据给定的数据，预测人们最可能在地图上的哪个位置进行签到。 12345import pandas as pd# 获取数据data = pd.read_csv(&quot;./FBlocation/train.csv&quot;)data.head() 123456789101112131415161718192021# 缩小范围，方便实验data = data.query(&quot;x &lt; 2.5 &amp; x &gt; 2 &amp; y &lt; 1.5 &amp; y &gt; 1.0&quot;)# 对数据进行处理，这个时间看的很难受# 时间处理成便于观察的形式time_value = pd.to_datetime(data[&quot;time&quot;], unit=&quot;s&quot;)time_value.head()# 年月的差别不大，可不考虑&quot;&quot;&quot;结果打印112 1970-01-08 05:06:14180 1970-01-08 01:29:55367 1970-01-07 17:01:07874 1970-01-02 15:52:461022 1970-01-03 09:46:33Name: time, dtype: datetime64[ns]&quot;&quot;&quot;# 得到时间列date = pd.DatetimeIndex(time_value)# 给数据增加日期，星期，时间这些特征值data[&quot;day&quot;] = date.daydata[&quot;weekday&quot;] = date.weekdaydata[&quot;hour&quot;] = date.hour 12# 根据 place_id 成组，过滤签到次数比较少的地点place_count = data.groupby(&quot;place_id&quot;).count()[&quot;row_id&quot;] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 过滤place_count[place_count &gt; 3].head()# 根据得到的 place_id 对数据进行过滤data_final = data[data[&quot;place_id&quot;].isin(place_count[place_count &gt; 3].index.values)]# 筛选特征值和目标值x = data_final[[&quot;x&quot;, &quot;y&quot;, &quot;accuracy&quot;, &quot;day&quot;, &quot;weekday&quot;, &quot;hour&quot;]]y = data_final[&quot;place_id&quot;]from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import GridSearchCV# 数据集划分x_train, x_test, y_train, y_test = train_test_split(x, y)# 特征工程：标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# KNN 算法预估器estimator = KNeighborsClassifier()# 加入网格搜索与交叉验证# 参数准备param_dict = &#123;&quot;n_neighbors&quot;: [3, 5, 7, 9]&#125;estimator = GridSearchCV(estimator, param_grid=param_dict, cv=3)estimator.fit(x_train, y_train)# 模型评估# 直接比对真实值和预测值y_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;直接比对真实值和预测值:\\n&quot;, y_test == y_predict)# 计算准确率score = estimator.score(x_test, y_test)print(&quot;准确率为：\\n&quot;, score)# 最佳参数：best_params_print(&quot;最佳参数：\\n&quot;, estimator.best_params_)# 最佳结果：best_score_print(&quot;最佳结果：\\n&quot;, estimator.best_score_)# 最佳估计器：best_estimator_print(&quot;最佳估计器:\\n&quot;, estimator.best_estimator_)# 交叉验证结果：cv_results_print(&quot;交叉验证结果:\\n&quot;, estimator.cv_results_) 案例分析：生存预测 根据人的信息预测此人在此灾难中存活下来的概率。 123456789101112131415161718192021222324252627282930313233343536import pandas as pdtitanic = pd.read_csv(&quot;titanic.csv&quot;)# 筛选特征值和目标值x = titanic[[&quot;pclass&quot;, &quot;age&quot;, &quot;sex&quot;]]y = titanic[&quot;survived&quot;]# 缺失值处理x[&quot;age&quot;].fillna(x[&quot;age&quot;].mean(), inplace=True)# 转换成字典，方便处理x = x.to_dict(orient=&quot;records&quot;)from sklearn.model_selection import train_test_split# 数据集划分x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)# 字典特征抽取from sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifiertransfer = DictVectorizer()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth=3)estimator.fit(x_train, y_train)y_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;直接比对真实值和预测值:\\n&quot;, y_test == y_predict)score = estimator.score(x_test, y_test)print(&quot;准确率为：\\n&quot;, score) 回归算法 线性回归 目标值为连续性的数据。 正规方程 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error# 正规方程的优化方法对波士顿房价进行预测# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = LinearRegression()estimator.fit(x_train, y_train)# 得出模型print(&quot;正规方程-权重系数为：\\n&quot;, estimator.coef_)print(&quot;正规方程-偏置为：\\n&quot;, estimator.intercept_)# 模型评估y_predict = estimator.predict(x_test)print(&quot;预测房价：\\n&quot;, y_predict)error = mean_squared_error(y_test, y_predict)print(&quot;正规方程-均方误差为：\\n&quot;, error) 梯度下降 一个不断逼近的，勤奋努力的算法。 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import SGDRegressorfrom sklearn.metrics import mean_squared_error# 梯度下降的优化方法对波士顿房价进行预测# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = SGDRegressor(learning_rate=&quot;constant&quot;, eta0=0.01, max_iter=10000, penalty=&quot;l1&quot;)estimator.fit(x_train, y_train)# 得出模型print(&quot;梯度下降-权重系数为：\\n&quot;, estimator.coef_)print(&quot;梯度下降-偏置为：\\n&quot;, estimator.intercept_)# 模型评估y_predict = estimator.predict(x_test)print(&quot;预测房价：\\n&quot;, y_predict)error = mean_squared_error(y_test, y_predict)print(&quot;梯度下降-均方误差为：\\n&quot;, error) 岭回归 带有 L2 正则化的线性回归：岭回归 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import Ridgefrom sklearn.metrics import mean_squared_error# 梯度下降的优化方法对波士顿房价进行预测# 获取数据boston = load_boston()# 划分数据集x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22)# 标准化transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 预估器estimator = Ridge(alpha=0.5, max_iter=10000)estimator.fit(x_train, y_train)# 得出模型print(&quot;岭回归-权重系数为：\\n&quot;, estimator.coef_)print(&quot;岭回归-偏置为：\\n&quot;, estimator.intercept_)# 模型评估y_predict = estimator.predict(x_test)print(&quot;预测房价：\\n&quot;, y_predict)error = mean_squared_error(y_test, y_predict)print(&quot;岭回归-均方误差为：\\n&quot;, error) 逻辑回归 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。 精确率 Precision 即，查的准不准。 召回率 Recall 即，查的全不全。 F1-score 模型的稳健性。 案例分析：肿瘤预测 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import pandas as pdimport numpy as np# 读取数据path = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;column_name = [&#x27;Sample code number&#x27;, &#x27;Clump Thickness&#x27;, &#x27;Uniformity of Cell Size&#x27;, &#x27;Uniformity of Cell Shape&#x27;, &#x27;Marginal Adhesion&#x27;, &#x27;Single Epithelial Cell Size&#x27;, &#x27;Bare Nuclei&#x27;, &#x27;Bland Chromatin&#x27;, &#x27;Normal Nucleoli&#x27;, &#x27;Mitoses&#x27;, &#x27;Class&#x27;]data = pd.read_csv(path, names=column_name)# 缺失值处理# 替换 》 np.nandata = data.replace(to_replace=&quot;?&quot;, value=np.nan)# 删除缺失样本data.dropna(inplace=True)# 检查是否还存在缺失值data.isnull().any()# 划分数据集from sklearn.model_selection import train_test_split# 筛选特征值和目标值x = data.iloc[:, 1:-1]y = data[&quot;Class&quot;]x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)# 标准化from sklearn.preprocessing import StandardScalertransfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 逻辑回归预测from sklearn.linear_model import LogisticRegressionestimator = LogisticRegression()estimator.fit(x_train, y_train)# 逻辑回归的模型参数：回归系数和偏置print(&quot;逻辑回归-回归系数：&quot;, estimator.coef_, &quot;逻辑回归-偏置：&quot; , estimator.intercept_)y_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;直接比对真实值和预测值:\\n&quot;, y_test == y_predict)score = estimator.score(x_test, y_test)print(&quot;准确率为：\\n&quot;, score)# 查看精确率、召回率、F1-scorefrom sklearn.metrics import classification_reportreport = classification_report(y_test, y_predict, target_names=[&quot;良性&quot;, &quot;恶性&quot;])# y_true：每个样本的真实类别，必须为 0 反，1 正标记# 将 y_test 转换成 0 1y_true = np.where(y_test &gt; 3, 1, 0)from sklearn.metrics import roc_auc_scoreroc_auc_score(y_true, y_predict) K-means 无监督算法。 购物车案例 123456789101112# 预估器流程from sklearn.cluster import KMeansestimator = KMeans(n_clusters=3)estimator.fit(data_new)y_predict = estimator.predict(data_new)y_predict[:300]# 模型评估-轮廓系数from sklearn.metrics import silhouette_scoresilhouette_score(data_new, y_predict)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"sklearn","slug":"sklearn","permalink":"https://wingowen.github.io/tags/sklearn/"}]},{"title":"RabbitMQ 初步使用","slug":"后台技术/RabbitMQ 初步使用","date":"2020-04-20T06:02:25.000Z","updated":"2023-09-20T07:12:54.220Z","comments":true,"path":"2020/04/20/后台技术/RabbitMQ 初步使用/","link":"","permalink":"https://wingowen.github.io/2020/04/20/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/RabbitMQ%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"RabbitMQ 的简单介绍及项目使用。","text":"RabbitMQ 的简单介绍及项目使用。 在虚拟机中基于 Docker 安装 RabbitMQ 镜像，启动镜像并暴露其端口。 1docker run -d -p 5672:5672 -p 15672:15672 --hostname my_rabbitmq --name rabbitmq rabbitmq:management 访问 [虚拟机 IP]:15672 进入 RabbitMQ 的控制台页面，默认账号密码皆为 guest。 基本概念 高级消息队列协议，即 Advanced Message Queuing Protocol（AMQP）是面向消息中间件提供的开放的应用层协议，其设计目标是对于消息的排序、路由（包括点对点和订阅-发布）、保持可靠性、保证安全性。 RabbitMQ 实现了 AMQP。 RabbitMQ 的基本使用流程： 客户端连接到消息队列服务器，打开一个 channel； 客户端声明一个 exchange，并设置相关属性； 客户端声明一个 queue，并设置相关属性； 客户端使用 routing key，在 exchange 和 queue 之间建立好绑定关系； 客户端投递消息到 exchange。 Broker：简单来说就是消息队列服务器实体。 Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。 Queue：消息队列载体，每个消息都会被投入到一个或多个队列。 Binding：绑定，它的作用就是把 exchange 和 queue 按照路由规则绑定起来。 Routing Key：路由关键字，exchange 根据这个关键字进行消息投递。 vhost：虚拟主机，一个 broker 里可以开设多个 vhost，用作不同用户的权限分离。 producer：消息生产者，就是投递消息的程序。 consumer：消息消费者，就是接受消息的程序。 channel：消息通道，在客户端的每个连接里，可建立多个 channel，每个 channel 代表一个会话任务。 简单示例 编写一个基本的 consumer 以及 provider 进行消息的接受与投递。 simple-consumer 编写一个简单的消费者连接到 RabbitMQ，并且初始化所需的 Exchange、Queue、Routing Key、Binding 等。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 123456789spring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000spring.rabbitmq.listener.simple.acknowledge-mode=manualspring.rabbitmq.listener.simple.concurrency=5spring.rabbitmq.listener.simple.max-concurrency=10 Receiver 通过 @RabbitListener进行初始化并绑定：可在配置文件中编写后通过$&#123;xxx.xxx...&#125;进行取值。 ！！！注意！！！此处的 Order 类依赖于 provider 模块的 Order 类。 1234567891011121314151617181920212223@Componentpublic class RabbitReceiver &#123; @RabbitListener( bindings = @QueueBinding( value = @Queue(value = &quot;queue-1&quot;, durable=&quot;true&quot;), exchange = @Exchange(value = &quot;exchange-1&quot;, durable = &quot;true&quot;, type = &quot;topic&quot;, ignoreDeclarationExceptions = &quot;true&quot;), key = &quot;springboot.*&quot; ) ) @RabbitHandler public void onOrderMessage(@Payload Order order, Channel channel, @Headers Map&lt;String, Object&gt; headers) throws Exception &#123; System.err.println(&quot;--------------------------------------&quot;); System.err.println(&quot;消费端 Order: &quot; + order.getId()); Long deliveryTag = (Long)headers.get(AmqpHeaders.DELIVERY_TAG); //手工ACK channel.basicAck(deliveryTag, false); &#125;&#125; 运行项目。 提示：连接成功。打开 RabbitMQ 控制台。 可以看到有一条来自本地端口 9371 的连接。 查看 Exchange 可以看到客户端所绑定的规则已经被自动初始化（当然也可以选择手工绑定） simple-provider 编写一个简单的提供者连接到 RabbitMQ，并发送消息到对应的队列中。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910spring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000# 启用回调spring.rabbitmq.publisher-confirm-type=correlatedspring.rabbitmq.publisher-returns=truespring.rabbitmq.template.mandatory=true 新建一个实体对象用于发送。 12345678910public class Order implements Serializable &#123; private static final long serialVersionUID = -3603050220195298978L; private String id; private String name; // ...&#125; Sender 编写发送方法，并自定义回调形式。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class RabbitSender &#123; @Autowired private RabbitTemplate rabbitTemplate; // 回调函数: confirm 确认 final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.err.println(&quot;correlationData: &quot; + correlationData); System.err.println(&quot;ack: &quot; + ack); if(!ack)&#123; System.err.println(&quot;异常处理....&quot;); &#125; &#125; &#125;; // 回调函数: return 返回 final RabbitTemplate.ReturnCallback returnCallback = new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(org.springframework.amqp.core.Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.err.println(&quot;return exchange: &quot; + exchange + &quot;, routingKey: &quot; + routingKey + &quot;, replyCode: &quot; + replyCode + &quot;, replyText: &quot; + replyText); &#125; &#125;; // 发送消息方法调用: 构建自定义对象消息 public void sendOrder(Order order) throws Exception &#123; rabbitTemplate.setConfirmCallback(confirmCallback); rabbitTemplate.setReturnCallback(returnCallback); // id + 时间戳 全局唯一 CorrelationData correlationData = new CorrelationData(&quot;987654321&quot;); rabbitTemplate.convertAndSend(&quot;exchange-1&quot;, &quot;springboot.order&quot;, order, correlationData); &#125;&#125; 编写测试方法进行测试。 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestclass SimpleProviderApplicationTests &#123; @Autowired private RabbitSender rabbitSender; @Test public void testSender() throws Exception &#123; Order order = new Order(&quot;001&quot;, &quot;第一个订单&quot;); rabbitSender.sendOrder(order); &#125;&#125; 控制台打印了了自定义回调的确认信息。 RabbitMQ 控制台可以查看到发送的消息的详细信息。 启动消费者后，消费者监听到队列中有消息并了进行消费。 可靠交付 可靠交付即将 RabbitMQ 处理订单的状态持久化到数据库中，并根据此状态表对不同状态的交付任务进行处理： 1）在达到规定的最大重试次数之前，定时任务在规定的时间间隔自动进行任务的重新交付； 2）任务达到了最大的重试次数后，为避免浪费资源将其状态更新为失败，之后人工处理。 12345678910111213141516171819-- 表 my_order 订单结构CREATE TABLE IF NOT EXISTS `my_order` ( `id` varchar(128) NOT NULL, -- 订单 ID `name` varchar(128), -- 订单名称 其他业务熟悉忽略 `message_id` varchar(128) NOT NULL, -- 消息唯一 ID PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;-- 表 log 消息记录结构CREATE TABLE IF NOT EXISTS `log` ( `message_id` varchar(128) NOT NULL, -- 消息唯一 ID `message` varchar(4000) DEFAULT NULL, -- 消息内容 `try_count` int(4) DEFAULT &#x27;0&#x27;, -- 重试次数 `status` varchar(10) DEFAULT &#x27;&#x27;, -- 消息投递状态：0 投递中 1 投递成功 2 投递失败 `next_retry` timestamp NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27;, -- 下一次重试时间 `create_time` timestamp NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27;, -- 创建时间 `update_time` timestamp NOT NULL DEFAULT &#x27;0000-00-00 00:00:00&#x27;, -- 更新时间 PRIMARY KEY (`message_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 项目依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- RabbitMQ --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mybatis-plus --&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- 常用工具类 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.12&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819## 配置数据源spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/rabbitmq_test?useSSL=falsespring.datasource.driver-class-name=com.mysql.jdbc.Driver# 打印 SQL 语句#mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImplmybatis-plus.mapper-locations=classpath:/mapper/*.xml## RabbitMQspring.rabbitmq.addresses=192.168.31.100:5672spring.rabbitmq.username=guestspring.rabbitmq.password=guestspring.rabbitmq.virtual-host=/spring.rabbitmq.connection-timeout=15000# 回调配置spring.rabbitmq.publisher-confirm-type=correlatedspring.rabbitmq.publisher-returns=truespring.rabbitmq.template.mandatory=true 新建一个 rabbitmq-entity 模块，编写传输实体类。 12345678910public class Order implements Serializable &#123; private static final long serialVersionUID = 2584584237793643231L; private String id; private String name; private String messageId; //...&#125; 新建一个 stable-provider 模块，编写可靠的交付者。 编写状态的实体类，以及常量类 12345678910111213@Datapublic class Log implements Serializable &#123; private static final long serialVersionUID = -8716210355448974538L; private String messageId; private String message; private Integer tryCount = 0; private String status; private Date nextRetry; private Date createTime; private Date updateTime;&#125; 1234567891011121314151617public class Constant &#123; private Constant() &#123; throw new IllegalStateException(&quot;Utility class&quot;); &#125; /** 订单状态 0 表示待发送 */ public static final String ORDER_SENDING = &quot;0&quot;; /** 订单状态 1 表示发送成功 */ public static final String ORDER_SEND_SUCCESS = &quot;1&quot;; /** 订单状态 2 表示发送失败 */ public static final String ORDER_SEND_FAILURE = &quot;2&quot;; /** 订单超时时间，即一分钟后订单进行交付重试 */ public static final Integer ORDER_TIMEOUT = 1; /** 订单最大重试次数 */ public static final Integer ORDER_RETRY_MAX_TIMES = 3;&#125; 编写 Mapper。 1234567891011121314151617181920212223242526@Componentpublic interface LogMapper extends BaseMapper&lt;Log&gt; &#123; /** * 更新最终消息发送结果 成功 or 失败 * @param messageId 消息的唯一 ID * @param status 订单状态 * @param updateTime 订单状态更新时间 */ void changeBrokerMessageLogStatus(@Param(&quot;messageId&quot;) String messageId , @Param(&quot;status&quot;) String status , @Param(&quot;updateTime&quot;) Date updateTime ); /** * 查询消息状态为 0 且已经超时的消息集合 * @return 符合条件的订单集合 */ List&lt;Log&gt; query4StatusAndTimeoutMessage(); /** * @param messageId 消息的唯一 ID * @param date 时间 */ void update4ReSend(@Param(&quot;messageId&quot;) String messageId, @Param(&quot;updateTime&quot;) Date date);&#125; 12345678910// Order 为 SQL 关键字，并且只有一个插入方法，故完全使用自定义方法不继承@Componentpublic interface OrderMapper&#123; /** * @param order 订单 */ void insert(Order order);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.wingo.stableprovider.mapper.LogMapper&quot;&gt; &lt;!-- 通用查询映射结果 --&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.wingo.stableprovider.entity.Log&quot;&gt; &lt;id column=&quot;message_id&quot; property=&quot;messageId&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;message&quot; property=&quot;message&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;try_count&quot; property=&quot;tryCount&quot; jdbcType=&quot;INTEGER&quot; /&gt; &lt;result column=&quot;status&quot; property=&quot;status&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;next_retry&quot; property=&quot;nextRetry&quot; jdbcType=&quot;TIMESTAMP&quot; /&gt; &lt;result column=&quot;create_time&quot; property=&quot;createTime&quot; jdbcType=&quot;TIMESTAMP&quot; /&gt; &lt;result column=&quot;update_time&quot; property=&quot;updateTime&quot; jdbcType=&quot;TIMESTAMP&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; message_id, message, try_count, status, next_retry, create_time, update_time &lt;/sql&gt; &lt;update id=&quot;changeBrokerMessageLogStatus&quot; &gt; UPDATE log SET status = #&#123;status&#125;, update_time = #&#123;updateTime&#125; WHERE message_id = #&#123;messageId&#125; &lt;/update&gt; &lt;select id=&quot;query4StatusAndTimeoutMessage&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT &lt;include refid=&quot;Base_Column_List&quot; /&gt; FROM log WHERE status = &#x27;0&#x27; AND next_retry &lt;![CDATA[ &lt;= ]]&gt; sysdate() &lt;/select&gt; &lt;update id=&quot;update4ReSend&quot; &gt; UPDATE log SET try_count = try_count + 1, update_time = #&#123;updateTime&#125; WHERE message_id = #&#123;messageId&#125; &lt;/update&gt;&lt;/mapper&gt; 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.wingo.stableprovider.mapper.OrderMapper&quot;&gt; &lt;!-- 通用查询映射结果 --&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.wingo.entity.Order&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;name&quot; property=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;message_id&quot; property=&quot;messageId&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; id, name, message_id &lt;/sql&gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;com.wingo.entity.Order&quot; &gt; INSERT INTO my_order (id, name, message_id) VALUES (#&#123;id&#125;, #&#123;name&#125;, #&#123;messageId&#125;) &lt;/insert&gt;&lt;/mapper&gt; 编写一个服务用于向数据库初始化订单数据以及消息状态信息。 123456789101112131415161718192021222324252627282930313233@Servicepublic class OrderService &#123; @Autowired private OrderMapper orderMapper; @Autowired private LogMapper logMapper; @Autowired private OrderSender orderSender; public void createOrder(Order order) throws Exception &#123; // 数据库插入订单信息 orderMapper.insert(order); // 初始化日志信息 Log log = new Log(); Date orderTime = new Date(); log.setMessageId(order.getMessageId()); // 将订单转为 JSON 字符串在重新发送时使用 String jOrder = JSON.toJSONString(order); log.setMessage(jOrder); log.setStatus(&quot;0&quot;); log.setNextRetry(DateUtils.addMinutes(orderTime, Constant.ORDER_TIMEOUT)); log.setCreateTime(new Date()); log.setUpdateTime(new Date()); // 数据库插入日志信息 logMapper.insert(log); // order message sender orderSender.sendOrder(order); &#125;&#125; Sender 编写 RabbitMQ 的交付类。 12345678910111213141516171819202122232425262728293031323334@Componentpublic class OrderSender &#123; @Autowired private RabbitTemplate rabbitTemplate; @Autowired private LogMapper logMapper; // 回调函数: confirm 确认 final RabbitTemplate.ConfirmCallback confirmCallback = new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.err.println(&quot;correlationData: &quot; + correlationData); String messageId = correlationData.getId(); if(ack)&#123; // 如果 confirm 返回成功，则对 Order 表中对应的订单状态进行更新 logMapper.changeBrokerMessageLogStatus(messageId, Constant.ORDER_SEND_SUCCESS, new Date()); &#125; else &#123; // 失败则进行具体的后续操作：重试或者补偿等手段 System.err.println(&quot;异常处理...&quot;); &#125; &#125; &#125;; // 发送消息方法调用: 构建自定义对象消息 public void sendOrder(Order order) throws Exception &#123; // 设置自定义回调方法 rabbitTemplate.setConfirmCallback(confirmCallback); // 消息唯一 ID CorrelationData correlationData = new CorrelationData(order.getMessageId()); rabbitTemplate.convertAndSend(&quot;order-exchange&quot;, &quot;order.stable&quot;, order, correlationData); &#125;&#125; Task 编写定时任务，定期对数据库中的任务状态进行读取，对重试满足条件的交付任务进行重新发送。 1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class RetryMessage &#123; @Autowired private OrderSender orderSender; @Autowired private LogMapper logMapper; // 3 秒后执行，之后每 10 秒执行一次 @Scheduled(initialDelay = 3000, fixedDelay = 10000) public void reSend()&#123; System.err.println(&quot;---------------定时任务开始---------------&quot;); // 取数据库中所有的状态为 0 并且已经超时的订单 List&lt;Log&gt; list = logMapper.query4StatusAndTimeoutMessage(); if(list.isEmpty())&#123; System.out.println(&quot;无交付失败订单&quot;); &#125; list.forEach(messageLog -&gt; &#123; if(messageLog.getTryCount() &gt;= Constant.ORDER_RETRY_MAX_TIMES)&#123; // 如果重试次数超过规定的最大次数则表示交付失败 logMapper.changeBrokerMessageLogStatus(messageLog.getMessageId(), Constant.ORDER_SEND_FAILURE, new Date()); System.out.println(&quot;有新的交付失败订单&quot;); &#125; else &#123; // 增加重试次数 logMapper.update4ReSend(messageLog.getMessageId(), new Date()); Order reSendOrder = JSON.parseObject(messageLog.getMessage(), Order.class); try &#123; orderSender.sendOrder(reSendOrder); System.out.println(&quot;重试订单交付&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); System.err.println(&quot;-----------异常处理-----------&quot;); &#125; &#125; &#125;); // forEach 结束 &#125;&#125; 主类上还需要加上几个注解。 123@MapperScan(&quot;com.wingo.stableprovider.mapper&quot;)@EnableScheduling@SpringBootApplication 交付失败时的控制台打印信息。 序列化错误 场景： consumer 和 provider 模块各自有一个可序列化的 entity.Order 类； 序列化与反序列化时，两个 Order 虽然代码相同，但底层序列化后的标记不同，虚拟机不会认为它们是同一个对象，出现反序列化错误。 1Caused by: org.springframework.messaging.converter.MessageConversionException: Cannot convert from 解决方案： 1）consumer 和 provider 引用同一个 entity.Order 类； 2）利用 jackson 工具进行转化，传递 Json 串。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"https://wingowen.github.io/tags/MQ/"}]},{"title":"Dubbo","slug":"后台技术/Dubbo 初步使用","date":"2020-04-17T01:50:34.000Z","updated":"2023-09-20T07:12:58.156Z","comments":true,"path":"2020/04/17/后台技术/Dubbo 初步使用/","link":"","permalink":"https://wingowen.github.io/2020/04/17/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Dubbo%20%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8/","excerpt":"Dubbo + Zookeeper 的简单介绍，以及整合使用。","text":"Dubbo + Zookeeper 的简单介绍，以及整合使用。 Dubbo 初步使用 基础理论 应用的发展与演变： 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架（ORM）是关键； 当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的 Web 框架（MVC）是关键； 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架（RPC）是关键； Dubbo 是一款高性能、轻量级的开源 Java RPC 框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心（SOA）即 Service Oriented Architecture 是关键。 环境搭建 Java yum search java-1.8 ！！！注意，需要下载两个 java-1.8.0-openjdk.x86_64 / java-1.8.0-openjdk-devel.x86_64 yum install ... whereis java查看 java 路径（/usr/lib/jvm/） 修改环境变量：vim /etc/profile 123export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-...export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar javac环境变量是否配置正确 Zookeeper 下载 zookeeper ：清华大学开源软件镜像站 wget [下载连接]下载压缩包 tar -zxvf zookeeper-x.x.x.tar.gz 在 /usr/local/ 下新建 zookeeper 目录进行解压 修改 zookeeper 配置文件名：zoo_sample.cfg 改为 zoo.cfg 配置环境变量：vi ~/.bash_profile 123export PATHexport ZOOKEEPER_HOME=/usr/local/zookeeper/zookeeper-3.4.11/export PATH=$ZOOKEEPER_HOME/bin:$PATH 常用操作 123zkServer.sh start # 开启 zookeeperjps # 显示当前所有 java 进程 pidzkServer.sh stop # 停止 zookeeper dubbo-admin netstat -aon|findstr “port-num” 查看占用端口的程序的 PID Dubbo 官方项目下载 修改 dubbo-admin 项目的配置（！！！zookeeper 的地址修改为自己配置的地址） 在 pom.xml 目录下：cmd 输入命令mvn clean package对项目进行打包 打包成功后生成 target 文件夹，target 目录下有一个可执行 jar 包 java -jar [jara_name]运行项目，运行成功后根据配置的路径进行访问 dubbo-monitor 生成的 target 文件中有一个压缩包，解压后的 assembly.bin 目录中运行 start.bat 脚本进行项目运行 项目架构 先来看一张官网给的架构图： 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 各节点间的调用关系说明： ​ 服务容器负责启动，加载，运行服务提供者。 ​ 服务提供者在启动时，向注册中心注册自己提供的服务。 ​ 服务消费者在启动时，向注册中心订阅自己所需的服务。 ​ 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 ​ 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 ​ 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 项目创建 基于对官网给出的架构分析得知我们需要两个角色：服务的提供者；服务的消费者。 消费者在自身的项目中可以如同调用本地方法一般调用提供者的方法；那么消费者和提供者必须都受到同一套接口的规范，在两个项目中都编写接口显得有些繁琐，所以可以新建一个 Maven 项目专门用于定义接口。 最外层套一层壳，添加三个 module，其中外壳项目和提供接口的项目为 Maven 项目即可。 dubbo-api 此项目用于提供接口规范，不需要添加任何依赖。编写一个实体类和一个服务类接口即可。 1234567891011public class User implements Serializable &#123; private static final long serialVersionUID = 5433406871746033298L; private Integer userId; private String username; private String password; // ...&#125; 定义接口： 123456789public interface UserService &#123; /** * 获取用户列表的方法 * * @return 用户的信息列表 */ List&lt;User&gt; getUserList();&#125; 要让 dubbo-consumer 以及 dubbo-provider 获取此接口，需要给这两个项目添加 dubbo-api 项目的依赖。 123456&lt;!-- 添加自定义接口依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.wingo&lt;/groupId&gt; &lt;artifactId&gt;dubbo-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; dubbo-provider 服务提供者编写了接口的具体实现方法，并且要把实现的方法通过 Dubbo 暴露出去。 123456&lt;!-- 添加 Dubbo 依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; Spring 以及 Dubbo 都要管理这个类。 12345678910@com.alibaba.dubbo.config.annotation.Service@Servicepublic class UserServiceImpl implements UserService &#123; @Override public List&lt;User&gt; getUserList() &#123; User user1 = new User(1,&quot;wingo&quot;,&quot;123456&quot;); User user2 = new User(2,&quot;admin&quot;,&quot;654321&quot;); return Arrays.asList(user1, user2); &#125;&#125; 在主类上标注 @EnableDubbo(scanBasePackages = “com.wingo.dubboprovider.service”)，并添加配置。 12345678910# 此服务的名称dubbo.application.name=user-service-provider# 注册中心的协议及地址dubbo.registry.address=127.0.0.1:2181dubbo.registry.protocol=zookeeper# 暴露的协议及端口dubbo.protocol.name=dubbodubbo.protocol.port=20881## 注册中心获取监控中心的信息dubbo.monitor.protocol=registry 启动项目，来到 dubbo-admin 的页面查看服务。 在 dubbo-admin 多了一个我们刚刚暴露出来的服务，由名称可以得知其暴露的是一个接口。 点击查看其对应的 IP 地址以及域名，正是本机的 IP 以及自定义的接口 20881。 dubbo-monitor 中可以看到这个应用的名称正是配置文件中的 user-service-provider。 dubbo-consumer 消费者向 dubbo 请求所需要的服务。 1234567891011@RestControllerpublic class UserController &#123; @Reference UserService userService; @GetMapping(&quot;/users&quot;) public List&lt;User&gt; userList ()&#123; return userService.getUserList(); &#125;&#125; 12345server.port=8082dubbo.application.name=dubbo-user-consumerdubbo.registry.address=zookeeper://127.0.0.1:2181dubbo.monitor.protocol=registry 启动消费者。 可以看出，提供者与消费者确实是通过接口通讯的。 成功请求数据。 Dubbo 特性 介绍一些 Dubbo 的使用特性。 高可用 Zookeeper 宕机与 Dubbo 直连：注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯。 集群负载均衡 Random LoadBalance： 随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance： 轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance： 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance： 一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 服务降级 当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 集群容错 Failfast Cluster：快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录； Failsafe Cluster：失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作； Failback Cluster：失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作； Forking Cluster：并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=“2” 来设置最大并行数； Broadcast Cluster：广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://wingowen.github.io/tags/Dubbo/"}]},{"title":"Spring Boot 任务","slug":"后台技术/Spring Boot/Spring Boot 任务","date":"2020-04-01T02:50:45.000Z","updated":"2023-09-20T07:13:14.137Z","comments":true,"path":"2020/04/01/后台技术/Spring Boot/Spring Boot 任务/","link":"","permalink":"https://wingowen.github.io/2020/04/01/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E4%BB%BB%E5%8A%A1/","excerpt":"Spring Boot 中的异步任务，定时任务，邮件任务的使用。","text":"Spring Boot 中的异步任务，定时任务，邮件任务的使用。 异步任务 在 Java 应用中，绝大多数情况下都是通过同步的方式来实现交互处理的。但是在处理与第三方系统交互的时候，容易造成响应迟缓的情况，之前大部分都是使用多线程来完成此类任务。其实，在Spring 3.x 之后，就已经内置了@Async、@EnableAysnc来完美解决这个问题。 在 Spring 中运用 Async 注解需要注意几点： 方法名必须是 public 进行修饰的，且不能是 static 方法； 不能与调用的方法在同一个类中； 需要把该方法注入到 Spring 容器中，就是在一个类中添加异步方法，并在此类上使用@Component之类的注解。 测试实例 定义 Task 类，创建三个处理函数分别模拟三个执行任务的操作，操作消耗时间随机取（10 秒内）。 12345678910111213141516171819202122232425262728293031323334353637@Componentpublic class Task &#123; public static Random random =new Random(); @Async // 注释为异步任务， public Future&lt;String&gt; doTaskOne() throws Exception &#123; System.out.println(&quot;Doing Task One&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(&quot;Task One Finished：Spend &quot; + (end - start) + &quot; Millisecond&quot;); // 若不用 Future 进行回调，无法确定任务已经完成 return new AsyncResult&lt;&gt;(&quot;Task One Finished&quot;); // 返回异步调用结果 &#125; @Async public Future&lt;String&gt; doTaskTwo() throws Exception &#123; System.out.println(&quot;Doing Task Two&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(&quot;Task Two Finished：Spend &quot; + (end - start) + &quot; Millisecond&quot;); return new AsyncResult&lt;&gt;(&quot;Task Two Finished&quot;); &#125; @Async public Future&lt;String&gt; doTaskThree() throws Exception &#123; System.out.println(&quot;Doing Task Three&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); System.out.println(&quot;Task Three Finished：Spend &quot; + (end - start) + &quot; Millisecond&quot;); return new AsyncResult&lt;&gt;(&quot;Task Three Finished&quot;); &#125;&#125; 12345678@SpringBootApplication@EnableAsync // 异步任务生效public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 123456789101112131415161718192021@Testpublic void test() throws Exception &#123; long start = System.currentTimeMillis(); Future&lt;String&gt; taskOne = task.doTaskOne(); Future&lt;String&gt; taskTwo = task.doTaskTwo(); Future&lt;String&gt; taskThree = task.doTaskThree(); while(true) &#123; if(taskOne.isDone() &amp;&amp; taskTwo.isDone() &amp;&amp; taskThree.isDone()) &#123; // 三个任务都调用完成，退出循环等待 break; &#125; Thread.sleep(1000); &#125; long end = System.currentTimeMillis(); System.out.println(&quot;All Finished，总耗时：&quot; + (end - start) + &quot;毫秒&quot;);&#125; 定时任务 项目开发中经常需要执行一些定时任务，比如需要在每天凌晨时候，分析一次前一天的日志信息。Spring 为我们提供了异步执行任务调度的方式，提供 TaskExecutor 、TaskScheduler 接口。 注解：@EnableScheduling、@Scheduled corn 表达式 *******从左到右分别代表：秒 分 时 日 月 星期 年份，这里 * 指所有可能的值。 在线 Cron 表达式生成器 测试 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 1234567891011121314151617@Servicepublic class TaskService &#123; /** * second（秒）, minute（分）, hour（时）, day of month（日）, month（月）, day of week（周几）. * 例子： * 0 0/5 14,18 * * ? 每天14点整，和18点整，每隔5分钟执行一次 * 0 15 10 ? * 1-6 每个月的周一至周六10:15分执行一次 * 0 0 2 ? * 6L 每个月的最后一个周六凌晨2点执行一次 * 0 0 2 LW * ? 每个月的最后一个工作日凌晨2点执行一次 * 0 0 2-4 ? * 1#1 每个月的第一个周一凌晨2点到4点期间，每个整点都执行一次； */ @Scheduled(cron = &quot;0,1,2,3,4 * * * * MON-SAT&quot;) public void runTask()&#123; System.out.println(new Date()+&quot;Doing Schedule Task&quot;); &#125;&#125; 邮件任务 最早期的时候使用 JavaMail 相关 API 来开发，需要自己去封装消息体，代码量比较庞大；后来 Spring 推出了 JavaMailSender 简化了邮件发送过程，JavaMailSender 提供了强大的邮件发送功能，可支持各种类型的邮件发送。现在 Spring Boot 在 JavaMailSender 的基础上又进行了封装，就有了现在的 spring-boot-starter-mail，让邮件发送流程更加简洁和完善。 测试 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 123456spring.mail.username=邮箱账号# 在邮件站点的个人中心中获取spring.mail.password=xxxxxspring.mail.host=smtp.163.com // 邮件协议对应的服务器spring.mail.protocol=smtp // 邮件协议spring.mail.properties.mail.smtp.ssl.enable=true 12345678910111213141516@AutowiredJavaMailSenderImpl mailSender;@Testpublic void testSimpleMessage()&#123; SimpleMailMessage message = new SimpleMailMessage(); message.setSubject(&quot;第一封测试邮件&quot;); message.setText(&quot;邮件测试...&quot;); message.setFrom(&quot;你的邮箱账号&quot;); message.setTo(&quot;发送对象的邮箱账号&quot;); mailSender.send(message);&#125; 123456789101112131415161718@Testpublic void testMimeMessage() throws Exception&#123; // 创建复杂消息 MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); helper.setSubject(&quot;第二封邮件&quot;); helper.setText(&quot;&lt;b style=&#x27;color:red&#x27;&gt;邮件测试....&lt;/b&gt;&quot;,true); helper.setFrom(&quot;你的邮箱账号&quot;); helper.setTo(&quot;发送对象的邮箱账号&quot;); // 发送邮件 helper.addAttachment(&quot;1.jpg&quot;,new File(&quot;C:\\\\Users\\\\12746\\\\Pictures\\\\22\\\\1.jpg&quot;)); helper.addAttachment(&quot;2.jpg&quot;,new File(&quot;C:\\\\Users\\\\12746\\\\Pictures\\\\22\\\\2.jpg&quot;)); mailSender.send(mimeMessage);&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Aysnc","slug":"Aysnc","permalink":"https://wingowen.github.io/tags/Aysnc/"},{"name":"Scheduled","slug":"Scheduled","permalink":"https://wingowen.github.io/tags/Scheduled/"},{"name":"Email","slug":"Email","permalink":"https://wingowen.github.io/tags/Email/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"}]},{"title":"Spring Boot 安全","slug":"后台技术/Spring Boot/Spring Boot 安全","date":"2020-04-01T02:50:45.000Z","updated":"2023-09-20T07:13:05.716Z","comments":true,"path":"2020/04/01/后台技术/Spring Boot/Spring Boot 安全/","link":"","permalink":"https://wingowen.github.io/2020/04/01/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E5%AE%89%E5%85%A8/","excerpt":"Shiro 轻量级安全框架的介绍及使用；Sping 安全框架 Spring Security 的介绍及使用。","text":"Shiro 轻量级安全框架的介绍及使用；Sping 安全框架 Spring Security 的介绍及使用。 Spring Security 简介 Spring Security 是针对 Spring 项目的安全框架，也是 Spring Boot 底层安全模块默认的技术选型。可以实现强大的 web 安全控制。对于安全控制，我们仅需引入 spring-boot-starter-security 模块，进行少量的配置，即可实现强大的安全管理。 WebSecurityConfigurerAdapter：自定义 Security 策略； AuthenticationManagerBuilder：自定义认证策略； @EnableWebSecurity：开启 WebSecurity。 应用程序的两个主要区域是“认证”和“授权”（或者访问控制）。这两个主要区域是 Spring Security 的两个目标。 “认证”（Authentication），是建立一个经过声明的主体的过程，即你是谁； “授权”（Authorization）指确定一个主体是否允许在你的应用程序执行一个动作的过程，即你能干什么。 入门实例 引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 建立一个 Web 层的请求接口： 12345678@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @GetMapping public String getUsers() &#123; return &quot;Hello Spring Security&quot;; &#125;&#125; 自定义用户认证逻辑 系统一般都有自定义的用户体系，Spring Security 提供了接口可以自定义认证逻辑以及登录界面。 1234567// 密码加密接口public interface PasswordEncoder &#123; // 对密码进行加密 String encode(CharSequence var1); // 对密码进行判断匹配 boolean matches(CharSequence var1, String var2);&#125; 12345// 实现 Spring Security 中的 PasswordEncoder 接，这里用提供的默认实现@Beanpublic PasswordEncoder passwordEncoder() &#123; return new BCryptPasswordEncoder(); &#125; 配置用户认证逻辑： UserDetails 就是封装了用户信息的对象，返回 UserDetails 的实现类 User 的时候，可以通过 User 的构造方法，设置对应的参数。 12345678910111213141516171819@Componentpublic class MyUserDetailsService implements UserDetailsService &#123; private Logger logger = LoggerFactory.getLogger(getClass()); @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; logger.info(&quot;用户的用户名: &#123;&#125;&quot;, username); // TODO 根据用户名，查找到对应的密码，与权限 // 封装用户信息，并返回。参数分别是：用户名，密码，用户权限 String password = passwordEncoder.encode(&quot;123456&quot;); logger.info(&quot;password: &#123;&#125;&quot;, password); // 每次打印的密码都不一样，因为经过了加密 // 参数分别是：用户名，密码，用户权限 User user = new User(username, password, AuthorityUtils.commaSeparatedStringToAuthorityList(&quot;admin&quot;)); return user; &#125;&#125; 自定义登录页面： 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;登录页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;自定义登录页面&lt;/h2&gt; &lt;form action=&quot;/user/login&quot; method=&quot;post&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td&gt;用户名：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;密码：&lt;/td&gt; &lt;td&gt;&lt;input type=&quot;password&quot; name=&quot;password&quot;&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=&quot;2&quot;&gt;&lt;button type=&quot;submit&quot;&gt;登录&lt;/button&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 配置模块 12345678910111213141516@Configurationpublic class BrowerSecurityConfig extends WebSecurityConfigurerAdapter &#123; protected void configure(HttpSecurity http) throws Exception &#123; http.formLogin() // 定义当需要用户登录时候，转到的登录页面。 .loginPage(&quot;/login.html&quot;) // 设置登录页面 .loginProcessingUrl(&quot;/user/login&quot;) // 自定义的登录接口 .and() .authorizeRequests() // 定义哪些 URL 需要被保护，哪些不需要被保护 .antMatchers(&quot;/login.html&quot;).permitAll() // 设置所有人都可以访问登录页面 .anyRequest() // 任何请求，登录后可以访问 .authenticated() .and() .csrf().disable(); // 关闭 csrf 防护 &#125;&#125; Shiro Apache Shiro是一个功能强大且易于使用的 Java安 全框架，它为开发人员提供了一种直观，全面的身份验证，授权，加密和会话管理解决方案。 核心 API Subject： 用户主体（把操作交给SecurityManager）； SecurityManager：安全管理器（关联Realm）； Realm：Shiro 连接数据的桥梁。 入门实例 SQL 建表语句： 1234567CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varbinary(64) DEFAULT NULL, `password` varchar(64) DEFAULT NULL, `perms` varchar(64) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 导入依赖： 12345678910111213141516&lt;!-- shiro --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-web&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.theborakompanioni&lt;/groupId&gt; &lt;artifactId&gt;thymeleaf-extras-shiro&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 自定义 Realm 类 12345678910111213141516171819202122232425262728293031323334353637383940@Slf4jpublic class UserRealm extends AuthorizingRealm &#123; @Autowired private UserRepository userRepository; @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection arg0) &#123; log.info(&quot;执行授权逻辑&quot;); Subject subject = SecurityUtils.getSubject(); User user = userRepository.findUserByUsername((String) subject.getSession().getAttribute(&quot;username&quot;)); // 给用户进行授权 SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addStringPermission(user.getPerms()); return info; &#125; @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken arg0) throws AuthenticationException &#123; log.info(&quot;执行认证逻辑&quot;); Subject subject = SecurityUtils.getSubject(); Session session = subject.getSession(); UsernamePasswordToken token = (UsernamePasswordToken)arg0; User user = userRepository.findUserByUsername(token.getUsername()); session.setAttribute(&quot;username&quot;, token.getUsername()); if(user==null)&#123; // shiro 底层会抛出UnKnowAccountException return null; &#125; // 判断密码 return new SimpleAuthenticationInfo(user,user.getPassword(),getName()); &#125;&#125; Shiro 配置类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Slf4j@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(&quot;shiroCacheManager&quot;) private ShiroCacheManager shiroCacheManager; @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Autowired DefaultWebSecurityManager securityManager)&#123; ShiroFilterFactoryBean shiroFilterFactoryBean = new ShiroFilterFactoryBean(); // 设置安全管理器 shiroFilterFactoryBean.setSecurityManager(securityManager); // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // 设置过滤器链 Map&lt;String,String&gt; filterMap = new LinkedHashMap&lt;&gt;(16); filterMap.put(&quot;/index&quot;, &quot;anon&quot;); filterMap.put(&quot;/favicon.ico&quot;, &quot;anon&quot;); // 添加访问权限 filterMap.put(&quot;/toAdd&quot;, &quot;perms[user:add]&quot;); filterMap.put(&quot;/toUpdate&quot;, &quot;perms[user:update]&quot;); // 注意：要放在允许访问的页面后面，否则允许访问无效 filterMap.put(&quot;/**&quot;, &quot;authc&quot;); // 如果没有认证通过 跳转到的 url 地址 shiroFilterFactoryBean.setLoginUrl(&quot;/login&quot;); shiroFilterFactoryBean.setSuccessUrl(&quot;/index&quot;); // 未授权跳转页面 shiroFilterFactoryBean.setUnauthorizedUrl(&quot;/error&quot;); shiroFilterFactoryBean.setFilterChainDefinitionMap(filterMap); return shiroFilterFactoryBean; &#125; // 安全管理器 @Bean public DefaultWebSecurityManager getDefaultWebSecurityManager(@Autowired UserRealm userRealm)&#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 关联 realm securityManager.setRealm(userRealm); return securityManager; &#125; @Bean public UserRealm getRealm()&#123; return new UserRealm(); &#125; /** 配置 ShiroDialect，用于 Thymeleaf 和 Shiro 标签配合使用 */ // 添加 xmlns:shiro=&quot;http://www.pollix.at/thymeleaf/shiro&quot; 标签校验规范 @Bean public ShiroDialect getShiroDialect()&#123; return new ShiroDialect(); &#125;&#125; 控制器类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Controllerpublic class UserController &#123; @GetMapping(&quot;/login&quot;) public String login()&#123; return &quot;login&quot;; &#125; @GetMapping(&quot;/index&quot;) public String hello(Model model)&#123; Subject subject = SecurityUtils.getSubject(); model.addAttribute(&quot;username&quot;, subject.getSession().getAttribute(&quot;username&quot;)); return &quot;index&quot;; &#125; @GetMapping(&quot;/toAdd&quot;) public String add()&#123; return &quot;/user/add&quot;; &#125; @GetMapping(&quot;/toUpdate&quot;) public String update()&#123; return &quot;/user/update&quot;; &#125; @GetMapping(&quot;/toExit&quot;) public String exit()&#123; Subject subject = SecurityUtils.getSubject(); subject.logout(); return &quot;redirect:/login&quot;; &#125; @PostMapping(&quot;/login&quot;) public String login(String username, String password, Model model)&#123; Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(username,password); try &#123; Session session = subject.getSession(); session.setAttribute(&quot;username&quot;, username); subject.login(token); return &quot;redirect:/index&quot;; &#125; catch (UnknownAccountException e) &#123; model.addAttribute(&quot;msg&quot;, &quot;用户名不存在&quot;); return &quot;login&quot;; &#125;catch (IncorrectCredentialsException e) &#123; model.addAttribute(&quot;msg&quot;, &quot;密码错误&quot;); return &quot;login&quot;; &#125; &#125;&#125; 高级 自定义缓存管理器：Spring 接管 Shiro 的缓存管理。 实现 Shiro 提供的 Cache 和 CacheManager 接口 👉 实现 Cache 缓存接口底层使用 SpringCache 实现。 ShiroCache 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/*** 自定义 Shiro 缓存，实现 ShiroCache 接口** @param &lt;K&gt;* @param &lt;V&gt;*/public class ShiroCache&lt;K,V&gt; implements org.apache.shiro.cache.Cache&lt;K,V&gt; &#123; // Spring 的缓存管理器 private CacheManager springCacheManager; // Spring 的缓存对象 private Cache springCache; // 构造器 public ShiroCache(org.springframework.cache.CacheManager springCacheManager, String cacheName) &#123; this.springCacheManager = springCacheManager; this.springCache = springCacheManager.getCache(cacheName); &#125; // 自定义实现，都由 Spring 的缓存对象进行操作 @Override public V get(K key) throws CacheException &#123; Cache.ValueWrapper valueWrapper = springCache.get(key); if (valueWrapper == null) &#123; return null; &#125; return (V) valueWrapper.get(); &#125; @Override public V put(K key, V value) throws CacheException &#123; springCache.put(key, value); return value; &#125; @Override public V remove(K key) throws CacheException &#123; V value = this.get(key); springCache.evict(key); return value; &#125; /** * 清空缓存 * @throws CacheException */ @Override public void clear() throws CacheException &#123; springCache.clear(); &#125; /** * 获取所有缓存key的集合 * * @return */ @Override public Set&lt;K&gt; keys() &#123; return (Set&lt;K&gt;) springCacheManager.getCacheNames(); &#125; /** * 获取所有缓存value值的集合 * * @return */ @Override public Collection&lt;V&gt; values() &#123; List&lt;V&gt; list = new ArrayList&lt;&gt;(); Set&lt;K&gt; keys = keys(); for (K k : keys) &#123; list.add(this.get(k)); &#125; return list; &#125; /** * 获取缓存对象的数量 * * @return */ @Override public int size() &#123; int size = keys().size(); return size; &#125;&#125; ShiroCacheManager 12345678910111213public class ShiroCacheManager&lt;K, V&gt; implements CacheManager &#123; // Spring 的缓存管理器 private org.springframework.cache.CacheManager springCacheManager; public ShiroCacheManager(org.springframework.cache.CacheManager springCacheManager) &#123; this.springCacheManager = springCacheManager; &#125; @Override public &lt;K, V&gt; Cache&lt;K, V&gt; getCache(String cacheName) throws CacheException &#123; // 通过缓存名在缓存管理器中获取对应的缓存 return new ShiroCache&lt;&gt;(springCacheManager, cacheName); &#125;&#125; 1234567891011121314151617/*** shiro 的缓存管理器* @return*/@Bean(name = &quot;shiroCacheManager&quot;)public ShiroCacheManager shiroCacheManager()&#123; // 创建一个 Redis 缓存的默认配置 RedisCacheConfiguration conf = RedisCacheConfiguration.defaultCacheConfig(); // 设置会话的有效期 conf = conf.entryTtl(Duration.ofSeconds(30)); RedisCacheManager cacheManager = RedisCacheManager .builder(redisConnectionFactory) .cacheDefaults(conf) .build(); // 不用 ShiroCacheManager 返回的其实是一个 RedisCacheManager return new ShiroCacheManager(cacheManager);&#125; 注入自定义的缓存管理器 12345678910111213141516@Slf4j@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(&quot;shiroCacheManager&quot;) private ShiroCacheManager shiroCacheManager; @Bean public ShiroFilterFactoryBean getShiroFilterFactoryBean(@Autowired DefaultWebSecurityManager securityManager)&#123; // ... // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // ... &#125;&#125; ShiroSessionDAO 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Repositorypublic class ShiroSessionDAO extends AbstractSessionDAO &#123; @Autowired private RedisTemplate&lt;Serializable, Session&gt; redisTemplate; private String name = &quot;shiro-session:&quot;; private long timeout = 30; @Override protected Serializable doCreate(Session session) &#123; // 生成会话的唯一id Serializable sessionId = generateSessionId(session); super.create(session); redisTemplate.opsForValue().set(name + sessionId, session); return sessionId; &#125; @Override protected Session doReadSession(Serializable sessionId) &#123; super.readSession(sessionId); // 更新缓存 redisTemplate.expire(sessionId, timeout, TimeUnit.MINUTES); return redisTemplate.opsForValue().get(sessionId); &#125; @Override public void update(Session session) throws UnknownSessionException &#123; redisTemplate.opsForValue().set(name + session.getId(), session); &#125; @Override public void delete(Session session) &#123; redisTemplate.delete(name + session.getId()); &#125; @Override public Collection&lt;Session&gt; getActiveSessions() &#123; Set&lt;Serializable&gt; set = redisTemplate.keys(name + &quot;*&quot;); List&lt;Session&gt; sessionList = new ArrayList&lt;&gt;(); for (Serializable sessionId : set) &#123; sessionList.add(redisTemplate.opsForValue().get(name + sessionId)); &#125; return sessionList; &#125;&#125; 配置类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100@Configurationpublic class ShiroConfig &#123; @Autowired @Qualifier(&quot;shiroCacheManager&quot;) private ShiroCacheManager shiroCacheManager; /** * 配置自定义SessionDAO * @return */ @Bean public SessionDAO sessionDAO() &#123; SessionDAO sessionDAO = new ShiroSessionDAO(shiroCacheManager); return sessionDAO; &#125; /** * 配置会话管理器 * @return */ @Bean public SessionManager sessionManager()&#123; // WEB 环境非 HttpSession 会话管理器 DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); // 注入自定义的 SessionDao sessionManager.setSessionDAO(sessionDAO()); return sessionManager; &#125; /** * 配置安全管理器 * * @return */ @Bean public SecurityManager securityManager() &#123; DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); // 注入自定义Realm securityManager.setRealm(shiroRealm()); // 注入缓存管理器 securityManager.setCacheManager(shiroCacheManager); // 注入会话管理器 securityManager.setSessionManager(sessionManager()); return securityManager; &#125; /** * 装配 自定义Realm * * @return */ @Bean public ShiroRealm shiroRealm() &#123; return new ShiroRealm(); &#125; /** * 配置权限权限过滤器 * @return */ @Bean public ShiroFilterFactoryBean shiroFilterFactoryBean() &#123; ShiroFilterFactoryBean filter = new ShiroFilterFactoryBean(); // 注入安全管理器 filter.setSecurityManager(securityManager()); // 未认证的跳转地址 filter.setLoginUrl(&quot;/login&quot;); Map&lt;String, String&gt; chain = new LinkedHashMap&lt;&gt;(); chain.put(&quot;/login&quot;, &quot;anon&quot;); // 登录链接不拦截 chain.put(&quot;/css/**&quot;, &quot;anon&quot;); chain.put(&quot;/img/**&quot;, &quot;anon&quot;); chain.put(&quot;/js/**&quot;, &quot;anon&quot;); chain.put(&quot;/lib/**&quot;, &quot;anon&quot;); chain.put(&quot;/**&quot;, &quot;user&quot;); filter.setFilterChainDefinitionMap(chain); return filter; &#125; /** * 启用 Shiro 注解 * @return */ @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor() &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); // 注入安全管理器 advisor.setSecurityManager(securityManager()); return advisor; &#125; /** * 启用 Shiro Thymeleaf 标签支持 * @return */ @Bean public ShiroDialect shiroDialect() &#123; return new ShiroDialect(); &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Security","slug":"Security","permalink":"https://wingowen.github.io/tags/Security/"},{"name":"Shiro","slug":"Shiro","permalink":"https://wingowen.github.io/tags/Shiro/"}]},{"title":"Spring Boot 检索","slug":"后台技术/Spring Boot/Spring Boot 检索","date":"2020-03-31T06:50:45.000Z","updated":"2023-09-20T07:13:11.447Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 检索/","link":"","permalink":"https://wingowen.github.io/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%A3%80%E7%B4%A2/","excerpt":"ElasticSearch 基本介绍以及在 Spring Boot 中的整合使用。","text":"ElasticSearch 基本介绍以及在 Spring Boot 中的整合使用。 ElasticSearch 如今的应用经常需要添加检索功能，开源的 ElasticSearch 是目前全文搜索引擎的首选。他可以快速的存储、搜索和分析海量数据。Spring Boot 通过整合 Spring Data ElasticSearch 为我们提供了非常便捷的检索功能支持。 Elasticsearch 是一个分布式搜索服务，提供 Restful API，底层基于 Lucene，采用多 shard（分片）的方式保证数据安全，并且提供自动 resharding 的功能，github 等大型的站点也是采用了 ElasticSearch 作为其搜索服务。 Elasticsearch 本身就是分布式的，即便只有一个节点，Elasticsearch 默认也会对的数据进行分片和副本操作，向集群添加新数据时，数据也会在新加入的节点中进行平衡 相关概念 一个 ElasticSearch 集群可以包含多个索引 ，相应的每个索引可以包含多个类型 。这些不同的类型存储着多个文档 ，每个文档又有多个属性 。 Elasticsearch 也是基于 Lucene 的全文检索库，本质也是存储数据，很多概念与关系型数据库是一致的，如下对照： 索引库 关系型数据库 类型（type） Table 数据表 文档（Document） Row 行 字段（Field） Columns 列 另外，在 Elasticsearch 有一些集群相关的概念： 索引集（Indices，index 的复数）：逻辑上的完整索引； 分片（shard）：数据拆分后的各个部分； 副本（replica）：每个分片的复制。 整合 Spring Boot 提供了两种方式操作 Elasticsearch，Jest 和 SpringData。 Docker 安装部署 ElasticSearch 12345# 下载镜像docker pull elasticsearch# Elasticsearch 启动是会默认分配 2G 的内存 ，我们启动是设置小一点，防止我们内存不够启动失败# 9200 是 Elasticsearch 默认的 web 通信接口，9300 是分布式情况下，Elasticsearch 各个节点的通信端口docker run -e ES_JAVA_OPTS=&quot;-Xms256m -Xmx256m&quot; -d -p 9200:9200 -p 9300:9300 --name es01 5c1e1ecfe33a Jest ElasticSearch already has a Java API which is also used by ElasticSearch internally, but Jest fills a gap, it is the missing client for ElasticSearch Http Rest interface. 添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;io.searchbox&lt;/groupId&gt; &lt;artifactId&gt;jest&lt;/artifactId&gt; &lt;version&gt;5.3.3&lt;/version&gt;&lt;/dependency&gt; 配置服务器： 1spring.elasticsearch.jest.uris=http://127.0.0.1:9200 ## Elasticsearch 服务器 编写实体类： 12345678910111213141516171819202122232425public class Article &#123; // If @JestId value is null, it will be set the value of ElasticSearch generated &quot;_id&quot;. @JestId // @JestId annotation can be used to mark a property of a bean as id private Integer id; private String author; private String title; private String content; // Getter / Setter @Override public String toString() &#123; final StringBuilder sb = new StringBuilder( &quot;&#123;\\&quot;Article\\&quot;:&#123;&quot; ); sb.append( &quot;\\&quot;id\\&quot;:&quot; ) .append( id ); sb.append( &quot;,\\&quot;author\\&quot;:\\&quot;&quot; ) .append( author ).append( &#x27;\\&quot;&#x27; ); sb.append( &quot;,\\&quot;title\\&quot;:\\&quot;&quot; ) .append( title ).append( &#x27;\\&quot;&#x27; ); sb.append( &quot;,\\&quot;content\\&quot;:\\&quot;&quot; ) .append( content ).append( &#x27;\\&quot;&#x27; ); sb.append( &quot;&#125;&#125;&quot; ); return sb.toString(); &#125;&#125; 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootElasticsearchApplicationTests &#123; @Autowired JestClient jestClient; @Test public void createIndex() &#123; // 初始化一个文档 Article article = new Article(); article.setId( 1 ); article.setTitle( &quot;好消息&quot; ); article.setAuthor( &quot;张三&quot; ); article.setContent( &quot;Hello World&quot; ); // 构建一个索引 Index index = new Index.Builder(article).index(&quot;shekou&quot;).type(&quot;news&quot;).build(); try &#123; // 执行 jestClient.execute(index); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Test public void search() &#123; // 查询表达式 String query = &quot;&#123;\\n&quot; + &quot; \\&quot;query\\&quot; : &#123;\\n&quot; + &quot; \\&quot;match\\&quot; : &#123;\\n&quot; + &quot; \\&quot;content\\&quot; : \\&quot;hello\\&quot;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;\\n&quot; + &quot;&#125;&quot;; // 构建搜索功能 Search search = new Search.Builder(query).addIndex(&quot;shekou&quot;).addType(&quot;news&quot;).build(); try &#123; // 执行 SearchResult result = jestClient.execute(search); System.out.println(result.getJsonString()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 搜索表达式查询： Searching Document Spring Data spring-data-elasticsearch 使用之前必须先确定 Elasticsearch 版本：点击查看官方文档 Spring Data 通过注解来声明字段的映射属性，注解的详细用法： @Document 作用在类，标记实体类为文档对象，一般有两个属性： indexName：对应索引库名称； type：对应在索引库中的类型； shards：分片数量，默认5； replicas：副本数量，默认1。 @Id作用在成员变量，标记一个字段作为 id 主键。 @Field作用在成员变量，标记为文档的字段，并指定字段映射属性： type：字段类型，是枚举：FieldType，可以是 text、long、short、date、integer、object 等： Text：存储数据时候，会自动分词，并生成索引； Keyword：存储数据时候，不会分词建立索引； Numerical：数值类型，分两类： 基本数据类型：long、interger、short、byte、double、float、half_float； 浮点数的高精度类型：scaled_float 需要指定一个精度因子，比如 10 或 100。Elasticsearch 会把真实值乘以这个因子后存储，取出时再还原。 Date：日期类型 Elasticsearch 可以对日期格式化为字符串存储，但是建议我们存储为毫秒值，存储为 long，节省空间。 index：是否索引，布尔类型，默认是true； store：是否存储，布尔类型，默认是false； analyzer：分词器名称。 CRUD Spring Data 的强大之处，就在于你不用写任何 DAO 处理，自动根据方法名或类的信息进行 CRUD 操作。只要你定义一个接口，然后继承 Repository 提供的一些子接口，就能具备各种基本的 CRUD 功能。 123public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123;&#125; 测试 12345678910111213141516171819202122// 创建一个实体类并标注为文档@Document(indexName = &quot;item&quot;, type = &quot;docs&quot;, shards = 1, replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text) private String title; @Field(type = FieldType.Keyword) private String category; @Field(type = FieldType.Keyword) private String brand; @Field(type = FieldType.Double) private Double price; @Field(index = false, type = FieldType.Keyword) private String images;&#125; 调用 elasticsearchTemplate 创建索引并映射： 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTest(classes = EsDemoApplication.class)public class EsDemoApplicationTests &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Test public void testCreateIndex() &#123; // 根据 Item 类的 @Document 注解信息来创建 elasticsearchTemplate.createIndex(Item.class); &#125; @Test public void insert() &#123; Item item = new Item(1L, &quot;Redmi&quot;, &quot; 手机&quot;, &quot;小米&quot;, 1499.00, &quot;RedmiImage.jpg&quot;); itemRepository.save(item); &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://wingowen.github.io/tags/ElasticSearch/"}]},{"title":"Spring Boot 消息中间件","slug":"后台技术/Spring Boot/Spring Boot 消息队列","date":"2020-03-31T03:50:45.000Z","updated":"2023-09-20T07:13:26.294Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 消息队列/","link":"","permalink":"https://wingowen.github.io/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"MQ 基本介绍以及与 Spring Boot 的整合使用。","text":"MQ 基本介绍以及与 Spring Boot 的整合使用。 概述 在大多数应用程序中，可通过消息服务中间件来提升系统异步通信、扩展解耦能力。 消息代理（message broker）； 目的地（destination）。 当消息发送者发送消息以后，将由消息代理接管，消息代理保证消息传递到指定目的地。消息队列主要有两种形式的目的地。 队列（queue）：点对点消息通信（point-to-point）； 主题（topic）：发布（publish）/ 订阅（subscribe）消息通信。 应用场景：异步处理（邮件）、应用解耦、流量削峰（秒杀）。 JMS：Java 消息服务（Java Message Service）应用程序接口是一个 Java 平台中关于面向消息中间件（MOM）的 API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java 消息服务是一个与具体平台无关的API，绝大多数 MOM 提供商都对 JMS 提供支持。 AMQP：高级消息队列协议（Advanced Message Queuing Protocol）, 对于面向消息中间件的应用层协议。 常见消息中间件 ActiveMQ Apache ActiveMQ 是 Apache 软件基金会所研发的开放源代码消息中间件。 RabbitMQ RabbitMQ 是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ 服务器是用 Erlang 语言编写的， Kafka 发布订阅消息系统、分布式日志服务。本身是做日志储存的，所以对消息的顺序有严格的要求。开源流处理平台，由 Scala 和 java 编写，目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个按照分布式事务日志架构的大规模发布 / 订阅消息队列。 编码接口 JMS 编码接口： ConnectionFactory：创建连接到消息中间件的连接工厂； Connection：通信链路； Destination：消息发布和接收的地点，包括队列和主题； Session：会话，表示一个单线程的上下文； MessageConsumer：会话创建，用于接收消息； MessageProducer：会话创建，用于发送消息； Message：消息对象，包括消息头，消息属性，消息体。 一个 Connection 可以创建多个会话，即一个连接可以供多个线程使用。 Spring 支持 spring-jms 提供了对 JMS 的支持；spring-rabbit 提供了对 AMQP 的支持。 需要 ConnectionFactory 的实现来连接消息代理； 提供 JmsTemplate、RabbitTemplate 来发送消息； @JmsListener（JMS）、@RabbitListener（AMQP）注解在方法上监听消息代理发布的消息； @EnableJms、@EnableRabbit 开启支持。 Spring Boot 自动配置 JmsAutoConfiguration、RabbitAutoConfiguration RabbitMQ 核心概念 Message 消息：消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者：是一个向交换器发布消息的客户端应用程序。 Exchange 交换器：用来接收生产者发送的消息并将这些消息路由给服务器中的队列。Exchange 有 4 种类型：direct（默认：routing key = binding key）、fanout（广播）、 topic,、和 headers（匹配消息的 Header 而不是路由键），不同类型的 Exchange 转发消息的策略有所区别。 topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符： # 匹配 0 个或多个单词；*匹配一个单词。 Queue 消息队列：用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Binding 绑定：用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Exchange 和 Queue 的绑定可以是多对多的关系。 Connection 网络连接：比如一个TCP连接。 Channel 信道：多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的 TCP 连接内的虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer 消息的消费者：表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker 表示消息队列服务器实体。 RabbitMQ 整合 导入模块依赖的 starter： 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot 配置： 123456## 配置 rabbitMq 服务器rabbitmq: host: 127.0.0.1 port: 5672 username: root password: root 配置一个直连型的交换机： 123456789101112131415161718192021@Configurationpublic class DirectRabbitConfig &#123; // 初始化队列名：TestDirectQueue @Bean public Queue TestDirectQueue() &#123; return new Queue(&quot;TestDirectQueue&quot;,true); // true 持久化 &#125; // 初始化 Direct 交换机：TestDirectExchange @Bean DirectExchange TestDirectExchange() &#123; return new DirectExchange(&quot;TestDirectExchange&quot;); &#125; // 定义队列和交换机绑定, 并设置用于匹配键：TestDirectRouting @Bean Binding bindingDirect() &#123; return BindingBuilder.bind(TestDirectQueue()).to(TestDirectExchange()).with(&quot;TestDirectRouting&quot;); &#125;&#125; 写个简单的接口进行消息推送（根据需求也可以改为定时任务等等，具体看需求）： 1234567891011121314151617181920@RestControllerpublic class SendMessageController &#123; @Autowired RabbitTemplate rabbitTemplate; // 使用 RabbitTemplate：提供了接收 / 发送等方法 @GetMapping(&quot;/sendDirectMessage&quot;) public String sendDirectMessage() &#123; String messageId = String.valueOf(UUID.randomUUID()); String messageData = &quot;test message, hello!&quot;; String createTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;)); Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;messageId&quot;,messageId); map.put(&quot;messageData&quot;,messageData); map.put(&quot;createTime&quot;,createTime); // 将消息携带绑定键值 TestDirectRouting 发送到交换机 TestDirectExchange rabbitTemplate.convertAndSend(&quot;TestDirectExchange&quot;, &quot;TestDirectRouting&quot;, map); return &quot;ok&quot;; &#125;&#125; consumer 创建一个消费者项目，消费者进行消息监听，需要手动创建消息接收的监听类。 123456789@Component@RabbitListener(queues = &quot;TestDirectQueue&quot;) // 监听的队列名称 TestDirectQueuepublic class DirectReceiver &#123; @RabbitHandler public void process(Map testMessage) &#123; System.out.println(&quot;DirectReceiver 消费者收到消息: &quot; + testMessage.toString()); &#125;&#125; 回调函数 12345678910111213141516171819202122232425262728293031323334@Configurationpublic class RabbitConfig &#123; @Bean public RabbitTemplate createRabbitTemplate(ConnectionFactory connectionFactory)&#123; RabbitTemplate rabbitTemplate = new RabbitTemplate(); rabbitTemplate.setConnectionFactory(connectionFactory); // 设置开启 Mandatory 才能触发回调函数，无论消息推送结果怎么样都强制调用回调函数 rabbitTemplate.setMandatory(true); // 交换机相关信息 rabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.out.println(&quot;ConfirmCallback: &quot;+&quot;相关数据：&quot;+correlationData); System.out.println(&quot;ConfirmCallback: &quot;+&quot;确认情况：&quot;+ack); System.out.println(&quot;ConfirmCallback: &quot;+&quot;原因：&quot;+cause); &#125; &#125;); // 队列相关信息 rabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() &#123; @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) &#123; System.out.println(&quot;ReturnCallback: &quot;+&quot;消息：&quot;+message); System.out.println(&quot;ReturnCallback: &quot;+&quot;回应码：&quot;+replyCode); System.out.println(&quot;ReturnCallback: &quot;+&quot;回应信息：&quot;+replyText); System.out.println(&quot;ReturnCallback: &quot;+&quot;交换机：&quot;+exchange); System.out.println(&quot;ReturnCallback: &quot;+&quot;路由键：&quot;+routingKey); &#125; &#125;); return rabbitTemplate; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"MQ","slug":"MQ","permalink":"https://wingowen.github.io/tags/MQ/"}]},{"title":"Spring Boot 缓存","slug":"后台技术/Spring Boot/Spring Boot 缓存","date":"2020-03-31T01:59:45.000Z","updated":"2023-09-20T07:13:09.117Z","comments":true,"path":"2020/03/31/后台技术/Spring Boot/Spring Boot 缓存/","link":"","permalink":"https://wingowen.github.io/2020/03/31/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E7%BC%93%E5%AD%98/","excerpt":"JSR107、Spring 缓存抽象。","text":"JSR107、Spring 缓存抽象。 JRS107 Java Caching 定义了 5 个核心接口，分别是 CachingProvider, CacheManager, Cache, Entry 和 Expiry。 • CachingProvider：定义了创建、配置、获取、管理和控制多个 CacheManager。一个应用可以在运行期访问多个 CachingProvider； • CacheManager：定义了创建、配置、获取、管理和控制多个唯一命名的 Cache，这些 Cache存在于 CacheManager 的上下文中。一个 CacheManager 仅被一个 CachingProvider 所拥有； • Cache：是一个类似 Map 的数据结构并临时存储以 Key 为索引的值。一个 Cache 仅被一个 CacheManager 所拥有； • Entry：是一个存储在 Cache 中的 key-value 对； • Expiry：每一个存储在 Cache 中的条目有一个定义的有效期。一旦超过这个时间，条目为过期的状态。一旦过期，条目将不可访问、更新和删除。缓存有效期可以通过ExpiryPolicy设置。 Spring 缓存抽象 Spring 3.1 以上版本定义了 org.springframework.cache.Cache 和 org.springframework.cache.CacheManager 接口来统一不同的缓存技术；并支持使用 JCache（JSR-107）注解简化我们开发。 Cache 接口为缓存的组件规范定义，包含缓存的各种操作集合； Cache 接口下 Spring 提供了各种 xxxCache 的实现；如 RedisCache、EhCacheCache 、ConcurrentMapCache等。 每次调用需要缓存功能的方法时，Spring 会检查检查指定参数的指定的目标方法是否已经被调用过：如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户。下次调用直接从缓存中获取。 确定方法需要被缓存以及他们的缓存策略； 从缓存中读取之前缓存存储的数据。 注解 基本流程 添加缓存模块的 stater： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-cache&lt;/artifactId&gt;&lt;/dependency&gt; 引入 Redis 的 starter，容器中默认保存的是 RedisCacheManager。RedisCacheManager 负责创建RedisCache，RedisCache 负责操作 Redis 缓存数据。默认保存数据 Key / Value 都是 Object，默认使用的是 JDK 序列化器。 在 Spring Boot 主类中添加 EnableCaching 注解开启缓存功能： 1234567@SpringBootApplication@EnableCachingpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 在类上添加CacheConfig进行缓存配置；在方法上添加@Cacheable，标注此方法返回值需要被缓存。 12345@CacheConfig(cacheNames = &quot;users&quot;)public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Cacheable User findByName(String name);&#125; 注解详解 @CacheConfig：主要用于配置该类中会用到的一些共用的缓存配置。在这里@CacheConfig(cacheNames = &quot;users&quot;)：配置了该数据访问对象中返回的内容将存储于名为 users 的缓存对象中，我们也可以不使用该注解，直接通过@Cacheable自己配置缓存集的名字来定义。 @Cacheable：配置了 findByName 函数的返回值将被加入缓存。同时在查询时，会先从缓存中获取，若不存在才再发起对数据库的访问。该注解主要有下面几个参数： value、cacheNames：两个等同的参数（cacheNames为 Spring 4 新增，作为value的别名），用于指定缓存存储的集合名。由于 Spring 4 中新增了@CacheConfig，因此在 Spring 3 中原本必须有的value属性，也成为非必需项了； key：缓存对象存储在 Map 集合中的 key 值，非必需，缺省按照函数的所有参数组合作为 key 值，若自己配置需使用 SpEL 表达式，比如：@Cacheable(key = &quot;#p0&quot;)：使用函数第一个参数作为缓存的 key 值，更多关于 SpEL 表达式的详细内容可参考官方文档； condition：缓存对象的条件，非必需，也需使用 SpEL 表达式，只有满足表达式条件的内容才会被缓存，比如：@Cacheable(key = &quot;#p0&quot;, condition = &quot;#p0.length() &lt; 3&quot;)，表示只有当第一个参数的长度小于3的时候才会被缓存； unless：另外一个缓存条件参数，非必需，需使用 SpEL 表达式。它不同于condition参数的地方在于它的判断时机，该条件是在函数被调用之后才做判断的，所以它可以通过对 result 进行判断； keyGenerator：用于指定 key 生成器，非必需。若需要指定一个自定义的 key 生成器，需要开发者实现org.springframework.cache.interceptor.KeyGenerator接口，并使用该参数来指定。需要注意的是：该参数与key是互斥的； cacheManager：用于指定使用哪个缓存管理器，非必需。只有当有多个时才需要使用； cacheResolver：用于指定使用那个缓存解析器，非必需。需通过org.springframework.cache.interceptor.CacheResolver接口来实现自己的缓存解析器，并用该参数指定。 @CachePut：配置于函数上，能够根据参数定义条件来进行缓存，它与@Cacheable不同的是，它每次都会真实调用函数，所以主要用于数据新增和修改操作上。它的参数与@Cacheable类似，具体功能可参考上面对@Cacheable参数的解析。 @CacheEvict：配置于函数上，通常用在删除方法上，用来从缓存中移除相应数据。除了同@Cacheable一样的参数之外，它还有下面两个参数： allEntries：非必需，默认为 false。当为 true 时，会移除所有数据； beforeInvocation：非必需，默认为 false，会在调用方法之后移除数据。当为 true 时，会在调用方法之前移除数据。 Template Spring Boot 提供了帮助操作 Redis 的 Helper 类 RedisTemplate：RedisTemplate的 K:V 均为 Object； StringRedisTemplate 继承自 RedisTemplate，是专为 String:String 类型的 K:V 提供服务。 在 RedisAutoConfiguration 中 Spring Boot 自动注册了 RedisTemplate 和 StringRedisTemplate。 数据操作 Redis 中常见的五大类数据类型：String，List，Set，Hash 和 ZSet。 RedisTemplate 封装了对五大类数据进行操作的方法，每一个方法都会返回一个 Operations 对象。 123456789RedisTemplate.opsForValue(); // StringRedisTemplate.opsForList();RedisTemplate.opsForSet();RedisTemplate.opsForHash();RedisTemplate.opsForZSet(); Redis 实现缓存 入门基础 String 最基础的数据类型； List 元素不具有唯一性；有序；是一个双向链表；既可以是栈，也可以是队列； Set 元素具有唯一性；无序； Hash 存储的是key-value结构，key必须是string；类似于 MySQL 中的一条记录。 Redis 命令接口 Redis 提供了许多命令供我们使用，同样在 Spring Boot 中也封装了对应类型的命令接口供开发者使用。 123456789101112131415161718192021/** * Interface for the commands supported by Redis. * * @author Costin Leau * @author Christoph Strobl */public interface RedisCommands extends RedisKeyCommands, RedisStringCommands, RedisListCommands, RedisSetCommands, RedisZSetCommands, RedisHashCommands, RedisTxCommands, RedisPubSubCommands, RedisConnectionCommands, RedisServerCommands, RedisScriptingCommands, RedisGeoCommands, HyperLogLogCommands &#123; /** * &#x27;Native&#x27; or &#x27;raw&#x27; execution of the given command along-side the given arguments. The command is executed as is, * with as little &#x27;interpretation&#x27; as possible - it is up to the caller to take care of any processing of arguments or * the result. * * @param command Command to execute * @param args Possible command arguments (may be null) * @return execution result. */ Object execute(String command, byte[]... args);&#125; 在 Spring Boot 配置文件中添加 Redis 配置： 1234567891011121314151617## 默认密码为空redis: host: 127.0.0.1 # Redis服务器连接端口 port: 6379 jedis: pool: # 连接池最大连接数（使用负值表示没有限制） max-active: 100 # 连接池中的最小空闲连接 max-idle: 10 # 连接池最大阻塞等待时间（使用负值表示没有限制） max-wait: 100000 # 连接超时时间（毫秒） timeout: 5000 # 默认是索引为 0 的数据库 database: 0 代码示例 1234567891011121314151617181920@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringBootCacheApplicationTests &#123; @Autowired EmployeeMapper employeeMapper; @Autowired StringRedisTemplate stringRedisTemplate; @Autowired RedisTemplate redisTemplate; // 向 Redis 中保存一个对象并取出打印 @Test public void test() &#123; Employee employee = employeeMapper.getEmpById(1); redisTemplate.opsForValue().set(&quot;emp01&quot;,employee); Object emp01 = redisTemplate.opsForValue().get(&quot;emp01&quot;); System.out.println(emp01); &#125;&#125; 注意，Redis 中对象的保存是需要进行序列化的，默认使用 JdkSerializationRedisSerializer 序列化器，所以在 Redis 中查看保存的对象是序列化后的二进制编码，正常使用是没有问题的，查询出来的时候会自动反序列化。 如果习惯于使用 String，那么可以将其 JSON 化，两种方式：使用第三方 JSON 或者向容器中添加自定义 RedisTemplate 改变其默认序列化器。 添加自定义 RedisTemplate： 123456789101112@Configurationpublic class MyRedisConfig &#123; @Bean public RedisTemplate&lt;Object,Employee&gt; empRedisTemplate(RedisConnectionFactory connectionFactory)&#123; RedisTemplate&lt;Object,Employee&gt; template = new RedisTemplate&lt;Object, Employee&gt;(); template.setConnectionFactory(connectionFactory); Jackson2JsonRedisSerializer&lt;Employee&gt; serializer = new Jackson2JsonRedisSerializer&lt;Employee&gt;(Employee.class); template.setDefaultSerializer(serializer); return template; &#125;&#125; Lettuce Jedis 在实现上是直接连接 Redis 服务器，在多个线程间共享一个 Jedis 实例时是线程不安全的，如果想要在多线程场景下使用 Jedis ，需要使用连接池，每个线程都使用自己的 Jedis 实例，当连接数量增多时，会消耗较多的物理资源。 与 Jedis 相比， Lettuce 则完全克服了其线程不安全的缺点： Lettuce 是一个可伸缩的线程安全的 Redis 客户端，支持同步、异步和响应式模式。多个线程可以共享一个连接实例，而 不必担心多线程并发问题。它基于优秀 Netty NIO 框架构建，支持 Redis 的更多高级功能。 12345678910&lt;!-- redis 访问启动器 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- redis 客户端 Lettuce 数据库连接池依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 123456789101112131415161718# Redis数据库索引（默认为0）spring.redis.database=0# Redis服务器地址 （默认localhost）spring.redis.host=localhost# Redis服务器连接端口 （默认6379）spring.redis.port=6379# Redis服务器连接密码（默认为空）spring.redis.password=123456# 连接超时时间spring.redis.timeout=10000ms# 最大连接数（使用负值表示没有限制） 默认 8spring.redis.lettuce.pool.max-active=8# 最小空闲连接 默认 0spring.redis.lettuce.pool.min-idle=0# 最大空闲连接 默认 8spring.redis.lettuce.pool.max-idle=8# 最大阻塞等待时间（使用负值表示没有限制） 默认 -1msspring.redis.lettuce.pool.max-wait=-1ms 缓存原理 要使用 Spring Boot 的缓存功能，还需要提供一个缓存的具体实现。 Spring Boot 一定的顺序去侦测缓存实现。 Spring 提供了一个统一访问缓存的接口：CacheManager ctrl + alt + b 可查看这个接口的实现 在 RedisCacheManager 中查看 Redis 缓存的默认配置类：RedisCacheConfiguration： 从源码了解到 Sping Boot 的 Reids 缓存对 Key 和 Value 默认的序列化器分别是 String 类型和 JDK 序列器。 在 RedisCacheManager 中可以看到如何实例化这个类： 实例化这个类必须传入一个 connectionFactory，用 redis 缓存所以传入一个 redisConnectionFactory。 123456789101112131415@Configurationpublic class CacheConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; /** * SpringCacheManager 缓存管理器：使用的缓存产品是Redis * @return */ @Bean(name = &quot;springCacheManager&quot;) public RedisCacheManager springCacheManager()&#123; RedisCacheManager cacheManager = RedisCacheManager.create(redisConnectionFactory); return cacheManager; &#125;&#125; Redis 配置 123456789101112131415161718192021222324252627282930313233@Configurationpublic class RedisConfig &#123; @Autowired private RedisConnectionFactory redisConnectionFactory; @Bean public StringRedisSerializer stringRedisSerializer() &#123; StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); return stringRedisSerializer; &#125; @Bean public StringRedisTemplate stringRedisTemplate() &#123; StringRedisTemplate stringRedisTemplate =new StringRedisTemplate(); stringRedisTemplate.setConnectionFactory(redisConnectionFactory); // 开启事务支持 stringRedisTemplate.setEnableTransactionSupport(true); return stringRedisTemplate; &#125; @Bean public RedisTemplate redisTemplate()&#123; RedisTemplate&lt;Object,Object&gt; redisTemplate= new RedisTemplate&lt;&gt;(); // 序列化器 redisTemplate.setConnectionFactory(redisConnectionFactory); redisTemplate.setKeySerializer(stringRedisSerializer()); //redisTemplate.setHashKeySerializer(stringRedisSerializer()); // 开启事务支持 redisTemplate.setEnableTransactionSupport(true); return redisTemplate; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Cache","slug":"Cache","permalink":"https://wingowen.github.io/tags/Cache/"}]},{"title":"开发常用工具类","slug":"后台技术/开发常用工具类","date":"2020-03-12T07:31:13.000Z","updated":"2023-09-20T07:13:00.954Z","comments":true,"path":"2020/03/12/后台技术/开发常用工具类/","link":"","permalink":"https://wingowen.github.io/2020/03/12/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/%E5%BC%80%E5%8F%91%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/","excerpt":"记录一下开发中用到的各种小工具类。","text":"记录一下开发中用到的各种小工具类。 加密类 Md5 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.security.MessageDigest;import org.apache.commons.lang3.StringUtils;public class Md5Util &#123; private Md5Util() &#123; throw new IllegalStateException(&quot;Utility class&quot;); &#125; /** * md5 加密通用工具类 * * @param encode 需加密的字符串 * @return md5 加密后的字符串（32位字符串） */ public static String md5(String encode) &#123; // 检验参数是否是 null 或者 &quot;&quot; 或者 &quot; &quot; 等 if (StringUtils.isBlank(encode)) &#123; return &quot;&quot;; &#125; char[] hexDigits = &#123;&#x27;0&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;&#125;; try &#123; // 设置字符集 byte[] strTemp = encode.getBytes(StandardCharsets.UTF_8); // 生成一个 md5 加密计算摘要 MessageDigest mdTemp = MessageDigest.getInstance(&quot;MD5&quot;); // 计算 md5 函数 mdTemp.update(strTemp); // digest() 最后确定返回 md5 hash 值，返回值为 8 位字符串。因为 md5 hash 值是 16 位的 hex 值，实际上就是 8 位的字符 byte[] md = mdTemp.digest(); int j = md.length; char[] str = new char[j * 2]; int k = 0; for (byte byte0 : md) &#123; // 不带符号右移四位(不管 byte0 的类型 位移处补 0)，&amp; 十六进制的 f 即 高四位清空 取低四位的值，&gt;&gt;&gt; 优先级高于 &amp; str[k++] = hexDigits[byte0 &gt;&gt;&gt; 4 &amp; 0xf]; str[k++] = hexDigits[byte0 &amp; 0xf]; &#125; return new String(str); &#125; catch (Exception e) &#123; return &quot;&quot;; &#125; &#125;&#125; Json FastJson 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.1.26&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128public class FastJsonConvertUtil &#123; private static final SerializerFeature[] featuresWithNullValue = &#123; SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullBooleanAsFalse, SerializerFeature.WriteNullListAsEmpty, SerializerFeature.WriteNullNumberAsZero, SerializerFeature.WriteNullStringAsEmpty &#125;; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSON字符串转换为实体对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSON字符串转换为实体对象&lt;BR&gt; * @param data JSON字符串 * @param clzss 转换对象 * @return T */ public static &lt;T&gt; T convertJSONToObject(String data, Class&lt;T&gt; clzss) &#123; try &#123; T t = JSON.parseObject(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSONObject对象转换为实体对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSONObject对象转换为实体对象&lt;BR&gt; * @param data JSONObject对象 * @param clzss 转换对象 * @return T */ public static &lt;T&gt; T convertJSONToObject(JSONObject data, Class&lt;T&gt; clzss) &#123; try &#123; T t = JSONObject.toJavaObject(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将JSON字符串数组转为List集合对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将JSON字符串数组转为List集合对象&lt;BR&gt; * @param data JSON字符串数组 * @param clzss 转换对象 * @return List&lt;T&gt;集合对象 */ public static &lt;T&gt; List&lt;T&gt; convertJSONToArray(String data, Class&lt;T&gt; clzss) &#123; try &#123; List&lt;T&gt; t = JSON.parseArray(data, clzss); return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将List&lt;JSONObject&gt;转为List集合对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将List&lt;JSONObject&gt;转为List集合对象&lt;BR&gt; * @param data List&lt;JSONObject&gt; * @param clzss 转换对象 * @return List&lt;T&gt;集合对象 */ public static &lt;T&gt; List&lt;T&gt; convertJSONToArray(List&lt;JSONObject&gt; data, Class&lt;T&gt; clzss) &#123; try &#123; List&lt;T&gt; t = new ArrayList&lt;T&gt;(); for (JSONObject jsonObject : data) &#123; t.add(convertJSONToObject(jsonObject, clzss)); &#125; return t; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将对象转为JSON字符串&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将对象转为JSON字符串&lt;BR&gt; * @param obj 任意对象 * @return JSON字符串 */ public static String convertObjectToJSON(Object obj) &#123; try &#123; String text = JSON.toJSONString(obj); return text; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;将对象转为JSONObject对象&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;将对象转为JSONObject对象&lt;BR&gt; * @param obj 任意对象 * @return JSONObject对象 */ public static JSONObject convertObjectToJSONObject(Object obj)&#123; try &#123; JSONObject jsonObject = (JSONObject) JSONObject.toJSON(obj); return jsonObject; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;&lt;BR&gt; * @param obj * @return */ public static String convertObjectToJSONWithNullValue(Object obj) &#123; try &#123; String text = JSON.toJSONString(obj, featuresWithNullValue); return text; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://wingowen.github.io/tags/Java/"}]},{"title":"Spring Boot 数据访问","slug":"后台技术/Spring Boot/Spring Boot 数据访问","date":"2020-03-05T07:42:42.000Z","updated":"2023-09-20T07:13:23.457Z","comments":true,"path":"2020/03/05/后台技术/Spring Boot/Spring Boot 数据访问/","link":"","permalink":"https://wingowen.github.io/2020/03/05/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/","excerpt":"Spring Boot 数据访问的相关知识。","text":"Spring Boot 数据访问的相关知识。 JDBC 123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 1234567spring: datasource: username: root password: 123456 url: jdbc:mysql://localhosthost:3306/jdbc driver-class-name: com.mysql.jdbc.Driver # Spring Boot 2.1.x 默认使用了 MySQL 8.0：com.mysql.cj.jdbc.Driver Sping Boot 默认使用 org.apache.tomcat.jdbc.pool.DataSource 作为数据源，相关配置都在 DataSourceProperties 里面。 默认建表配置规则 1schema-*.sql 指定建表文件 sql 文件的文件名 1234spring: datasource: schema: - classpath:user.sql Sping Boot 自动配置了 JdbcTemplate 操作数据库 JdbcTempale 创建一个 User 表： 1234CREATE TABLE `User` ( `name` varchar(100) COLLATE utf8mb4_general_ci NOT NULL, `age` int NOT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci 编写实体对象： 12345678@Data // Getter / Setter@NoArgsConstructor // 无参构造器public class User &#123; private String name; private Integer age;&#125; 数据访问对象接口： 123456789101112131415161718public interface UserService &#123; // 新增一个用户 int create(String name, Integer age); // 根据 name 查询用户 List&lt;User&gt; getByName(String name); // 根据 name 删除用户 int deleteByName(String name); // 获取用户总量 int getAllUsers(); // 删除所有用户 int deleteAllUsers();&#125; 数据访问操作的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private JdbcTemplate jdbcTemplate; UserServiceImpl(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; &#125; @Override public int create(String name, Integer age) &#123; return jdbcTemplate.update(&quot;insert into USER(NAME, AGE) values(?, ?)&quot;, name, age); &#125; @Override public List&lt;User&gt; getByName(String name) &#123; List&lt;User&gt; users = jdbcTemplate.query (&quot;select NAME, AGE from USER where NAME = ?&quot;, (resultSet, i) -&gt; &#123; User user = new User(); user.setName(resultSet.getString(&quot;NAME&quot;)); user.setAge(resultSet.getInt(&quot;AGE&quot;)); return user; &#125;, name); return users; &#125; @Override public int deleteByName(String name) &#123; return jdbcTemplate.update(&quot;delete from USER where NAME = ?&quot;, name); &#125; @Override public int getAllUsers() &#123; return jdbcTemplate.queryForObject(&quot;select count(1) from USER&quot;, Integer.class); &#125; @Override public int deleteAllUsers() &#123; return jdbcTemplate.update(&quot;delete from USER&quot;); &#125;&#125; 整合 Druid 数据源 123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--数据库连接池--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.20&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445spring.datasource.username=rootspring.datasource.password=123456spring.datasource.url=jdbc:mysql://localhost:3306/i-auth?useSSL=false# 可以不配置 Druid 可以根据 url 自动识别数据库驱动# spring.datasource.driver-class-name=com.mysql.jdbc.Driver# 初始化连接数spring.datasource.druid.initial-size=5# 最小连接数spring.datasource.druid.min-idle=10# 最大连接数spring.datasource.druid.max-active=10# 获取连接最长等待时间 单位毫秒spring.datasource.druid.max-wait=10000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.druid.timeBetweenEvictionRunsMillis=30000# 指定获取连接时连接校验的sql查询语句spring.datasource.druid.validationQuery=select &#x27;x&#x27;# 获取连接后，确实是否要进行连接空闲时间的检查spring.datasource.druid.testWhileIdle=true# 获取连接检测spring.datasource.druid.testOnBorrow=false# 归还连接检测spring.datasource.druid.testOnReturn=false# 指定连接校验查询的超时时间spring.datasource.druid.validationQueryTimeout=600000# 配置一个连接在池总最小生存的时间spring.datasource.druid.minEvictableIdleTimeMillis=300000# 打开 PSCache，并且指定每个连接上 Cache 的大小spring.datasource.druid.poolPreparedStatements=truespring.datasource.druid.maxPoolPreparedStatementPerConnectionSize=20# 配置监控统计拦截的filter，去掉后监控界面sql无法统计，&#x27;wall&#x27;用于防火墙spring.datasource.druid.filters=stat,wall,slf4j# 通过connectProperties属性来打开mergeSql功能，慢sql记录等spring.datasource.druid.connectionProperties=druid.stat.mergeSql=true;druid.stat.logSlowSql=true;druid.stat.slowSqlMillis=5000## druid连接池监控# 需要账号密码才能访问控制台，默认为rootspring.datasource.druid.stat-view-servlet.login-username=adminspring.datasource.druid.stat-view-servlet.login-password=123# 访问路径为/druid时，跳转到StatViewServletspring.datasource.druid.stat-view-servlet.url-pattern=/druid/*# 是否能够重置数据spring.datasource.druid.stat-view-servlet.reset-enable=false# 排除一些静态资源，以提高效率spring.datasource.druid.web-stat-filter.exclusions=*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/* 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 导入 Druid 数据源@Configurationpublic class DruidConfig &#123; // 一个自定义的 DataSource 组件 @ConfigurationProperties(prefix = &quot;spring.datasource&quot;) @Bean public DataSource druid()&#123; return new DruidDataSource(); &#125; // 配置 Druid 的监控 // 配置一个管理后台的 Servlet @Bean public ServletRegistrationBean statViewServlet()&#123; ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), &quot;/druid/*&quot;); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(&quot;loginUsername&quot;,&quot;admin&quot;); initParams.put(&quot;loginPassword&quot;,&quot;123456&quot;); initParams.put(&quot;allow&quot;,&quot;&quot;); // 默认就是允许所有访问 initParams.put(&quot;deny&quot;,&quot;192.168.15.21&quot;); bean.setInitParameters(initParams); return bean; &#125; // 配置一个 Web 监控的 Filter @Bean public FilterRegistrationBean webStatFilter()&#123; FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map&lt;String,String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(&quot;exclusions&quot;,&quot;*.js,*.css,/druid/*&quot;); bean.setInitParameters(initParams); bean.setUrlPatterns(Arrays.asList(&quot;/*&quot;)); return bean; &#125;&#125; 整合 MyBatis 可直接使用插件：Mybatis-Plus 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 注解版 1234567891011121314151617// 指定这是一个操作数据库的 Mapper@Mapperpublic interface DepartmentMapper &#123; @Select(&quot;select * from department where id=#&#123;id&#125;&quot;) public Department getDeptById(Integer id); @Delete(&quot;delete from department where id=#&#123;id&#125;&quot;) public int deleteDeptById(Integer id); @Options(useGeneratedKeys = true,keyProperty = &quot;id&quot;) @Insert(&quot;insert into department(departmentName) values(#&#123;departmentName&#125;)&quot;) public int insertDept(Department department); @Update(&quot;update department set departmentName=#&#123;departmentName&#125; where id=#&#123;id&#125;&quot;) public int updateDept(Department department);&#125; 123456789// 使用 MapperScan 批量扫描所有的 Mapper 接口，或在 Mapper 类使用 Mapper@MapperScan(value = &quot;com.wingo.springboot.mapper&quot;)@SpringBootApplicationpublic class SpringBootDataMybatisApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootDataMybatisApplication.class, args); &#125;&#125; 自定义 MyBatis 配置规则：驼峰命名法。 1234567891011121314@Configuration public class MyBatisConfig &#123; @Bean public ConfigurationCustomizer configurationCustomizer()&#123; return new ConfigurationCustomizer()&#123; @Override public void customize(Configuration configuration) &#123; configuration.setMapUnderscoreToCamelCase(true); &#125; &#125;; &#125; &#125; 配置文件版 12345mybatis: # 指定全局配置文件的位置 config-location: classpath:mybatis/mybatis-config.xml # 指定 Sql 映射文件的位置 mapper-locations: classpath:mybatis/mapper/*.xml 整合 SpringData JPA 12345678910111213&lt;!-- JPA --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- mysql驱动 --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;!-- 启动器中默认的版本较高 --&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 编写一个实体类 Bean 和数据表进行映射，并且配置好映射关系： 1234567891011121314// 使用 JPA 注解配置映射关系@Entity // 告诉 JPA 这是一个实体类（和数据表映射的类）@Table(name = &quot;tbl_user&quot;) // @Table 来指定和哪个数据表对应，如果省略默认表名就是 userpublic class User &#123; @Id // 这是一个主键 @GeneratedValue(strategy = GenerationType.IDENTITY) // 自增主键 private Integer id; @Column(name = &quot;last_name&quot;,length = 50) // 这是和数据表对应的一个列 private String lastName; @Column // 省略默认列名就是属性名 private String email;&#125; 编写一个 Dao 接口来操作实体类对应的数据表： 123// 继承 JpaRepository 来完成对数据库的操作，传参：实体类型 主键类型public interface UserRepository extends JpaRepository&lt;User,Integer&gt; &#123;&#125; JpaProperties 基本配置 1234567spring: jpa: hibernate: # 更新或者创建数据表结构 ddl-auto: update # 控制台显示SQL show-sql: true","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wingowen.github.io/tags/MyBatis/"},{"name":"JPA","slug":"JPA","permalink":"https://wingowen.github.io/tags/JPA/"},{"name":"Druid","slug":"Druid","permalink":"https://wingowen.github.io/tags/Druid/"},{"name":"JDBC","slug":"JDBC","permalink":"https://wingowen.github.io/tags/JDBC/"}]},{"title":"Spring Boot Web 开发","slug":"后台技术/Spring Boot/Spring Boot Web 开发","date":"2020-03-03T06:08:47.000Z","updated":"2023-09-20T07:13:29.023Z","comments":true,"path":"2020/03/03/后台技术/Spring Boot/Spring Boot Web 开发/","link":"","permalink":"https://wingowen.github.io/2020/03/03/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20Web%20%E5%BC%80%E5%8F%91/","excerpt":"Spring Boot Web 方面的开发介绍。","text":"Spring Boot Web 方面的开发介绍。 自动配置原理： xxxAutoConfiguration：帮我们给容器中自动配置组件； xxxProperties：配置类封装配置文件的内容。 静态资源映射 以 JAR 包的方引入静态资源： 123456&lt;!-- 引入 jquery-webjar 在访问的时候只需要写 webjars 下面资源的名称即可 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt; 访问 jquery.js：localhost:8080/webjars/jquery/3.3.1/jquery.js 访问当前项目的任何资源，都去静态资源文件夹找映射。 12345&quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;,&quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; &quot;/&quot;：当前项目的根路径 Thymeleaf 模板引擎 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 默认是 2.1.6 版本 --&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 切换 Thymeleaf 版本 --&gt;&lt;properties&gt; &lt;thymeleaf.version&gt;3.0.9.RELEASE&lt;/thymeleaf.version&gt; &lt;!-- 布局功能的支持程序 Thymeleaf3 主程序 Layout2 以上版本 --&gt; &lt;!-- Thymeleaf2 layout1--&gt; &lt;thymeleaf-layout-dialect.version&gt;2.2.2&lt;/thymeleaf-layout-dialect.version&gt;&lt;/properties&gt; 自动配置 Spriing Boot 对于 Thymeleaf 的自动配置 12345678910# Enable template cachingspring thymeleaf cache=true# Template encodingspring thymeleaf encoding=UTF-8# Template mode to be applied to templates. See also StandardTemplateModeHandlersspring thymeleaf mode=HTML5# Prefix that gets prepended to view names when building a URLspring thymeleaf prefix=classpath: /templates/# Suffix that gets appended to view names when building a URLspring thymeleaf suffix=.html 12345678910111213@ConfigurationProperties(prefix = &quot;spring.thymeleaf&quot;)public class ThymeleafProperties &#123; private static final Charset DEFAULT_ENCODING = Charset.forName(&quot;UTF-8&quot;); private static final MimeType DEFAULT_CONTENT_TYPE = MimeType.valueOf(&quot;text/html&quot;); public static final String DEFAULT_PREFIX = &quot;classpath:/templates/&quot;; public static final String DEFAULT_SUFFIX = &quot;.html&quot;; // ...&#125; 从自动配置类中可以看出只要我们把 HTML 页面放在classpath:/templates/目录下 Thymeleaf 就能自动渲染。 Thymeleaf 的使用 123456789101112&lt;!DOCTYPE html&gt;&lt;!-- 导入 Thymeleaf 的名称空间 --&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- th:text 设置 div 里面的文本内容 --&gt; &lt;div th:text=&quot;$&#123;hello&#125;&quot;&gt;这是显示欢迎信息&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 常用标签 常用表达式 123456789101112131415161718192021行内写法：[[]] -&gt; th:text 会转义特殊字符[()] -&gt; th:utext 不会转义特殊字符内置工具类：#execInfo : information about the template being processed.#messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #&#123;…&#125; syntax.#uris : methods for escaping parts of URLs/URIs#conversions : methods for executing the configured conversion service (if any).#dates : methods for java.util.Date objects: formatting, component extraction, etc.#calendars : analogous to #dates , but for java.util.Calendar objects.#numbers : methods for formatting numeric objects.#strings : methods for String objects: contains, startsWith, prepending/appending, etc.#objects : methods for objects in general.#bools : methods for boolean evaluation.#arrays : methods for arrays.#lists : methods for lists.#sets : methods for sets.#maps : methods for maps.#aggregates : methods for creating aggregates on arrays or collections.#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration). Spring MVC 自动配置 Sping Boot 对 Spring MVC 的默认配置：WebMvcAutoConfiguration 12345678910//使用 WebMvcConfigurerAdapter 可以来扩展 SpringMVC 的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; // 浏览器发送 /wingo 请求来到 success registry.addViewController(&quot;/wigno&quot;).setViewName(&quot;success&quot;); &#125;&#125; 不能标注@EnableWebMvc，标注后 Spring MVC 的默认自动装配将不会进行，即开发者全面接管 Spring MVC Restful CRUD 默认访问首页： 1234567891011121314151617// 使用 WebMvcConfigurerAdapter 可以来扩展 Spring MVC 的功能@Configurationpublic class MyMvcConfig extends WebMvcConfigurerAdapter &#123; // 所有的 WebMvcConfigurerAdapter 组件都会一起起作用 @Bean // 将组件注册在容器 public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/&quot;).setViewName(&quot;login&quot;); registry.addViewController(&quot;/index.html&quot;).setViewName(&quot;login&quot;); &#125; &#125;; return adapter; &#125;&#125; 国际化 IDEA 会自动识别国际化目录。 IDEA 进行优化后的便于编辑的视图：（点击左下角的 Resource Bundle） Spring Boot 中的消息自动配置： 12345678public class MessageSourceAutoConfiguration &#123; // ... // 默认配置为 spring.messages.basename=messages 即国际化资源默认放在类路径下的 messages.properties String basename = context.getEnvironment().getProperty(&quot;spring.messages.basename&quot;, &quot;messages&quot;); //... &#125; 12# 自定义国际化文件的位置spring.message.basename=i18n.login 页面获取国际化的信息： 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, shrink-to-fit=no&quot;&gt; &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt; &lt;meta name=&quot;author&quot; content=&quot;&quot;&gt; &lt;title&gt;Signin Template for Bootstrap&lt;/title&gt; &lt;!-- webjars 形式的静态资源映射 --&gt; &lt;!-- Bootstrap core CSS --&gt; &lt;link href=&quot;asserts/css/bootstrap.min.css&quot; th:href=&quot;@&#123;/webjars/bootstrap/4.0.0/css/bootstrap.css&#125;&quot; rel=&quot;stylesheet&quot;&gt; &lt;!-- Custom styles for this template --&gt; &lt;link href=&quot;asserts/css/signin.css&quot; th:href=&quot;@&#123;/asserts/css/signin.css&#125;&quot; rel=&quot;stylesheet&quot;&gt; &lt;/head&gt; &lt;body class=&quot;text-center&quot;&gt; &lt;form class=&quot;form-signin&quot; action=&quot;dashboard.html&quot;&gt; &lt;img class=&quot;mb-4&quot; th:src=&quot;@&#123;/asserts/img/bootstrap-solid.svg&#125;&quot; src=&quot;asserts/img/bootstrap-solid.svg&quot; alt=&quot;&quot; width=&quot;72&quot; height=&quot;72&quot;&gt; &lt;!-- Thymeleaf 用 #&#123;&#125; 表达式取出国际化信息 --&gt; &lt;h1 class=&quot;h3 mb-3 font-weight-normal&quot; th:text=&quot;#&#123;login.tip&#125;&quot;&gt;Please sign in&lt;/h1&gt; &lt;label class=&quot;sr-only&quot; th:text=&quot;#&#123;login.username&#125;&quot;&gt;Username&lt;/label&gt; &lt;input type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;Username&quot; th:placeholder=&quot;#&#123;login.username&#125;&quot; required=&quot;&quot; autofocus=&quot;&quot;&gt; &lt;label class=&quot;sr-only&quot; th:text=&quot;#&#123;login.password&#125;&quot;&gt;Password&lt;/label&gt; &lt;input type=&quot;password&quot; class=&quot;form-control&quot; placeholder=&quot;Password&quot; th:placeholder=&quot;#&#123;login.password&#125;&quot; required=&quot;&quot;&gt; &lt;div class=&quot;checkbox mb-3&quot;&gt; &lt;label&gt; &lt;input type=&quot;checkbox&quot; value=&quot;remember-me&quot;/&gt; [[#&#123;login.remember&#125;]] &lt;/label&gt; &lt;/div&gt; &lt;button class=&quot;btn btn-lg btn-primary btn-block&quot; type=&quot;submit&quot; th:text=&quot;#&#123;login.btn&#125;&quot;&gt;Sign in&lt;/button&gt; &lt;p class=&quot;mt-5 mb-3 text-muted&quot;&gt;© 2017-2018&lt;/p&gt; &lt;a class=&quot;btn btn-sm&quot;&gt;中文&lt;/a&gt; &lt;a class=&quot;btn btn-sm&quot;&gt;English&lt;/a&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; 根据浏览器请求中所携带的国际化 Locale 信息进行语言的转换。 123456789101112131415@Bean@ConditionalOnMissingBean@ConditionalOnProperty( prefix = &quot;spring.mvc&quot;, name = &#123;&quot;locale&quot;&#125;)public LocaleResolver localeResolver() &#123; if (this.mvcProperties.getLocaleResolver() == org.springframework.boot.autoconfigure.web.servlet.WebMvcProperties.LocaleResolver.FIXED) &#123; return new FixedLocaleResolver(this.mvcProperties.getLocale()); &#125; else &#123; AcceptHeaderLocaleResolver localeResolver = new AcceptHeaderLocaleResolver(); localeResolver.setDefaultLocale(this.mvcProperties.getLocale()); return localeResolver; &#125;&#125; 可以在切换语言的的链接上携带区域信息，进行语言切换。 1234567891011121314151617181920212223public class MyLocaleResolver implements LocaleResolver &#123; @Override public Locale resolveLocale(HttpServletRequest request) &#123; String myLocale = request.getParameter(&quot;locale&quot;); Locale locale = Locale.getDefault(); if(!StringUtils.isEmpty(myLocale))&#123; String[] split = myLocate.split(&quot;_&quot;); //国家 语言 locale = new Locale(split[0],split[1]); &#125; return locale; &#125; @Override public void setLocale(HttpServletRequest request, HttpServletResponse response, Locale locale) &#123;&#125; @Bean public LocaleResolver localeResolver()&#123; return new MyLocaleResolver(); &#125;&#125; 用户登录 12# 禁用缓存spring.thymeleaf.cache=false 修改了页面后 Ctrl + F9 进行重新编译 123&lt;!-- 登录错误消息显示 --&gt;&lt;!-- 使用 Thymeleaf 内置工具类 #Srting, 取传递变量的值 $&#123;&#125; 表达式--&gt;&lt;p style=&quot;color: red&quot; th:text=&quot;$&#123;msg&#125;&quot; th:if=&quot;$&#123;not #strings.isEmpty(msg)&#125;&quot;&gt;&lt;/p&gt; 拦截器 拦截器的编写： 12345678910111213141516171819public class LoginHandlerInterceptor implements HandlerInterceptor &#123; // 目标方法执行之前 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 用户登录后在 Session 中保存用户标志 Object user = request.getSession().getAttribute(&quot;loginUser&quot;); if(user == null)&#123; // 未登陆，返回登陆页面 request.setAttribute(&quot;msg&quot;,&quot;没有权限请先登陆&quot;); request.getRequestDispatcher(&quot;/index.html&quot;).forward(request,response); return false; &#125;else&#123; // 已登陆，放行请求 return true; &#125; &#125; // 省略其它不需要编写的抽象方法&#125; 注册拦截器 123456789101112131415161718192021// 所有的 WebMvcConfigurerAdapter 组件都会一起起作用@Bean // 将组件注册在容器public WebMvcConfigurerAdapter webMvcConfigurerAdapter()&#123; WebMvcConfigurerAdapter adapter = new WebMvcConfigurerAdapter() &#123; @Override public void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/&quot;).setViewName(&quot;login&quot;); registry.addViewController(&quot;/index.html&quot;).setViewName(&quot;login&quot;); &#125; // 注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; // super.addInterceptors(registry); // 静态资源 *.css , *.js SpringBoot 已经做好了映射 // 除了访问登录页面的 URL 其余的 URL 全部进行拦截认证 registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns(&quot;/**&quot;) .excludePathPatterns(&quot;/index.html&quot;,&quot;/&quot;,&quot;/user/login&quot;); &#125; &#125;; return adapter;&#125; 员工列表 普通CRUD（URI 来区分操作） RestfulCRUD 查询 getEmp emp—GET 添加 addEmp?xxx emp—POST 修改 updateEmp?id=xxx&amp;xxx=xx emp/{id}—PUT 删除 deleteEmp?id=1 emp/{id}—DELETE 本次实验的请求架构： 实验功能 请求URI 请求方式 查询所有员工 emps GET 查询某个员工（来到修改页面） emp/{id} GET 来到添加页面 emp GET 添加员工 emp POST 来到修改页面（查出员工进行信息回显） emp/{id} GET 修改员工 emp PUT 删除员工 emp/{id} DELETE 员工列表，Thymeleaf 公共页面元素抽取。 123456789101112131415161718192021222324252627282930&lt;!-- common/footer.html 抽取公共片段 --&gt;&lt;footer th:fragment=&quot;copy&quot;&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;!-- 引入公共片段 --&gt;&lt;div th:insert=&quot;footer::copy&quot;&gt;&lt;/div&gt;&lt;div th:replace=&quot;footer::copy&quot;&gt;&lt;/div&gt;&lt;div th:include=&quot;footer::copy&quot;&gt;&lt;/div&gt;&lt;!-- ~&#123;templatename::selector&#125;：模板名::选择器~&#123;templatename::fragmentname&#125;:模板名::片段名insert 的公共片段在 div 标签中，如果使用 th:insert 等属性进行引入，可以不用写 ~&#123;&#125;；；行内写法可以加上：[[~&#123;&#125;]] [(~&#123;&#125;)]--&gt;&lt;!-- 效果 --&gt;&lt;div&gt; &lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery &lt;/footer&gt;&lt;/div&gt;&lt;footer&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/footer&gt;&lt;div&gt; &amp;copy; 2011 The Good Thymes Virtual Grocery&lt;/div&gt; 引入片段时传递参数： 12345678&lt;!--引入侧边栏并传入参数；此处用的 id 选择器来引入片段 --&gt;&lt;div th:replace=&quot;commons/bar::#sidebar(activeUri=&#x27;emps&#x27;)&quot;&gt;&lt;/div&gt;&lt;!-- 应用场景：点击侧边栏的选项，被点击的选项样式发生改变，变为高亮 --&gt;&lt;!-- 解决方式：页面引用时所传递的参数进行样式的修改，生成的侧边栏样式根据传入的参数改变样式 --&gt;&lt;a class=&quot;nav-link active&quot; th:class=&quot;$&#123;activeUri==&#x27;main.html&#x27;?&#x27;nav-link active&#x27;:&#x27;nav-link&#x27;&#125;&quot; href=&quot;#&quot; th:href=&quot;@&#123;/main.html&#125;&quot;&gt; 添加及修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!-- 需要区分是员工修改还是添加 --&gt;&lt;form th:action=&quot;@&#123;/emp&#125;&quot; method=&quot;post&quot;&gt;&lt;!-- 发送 put 请求修改员工数据--&gt; &lt;!-- Spring MVC 中配置 HiddenHttpMethodFilter；（SpringBoot自动配置好的） 页面创建一个post表单； 创建一个 input 项，name=&quot;_method&quot;；value 就是我们指定的请求方式。 --&gt; &lt;!-- 若 emp 携带信息，则此表单提交的是 put 请求 --&gt; &lt;input type=&quot;hidden&quot; name=&quot;_method&quot; value=&quot;put&quot; th:if=&quot;$&#123;emp!=null&#125;&quot;/&gt; &lt;!-- 修改需要携带员工 id 用于保存修改 --&gt; &lt;input type=&quot;hidden&quot; name=&quot;id&quot; th:if=&quot;$&#123;emp!=null&#125;&quot; th:value=&quot;$&#123;emp.id&#125;&quot;&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;LastName&lt;/label&gt; &lt;input name=&quot;lastName&quot; type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;zhangsan&quot; th:value=&quot;$&#123;emp!=null&#125;?$&#123;emp.lastName&#125;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Email&lt;/label&gt; &lt;input name=&quot;email&quot; type=&quot;email&quot; class=&quot;form-control&quot; placeholder=&quot;zhangsan@atguigu.com&quot; th:value=&quot;$&#123;emp!=null&#125;?$&#123;emp.email&#125;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Gender&lt;/label&gt;&lt;br/&gt; &lt;div class=&quot;form-check form-check-inline&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;1&quot; th:checked=&quot;$&#123;emp!=null&#125;?$&#123;emp.gender==1&#125;&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt;男&lt;/label&gt; &lt;/div&gt; &lt;div class=&quot;form-check form-check-inline&quot;&gt; &lt;input class=&quot;form-check-input&quot; type=&quot;radio&quot; name=&quot;gender&quot; value=&quot;0&quot; th:checked=&quot;$&#123;emp!=null&#125;?$&#123;emp.gender==0&#125;&quot;&gt; &lt;label class=&quot;form-check-label&quot;&gt;女&lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;department&lt;/label&gt; &lt;!--提交的是部门的id--&gt; &lt;select class=&quot;form-control&quot; name=&quot;department.id&quot;&gt; &lt;option th:selected=&quot;$&#123;emp!=null&#125;?$&#123;dept.id == emp.department.id&#125;&quot; th:value=&quot;$&#123;dept.id&#125;&quot; th:each=&quot;dept:$&#123;depts&#125;&quot; th:text=&quot;$&#123;dept.departmentName&#125;&quot;&gt;1&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=&quot;form-group&quot;&gt; &lt;label&gt;Birth&lt;/label&gt; &lt;!-- #dates 工具类进行时间格式化 --&gt; &lt;input name=&quot;birth&quot; type=&quot;text&quot; class=&quot;form-control&quot; placeholder=&quot;zhangsan&quot; th:value=&quot;$&#123;emp!=null&#125;?$&#123;#dates.format(emp.birth, &#x27;yyyy-MM-dd HH:mm&#x27;)&#125;&quot;&gt; &lt;/div&gt; &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary&quot; th:text=&quot;$&#123;emp!=null&#125;?&#x27;修改&#x27;:&#x27;添加&#x27;&quot;&gt;添加&lt;/button&gt;&lt;/form&gt; 删除 12345678910111213141516171819202122&lt;tr th:each=&quot;emp:$&#123;emps&#125;&quot;&gt; &lt;td th:text=&quot;$&#123;emp.id&#125;&quot;&gt;&lt;/td&gt; &lt;td&gt;[[$&#123;emp.lastName&#125;]]&lt;/td&gt; &lt;td th:text=&quot;$&#123;emp.email&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;emp.gender&#125;==0?&#x27;女&#x27;:&#x27;男&#x27;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;emp.department.departmentName&#125;&quot;&gt;&lt;/td&gt; &lt;td th:text=&quot;$&#123;#dates.format(emp.birth, &#x27;yyyy-MM-dd HH:mm&#x27;)&#125;&quot;&gt;&lt;/td&gt; &lt;td&gt; &lt;a class=&quot;btn btn-sm btn-primary&quot; th:href=&quot;@&#123;/emp/&#125;+$&#123;emp.id&#125;&quot;&gt;编辑&lt;/a&gt; &lt;!-- th:attr 标签自定义属性及值 --&gt; &lt;button th:attr=&quot;del_uri=@&#123;/emp/&#125;+$&#123;emp.id&#125;&quot; class=&quot;btn btn-sm btn-danger deleteBtn&quot;&gt;删除&lt;/button&gt; &lt;/td&gt;&lt;/tr&gt;&lt;script&gt; $(&quot;.deleteBtn&quot;).click(function()&#123; // 删除当前员工 $(&quot;#deleteEmpForm&quot;).attr(&quot;action&quot;,$(this).attr(&quot;del_uri&quot;)).submit(); return false; &#125;);&lt;/script&gt; 错误处理机制 Spring Boot 默认的错误处理机制。 默认效果：对于浏览器，Spring Boot 返回一个默认的错误页面；对于其它客户端，默认响应一个 Json 数据。 1234567891011// 帮我们在页面共享信息@Overridepublic Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; Map&lt;String, Object&gt; errorAttributes = new LinkedHashMap&lt;String, Object&gt;(); errorAttributes.put(&quot;timestamp&quot;, new Date()); addStatus(errorAttributes, requestAttributes); addErrorDetails(errorAttributes, requestAttributes, includeStackTrace); addPath(errorAttributes, requestAttributes); return errorAttributes;&#125; 1234567891011121314151617181920212223242526// 处理默认 /error 请求@Controller@RequestMapping(&quot;$&#123;server.error.path:$&#123;error.path:/error&#125;&#125;&quot;) // 默认为错误页面路径 /errorpublic class BasicErrorController extends AbstractErrorController &#123; @RequestMapping(produces = &quot;text/html&quot;) // 产生 HTML 类型的数据；浏览器发送的请求来到这个方法处理 public ModelAndView errorHtml(HttpServletRequest request, HttpServletResponse response) &#123; HttpStatus status = getStatus(request); Map&lt;String, Object&gt; model = Collections.unmodifiableMap(getErrorAttributes( request, isIncludeStackTrace(request, MediaType.TEXT_HTML))); response.setStatus(status.value()); // 去哪个页面作为错误页面，包含页面地址和页面内容 ModelAndView modelAndView = resolveErrorView(request, response, status, model); return (modelAndView == null ? new ModelAndView(&quot;error&quot;, model) : modelAndView); &#125; @RequestMapping @ResponseBody // 产生 Json 数据，其他客户端来到这个方法处理 public ResponseEntity&lt;Map&lt;String, Object&gt;&gt; error(HttpServletRequest request) &#123; Map&lt;String, Object&gt; body = getErrorAttributes(request, isIncludeStackTrace(request, MediaType.ALL)); HttpStatus status = getStatus(request); return new ResponseEntity&lt;Map&lt;String, Object&gt;&gt;(body, status); &#125; 1234567891011121314151617181920212223@Overridepublic ModelAndView resolveErrorView(HttpServletRequest request, HttpStatus status, Map&lt;String, Object&gt; model) &#123; ModelAndView modelAndView = resolve(String.valueOf(status), model); if (modelAndView == null &amp;&amp; SERIES_VIEWS.containsKey(status.series())) &#123; modelAndView = resolve(SERIES_VIEWS.get(status.series()), model); &#125; return modelAndView;&#125;private ModelAndView resolve(String viewName, Map&lt;String, Object&gt; model) &#123; // 默认 Spring Boot 可以去找到一个页面 error/404 String errorViewName = &quot;error/&quot; + viewName; // 模板引擎可以解析这个页面地址就用模板引擎解析 TemplateAvailabilityProvider provider = this.templateAvailabilityProviders .getProvider(errorViewName, this.applicationContext); if (provider != null) &#123; // 模板引擎可用的情况下返回到 errorViewName 指定的视图地址 return new ModelAndView(errorViewName, model); &#125; //模板引擎不可用，就在静态资源文件夹下找 errorViewName 对应的页面 error/404.html return resolveResource(errorViewName, model);&#125; 一但系统出现 4xx 或者 5xx 之类的错误，ErrorPageCustomizer 就会生效（定制错误的响应规则），就会来到 /error 请求，就会被BasicErrorController处理。 错误页面的定制 有模板引擎的情况下：error/状态码（将错误页面命名为 错误状态码 .html 放在模板引擎文件夹里面的 error 文件夹下）发生此状态码的错误就会来到对应的页面。 可以使用 4xx 和 5xx 作为错误页面的文件名来匹配这种类型的所有错误，精确优先。（优先寻找精确的 [状态码].html） 1234567页面能获取的信息： timestamp：时间戳 status：状态码 error：错误提示 exception：异常对象 message：异常消息 errors：JSR303 数据校验的错误都在这里 没有模板引擎（模板引擎找不到这个错误页面），静态资源文件夹下找，以上都没有错误页面，就是默认来到SpringBoot默认的错误提示页面。 12345678910111213141516// 自定义异常处理 &amp; 返回定制 Json 数据@ControllerAdvicepublic class MyExceptionHandler &#123; @ResponseBody @ExceptionHandler(UserNotExistException.class) public Map&lt;String,Object&gt; handleException(Exception e)&#123; Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 传入我们自己的错误状态码 4xx 5xx，否则就不会进入定制错误页面的解析流程 request.setAttribute(&quot;javax.servlet.error.status_code&quot;,500); map.put(&quot;code&quot;,&quot;user.notexist&quot;); map.put(&quot;message&quot;,e.getMessage()); // 用于错误信息的读取 request.setAttribute(&quot;userNotExist&quot;,map); return &quot;forward:/error&quot;; &#125;&#125; 12345678910111213141516// 给容器中加入我们自己定义的 ErrorAttributes@Componentpublic class MyErrorAttributes extends DefaultErrorAttributes &#123; @Override public Map&lt;String, Object&gt; getErrorAttributes(RequestAttributes requestAttributes, boolean includeStackTrace) &#123; // 这个 map 就是页面和 Json 能获取的所有字段 Map&lt;String, Object&gt; map = super.getErrorAttributes(requestAttributes, includeStackTrace); map.put(&quot;company&quot;, &quot;Welab&quot;); // 异常处理器携带的数据 Map&lt;String, Object&gt; map userNotExist = (Map&lt;String, Object&gt;)requestAttributes.getAttribute(&quot;userNotExist&quot;,0); map.put(&quot;userNotExist&quot;, userNotExist) return map; &#125;&#125; 嵌入式 Servlet 容器 由于 Spring Boot 默认是以 Jar 包的方式启动嵌入式的 Servlet 容器来启动 Spring Boot 的 web 应用，没有 web.xml 文件。 Spring Boot 修改和 server 有关的配置： 123456# 方式一：propertiesserver.port=8081server.context-path=/crudserver.tomcat.uri-encoding=UTF-8# 通用的 Servlet 容器设置 server.xxx# Tomcat 的设置 server.tomcat.xxx 1234567891011// 编写嵌入式的 Servlet 容器的定制器来修改 Servlet 容器的配置@Bean //一定要将这个定制器加入到容器中public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer()&#123; return new EmbeddedServletContainerCustomizer() &#123; //定制嵌入式的Servlet容器相关的规则 @Override public void customize(ConfigurableEmbeddedServletContainer container) &#123; container.setPort(8083); &#125; &#125;;&#125; Servlet 三大组件的注册 123456@Beanpublic ServletRegistrationBean myServlet()&#123; ServletRegistrationBean registrationBean = new ServletRegistrationBean( new MyServlet(),&quot;/myServlet&quot;); return registrationBean;&#125; 1234567@Beanpublic FilterRegistrationBean myFilter()&#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(new MyFilter()); registrationBean.setUrlPatterns(Arrays.asList(&quot;/hello&quot;,&quot;/myServlet&quot;)); return registrationBean;&#125; 123456@Beanpublic ServletListenerRegistrationBean myListener()&#123; ServletListenerRegistrationBean&lt;MyListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new MyListener()); return registrationBean;&#125; 替换 Servlet 容器 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 引入 web 模块默认就是使用嵌入式的 Tomcat 作为 Servlet 容器 --&gt;&lt;/dependency&gt; 123456789101112131415161718&lt;!-- 引入web模块 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;!--引入其他的Servlet容器--&gt;&lt;dependency&gt; &lt;artifactId&gt;spring-boot-starter-jetty&lt;/artifactId&gt; &lt;!-- spring-boot-starter-undertow --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;/dependency&gt; 外置 Servlet 容器 ！！！创建的必须是一个 war 项目（利用 IDEA 创建好目录结构） 123456&lt;!-- 将嵌入式的 Tomcat 指定为 provided --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 必须要编写一个 SpringBootServletInitializer 的子类，并调用 configure() 方法。 123456789public class ServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; // 传入 SpringBoot 应用的主程序 return application.sources(SpringBootMyApplication.class); &#125;&#125; REST 架构风格","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Web","slug":"Web","permalink":"https://wingowen.github.io/tags/Web/"}]},{"title":"Spring Boot 日志","slug":"后台技术/Spring Boot/Spring Boot 日志框架","date":"2020-03-02T02:27:24.000Z","updated":"2023-09-20T07:13:16.431Z","comments":true,"path":"2020/03/02/后台技术/Spring Boot/Spring Boot 日志框架/","link":"","permalink":"https://wingowen.github.io/2020/03/02/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/","excerpt":"Spring Boot 日志框架的使用。","text":"Spring Boot 日志框架的使用。 日志框架 市面上常见的日志框架： 日志门面 （日志的抽象层） 日志实现 JCL: Jakarta Commons Logging;SLF4j: Simple Logging Facade for Java;Jboss-Logging Log4JJUL: java.util.logging;Log4J2 Logback Spring Boot 选用 SLF4J 和 Logback Q：Spring Boot 整合了很多框架，若整合的框架默认使用的日志框架与 Spring Boot 默认使用的日志框架不一样时，要怎么做？ A：排除整合框架的默认日志框架，用中间包来替代原有的日志框架。 SLF4J 在开发的中，日志记录方法的调用不应该来直接调用日志的实现类，而是调用日志抽象层里面的方法。 12345678910// 给系统里面导入 SLF4J 的 JAR 和 Logback 的实现 JARimport org.slf4j.Logger;import org.slf4j.LoggerFactory;public class HelloWorld &#123; public static void main(String[] args) &#123; Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(&quot;Hello World&quot;); &#125;&#125; SLF4J 做为不同的日志实现框架的门面的配置结构： 统一日志记录： 将系统中其他日志框架先排除出去； 用中间包来替换原有的日志框架； 导入 SLF4J 的其他的实现。 Spring Boot 日志关系 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot 内配置的排除 Spring 的 Commons-Logging： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 日志的使用 日志的级别： 12345678// 由低到高 trace &lt; debug &lt; info &lt; warn &lt; error// 可以调整输出的日志级别；日志就只会在这个级别以以后的高级别生效logger.trace(&quot;这是trace日志...&quot;);logger.debug(&quot;这是debug日志...&quot;);// SpringBoot 默认给我们使用的是 info 级别（root 级别）logger.info(&quot;这是info日志...&quot;);logger.warn(&quot;这是warn日志...&quot;);logger.error(&quot;这是error日志...&quot;); 日志输出格式： 1234567日志输出格式： %d表示日期时间; %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %thread表示线程名; %-5level：级别从左显示5个字符宽度; %logger&#123;50&#125; 表示logger名字最长50个字符，否则按照句点分割; %msg：日志消息; %n是换行符。 Spring Boot 修改日志的默认设置 1234567891011121314logging.level.com.wingo=trace # 指定某个包路径下的日志输出等级# logging.path=# 不指定路径在当前项目下生成 springboot.log 日志# 可以指定完整的路径；# logging.file=G:/springboot.log# 在当前磁盘的根路径下创建 spring 文件夹和里面的 log 文件夹；使用 spring.log 作为默认文件# logging.path=/spring/log# 在控制台输出的日志的格式logging.pattern.console=%d&#123;yyyy-MM-dd&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n# 指定文件中日志输出的格式logging.pattern.file=%d&#123;yyyy-MM-dd&#125; === [%thread] === %-5level === %logger&#123;50&#125; ==== %msg%n 指定配置 给类路径下放上每个日志框架自己的配置文件即可，SpringBoot 就不使用他默认配置的了。 Logging System Customization Logback logback-spring.xml, logback-spring.groovy, logback.xml or logback.groovy Log4J2 log4j2-spring.xml or log4j2.xml JDK (Java Util Logging) logging.properties logback.xml：直接就被日志框架识别了； logback-spring.xml：日志框架就不直接加载日志的配置项，由 SpringBoot 解析日志配置，可以使用 SpringBoot 的高级 Profile 功能。 SpringBoot 的高级 Profile 功能： 123&lt;springProfile name=&quot;staging&quot;&gt; &lt;!-- configuration to be enabled when the &quot;staging&quot; profile is active --&gt;&lt;/springProfile&gt; 配置文件 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;!-- 先定义所有的 appender --&gt; &lt;appenders&gt; &lt;!-- 输出控制台的配置 --&gt; &lt;console name=&quot;Console target=SYSTEM_OUT&quot;&gt; &lt;!-- 输出日志的格式 --&gt; &lt;patternlayout pattern=&quot;[%p] &gt;&gt;&gt; %m%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;!-- 定义各种包下的日志的输出级别 --&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;!-- 输出到控制台 --&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;logger name=&quot;org.springframework&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;com.baomidou&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;org.hibernate&quot; level=&quot;ERROR&quot;/&gt; &lt;logger name=&quot;com. alibaba druid&quot; level=&quot;ERROR&quot;/&gt; &lt;/loggers&gt;&lt;/configuration&gt; 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;property name=&quot;log.path&quot; value=&quot;C:\\Users\\wingo\\Documents\\Recent\\log&quot; /&gt; &lt;!--输出到控制台--&gt; &lt;appender name=&quot;console&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;!-- &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt;--&gt; &lt;encoder&gt; &lt;pattern&gt;[%p] &gt;&gt;&gt; %m%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--输出到文件--&gt; &lt;appender name=&quot;file&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/logback.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %contextName [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;console&quot; /&gt; &lt;appender-ref ref=&quot;file&quot; /&gt; &lt;/root&gt; &lt;!-- logback为java中的包 --&gt; &lt;logger name=&quot;com.wingo.controller&quot;/&gt; &lt;!--logback.LogbackDemo：类的全路径 --&gt; &lt;logger name=&quot;com.wingo.controller.LearnController&quot; level=&quot;WARN&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;console&quot;/&gt; &lt;/logger&gt;&lt;/configuration&gt;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Log","slug":"Log","permalink":"https://wingowen.github.io/tags/Log/"}]},{"title":"Spring Boot 入门配置","slug":"后台技术/Spring Boot/Spring Boot 入门配置","date":"2020-02-29T02:48:07.000Z","updated":"2023-09-20T07:13:20.576Z","comments":true,"path":"2020/02/29/后台技术/Spring Boot/Spring Boot 入门配置/","link":"","permalink":"https://wingowen.github.io/2020/02/29/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Spring%20Boot/Spring%20Boot%20%E5%85%A5%E9%97%A8%E9%85%8D%E7%BD%AE/","excerpt":"Spring Boot 2.x 的 Hello World 以及配置详解。","text":"Spring Boot 2.x 的 Hello World 以及配置详解。 背景：J2EE 笨重的开发、繁多的配置、底下的开发效率吧、复杂的部署流程、第三方技术集成难度大。 解决：Spring 全家桶 👉 Spring Boot J2EE 一站式解决方案 👉 Spring Cloud 分布式整体解决方案 Spring Boot 简介 简化 Spring 应用开发的一个框架； 整合 Spring 技术栈的一个大集合； J2EE 一站式解决方案。 微服务：一种架构风格，即一个应用应该是一组小型服务，这些小服务之间可以通过 HTTP 的方式进行通信，每一个功能元素最终都是一个可独立替换和独立升级的软件单元。 Spring Boot HelloWorld 功能：浏览器发送 hello 请求，服务器接受请求并处理，响应 Hello World 字符串 项目生成 项目生成地址：Spring Initiizr 填写项目的基本信息后点击 Generate 下载生成的项目。 将项目解压到相应目录，打开 IDEA 导入项目。（File 👉 New 👉 Project from Existing Sources） 选择以 Maven 项目的方式导入。 设置 JDK ，点击 Environment settings 配置 Maven。 项目导入成功后的项目目录。 resources文件夹中目录结构 static：保存所有的静态资源； js css images 等； templates：保存所有的模板页面；（Spring Boot 默认 jar 包使用嵌入式的 Tomcat，默认不支持 JSP 页面）；可以使用模板引擎（freemarker、thymeleaf）； application.properties：Spring Boot 应用的配置文件；可以修改一些默认设置； 代码编写 在 pom.xml 中加入 Spring Boot 的相关依赖。 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 功能场景抽取成 Starter 来导入相关依赖 --&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启动 Spring Boot 应用的主程序 1234567891011/** * @author wingo */@SpringBootApplicationpublic class JavaSpringbootWingoApplication &#123; public static void main(String[] args) &#123; run(JavaSpringbootWingoApplication.class, args); &#125;&#125; 编写相关的 Controller 123456789@Controllerpublic class HelloController &#123; @ResponseBody @RequestMapping(&quot;/hello&quot;) public String hello()&#123; return &quot;Hello World!&quot;; &#125;&#125; 运行测试 控制台打印中可看到 Tomcat 在其默认端口 8080 被开启。 访问：localhost:8080/hello 访问成功，此 Spring Boot 项目可正常运行。 简化部署 123456789&lt;!-- 这个插件可以将应用打包成一个可执行的jar包，在 cmd 直接用 java -jar 命令执行 --&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; Hello World 探究 项目依赖 spring-boot-starter-Xxx：启动器 Spring Boot 场景启动器，用于自动导入了场景运行所依赖的组件。 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- Starter 启动器的父项目 --&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/parent&gt; 点击进入 spring-boot-starter-parent，它的父项目是 spring-boot-dependencies。 1234567&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;!-- 定义了各种依赖的版本号 --&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt; 主程序类（入口类） Spring Boot 用@SpringBootApplication来标注一个主程序类，说明这是一个 Spring Boot 应用。 123456@SpringBootConfiguration // 这是一个 Spring Boot 配置类@EnableAutoConfiguration // 开启自动配置功能@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123;&#125; Spring Boot 在启动的时候从类路径下的 META-INF/spring.factories 中获取 EnableAutoConfiguration 指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作。 按住 Ctrl 点击一个自动装配类： 发现有大量的@Conditional派生类，必须是在@Conditional指定的条件成立的情况下，才给容器中添加组件，配置里面的所有内容才生效。 可以通过 debug=true 属性让控制台打印自动配置报告。 Spring Boot 配置文件 application.properties application.yml YAML 以数据为中心。 123server: port: 8081 path: /hello 基本语法： k:(空格)v 表示一对键值对（大小写敏感）； 以空格的缩进来控制层级关系；只要是左对齐的一列数据，都是同一个层级的。 字符串字符串默认不用加上单引号或者双引号： 单引号：会转义特殊字符，特殊字符最终只是一个普通的字符串数据； 双引号：不会转义字符串里面的特殊字符，特殊字符会作为本身想表示的意思。 对象、Map： 123456friends: lastName: Wen age: 23# 行内写法friends: &#123;lastName: Wen,age: 23&#125; 数组、List、Set： 123456pets: - cat - dog # 行内写法friends: [cat,dog] 配置文件值的注入 配置文件： 123456789101112person: lastName: hello age: 18 boss: false birth: 2017/12/12 maps: &#123;k1: v1,k2: 12&#125; lists: - lisi - zhaoliu dog: name: 小狗 age: 12 Java Bean 将配置文件中配置的每一个属性的值，映射到这个组件中。 只有这个组件是容器中的组件，才能使用容器提供的@ConfigurationProperties功能 @ConfigurationProperties告诉 SpringBoot 将本类中的所有属性和配置文件中相关的配置进行绑定； prefix = &quot;person&quot;配置文件中哪个下面的所有属性进行一一映射。 123456789101112131415161718192021/** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定； * prefix = &quot;person&quot;：配置文件中哪个下面的所有属性进行一一映射 * * 只有这个组件是容器中的组件，才能使用容器提供的@ConfigurationProperties功能； * */@Component@ConfigurationProperties(prefix = &quot;person&quot;)public class Person &#123; private String lastName; private Integer age; private Boolean boss; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog;&#125; 导入配置文件处理器，这样编写配置时会有提示。 123456&lt;!-- 导入配置文件处理器，配置文件进行绑定就会有提示 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; properties 配置文件在 IDEA 中默认 UTF-8 可能会乱码，勾选 Setting - File Encodings - Transparent native-to-ascii conversion。 @Value获取值和@ConfigurationProperties获取值比较 @ConfigurationProperties @Value 功能 批量注入配置文件中的属性 一个个指定 松散绑定（松散语法） 支持 不支持 SpEL 不支持 支持 JSR303数据校验 支持 不支持 复杂类型封装 支持 不支持 配置文件 yml 还是 properties 它们都能获取到值： 只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value； 专门编写了一个 JavaBean 来和配置文件进行映射，我们就直接使用@ConfigurationProperties； 配置文件注入校验数据。 12345678//...@Validated // 开启校验public class Person &#123; //... @Email // 必须为邮件格式 private String lastName; //...&#125; 配置文件的加载 @PropertySource：加载指定的配置文件。 1@PropertySource(value = &#123;&quot;classpath:person.properties&quot;&#125;) @ImportResource：让 Spring 的配置文件生效，加载进来。Spring Boot 里面没有 Spring 的配置文件，我们自己编写的配置文件，也不能自动识别。 1@ImportResource(locations = &#123;&quot;classpath:beans.xml&quot;&#125;) Spring Boot 推荐使用全注解的方式。 12345678910@Configuration // 指明当前类是一个 Spring 配置类public class MyAppConfig &#123; //将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名 @Bean public HelloService helloService()&#123; System.out.println(&quot;配置类 @Bean 给容器中添加组件了...&quot;); return new HelloService(); &#125;&#125; 配置文件占位符 随机数： 12$&#123;random.value&#125;、$&#123;random.int&#125;、$&#123;random.long&#125;$&#123;random.int(10)&#125;、$&#123;random.int[1024,65536]&#125; 占位符获取之前配置过的值，可用:指定默认值 123456789person.last-name=张三$&#123;random.uuid&#125;person.age=$&#123;random.int&#125;person.birth=2017/12/15person.boss=falseperson.maps.k1=v1person.maps.k2=14person.lists=a,b,cperson.dog.name=$&#123;person.hello:hello&#125;_dog // 指定默认值person.dog.age=15 Profile 多 Profile 文件：我们在主配置文件编写的时候，文件名可以是 application-{profile}.properties / yml，默认使用 application.properties 的配置。 Yaml 支持多文档块的方式： 12345678910111213141516171819server: port: 8081spring: profiles: active: prod # 指定激活哪个环境--- # 文档块分隔符server: port: 8083spring: profiles: dev # 属于 dev 环境---server: port: 8084spring: profiles: prod # 属于 prod 环境 激活指定的 Profile 在配置文件中指定spring.profiles.active=dev 命令行运行java -jar [项目 JAR 包] --spring.profiles.active=dev; 虚拟机参数-Dspring.profiles.active=dev 配置文件加载位置 springboot 启动会扫描以下位置的 application.properties 或者 application.yml 文件作为 Spring boot 的默认配置文件 file:./config/ file:./ classpath:/config/ classpath:/ 优先级由高到底，高优先级的配置会覆盖低优先级的配置；SpringBoot 会从这四个位置加载全部配置文件形成互补配置。 项目打包好后，若要改变配置文件，可以使用命令行参数的形式 java -jar [项目 JAR 包] --spring.config.location=D:/application.properties 或者直接使用 --配置项=值的方式改变某一项配置 java -jar [项目 JAR 包] --server.port=8087 --server.context-path=/abc 优先级：外部 &gt; 内部、带 profile &gt; 不带 profile 启动原理解析","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"}]},{"title":"Netty 常用配置","slug":"后台技术/Netty/Netty 常用配置","date":"2020-02-27T08:35:34.000Z","updated":"2023-09-20T07:13:34.115Z","comments":true,"path":"2020/02/27/后台技术/Netty/Netty 常用配置/","link":"","permalink":"https://wingowen.github.io/2020/02/27/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"在使用 Netty 的过程中会对 serverBootStrap 的 option 和 childOption 进行自定义的一些配置，具体就是使用ChannelOption 里面声明的常量进行配置，在此介绍一下这些常量。","text":"在使用 Netty 的过程中会对 serverBootStrap 的 option 和 childOption 进行自定义的一些配置，具体就是使用ChannelOption 里面声明的常量进行配置，在此介绍一下这些常量。 ChannelOption 服务端： option() 用于设置 ServerChannel 的选项，负责监听和接收连接； childOption()：用于设置 SocketChannel，负责处理 I/O 操作。 客户端： 没有 childOption()。 ChannelOption.SO_BACKLOG SO_BACKLOG 对应的是 TCP/IP 协议 listen() 函数中的 backlog 参数，函数 listen(int socketfd, int backlog) 用来初始化服务端可连接队列，服务端处理客户端链接是顺序处理的，所以同一时间只能处理一个链接，多客户端请求过来时，服务端会将未处理的请求放入请求队列，backlogb就是制定了队列的大小。 ChannelOption.SO_REUSEADDR SO_REUSEADDR 对应的是 Socket 选项中 SO_REUSEADDR，这个参数表示允许重复使用本地地址和端口，例如，某个服务占用了 TCP 的 8080 端口，其他服务再对这个端口进行监听就会报错，SO_REUSEADDR 这个参数就是用来解决这个问题的，该参数允许服务公用一个端口，这个在服务器程序中比较常用，例如某个进程非正常退出，对一个端口的占用可能不会立即释放，这时候如果不设置这个参数，其他进程就不能立即使用这个端口。 ChannelOption.SO_KEEPALIVE SO_KEEPALIVE 这个参数对应 Socket 中的 SO_KEEPALIVE，当设置这个参数为 true 后，TCP 连接会测试这个连接的状态，如果该连接长时间没有数据交流，TCP 会自动发送一个活动探测数据报文，来检测链接是否存活。 ChannelOption.TCP_NODELAY TCP_NODELAY 对应于 socket 选项中的 TCP_NODELAY，该参数的使用和 Nagle 算法有关，Nagle 算法是将小的数据包组装为更大的帧进行发送，而不会来一个数据包发送一次，目的是为了提高每次发送的效率，因此在数据包没有组成足够大的帧时，就会延迟该数据包的发送，虽然提高了网络负载却造成了延时，TCP_NODELAY 参数设置为 true，就可以禁用 Nagle 算法，即使用小数据包即时传输。与 TCP_NODELAY 对应的就是 TCP_CORK，该选项会等到发送的数据量最大的时候，一次性发送，适合进行文件传输。 ChannelOption.SO_SNDBUF 和 ChannelOption.SO_RCVBUF SO_SNDBUF 和 SO_RCVBUF 对应 Socket 中的 SO_SNDBUF 和 SO_RCVBUF 参数，即设置发送缓冲区和接收缓冲区的大小，发送缓冲区用于保存发送数据，直到发送成功，接收缓冲区用于保存网络协议站内收到的数据，直到程序读取成功。 Codec Http 服务 HttpServerCodec 将二进制报文转为 HTTP 文本报文。 HttpObjectAggregator(512*1024) 合并分批次过来的 HTTP 报文，合并后 Handler 拿到的将是一个完整的 HTTP 报文。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"WebSocket","slug":"后台技术/Netty/WebSocket","date":"2020-02-17T04:12:30.000Z","updated":"2023-09-20T07:13:55.522Z","comments":true,"path":"2020/02/17/后台技术/Netty/WebSocket/","link":"","permalink":"https://wingowen.github.io/2020/02/17/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/WebSocket/","excerpt":"","text":"WebSocket 入门介绍以及 Netty WebSocket 协议开发。 HTTP 请求 / 响应模式：客户端加载一个网页，然后直到用户点击下一页之前，什么都不会发生。 Ajax：网络开始变得更加动态了，但所有的 HTTP 通信仍然由客户端控制，这就需要用户进行互动或定期轮询，以便从服务器加载新数据。 长期以来存在着各种技术让服务器得知有新数据可用时，立即将数据发送到客户端这些技术种类繁多。最常用的一种黑客手段是对服务器发起链接创建假象，被称为长轮询。利用长轮询，客户端可以打开指向服务器的 HTTP 连接，而服务器会一直保持连接打开，直到发送响应。服务器只要实际拥有新数据，就会发送响应。 问题：由于 HTTP 协议的开销，导致它们不适用于低延迟应用。 WebSocket：将网络套接字引入到了客户端和服务端来解决这一问题，浏览器和服务器之间可以通过套接字建立持久的连接，双方随时都可以互发数据给对方，而不是之前由客户端控制的一请求一应答模式。 WebSocket 入门 在 WebSocket API 中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道，两者就可以直接互相传送数据了。WebSocket 基于 TCP 双向全双工进行消息传递，在同一时刻，既可以发送消息，也可以接收消息，相比于 HTTP 的半双工协议，性能得到很大提升。 WebSocket 连接 建立 WebSocket 连接时，需要通过客户端或者浏览器发出握手请求。 握手请求消息如下： 为了建立一个 WebSocket 连接，客户端浏览器首先要向服务器发起一个 HTTP 请求，这个请求和通常的 HTTP 请求不同，包含了一些附加头信息，其中附加头信息 Upgrade: WebSocket表明这是一个申请协议升级的 HTTP 请求。服务器端解析这些附加的头信息，然后生成应答信息返回给客户端，客户端和服务器端的 WebSocket 连接就建立起来了，双方可以通过这个连接通道自由地传递信息，并且这个连接会持续存在直到客户端或者服务器端的某一方主动关闭连接。 握手应答消息如下： 请求消息中的Sec-WebSocket-Key是随机的,服务器端会用这些数据来构造出一个 SHA-1 的信息摘要。使用 SHA-1 加密,然后进行 BASE-64 编码，将结果做为Sec- WebSocket- Accept头的值，返回给客户端。 WebSocket 生命周期 握手成功之后,服务端和客户端就可以通过 messages 的方式进行通信了，一个消息由一个或者多个帧组成，WebSocket的消息并不一定对应一个特定网络层的帧，它可以被分割成多个帧或者被合并。 WebSocket 的握手关闭消息带有一个状态码和一个可选的关闭原因，它必须按照协议要求发送一个 Close 控制帧，当对端接收到关闭控制帧指令时，需要主动关闭 WebSocket连接。 Netty WebSocket 协议开发 Netty 基于 HTTP 协议栈开发了 WebSocket 协议栈,利用 Netty 的 WebSocket 协议栈可以非常方便地开发出 WebSocket 客户端和服务端。 Netty 服务端实例 功能介绍 支持 WebSocket 的浏览器通过 WebSocket 协议发送请求消息给服务端，服务端对请求消息进行判断； 如果是合法的 WebSocket 请求,则获取请求消息体（文本）并在后面追加字符串“欢迎使用 Netty WebSocket服务,现在时刻:[系统时间]”； 客户端 HTML 通过内嵌的 JS 脚本创建 WebSocket 连接，握手成功 / 失败，在文本框中打印“打开 Web Socket服务正常,浏览器支持 Web Socket!” / “抱歉，您的浏览器不支持 Web Socket协议!”。 功能开发 12345678&lt;!-- pom.xml --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;5.0.0.Alpha1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// WebSocket 服务端启动类public class WebSocketServer&#123; public void run(int port) throws Exception&#123; EventLoopGroup bossGroup = new NioEventLoopGroup (); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap b= new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline (); // 将请求和应答消息编码 / 解码为 HTTP 消息 pipeline.addLast(&quot;http-codec&quot;, new HttpServerCodec()); // 将 HTTP 消息的多个部分组合成一条完整的 HTTP 消息 pipeline.addLast (&quot;aggregator&quot;, new HttpObjectAggregator(65536)); // 主要用于支持浏览器和服务端进行 WebSocket 通信 ch.pipeline().addLast(&quot;http-chunked&quot;, new ChunkedWriteHandler()); // 增加 WebSocket 服务端 handler pipeline.addLast(&quot;handler&quot;, new WebSocketServerHandler()); &#125; &#125;); Channel ch = b.bind(port).sync().channel(); System.out.println(&quot;Web socket server started at port&quot; + port +&#x27;.&#x27;); System.out.println (&quot;Open our browser and navigate to http://localhost:&quot; + port + &#x27;/&#x27;); ch.closeFuture().sync(); &#125; finally &#123; bossGroup. shutdownGracefully(); workerGroup. shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws Exception &#123; int port = 8080; if (args.length &gt;0) &#123; try&#123; port = Integer.parseInt(args[0]); &#125; catch(NumberFormatException e) &#123; e.printStackTrace(); &#125; &#125; new WebSocketServer().run(port); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// Websocket 服务端处理类public class WebSocketServerHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; private static final Logger logger = Logger.getLogger(WebSocketServerHandler.class.getName()); private WebSocketServerHandshaker handshaker; @Override protected void messageReceived(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 传统的 HTTP 接入 if (msg instanceof FullHttpRequest)&#123; // 判断请求消息有中是否包含 Upgrade: websocket handleHttpRequest(ctx,(FullHttpRequest)msg); &#125;// WebSocket 接入 else if (msg instanceof WebSocketFrame)&#123; handleWebSocketFrame(ctx,(WebSocketFrame)msg); &#125; &#125; private void handleHttpRequest(ChannelHandlerContext ctx, FullHttpRequest req) throws Exception&#123; // 如果 HTTP 解码失败，返回 HTTP 400 响应异常 if (!req.getDecoderResult().isSuccess() || (!&quot;websocket&quot;.equals(req.headers().get(&quot;Upgrade&quot;))))&#123; sendHttpResponse(ctx, req,new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.BAD_REQUEST)); return; &#125; // 构造握手工厂，本机测试 WebSocketServerHandshakerFactory wsFactory = new WebSocketServerHandshakerFactory(&quot;ws://localhost:8080/websocket&quot;,null,false); // 创建握手处理类 handshaker = wsFactory.newHandshaker(req); if (handshaker == null)&#123; WebSocketServerHandshakerFactory.sendUnsupportedWebSocketVersionResponse(ctx.channel()); &#125;else &#123; // 构造握手响应消息返回给客户端 // 将 WebSocket 相关的编码和解码类动态添加到 ChannelPipeline 中用于 WebSocket 消息的编解码 handshaker.handshake(ctx.channel(),req); &#125; &#125; // 链路建立成功之后分别对控制帧进行判断 private void handleWebSocketFrame(ChannelHandlerContext ctx, WebSocketFrame frame) &#123; // 判断是否关闭链路指令 if (frame instanceof CloseWebSocketFrame)&#123; handshaker.close(ctx.channel(), (CloseWebSocketFrame) frame.retain()); return; &#125; // 判断是否是维持链路的 Ping 消息 if (frame instanceof PingWebSocketFrame)&#123; // 构造 pong 消息返回 ctx.channel().write(new PongWebSocketFrame(frame.content()).retain()); return; &#125; // 本例程仅支持文本信息，故对非文本消息抛出异常 if (!(frame instanceof TextWebSocketFrame))&#123; throw new UnsupportedOperationException(String.format(&quot;%s frame types not supported&quot;,frame.getClass().getName())); &#125; // 返回应答消息 String request = ((TextWebSocketFrame) frame).text(); if (logger.isLoggable(Level.FINE))&#123; logger.fine(String.format(&quot;%s received %s&quot;,ctx.channel(),request)); &#125; // 构造新的 TextWebSocketFrame 消息返回给客户端 // 由于握手应答时动态增加了 TextWebSocketframe 的编码类,所以可以直接发送 TextWebSocketFrame对象 ctx.channel().write(new TextWebSocketFrame(request+&quot;欢迎使用Netty WebSocket服务，现在时刻:&quot; + new Date().toString())); &#125; private static void sendHttpResponse(ChannelHandlerContext ctx,FullHttpRequest req,FullHttpResponse res)&#123; // 返回应答给客户端 if (res.getStatus().code() != 200)&#123; ByteBuf buf = Unpooled.copiedBuffer(res.getStatus().toString(), CharsetUtil.UTF_8); res.content().writeBytes(buf); // 明确释放ByteBuf的引用计数 buf.release(); setContentLength(res,res.content().readableBytes()); &#125; // 如果是非 Keep-Alive，关闭连接 ChannelFuture f = ctx.channel().writeAndFlush(res); if (!isKeepAlive(req) || res.getStatus().code() != 200)&#123; f.addListener(ChannelFutureListener.CLOSE); &#125; &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;WebSoket Demo&lt;/title&gt; &lt;script type=&quot;text/JavaScript&quot;&gt; var WebSocket = WebSocket || window.WebSocket || window.MozWebSocket; if (!WebSocket) &#123; alert(&quot;WebSocket not supported by this browser!&quot;); &#125; else &#123; var ws = null; function Display() &#123; console.log(&quot;websocket 测试&quot;); &#125; var log = function (s) &#123; if (document.readyState !== &quot;complete&quot;) &#123; log.buffer.push(s); &#125; else &#123; document.getElementById(&quot;contentId&quot;).value += (s + &quot;/n&quot;); &#125; &#125; function CreateConnect () &#123; var msg = document.getElementById(&quot;wsUrlId&quot;); console.log(&quot;CreateConnect(), url: &quot; + msg.value); if (ws == null) &#123; var wsUrlValue = msg.value; try &#123; ws = new WebSocket(wsUrlValue); ws.onmessage = function (event) &#123; log(&quot;onmessage(), 接收到服务器消息: &quot; + event.data); &#125;; ws.onclose = function (event) &#123; log(&quot;onclose(), Socket 已关闭!&quot;); ws = null; &#125;; ws.onopen = function (event) &#123; log(&quot;onopen(), Socket 连接成功!&quot;); &#125;; ws.onerror = function (event) &#123; &#125;; &#125; catch (e) &#123; ws = null; log(&quot;连接异常, 重置 websocket&quot;); &#125; &#125; &#125; function SendMsg() &#123; var msg = document.getElementById(&quot;messageId&quot;); console.log(&quot;SendMsg(), msg: &quot; + msg.value); if (ws != null) &#123; log(&quot;发送 Socket 消息: &quot; + msg.value); ws.send(msg.value); &#125; else &#123; log(&quot;Socket 还未创建!, msg: &quot; + msg.value); &#125; &#125; function CloseConnect () &#123; console.log(&quot;CloseConnect()&quot;); if (ws != null) &#123; ws.close(); &#125; &#125; &#125; &lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;Display()&quot; &gt; &lt;div id=&quot;valueLabel&quot;&gt;&lt;/div&gt; &lt;textarea rows=&quot;10&quot; cols=&quot;40&quot; id=&quot;contentId&quot;&gt;&lt;/textarea&gt; &lt;br/&gt; &lt;input name=&quot;wsUrl&quot; id=&quot;wsUrlId&quot; value=&quot;ws://localhost:8080/websocket&quot;/&gt; &lt;button id=&quot;createButton&quot; onClick=&quot;javascript:CreateConnect()&quot;&gt;Create&lt;/button&gt; &lt;button id=&quot;closeButton&quot; onClick=&quot;javascript:CloseConnect()&quot;&gt;Close&lt;/button&gt; &lt;br/&gt; &lt;input name=&quot;message&quot; id=&quot;messageId&quot; value=&quot;Hello, Server!&quot;/&gt; &lt;button id=&quot;sendButton&quot; onClick=&quot;javascript:SendMsg()&quot;&gt;Send&lt;/button&gt; &lt;/body&gt;&lt;/html&gt; 测试结果 控制台打印 浏览器调试","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://wingowen.github.io/tags/WebSocket/"}]},{"title":"Netty 引导","slug":"后台技术/Netty/Netty 引导","date":"2020-01-25T16:25:24.000Z","updated":"2023-09-20T07:13:43.324Z","comments":true,"path":"2020/01/26/后台技术/Netty/Netty 引导/","link":"","permalink":"https://wingowen.github.io/2020/01/26/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%BC%95%E5%AF%BC/","excerpt":"引导一个应用程序是指对它进行配置，并使它运行起来的过程。Netty处理引导的方式使你的应用程序和网络层相隔离，无论它是客户端还是服务器。所有的框架组件都将会在后台结合在 一起并且启用。","text":"引导一个应用程序是指对它进行配置，并使它运行起来的过程。Netty处理引导的方式使你的应用程序和网络层相隔离，无论它是客户端还是服务器。所有的框架组件都将会在后台结合在 一起并且启用。 Bootstrap 类 引导类的层次结构包括一个抽象的父类和两个具体的引导子类。 相对于将具体的引导类分别看作用于服务器和客户端的引导来说，记住它们的本意是用来支 撑不同的应用程序的功能。也就是说，服务器致力于使用一个父 Channel 来接受来自客户端的连接，并创建子 Channel 以用于它们之间的通信；而客户端将最可能只需要一个单独的、没有父 Channel 的 Channel 来用于所有的网络交互。 为什么引导类是 Cloneable 的： 你有时可能会需要创建多个具有类似配置或者完全相同配置的Channel。为了支持这种模式而又不需要为每个 Channel 都创建并配置一个新的引导类实例， AbstractBootstrap 被标记为了 Cloneable。 注意，这种方式只会创建引导类实例的 EventLoopGroup 的一个浅拷贝，所以，后者将在所有克隆的 Channel 实例之间共享。这是可以接受的，因为通常这些克隆的 Channel 的生命周期都很短暂，一个典型的场景是：创建一个 Channel 以进行一次HTTP请求。 引导客户端 Bootstrap 类负责为客户端和使用无连接协议的应用程序创建 Channel。 12345678910111213141516171819202122232425262728293031323334// 引导一个客户端EventLoopGroup group = new NioEventLoopGroup();// 创建一个 Bootstrap 类的实例用来创建和连接新的客户端 ChannelBootstrap bootstrap = new Bootstrap();// 设置 EventLoopGroup 提供用于处理 Channel 事件的 EventLoopbootstrap.group(group) // 指定要使用的 Channel 实现 .channel(NioSocketChannel.class) .handler( // 设置用于 Channel 事件和数据的 ChannelInboundHandler new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channeRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; );ChannelFuture future = bootstrap.connect( // 连接到远程主机 new InetSocketAddress(&quot;www.manning.com&quot;, 80));future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Connection established&quot;); &#125; else &#123; System.err.println(&quot;Connection attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 引导服务器 负责引导 ServerChannel 的 ServerBootstrap 提供了负责创建子 Channel 的方法 childXxx()，这些子 Channel 代表了已被接收的连接。 1234567891011121314151617181920212223242526272829// 引导服务器NioEventLoopGroup group = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(group) .channel(NioServerSocketChannel.class) // 子线程 .childHandler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf byteBuf) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; );// 监听端口ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Server bound&quot;); &#125; else &#123; System.err.println(&quot;Bound attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 从 Channel 引导客户端 假设你的服务器正在处理一个客户端的请求，这个请求需要它充当第三方系统的客户端。 1234567891011121314151617181920212223242526272829303132333435ServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(new NioEventLoopGroup(), new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; ChannelFuture connectFuture; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap.channel(NioSocketChannel.class).handler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; ); // 使用与分配给已被接受的子 Channel 相同的 EventLoop bootstrap.group(ctx.channel().eventLoop()); connectFuture = bootstrap.connect( new InetSocketAddress(&quot;www.manning.com&quot;, 80)); &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; if (connectFuture.isDone()) &#123; // do something with the data &#125; &#125; &#125; );ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture)throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Server bound&quot;); &#125; else &#123; System.err.println(&quot;Bind attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125; &#125; ); 尽可能地重用 EventLoop，以减少线程创建所带来的开销。 在引导过程中添加多个 ChannelHandler 在前面的几个例子中，在引导的过程中调用了 handler() 或者 childHandler() 方法来添加单个的 ChannelHandler。这对于简单的应用程序来说可能已经足够了，但是它不能满足更加复杂的需求。 protected abstract void initChannel(C ch) throws Exception; 这个方法提供了一种将多个 ChannelHandler 添加到一个 ChannelPipeline 中的简便方法 12345678910111213141516// 引导和使用 ChannelInitializerServerBootstrap bootstrap = new ServerBootstrap();bootstrap.group(new NioEventLoopGroup(), new NioEventLoopGroup()) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializerImpl());ChannelFuture future = bootstrap.bind(new InetSocketAddress(8080));future.sync();// ChannelInitializer 的实现final class ChannelInitializerImpl extends ChannelInitializer&lt;Channel&gt; &#123; @Override protected void initChannel(Channel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new HttpClientCodec()); pipeline.addLast(new HttpObjectAggregator(Integer.MAX_VALUE)); &#125; &#125; 如果你的应用程序使用了多个 ChannelHandler，请定义你自己的 ChannelInitializer 实现来将它们安装到 ChannelPipeline 中。 ChannelOption 和属性 你可以使用 option() 方法来将 ChannelOption 应用到引导。你所提供的值将会被自动应用到引导所创建的所有 Channel。可用的 ChannelOption 包括了底层连接的详细信息，如 keep-alive 或者超时属性以及缓冲区设置。 Netty 提供了 AttributeMap 抽象（一个由 Channel 和引导类提供的集合）以及 AttributeKey（一 个用于插入和获取属性值的泛型类）。使用这些工具，便可以安全地将任何类型的数据项与客户端和服务器 Channel（包含 ServerChannel 的子 Channel）相关联了。 123456789101112131415161718192021222324252627// 使用属性值// 创建一个 AttributeKey 以标识该属性final AttributeKey&lt;Integer&gt; id = new AttributeKey&lt;Integer&gt;(&quot;ID&quot;);Bootstrap bootstrap = new Bootstrap();bootstrap.group(new NioEventLoopGroup()) .channel(NioSocketChannel.class) .handler( new SimpleChannelInboundHandler&lt;ByteBuf&gt;() &#123; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // 使用 AttributeKey 检索属性以及它的值 Integer idValue = ctx.channel().attr(id).get(); // do something with the idValue &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; System.out.println(&quot;Received data&quot;); &#125; &#125; );// 引导的每一个 Channel 都将具有这些属性bootstrap.option(ChannelOption.SO_KEEPALIVE,true) .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, 5000);// 存储该 id 属性bootstrap.attr(id, 123456);ChannelFuture future = bootstrap.connect(new InetSocketAddress(&quot;www.manning.com&quot;, 80));future.syncUninterruptibly(); 引导 DatagramChannel 除了基于 TCP 协议的 SocketChannel，Bootstrap 类也可以被用于无连接协议。为此。Netty 提供了各种 DatagramChannel 的实现。唯一区别就是，不再调用 connect() 方法，而是只调用 bind() 方法。 1234567891011121314151617181920212223242526// 使用 Bootstrap 和 DatagramChannelBootstrap bootstrap = new Bootstrap();bootstrap.group(new OioEventLoopGroup()) .channel(OioDatagramChannel.class) .handler( new SimpleChannelInboundHandler&lt;DatagramPacket&gt;()&#123; @Override public void channelRead0(ChannelHandlerContext ctx, DatagramPacket msg) throws Exception &#123; // Do something with the packet &#125; &#125; );// 调用 bind() 方法，因为该协议是无连接的ChannelFuture future = bootstrap.bind(new InetSocketAddress(0));future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture channelFuture) throws Exception &#123; if (channelFuture.isSuccess()) &#123; System.out.println(&quot;Channel bound&quot;); &#125; else &#123; System.err.println(&quot;Bind attempt failed&quot;); channelFuture.cause().printStackTrace(); &#125; &#125;&#125;); 关闭 引导使你的应用程序启动并且运行起来，但是迟早你都需要优雅地将它关闭。 123456789// 优雅的关闭EventLoopGroup group = new NioEventLoopGroup();Bootstrap bootstrap = new Bootstrap();bootstrap.group(group) .channel(NioSocketChannel.class);...;Future&lt;?&gt; future = group.shutdownGracefully();// block until the group has shutdownfuture.syncUninterruptibly(); 注意，shutdownGracefully() 方法也是一个异步的操作，所以你需要阻塞等待直到它完成，或者向所返回的 Future 注册一个监听器以在关闭完成时获得通知。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty EventLoop 和线程模型","slug":"后台技术/Netty/Netty EventLoop 和线程模型","date":"2020-01-25T08:09:20.000Z","updated":"2023-09-20T07:13:53.018Z","comments":true,"path":"2020/01/25/后台技术/Netty/Netty EventLoop 和线程模型/","link":"","permalink":"https://wingowen.github.io/2020/01/25/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20EventLoop%20%E5%92%8C%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","excerpt":"线程模型指定了操作系统、编程语言、框架或者应用程序的上下文中的线程管理的关键方面，如何以及何时创建线程将对应用程序代码的执行产生显著的影响。","text":"线程模型指定了操作系统、编程语言、框架或者应用程序的上下文中的线程管理的关键方面，如何以及何时创建线程将对应用程序代码的执行产生显著的影响。 线程模型概述 多核心或多个 CPU 的计算机现在已经司空见惯，大多数的现代应用程序都利用了 复杂的多线程处理技术以有效地利用系统资源。 在早期的 Java 语言中，我们使用多线程处理的主要方式无非是按需创建和启动新的 Thread 来执行并发的任务单元，这是一种在高负载下工作得很差的原始方式。Java 5 随后引入了 Executor API，其线程池通过缓存和重用 Thread 极大地提高了性能。 虽然池化和重用线程相对于简单地为每个任务都创建和销毁线程是一种进步，但是它并不能 消除由上下文切换所带来的开销，其将随着线程数量的增加很快变得明显，并且在高负载下愈演愈烈。Netty 框架帮助简化了这一处理。 EventLoop 接口 运行任务来处理在连接的生命周期内发生的事件是任何网络框架的基本功能。与之相应的编程上的构造通常被称为事件循环：Netty 使用了 interface io.netty.channel.EventLoop 来适配的术语。 1234567// 在事件循环中执行任务while (!terminated) &#123; List&lt;Runnable&gt; readyEvents = blockUntilEventsReady(); for (Runnable ev: readyEvents) &#123; ev.run(); &#125; &#125; Netty 的 EventLoop 是协同设计的一部分，它采用了两个基本的 API：并发和网络编程。首先，io.netty.util.concurrent 包构建在 JDK 的 java.util.concurrent 包上，用来提供线程执行器。其次，io.netty.channel 包中的类，为了与 Channel 的事件进行交互，扩展了这些接口/类。 在这个模型中，一个 EventLoop 将由一个永远都不会改变的 Thread 驱动，同时任务 （Runnable 或者 Callable）可以直接提交给 EventLoop 实现，以立即执行或者调度执行。根据配置和可用核心的不同，可能会创建多个 EventLoop 实例用以优化资源的使用，并且单个 EventLoop 可能会被指派用于服务多个 Channel。 事件/任务的执行顺序 ： 事件和任务是以先进先出（FIFO）的顺序执行的。这样可以通过保证字节内容总是按正确的顺序被处理，消除潜在的数据损坏的可能性。 任务调度 使用核心的 Java API 和 Netty 的 EventLoop 来调度任务 JDK 的任务调度 API 12345678910111213// 使用 ScheduledExecutorService 来在 60 秒的延迟之后执行一个任务ScheduledExecutorService executor = Executors.newScheduledThreadPool(10);ScheduledFuture&lt;?&gt; future = executor.schedule( new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;60 seconds later&quot;); &#125; &#125;, 60, TimeUnit.SECONDS); // 调度任务在从现在开始的 60 秒之后执行...;executor.shutdown(); 使用 EventLoop 调度任务 12345678910// 使用 EventLoop 来在 60 秒的延迟之后执行一个任务Channel ch = ...;ScheduledFuture&lt;?&gt; future = ch.eventLoop().schedule( new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;60 seconds later&quot;); &#125; &#125;, 60, TimeUnit.SECONDS); 123456789101112// 使用 EventLoop 调度周期性的任务Channel ch = ...;ScheduledFuture&lt;?&gt; future = ch.eventLoop().scheduleAtFixedRate( new Runnable() &#123; @Override public void run() &#123; System.out.println(&quot;Run every 60 seconds&quot;); &#125; &#125;, 60, 60, TimeUnit.Seconds);// 利用每个异步操作所返回的 ScheduledFuture 取消任务future.cancel(false); 实现细节 更加详细地探讨 Netty 的线程模型和任务调度实现的主要内容。 线程管理 Netty 线程模型的卓越性能取决于对于当前执行的 Thread 的身份的确定。 永远不要将一个长时间运行的任务放入到执行队列中，因为它将阻塞需要在同一线程上执行的任何其他任务。如果必须要进行阻塞调用或者执行长时间运行的任务，我们建议使用一个专门的 EventExecutor。 EventLoop / 线程的分配 服务于 Channel 的 I/O 和事件的 EventLoop 包含在 EventLoopGroup 中。根据不同的传输实现，EventLoop 的创建和分配方式也不同。 异步传输 异步传输实现只使用了少量的 EventLoop（以及和它们相关联的 Thread），而且在当前的线程模型中，它们可能会被多个 Channel 所共享。这使得可以通过尽可能少量的 Thread 来支撑大量的 Channel，而不是每个 Channel 分配一个 Thread。 一旦一个 Channel 被分配给一个 EventLoop，它将在它的整个生命周期中都使用这个 EventLoop（以及相关联的 Thread）。 阻塞传输 这里每一个 Channel 都将被分配给一个 EventLoop（以及它的 Thread）。 每个 Channel 的 I/O 事件都将只会被一个 Thread （用于支撑该 Channel 的 EventLoop 的那个 Thread）处理（Netty 的一致性体现）。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty ChannelHandler 和 ChannelPipeline","slug":"后台技术/Netty/Netty ChannelHandler 和 ChannelPipeline","date":"2020-01-24T14:09:35.000Z","updated":"2023-09-20T07:13:50.680Z","comments":true,"path":"2020/01/24/后台技术/Netty/Netty ChannelHandler 和 ChannelPipeline/","link":"","permalink":"https://wingowen.github.io/2020/01/24/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20ChannelHandler%20%E5%92%8C%20ChannelPipeline/","excerpt":"ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑，还有一个重要的关系 ChannelHandlerContext。","text":"ChannelPipeline 中将 ChannelHandler 链接在一起以组织处理逻辑，还有一个重要的关系 ChannelHandlerContext。 ChannelHandler 家族 Channel 的生命周期 当 Channel 的状态发生改变时，将会生成对应的事件。这些事件将会被转发给 ChannelPipeline 中的 ChannelHandler，其可以随后对它们做出响应。 ChannelHandler 的生命周期 类型 描述 handlerAdded 当把 ChannelHandler 添加到 ChannelPipeline 中时被调用 handlerRemoved 当从 ChannelPipeline 中移除 ChannelHandler 时被调用 exceptionCaught 当处理过程中在 ChannelPipeline 中有错误产生时被调用 Netty 定义了两个重要的 Channelhandler 子接口： ChannelInboundHandler——处理入站数据以及各种状态变化； ChannelOutboundHandler——处理出站数据并且允许拦截所有的操作。 ChannelInboundHandler 接口 12345678910// 释放消息资源@Sharable// 扩展了 ChannelInboundHandlerAdapterpublic class DiscardHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; // 丢弃已接收的消息 ReferenceCountUtil.release(msg); &#125; &#125; 123456789// 使用 SimpleChannelInboundHandler 会自动释放资源@Sharablepublic class SimpleDiscardHandler extends SimpleChannelInboundHandler&lt;Object&gt; &#123; @Override public void channelRead0(ChannelHandlerContext ctx, Object msg) &#123; // 不需要任何显式的资源释放 // No need to do anything special &#125; &#125; ChannelOutboundHandler 接口 ChannelOutboundHandler 的一个强大的功能是可以按需推迟操作或者事件，这使得可 以通过一些复杂的方法来处理请求。 ChannelPromise 与 ChannelFuture：ChannelOutboundHandler 中的大部分方法都需要一个 ChannelPromise 参数，以便在操作完成时得到通知。ChannelPromise 是 ChannelFuture 的一个子类，其定义了一些可写的方法，如 setSuccess() 和 setFailure()，从而使 ChannelFuture 不可变。 ChannelHandler 适配器 适配器对 ChannelHandler 进行了简单的实现，只需要简单地扩展它们，并且重写那些你想要自定义的方法。 ChannelInboundHandlerAdapter 和 ChannelOutboundHandlerAdapter 中所提供的方法体调用了其相关联的 ChannelHandlerContext 上的等效方法，从而将事件转发到了 ChannelPipeline 中的下一个 ChannelHandler 中。 资源管理 每当通过调用 ChannelInboundHandler.channelRead() 或者 ChannelOutboundHandler.write() 方法来处理数据时，你都需要确保没有任何的资源泄漏。 Netty 使用引用计数来处理池化的 ByteBuf。所以在完全使用完某个 ByteBuf 后，调整其引用计数是很重要的。 12345678910111213// 丢弃并释放出战消息@Sharablepublic class DiscardOutboundHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) &#123; // 释放资源 ReferenceCountUtil.release(msg); // 通知 ChannelPromise 数据已经被处理了 promise.setSuccess(); &#125;&#125; 重要的是，不仅要释放资源，还要通知 ChannelPromise。否则可能会出现 ChannelFutureListener 收不到某个消息已经被处理了的通知的情况。 如果一个消息被消费或者丢弃了，并且没有传递给 ChannelPipeline 中的下一个 ChannelOutboundHandler，那么用户就有责任调用 ReferenceCountUtil.release()。 ChannelPipeline 接口 每一个新创建的 Channel 都将会被分配一个新的 ChannelPipeline。这项关联是永久性的；Channel 既不能附加另外一个 ChannelPipeline，也不能分离其当前的。 根据事件的起源，事件将会被 ChannelInboundHandler 或者 ChannelOutboundHandler 处理。随后，通过调用 ChannelHandlerContext 实现，它将被转发给同一超类型的下一个 ChannelHandler。 修改 ChannelPipeline ChannelHandler 可以通过添加 addXxx()、删除 remove() 或者替换 replace() 其他的 ChannelHandler 来实时地修改 ChannelPipeline 的布局。 ChannelHandler 的执行和阻塞 通常 ChannelPipeline 中的每一个 ChannelHandler 都是通过它的 EventLoop（I/O 线程）来处理传递给它的事件的。所以至关重要的是不要阻塞这个线程，因为这会对整体的 I/O 处理产生负面的影响。 但有时可能需要与那些使用阻塞 API 的遗留代码进行交互。对于这种情况，ChannelPipeline 有一些接受一个 EventExecutorGroup 的 add()方法。如果一个事件被传递给一个自定义的 EventExecutorGroup，它将被包含在这个 EventExecutorGroup 中的某个 EventExecutor 所处理，从而被从该 Channel 本身的 EventLoop 中移除。对于这种用例，Netty 提供了一个叫 DefaultEventExecutorGroup 的默认实现。 getXxx()：通过类型或者名称来访问 ChannelHandler 的方法。 context()：返回和 ChannelHandler 绑定的 ChannelHandlerContext。 names()：返回 ChannelPipeline 中所有 ChannelHandler 的名称。 触发事件 ChannelPipeline 的 API 公开了用于调用入站和出站操作的附加方法。 小结 ChannelPipeline 保存了与 Channel 相关联的 ChannelHandler； ChannelPipeline 可以根据需要，通过添加或者删除 ChannelHandler 来动态地修改； ChannelPipeline 有着丰富的 API 用以被调用，以响应入站和出站事件。 ChannelHandlerContext 接口 ChannelHandlerContext 代表了 ChannelHandler 和 ChannelPipeline 之间的关联，每当有 ChannelHandler 添加到 ChannelPipeline 中时，都会创建 ChannelHandlerContext。 ChannelHandlerContext 的主要功能是管理它所关联的 ChannelHandler 和在同一个 ChannelPipeline 中的其他 ChannelHandler 之间的交互。 ChannelHandlerContext 和 ChannelHandler 之间的关联（绑定）是永远不会改变的，所以缓存对它的引用是安全的； 使用 ChannelHandlerContext Channel、ChannelPipeline、ChannelHandler 以及 ChannelHandlerContext 之间的关系 1234// 从 ChannelHandlerContext 访问 ChannelChannelHandlerContext ctx = ..;Channel channel = ctx.channel();channel.write(Unpooled.copiedBuffer(&quot;Netty in Action&quot;, CharsetUtil.UTF_8)); 1234// 通过 ChannelHandlerContext 访问 ChannelPipelineChannelHandlerContext ctx = ..;ChannelPipeline pipeline = ctx.pipeline();pipeline.write(Unpooled.copiedBuffer(&quot;Netty in Action&quot;, CharsetUtil.UTF_8)); 重要的是要注意到，虽然被调用的 Channel 或 ChannelPipeline 上的 write()方法将一直传播事件通 过整个 ChannelPipeline，但是在 ChannelHandler 的级别上，事件从一个 ChannelHandler 到下一个 ChannelHandler 的移动是由 ChannelHandlerContext 上的调用完成的. 1234// 调用 ChannelHandlerContext 的 write()方法ChannelHandlerContext ctx = ..;// write()方法将把缓冲区数据发送到下一个 ChannelHandlerctx.write(Unpooled.copiedBuffer(&quot;Netty in Action&quot;, CharsetUtil.UTF_8)); 高级用法 缓存 ChannelHandlerContext 的引用以供稍后使用，这可能会发生在任何的 ChannelHandler 方法之外，甚至来自于不同的线程。 12345678910111213// 缓存到 ChannelHandlerContext 的引用public class WriteHandler extends ChannelHandlerAdapter &#123; private ChannelHandlerContext ctx; @Override public void handlerAdded(ChannelHandlerContext ctx) &#123; //存储到 ChannelHandlerContext 的引用以供稍后使用 this.ctx = ctx; &#125; public void send(String msg) &#123; // 使用之前存储的到 ChannelHandlerContext 的引用来发送消息 ctx.writeAndFlush(msg); &#125; &#125; 因为一个 ChannelHandler 可以从属于多个 ChannelPipeline，所以它也可以绑定到多个 ChannelHandlerContext 实例。对于这种用法指在多个 ChannelPipeline 中共享同一 个 ChannelHandler，对应的 ChannelHandler 必须要使用 @Sharable 注解标注；否则， 试图将它添加到多个 ChannelPipeline 时将会触发异常。显而易见，为了安全地被用于多个 并发的 Channel（即连接），这样的 ChannelHandler 必须是线程安全的。 123456789// 可共享的 ChannelHandler@Sharable public class SharableHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; System.out.println(&quot;Channel read message: &quot; + msg); ctx.fireChannelRead(msg); &#125; &#125; 异常处理 Netty 提供了几种方式用于处理入站或者出站处理过程中所抛出的异常。 处理入站异常 12345678// 基本的入站异常处理public class InboundExceptionHandler extends ChannelInboundHandlerAdapter &#123; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; &#125; 处理出站异常 出站的异常处理基于一下通知机制： 每个出站操作都将返回一个 ChannelFuture。注册到 ChannelFuture 的 ChannelFutureListener 将在操作完成时被通知该操作是成功了还是出错了。 几乎所有的 ChannelOutboundHandler 上的方法都会传入一个 ChannelPromise 的实例。作为 ChannelFuture 的子类，ChannelPromise 也可以被分配用于异步通知的监听器。但是，ChannelPromise 还具有提供立即通知的可写方法。 ChannelPromise 的可写方法 通过调用 ChannelPromise 上的 setSuccess() 和 setFailure() 方法，可以使一个操作的状态在 ChannelHandler 的方法返回给其调用者时便即刻被感知到。 12345678910// 添加 ChannelFutureListener 到 ChannelFutureChannelFuture future = channel.write(someMessage);future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture f) &#123; if (!f.isSuccess()) &#123; f.cause().printStackTrace(); f.channel().close(); &#125; &#125;&#125;); 123456789101112131415// 添加 ChannelFutureListener 到 ChannelPromisepublic class OutboundExceptionHandler extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) &#123; promise.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture f) &#123; if (!f.isSuccess()) &#123; f.cause().printStackTrace(); f.channel().close(); &#125; &#125; &#125;); &#125; &#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty ByteBuf","slug":"后台技术/Netty/Netty ByteBuf","date":"2020-01-24T03:29:09.000Z","updated":"2023-09-20T07:13:48.288Z","comments":true,"path":"2020/01/24/后台技术/Netty/Netty ByteBuf/","link":"","permalink":"https://wingowen.github.io/2020/01/24/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20ByteBuf/","excerpt":"总所周知，网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。因此，Netty 提供了一个替代品 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。","text":"总所周知，网络数据的基本单位总是字节。Java NIO 提供了 ByteBuffer 作为它的字节容器，但是这个类使用起来过于复杂，而且也有些繁琐。因此，Netty 提供了一个替代品 ByteBuf，一个强大的实现，既解决了 JDK API 的局限性，又为网络应用程序的开发者提供了更好的 API。 ByteBuf 的 API Netty 的数据处理 API 通过两个组件暴露：abstract class ByteBuf 和 interface ByteBufHolder。 ByteBuf 类 ByteBuf 类是 Netty 的数据容器，通过使用不同的索引来简化对它所包含的数据的访问。 How ByteBuf 维护了两个不同的索引：一个用于读取，一个用于写入。读取时，readerIndex 将会被递增已经被读取的字节数；写入时，writerIndex 也会被递增。readerIndex == writerIndex 时则到达可读取数据的末尾。 名称以 read 或者 write 开头的 ByteBuf 方法，将会推进其对应的索引，而名称以 set 或者 get 开头的操作则不会。后面的这些方法将在作为一个参数传入的一个相对索引上执行操作。 Mode 在使用 Netty 时，有几种常见的围绕 ByteBuf 而构建的使用模式。 堆缓冲区 最常用的 ByteBuf 模式是将数据存储在 JVM 的堆空间中。这种模式被称为支撑数组。 12345678910111213// 支撑数组ByteBuf heapBuf = ...;// 检查 ByteBuf 是否有一个支撑数组if (heapBuf.hasArray()) &#123; // 获取对该数组的引用 byte[] array = heapBuf.array(); // 计算第一个字节的偏移量 int offset = heapBuf.arrayOffset() + heapBuf.readerIndex(); // 获取可读字节数 int length = heapBuf.readableBytes(); // 使用获取的信息作为参数调用自定义方法 handleArray(array, offset, length);&#125; 直接缓冲区（Updating） 复合缓冲器（Updating） 字节级操作 ByteBuf 提供了许多超出基本读、写操作的方法用于修改它的数据。 随机访问索引 123456// 访问数据ByteBuf buffer = ...;for (int i = 0; i &lt; buffer.capacity(); i++) &#123; byte b = buffer.getByte(i); System.out.println((char)b);&#125; 顺序访问索引 虽然 ByteBuf 同时具有读索引和写索引，但是 JDK 的 ByteBuffer 却只有一个索引，这也就是为什么必须调用 flip() 方法来在读模式和写模式之间进行切换的原因 ByteBuf 的内部分段 可丢弃字节 可丢弃字节的分段包含了已经被读过的字节。通过调用 discardReadBytes() 方法，可以丢弃它们并回收空间。这个分段的初始大小为 0，存储在 readerIndex 中，会随着 read 操作的执行而增加（get*操作不会移动 readerIndex）。 可读字节 ByteBuf 的可读字节分段存储了实际数据。 12345// 读取所有数据ByteBuf buffer = ...;while (buffer.isReadable()) &#123; System.out.println(buffer.readByte());&#125; 可写字节 可写字节分段是指一个拥有未定义内容的、写入就绪的内存区域。 12345// 写数据ByteBuf buffer = ...;while (buffer.writableBytes() &gt;= 4) &#123; buffer.writeInt(random.nextInt());&#125; 索引管理 可以通过调用 markReaderIndex()、markWriterIndex()、resetWriterIndex() 和 resetReaderIndex() 来标记和重置 ByteBuf 的 readerIndex 和 writerIndex。 也可以通过调用 readerIndex(int) 或者 writerIndex(int) 来将索引移动到指定位置。试图将任何一个索引设置到一个无效的位置都将导致一个 IndexOutOfBoundsException。 可以通过调用 clear()方法来将 readerIndex 和 writerIndex 都设置为 0（不会清除内存中的内容）。 查找操作（Updating） 123// 使用 ByteBufProcessor 来寻找\\rByteBuf buffer = ...;int index = buffer.forEachByte(ByteBufProcessor.FIND_CR); 派生缓冲区 派生缓冲区为 ByteBuf 提供了以专门的方式来呈现其内容的视图。修改了其中一个，源实例也会被修改（共享）。 12345678910// 对 ByteBuf 进行切片Charset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);ByteBuf sliced = buf.slice(0, 15);// 打印 Netty in ActionSystem.out.println(sliced.toString(utf8));// 更改索引 0 处的字节buf.setByte(0, (byte)&#x27;J&#x27;);// True 因为数据是共享关系assert buf.getByte(0) == sliced.getByte(0); ByteBuf 复制 如果需要一个现有缓冲区的真实副本，请使用 copy() 或者 copy(int, int) 方法。不同于派生缓冲区，由这个调用所返回的 ByteBuf 拥有独立的数据副本 12345678// 复制一个 ByteBufCharset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);ByteBuf copy = buf.copy(0, 15);System.out.println(copy.toString(utf8));buf.setByte(0, (byte) &#x27;J&#x27;);// True 因为数据是非共享关系assert buf.getByte(0) != copy.getByte(0); 除了修改原始 ByteBuf 的切片或者副本的效果以外，这两种场景是相同的。只要有可能， 使用 slice() 方法来避免复制内存的开销。 读写操作 getXxx() 和 setXxx() 操作，从给定的索引开始，并且保持索引不变。 12345678910111213// getXxx() And setXxx()Charset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);// 打印第一个字符 NSystem.out.println((char)buf.getByte(0));int readerIndex = buf.readerIndex();int writerIndex = buf.writerIndex();// 更改索引 0 处的字节buf.setByte(0, (byte)&#x27;B&#x27;);System.out.println((char)buf.getByte(0));// True 不会影响相应的索引assert readerIndex == buf.readerIndex();assert writerIndex == buf.writerIndex(); readXxx() 和 writeXxx() 操作，从给定的索引开始，并且会根据已经访问过的字节数对索 引进行调整。 1234567891011// readXxx() And writeXxx()Charset utf8 = Charset.forName(&quot;UTF-8&quot;);ByteBuf buf = Unpooled.copiedBuffer(&quot;Netty in Action rocks!&quot;, utf8);System.out.println((char)buf.readByte());int readerIndex = buf.readerIndex();int writerIndex = buf.writerIndex();// 将字符 ？ 追加到缓存区，writerIndex 改变buf.writeByte((byte)&#x27;?&#x27;);// Trueassert readerIndex == buf.readerIndex();assert writerIndex != buf.writerIndex(); ByteBufHolder 接口 如果想要实现一个将其有效负载存储在 ByteBuf 中的消息对象，那么 ByteBufHolder 将是个不错的选择。 ByteBuf 分配 常见的几种管理 ByteBuf 实例的不同方式。 按需分配：ByteBufAllocator 接口 为了降低分配和释放内存的开销，Netty 通过 interface ByteBufAllocator 实现了（ByteBuf 的）池化，它可以用来分配我们所描述过的任意类型的 ByteBuf 实例。 可以通过 Channel（每个都可以有一个不同的 ByteBufAllocator 实例）或者绑定到 ChannelHandler 的 ChannelHandlerContext 获取一个到 ByteBufAllocator 的引用。 1234567// 获取一个到 ByteBufAllocator 的引用Channel channel = ...;ByteBufAllocator allocator = channel.alloc();....ChannelHandlerContext ctx = ...;ByteBufAllocator allocator2 = ctx.alloc();... Netty提供了两种ByteBufAllocator的实现：PooledByteBufAllocator 和 UnpooledByteBufAllocator。 Unpooled 缓冲池 可能某些情况下，你不需要一个 ByteBufAllocator 的引用。对于这种情况，Netty 提供了一个简单的称为 Unpooled 的工具类，它提供了静态的辅助方法来创建未池化的 ByteBuf 实例。 ByteBufUtil 类 ByteBufUtil 提供了用于操作 ByteBuf 的静态的辅助方法。 引用计数 引用计数是一种通过在某个对象所持有的资源不再被其他对象引用时释放该对象所持有的资源来优化内存使用和性能的技术。Netty 在第 4 版中为 ByteBuf 和 ByteBufHolder 引入了引用计数技术，它们都实现了 interface ReferenceCounted。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty 传输","slug":"后台技术/Netty/Netty 传输","date":"2020-01-22T11:03:21.000Z","updated":"2023-09-20T07:13:36.591Z","comments":true,"path":"2020/01/22/后台技术/Netty/Netty 传输/","link":"","permalink":"https://wingowen.github.io/2020/01/22/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E4%BC%A0%E8%BE%93/","excerpt":"流经网络的数据总是具有相同的类型：字节。这些字节是如何流动的主要取决于我们所说的网络传输（一个帮助我们抽象底层数据传输机制的概念）","text":"流经网络的数据总是具有相同的类型：字节。这些字节是如何流动的主要取决于我们所说的网络传输（一个帮助我们抽象底层数据传输机制的概念） 案例研究：JDK VS. Netty 原生 JDK 123456789101112131415161718192021222324252627282930313233343536// JDK 的阻塞网络编程public class PlainOioServer &#123; public void serve(int port) throws IOException &#123; final ServerSocket socket = new ServerSocket(port); try &#123; for (;;) &#123; final Socket clientSocket = socket.accept(); System.out.println(&quot;Accepted connection from &quot; + clientSocket); new Thread( // 创建一个新的线程来处理该连接 new Runnable() &#123; @Override public void run() &#123; OutputStream out; try &#123; out = clientSocket.getOutputStream(); // 将消息写给已连接的客户端 out.write(&quot;Hi!\\r\\n&quot;.getBytes(Charset.forName(&quot;UTF-8&quot;))); out.flush(); clientSocket.close(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; clientSocket.close(); &#125;catch (IOException ex) &#123; // ignore on close &#125; &#125; &#125; &#125;).start(); // 启动线程 &#125; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 随着应用程序的用户越来越多，阻塞 I/O 并不能很好地伸缩到支撑成千上万地并发连入连接。此时异步 I/O 可以解决这个问题，但异步 I/O 的 API 是完全不同的，因此不得不重写此应用程序。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// JDK 非阻塞网络处理public class PlainNioServer &#123; public void serve(int port) throws IOException &#123; ServerSocketChannel serverChannel = ServerSocketChannel.open(); serverChannel.configureBlocking(false); ServerSocket ssocket = serverChannel.socket(); InetSocketAddress address = new InetSocketAddress(port); ssocket.bind(address); // 打开 Selector 来处理 Channel Selector selector = Selector.open(); // 将 ServerChannel 注册到 Selector 以便接受连接（报个名） serverChannel.register(selector, SelectionKey.OP_ACCEPT); final ByteBuffer msg = ByteBuffer.wrap(&quot;Hi!\\r\\n&quot;.getBytes()); for (;;) &#123; try &#123; // 等待 select() 需要处理的新事件，阻塞将一直持续到下一个传入事件 selector.select(); &#125; catch (IOException ex) &#123; ex.printStackTrace(); // handle exception break; &#125; // 获取所有接收事件的标识 SelectionKey() Set&lt;SelectionKey&gt; readyKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; // 取出标识，以便标识所对应的事件进行业务操作 SelectionKey key = iterator.next(); // 此事件已经取出处理，将其标识符移除 iterator.remove(); try &#123; // 检查事件类型 if (key.isAcceptable()) &#123; ServerSocketChannel server = (ServerSocketChannel)key.channel(); // 接受客户端 SocketChannel client = server.accept(); client.configureBlocking(false); // 将客户端注册到选择器 client.register(selector, SelectionKey.OP_WRITE | SelectionKey.OP_READ, msg.duplicate()); System.out.println(&quot;Accepted connection from &quot; + client); &#125; // 检查事件类型 if (key.isWritable()) &#123; SocketChannel client = (SocketChannel)key.channel(); ByteBuffer buffer =(ByteBuffer)key.attachment(); while (buffer.hasRemaining()) &#123; // 将数据写到已连接的客户端 if (client.write(buffer) == 0) &#123; break; &#125; &#125; client.close(); &#125; &#125; catch (IOException ex) &#123; key.cancel(); try &#123; key.channel().close(); &#125; catch (IOException cex) &#123; // ignore on close &#125; &#125; &#125; // while &#125; // for &#125; &#125; Netty 框架 因为 Netty 为每种传输的实现都暴露了相同的 API，阻塞与非阻塞网络传输的实现都依赖于 interface Channel、 ChannelPipeline 和 ChannelHandler。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 使用 Netty 的网络处理public class NettyOioServer &#123; public void server(int port) throws Exception &#123; final ByteBuf buf = Unpooled.unreleasableBuffer( Unpooled.copiedBuffer(&quot;Hi!\\r\\n&quot;, Charset.forName(&quot;UTF-8&quot;)) ); // 阻塞模式 EventLoopGroup group = new OioEventLoopGroup(); // 非阻塞模式 // EventLoopGroup group = new OioEventLoopGroup(); try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(group) // 非阻塞模式 // .channel(NioServerSocketChannel.class) .channel(OioServerSocketChannel.class) .localAddress(new InetSocketAddress(port)) // 每个连接的通道都需要调用此初始化方法 .childHandler( new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast( // 拦截和处理事件 new ChannelInboundHandlerAdapter() &#123; @Override public void channelActive(ChannelHandlerContext ctx)throws Exception &#123; ctx.writeAndFlush(buf.duplicate()) // 监听，以便在消息写完后关闭连接 .addListener(ChannelFutureListener.CLOSE); &#125; &#125; ); &#125; &#125; ); ChannelFuture f = b.bind().sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; &#125; 传输 API 传输 API 的核心是 interface Channel，它被用于所有的 I/O 操作 每个 Channel 都将会被分配一个 ChannelPipeline 和 ChannelConfig。ChannelConfig 包含了该 Channel 的所有配置设置，并且支持热更新。 由于 Channel 是独一无二的，为了保证顺序将 Channel 声明为 java.lang. Comparable 的一个子接口。 ChannelPipeline 持有所有将应用于入站和出站数据以及事件的 ChannelHandler 实例，这些 ChannelHandler 实现了应用程序用于处理状态变化以及数据处理的逻辑。你也可以根据需要通过添加或者移除ChannelHandler实例来修改 ChannelPipeline。通过利用Netty的这项能力可以构建出高度灵活的应用程序。 123456789101112131415161718192021// 写出到 ChannelChannel channel = ...;// 创建持有要写数据的 ByteBufByteBuf buf = Unpooled.copiedBuffer(&quot;your data&quot;, CharsetUtil.UTF_8);// 写数据并且冲刷它ChannelFuture cf = channel.writeAndFlush(buf);// 添加监听器以便在操作完成后接收到通知cf.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; // 操作完成并且没有发生错误 if (future.isSuccess()) &#123; System.out.println(&quot;Write successful&quot;); &#125; else &#123; System.err.println(&quot;Write error&quot;); future.cause().printStackTrace(); &#125; &#125; &#125;); Netty 的 Channel 实现是线程安全的，因此你可以存储一个到 Channel 的引用，并且每当你需要向远程节点写数据时，都可以使用它，即使当时许多线程都在使用它。 1234567891011121314151617// 从多个线程使用同一个 Channelfinal Channel channel = ...;final ByteBuf buf = Unpooled.copiedBuffer(&quot;your data&quot;,CharsetUtil.UTF_8).retain();// 创建将数据写到 Channel 的 RunnableRunnable writer = new Runnable() &#123; @Override public void run() &#123; channel.writeAndFlush(buf.duplicate()); &#125;&#125;;// 获取到线程池 Executor 的引用Executor executor = Executors.newCachedThreadPool();// write in one threadexecutor.execute(writer);// write in another threadexecutor.execute(writer);... 内置的传输 Netty 内置了一些可开箱即用的传输。因为并不是它们所有的传输都支持每一种协议，所以你必须选择一个和你的应用程序所使用的协议相容的传输。 NIO NIO 提供了一个所有 I/O 操作的全异步的实现，利用了 JDK1.4 时便可用的基于选择器的 API。 选择器背后的基本概念是充当一个注册表，在那里你将可以请求在 Channel 的状态发生变化时得到通知。 class java.nio.channels.SelectionKey 定义了一组应用程序正在请求通知的状态变化集。 选择器选择并处理状态的变化 零拷贝（zero-copy）：它使你可以快速高效地将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间，其在像 FTP 或者 HTTP 这样的协议中可以显著地提升性能。 Epoll 用于 Linux 的本地非阻塞传输，Linux JDK NIO API 使用了这些 epoll 调用。 OIO Netty 的 OIO 传输实现代表了一种折中：它可以通过常规的传输 API 使用，但是由于它是建立在 java.net 包的阻塞实现之上的，所以它不是异步的。但是，它仍然非常适合于某些用途。 Local 传输 用于在同一个 JVM 中运行的客户端和服务器程序之间的异步通信。 Embedded 传输 Netty 提供了一种额外的传输，使得你可以将一组 ChannelHandler 作为帮助器类嵌入到其他的 ChannelHandler 内部。通过这种方式，你将可以扩展一个 ChannelHandler 的功能，而又不需要修改其内部代码。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty 组件和设计","slug":"后台技术/Netty/Netty 组件和设计","date":"2020-01-22T06:09:03.000Z","updated":"2023-09-20T07:13:45.922Z","comments":true,"path":"2020/01/22/后台技术/Netty/Netty 组件和设计/","link":"","permalink":"https://wingowen.github.io/2020/01/22/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E7%BB%84%E4%BB%B6%E5%92%8C%E8%AE%BE%E8%AE%A1/","excerpt":"Channel、EventLoop 和 ChannelFuture Channel、EventLoop 和 ChannelFuture 类合在一起可以被认为是 Netty 的网络抽象代表","text":"Channel、EventLoop 和 ChannelFuture Channel、EventLoop 和 ChannelFuture 类合在一起可以被认为是 Netty 的网络抽象代表 Channel：Socket EventLoop：控制流、多线程处理、并发 ChannelFuture：异步通知 Channel 接口 基本的 I/O 操作（bind()、connect()、read()和 write()）依赖于底层网络传输所提供的原语。在基于 Java 的网络编程中，其基本的构造是 class Socket。Netty 的 Channel 接口所提供的 API，大大地降低了直接使用 Socket 类的复杂性。此外，Channel 也是拥有许多预定义的、专门化实现的广泛类层次结构的根。 EvenLoop 接口 一个 EventLoopGroup 包含一个或者多个 EventLoop； 一个 EventLoop 在它的生命周期内只和一个 Thread 绑定； 所有由 EventLoop 处理的 I/O 事件都将在它专有的 Thread 上被处理； 一个 Channel 在它的生命周期内只注册于一个 EventLoop； 一个 EventLoop 可能会被分配给一个或多个 Channel。 在这种设计中，一个给定 Channel 的 I/O 操作都是由相同的 Thread 执行的，实际上消除了对于同步的需要。 channelFuture 接口 Netty 中所有的 I/O 操作都是异步的，因为一个操作可能不会立即返回，所以我们需要一种用于在之后的某个时间点确定其结果的方法。为此，Netty 提供了 ChannelFuture 接口，其 addListener() 方法注册了一个 ChannelFutureListener，以便在某个操作完成时（无论是否成功）得到通知。 ChannelHandler 和 ChannelPipeline 接下来，让我们更加细致的看一看哪那些管理数据流以及执行应用程序处理逻辑的组件。 ChannelHandler 接口 从应用程序开发人员的角度来看，Netty 的主要组件是 ChannelHandler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。这是可行的，因为 ChannelHandler 的方法是由网络事件（其中术语“事件”的使用非常广泛）触发的。事实上，ChannelHandler 可专门用于几乎任何类型的动作，例如将数据从一种格式转换为另外一种格式，或者处理转换过程中所抛出的异常。 你的应用程序的业务逻辑通常驻留在一个或者多个 ChannelInboundHandler 中。 ChannelPipeline 接口 ChannelPipeline 提供了 ChannelHandler 链的容器，并定义了用于在该链上传播入站和出站事件流的 API。当 Channel 被创建时，它会被自动地分配到它专属的 ChannelPipeline。 ChannelHandler 安装到 ChannelPipeline 中的过程： 一个ChannelInitializer的实现被注册到了ServerBootstrap中； 当 ChannelInitializer.initChannel() 方法被调用时，ChannelInitializer 将在 ChannelPipeline 中安装一组自定义的 ChannelHandler； ChannelInitializer 将它自己从 ChannelPipeline 中移除。 ChannelHandler 是专为支持广泛的用途而设计的，可以将它看作是处理往来 ChannelPipeline 事件（包括数据）的任何代码的通用容器。使得事件流经 ChannelPipeline 是 ChannelHandler 的工作，它们是在应用程序的初始化或者引导阶段被安装的。这些对象接收事件、执行它们所实现的处理逻辑，并将数据传递给链中的下一个 ChannelHandler。它们的执行顺序是由它们被添加的顺序所决定的。实际上，被我们称为 ChannelPipeline 的是这些 ChannelHandler 的编排顺序。 如同上图所示，入站和出站 ChannelHandler 可以被安装到同一个 ChannelPipeline 中。虽然 ChannelInboundHandle 和 ChannelOutboundHandle 都扩展自 ChannelHandler，但是 Netty 能区分 ChannelInboundHandler 实现和 ChannelOutboundHandler 实现，并确保数据只会在具有相同定向类型的两个 ChannelHandler 之间传递。 当 ChannelHandler 被添加到 ChannelPipeline 时，它将会被分配一个 ChannelHandlerContext，其代表了 ChannelHandler 和 ChannelPipeline 之间的绑定。虽然这个对象可以被用于获取底层的 Channel，但是它主要还是被用于写出站数据。 在 Netty 中，有两种发送消息的方式。你可以直接写到 Channel 中，也可以写到和 ChannelHandler 相关联的 ChannelHandlerContext 对象中。前一种方式将会导致消息从 ChannelPipeline 的尾端开始流动，而后者将导致消息从 ChannelPipeline 中的下一个 ChannelHandler 开始流动。 编码器和解码器 正如有用来简化 ChannelHandler 的创建的适配器类一样，所有由 Netty 提供的编码器/解码器适配器类都实现 了 ChannelOutboundHandler 或者 ChannelInboundHandler 接口。 通常来说，这些基类的名称将类似于 ByteToMessageDecoder 或 MessageToByteEncoder。对于特殊的类型，你可能会发现类似于 ProtobufEncoder 和 ProtobufDecoder 这样的名称——预置的用来支持 Google 的 Protocol Buffers。 抽象类 SimpleChannelInboundHandler 最常见的情况是，你的应用程序会利用一个 ChannelHandler 来接收解码消息，并对该数据应用业务逻辑。要创建一个这样的 ChannelHandler，你只需要扩展基类 SimpleChannelInboundHandler&lt;T&gt;，其中 T 是你要处理的消息的 Java 类型 。在这个 ChannelHandler 中，你将需要重写基类的一个或者多个方法，并且获取一个到 ChannelHandlerContext 的引用， 这个引用将作为输入参数传递给 ChannelHandler 的所有方法。 引导 Netty 的引导类为应用程序的网络层配置提供了容器，这涉及将一个进程绑定到某个指定的端口，或者将一个进程连接到另一个运行在某个指定主机的指定端口上的进程。 “服务器”和“客户端”实际上表示了不同的网络行为；换句话说，是监听传入的连接还是建立到一个或者多个进程的连接。 因此，有两种类型的引导：一种用于客户端（简单地称为 Bootstrap），而另一种（ServerBootstrap）用于服务器。无论你的应用程序使用哪种协议或者处理哪种类型的数据，唯一决定它使用哪种引导类的是它是作为一个客户端还是作为一个服务器。 Bootstrap 将连接远程主机和端口，数量通常为一个。 ServerBootstrap 将绑定到一个端口，因为服务器必须要监听连接，数量通常为两个。 第一组将只包含一个 ServerChannel，代表服务器自身的已绑定到某个本地端口的正在监听的套接字。 第二组将包含所有已创建的用来处理传入客户端连接（对于每个服务器已经接受的连接都有一个）的 Channel。 与 ServerChannel 相关联的 EventLoopGroup 将分配一个负责为传入连接请求创建 Channel 的 EventLoop。一旦连接被接受，第二个 EventLoopGroup 就会给它的 Channel 分配一个 EventLoop。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"一个简单的 Netty 应用程序","slug":"后台技术/Netty/Netty 简单应用程序","date":"2020-01-21T14:46:53.000Z","updated":"2023-09-20T07:13:38.772Z","comments":true,"path":"2020/01/21/后台技术/Netty/Netty 简单应用程序/","link":"","permalink":"https://wingowen.github.io/2020/01/21/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","excerpt":"应用程序 Echo Echo 客户端和服务器之间的交互是非常简单的；在客户端建立一个连接之后，它会向服务器发送一个或多个消息，反过来，服务器又会将每个消息回送给客户端。这一简单的功能充分体现了客户端/服务器系统中典型的请求-响应交互模式。","text":"应用程序 Echo Echo 客户端和服务器之间的交互是非常简单的；在客户端建立一个连接之后，它会向服务器发送一个或多个消息，反过来，服务器又会将每个消息回送给客户端。这一简单的功能充分体现了客户端/服务器系统中典型的请求-响应交互模式。 编写 Echo 服务器 所有的 Netty 服务器都需要以下两部分 至少一个 ChannelHandler：该组件实现了服务器对从客户端接收的数据的处理，即它的业务逻辑。 引导：这是配置服务器的启动代码。至少，它会将服务器绑定到它要监听连接请求的端口上。 ChannelHandler 和业务逻辑 因为你的 Echo 服务器会响应传入的消息，所以它需要实现 ChannelInboundHandler 接口，用来定义响应入站事件的方法。这个简单的应用程序只需要用到少量的这些方法，所以继承 ChannelInboundHandlerAdapter 类也就足够了，它提供了 ChannelInboundHandler 的默认实现。 123456789101112131415161718192021222324// EchoServerHandler 类@Sharable // 标示一个 ChannelHandler 可以被多个 Channel 安全地共享public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; // 对于每个传入的消息都要调用 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; ByteBuf in = (ByteBuf) msg; System.out.println(&quot;Server received: &quot; + in.toString(CharsetUtil.UTF_8)); // 将接收到的数据写给发送者 ctx.write(in); &#125; // 通知 ChannelInboundHandler 最后一次对 channelRead() 的调用是当前批量读取中的最后一条消息 @Override public void channelReadComplete(ChannelHandlerContext ctx) &#123; // 将消息冲刷到远程节点并且关闭该 Channel ctx.writeAndFlush(Unpooled.EMPTY_BUFFER).addListener(ChannelFutureListener.CLOSE); &#125; // 在读取操作期间，有异常抛出时会调用 @Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; ChannelHandler 的一些关键点： 针对不同类型的事件来调用 ChannelHandler； 应用程序通过实现或者扩展 ChannelHandler 来挂钩到事件的生命周期，并且提供自定义的应用程序逻辑； 在架构上，ChannelHandler 有助于保持业务逻辑与网络处理代码的分离。这简化了开发过程，因为代码必须不断地演化以响应不断变化的需求。 引导服务器 绑定到服务器将在其上监听并接受传入连接请求的端口； 配置 Channel，以将有关的入站消息通知给 EchoServerHandler 实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// EchoServer 类public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.println(&quot;Usage: &quot; + EchoServer.class.getSimpleName() + &quot; &lt;port&gt;&quot;); &#125; int port = Integer.parseInt(args[0]); new EchoServer(port).start(); &#125; public void start() throws Exception &#123; final EchoServerHandler serverHandler = new EchoServerHandler(); // 创建一个循环组 EventLoopGroup EventLoopGroup group = new NioEventLoopGroup(); try &#123; // 创建一个 ServerBootstrap 的实例以引导和绑定服务器 ServerBootstrap b = new ServerBootstrap(); b.group(group) .channel(NioServerSocketChannel.class) // 指定服务器绑定的本地的 InetSocketAddress .localAddress(new InetSocketAddress(port)) .childHandler( // 接受一个新连接则一个新的子 Channel 将会被创建 new ChannelInitializer&lt;SocketChannel&gt;()&#123; // 使用一个 EchoServerHandler 的实例初始化每一个新的 Channel @Override public void initChannel(SocketChannel ch)throws Exception &#123; ch.pipeline().addLast(serverHandler); &#125; &#125; ); // 异步地绑定服务器，sync() 方法的调用将导致当前 Thread 阻塞，一直到绑定操作完成为止 ChannelFuture f = b.bind().sync(); // 异步地关闭服务器 f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; &#125; 编写 Echo 客户端 Echo 客户端将会： 连接到服务器； 发送一个或者多个消息； 对于每个消息，等待并接受从服务器发送回的相同消息 关闭连接。 通过 ChannelHandler 实现客户端逻辑 如同服务器，客户端将拥有一个用来处理数据的 ChannelInboundHandler。 123456789101112131415161718192021// 客户端的 ChannelHandler// ByteBuf 为 Nutty 的字节容器@Sharablepublic class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 在到服务器的连接已经建立之后将被调用 @Override public void channelActive(ChannelHandlerContext ctx) &#123; ctx.writeAndFlush(Unpooled.copiedBuffer(&quot;Netty rocks!&quot;,CharsetUtil.UTF_8)); &#125; // 当从服务器接收到一条消息时被调用 @Override public void channelRead0(ChannelHandlerContext ctx, ByteBuf in) &#123; System.out.println(&quot;Client received: &quot; + in.toString(CharsetUtil.UTF_8)); &#125; // 在处理过程中引发异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; SimpleChannelInboundHandler 与 ChannelInboundHandler 你可能会想：为什么我们在客户端使用的是 SimpleChannelInboundHandler，而不是在 EchoServerHandler 中所使用的 ChannelInboundHandlerAdapter 呢？这和两个因素的相互作用有关：业务逻辑如何处理消息以及 Netty 如何管理资源。 在客户端，当 channelRead0() 方法完成时，你已经有了传入消息，并且已经处理完它了。当该方法返回时，SimpleChannelInboundHandler 负责释放指向保存该消息的 ByteBuf 的内存引用。 在 EchoServerHandler 中，你仍然需要将传入消息回送给发送者，而 write()操作是异步的，直到 channelRead() 方法返回后可能仍然没有完成。为此，EchoServerHandler 扩展了 ChannelInboundHandlerAdapter，其在这个时间点上不会释放消息。 消息在 EchoServerHandler 的 channelReadComplete() 方法中，当 writeAndFlush() 方法被调用时被释放 引导客户端 引导客户端类似于引导服务器，不同的是，客户端是使用主机和端口参数来连接远程地址，也就是这里的 Echo 服务器的地址，而不是绑定到一个一直被监听的端口。 12345678910111213141516171819202122232425262728293031323334353637383940// 客户端的主类public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .remoteAddress(new InetSocketAddress(host, port)) .handler( new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler()); &#125; &#125; ); ChannelFuture f = b.connect().sync(); f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println(&quot;Usage: &quot; + EchoClient.class.getSimpleName() + &quot; &lt;host&gt; &lt;port&gt;&quot;); return; &#125; String host = args[0]; int port = Integer.parseInt(args[1]); new EchoClient(host, port).start(); &#125; &#125;","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]},{"title":"Netty 入门简介","slug":"后台技术/Netty/Netty 入门简介","date":"2020-01-21T01:37:11.000Z","updated":"2023-09-20T07:13:41.071Z","comments":true,"path":"2020/01/21/后台技术/Netty/Netty 入门简介/","link":"","permalink":"https://wingowen.github.io/2020/01/21/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/Netty/Netty%20%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B/","excerpt":"Java IO 无论是 C 语言，还是 Java，在进行网络编程的开发时都较为不友好。早期的 Java API（ java.net ）只支持由本地系统套接字库提供所谓的阻塞函数。","text":"Java IO 无论是 C 语言，还是 Java，在进行网络编程的开发时都较为不友好。早期的 Java API（ java.net ）只支持由本地系统套接字库提供所谓的阻塞函数。 12345678910111213141516171819202122// 阻塞 I/O 示例I/O// 创建一个 ServerSocket 用以监听端口上的连接请求ServerSocket serverSocket = new ServerSocket(portNumber);// accept() 方法调用将被阻塞，直到一个连接建立Socket clientSocket = serverSocket.accept();BufferReader in = new BufferReader( new InputStreamReader( clientSocket.getInputStream() ));// PrintWriter(OutputStream out, boolean autoFlush) ???PrintWriter out = new PrintWriter(clientSocket.getOutputStream(), true);String request, response;while ((request = in.readLine()) != null) &#123; if (&quot;Done&quot;.equals(request)) &#123; break; // 客户端发送了 Done 则退出处理循环 &#125; // You can use ProcessRequest to handle your request response = processRequest(request); out.println(response);&#125; 这段代码片段将只能同时处理一个连接，要管理多个并发客户端，需要为每个新的客户端 Socket 创建一个新的 Thread。 使用阻塞 I/O处理多个连接： Java NIO class java.nio.channels.Selector 是 Java 的非阻塞 I/O 实现的关键。它使用了事件通知 API 以确定在一组非阻塞套接字中有哪些已经就绪能够进 行 I/O 相关的操作。因为可以在任何的时间检查任意 的读操作或者写操作的完成状态。 使用 Selector 的非阻塞 I/O Netty 简介 在网络编程领域，Netty 是 Java 的卓越框架。它驾驭了 Java 高 API 的能力，并将其隐藏在一个易于使用的 API 之后。Netty 使你可以专注于自己真正感兴趣的：你的应用程序的独一无二的价值。 一个既是异步的又是事件驱动的系统会表现出一种特殊的、对我们来说极具价值的行为：它可以以任意的顺序响应在任意的时间点产生的事件。 完全异步的 I/O：非阻塞网络调用使得我们可以不必等待一个操作的完成。异步方法会立即返回，并且在它完成时，会直接或者在稍后的某个时间点通知用户。 选择器使得我们能够通过较少的线程便可监视许多连接上的事件。 Netty 的核心组件 Channel Channel 时 Java NIO 的一个基本构造。 它代表一个到实体（如一个硬件设备、一个文件、一个网络套接字或者一个能够执 行一个或者多个不同的I/O操作的程序组件）的开放连接，如读操作和写操作。 在一定程度上可以把 Channel 看作是传入（入站）或者传出（出站）数据的载体。因此，它可以被打开或者被关闭，连接或者断开连接。 回调 Netty 在内部使用了回调来处理事件；当一个回调被触发时，相关的事件可以被一个 interfaceChannelHandler 的实现处理 12345678910// 被回调触发的 ChannelHandlerpublic class ConnectHandler extends ChannelInboundHandlerAdapter &#123; @Override // 当一个新的连接被建立完成时，此方法将会被调用 public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println( &quot;Client &quot; + ctx.channel().remoteAddress() + &quot; connected&quot; ); &#125;&#125; Future Future 提供了另一种在操作完成时通知应用程序的方式。这个对象可以看作是一个异步操作的结果的占位符；它将在未来的某个时刻完成，并提供对其结果的访问。 JDK 预置了 interface java.util.concurrent.Future，但是其所提供的实现，只允许手动检查对应的操作是否已经完成，或者一直阻塞直到它完成。这是非常繁琐的，所以 Netty 提供了它自己的实现：ChannelFuture，用于在执行异步操作的时候使用。 ChannelFuture提供了几种额外的方法，这些方法使得我们能够注册一个或者多个 ChannelFutureListener 实例。由 ChannelFutureListener 提供的通知机制消除了手动检查对应的操作是否完成的必要。 监听器的回调方法 operationComplete()：将会在对应的操作完成时被调用。 监听器可以判断操作是成功地完成了还是出错了（ 检索产生的 Throwable ）。 每个 Netty 的出站 I/O 操作都将返回一个 ChannelFuture；也就是说，它们都不会阻塞。正如我们前面所提到过的一样，Netty 完全是异步和事件驱动的。 12345678910111213141516171819202122232425262728// 异步的建立连接Channel channel = ...;// Does not blockChannelFuture future = channel.connect( // 异步地连接到远程节点 new InetSocketAddress(&quot;192.168.0.1&quot;, 25));// 注册一个 ChannelFutureListener 以便在操作完成时获得通知future.addListener( new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; // 检查操作的状态 if (future.isSuccess())&#123; ByteBuf buffer = Unpooled.copiedBuffer( &quot;Hello&quot;,Charset.defaultCharset() ); // 将数据异步发送到远程节点 ChannelFuture wf = future.channel().writeAndFlush(buffer); .... &#125; else &#123; Throwable cause = future.cause(); cause.printStackTrace(); &#125; &#125; &#125;); 回调和 Future 是相互补充的机制；它们相互结合，构成了 Netty 本身的关键构件块之一。 事件和 ChannelHandler Netty 是一个网络编程框架，所以事件是按照它们与入站或出站数据流的相关性进行分类的。 可能由入站数据或者相关的状态更改而触发的事件： 连接已被激活或者连接失活； 数据读取； 用户事件； 错误事件。 出站事件是未来将会触发的某个动作的操作结果，这些动作包括： 打开或者关闭到远程节点的连接； 将数据写到或者冲刷到套接字。 每个事件都可以被分发给 ChannelHandler 类中的某个用户实现的方法，后面会对此类进行更进一步的说明，目前你可以每个 ChannelHandler 的实例都类似于一种为了响应特定事件而被执行的回调。 流经 ChannelHandler 链的入站事件和出站事件 各组件的整合 Future、回调和 ChannelHandler Netty 的异步编程模型是建立在 Future 和回调的概念之上的，而将事件派发到 ChannelHandler 的方法则发生在更深的层次上。结合在一起，这些元素就提供了一个处理环境，使你的应用程序逻 辑可以独立于任何网络操作相关的顾虑而独立地演变。这也是 Netty 的设计方式的一个关键目标。 拦截操作以及高速地转换入站数据和出站数据，都只需要你提供回调或者利用操作所返回的 Future。这使得链接操作变得既简单又高效，并且促进了可重用的通用代码的编写。 选择器、事件和 EventLoop Netty 通过触发事件将 Selector 从应用程序中抽象出来，消除了所有本来将需要手动编写 的派发代码。在内部，将会为每个 Channel 分配一个 EventLoop，用以处理所有事件。 EventLoop 本身只由一个线程驱动，其处理了一个 Channel 的所有 I/O 事件，并且在该 EventLoop 的整个生命周期内都不会改变（ 无需顾虑同步 ）。","categories":[{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"}]}],"categories":[{"name":"杂项","slug":"杂项","permalink":"https://wingowen.github.io/categories/%E6%9D%82%E9%A1%B9/"},{"name":"数据库","slug":"数据库","permalink":"https://wingowen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据","slug":"大数据","permalink":"https://wingowen.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Python","slug":"Python","permalink":"https://wingowen.github.io/categories/Python/"},{"name":"基础科学","slug":"基础科学","permalink":"https://wingowen.github.io/categories/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/"},{"name":"运维","slug":"运维","permalink":"https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"计算机科学","slug":"计算机科学","permalink":"https://wingowen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"},{"name":"机器学习","slug":"机器学习","permalink":"https://wingowen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"后台技术","slug":"后台技术","permalink":"https://wingowen.github.io/categories/%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"InooDB","slug":"InooDB","permalink":"https://wingowen.github.io/tags/InooDB/"},{"name":"Hive","slug":"Hive","permalink":"https://wingowen.github.io/tags/Hive/"},{"name":"Kudu","slug":"Kudu","permalink":"https://wingowen.github.io/tags/Kudu/"},{"name":"Impala","slug":"Impala","permalink":"https://wingowen.github.io/tags/Impala/"},{"name":"Pandas","slug":"Pandas","permalink":"https://wingowen.github.io/tags/Pandas/"},{"name":"SQL","slug":"SQL","permalink":"https://wingowen.github.io/tags/SQL/"},{"name":"考研","slug":"考研","permalink":"https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"},{"name":"脚本命令","slug":"脚本命令","permalink":"https://wingowen.github.io/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"name":"网络","slug":"网络","permalink":"https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C/"},{"name":"部署","slug":"部署","permalink":"https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"},{"name":"算法","slug":"算法","permalink":"https://wingowen.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"gRPC","slug":"gRPC","permalink":"https://wingowen.github.io/tags/gRPC/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wingowen.github.io/tags/Spring-Boot/"},{"name":"Flume","slug":"Flume","permalink":"https://wingowen.github.io/tags/Flume/"},{"name":"Kafka","slug":"Kafka","permalink":"https://wingowen.github.io/tags/Kafka/"},{"name":"HBase","slug":"HBase","permalink":"https://wingowen.github.io/tags/HBase/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://wingowen.github.io/tags/Hadoop/"},{"name":"sklearn","slug":"sklearn","permalink":"https://wingowen.github.io/tags/sklearn/"},{"name":"MQ","slug":"MQ","permalink":"https://wingowen.github.io/tags/MQ/"},{"name":"Dubbo","slug":"Dubbo","permalink":"https://wingowen.github.io/tags/Dubbo/"},{"name":"Aysnc","slug":"Aysnc","permalink":"https://wingowen.github.io/tags/Aysnc/"},{"name":"Scheduled","slug":"Scheduled","permalink":"https://wingowen.github.io/tags/Scheduled/"},{"name":"Email","slug":"Email","permalink":"https://wingowen.github.io/tags/Email/"},{"name":"Security","slug":"Security","permalink":"https://wingowen.github.io/tags/Security/"},{"name":"Shiro","slug":"Shiro","permalink":"https://wingowen.github.io/tags/Shiro/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://wingowen.github.io/tags/ElasticSearch/"},{"name":"Cache","slug":"Cache","permalink":"https://wingowen.github.io/tags/Cache/"},{"name":"Java","slug":"Java","permalink":"https://wingowen.github.io/tags/Java/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wingowen.github.io/tags/MyBatis/"},{"name":"JPA","slug":"JPA","permalink":"https://wingowen.github.io/tags/JPA/"},{"name":"Druid","slug":"Druid","permalink":"https://wingowen.github.io/tags/Druid/"},{"name":"JDBC","slug":"JDBC","permalink":"https://wingowen.github.io/tags/JDBC/"},{"name":"Web","slug":"Web","permalink":"https://wingowen.github.io/tags/Web/"},{"name":"Log","slug":"Log","permalink":"https://wingowen.github.io/tags/Log/"},{"name":"Netty","slug":"Netty","permalink":"https://wingowen.github.io/tags/Netty/"},{"name":"WebSocket","slug":"WebSocket","permalink":"https://wingowen.github.io/tags/WebSocket/"}]}