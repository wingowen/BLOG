<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WINGO&#39;S BLOG</title>
  
  
  <link href="https://wingowen.github.io/atom.xml" rel="self"/>
  
  <link href="https://wingowen.github.io/"/>
  <updated>2022-07-29T23:58:50.538Z</updated>
  <id>https://wingowen.github.io/</id>
  
  <author>
    <name>Wingo Wen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>模型评估与选择</title>
    <link href="https://wingowen.github.io/2022/07/29/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>https://wingowen.github.io/2022/07/29/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2022-07-29T10:24:58.000Z</published>
    <updated>2022-07-29T23:58:50.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="错误率和精度"><a href="#错误率和精度" class="headerlink" title="错误率和精度"></a>错误率和精度</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实的数据标签</span></span><br><span class="line">real_label = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>])                                         \                              \   \</span><br><span class="line"><span class="comment"># 分类器的预测标签</span></span><br><span class="line">classifier_pred = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">compare_result = (real_label == classifier_pred)</span><br><span class="line">compare_result = compare_result.astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># m 为样本数量 b 为预测错误样本</span></span><br><span class="line">m = <span class="built_in">len</span>(real)</span><br><span class="line">b = m - np.<span class="built_in">sum</span>(cmp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误率</span></span><br><span class="line">error_rate = (b / m)*<span class="number">100</span></span><br><span class="line"><span class="comment"># 精确度 acc</span></span><br><span class="line">accuracy = (<span class="number">1</span> - b / m)*<span class="number">100</span></span><br></pre></td></tr></table></figure><h1 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h1><p>模型在训练样本上的误差称为<strong>训练误差</strong>或<strong>经验误差</strong>；模型在新样本上的误差称为<strong>泛化误差。</strong></p><p>过拟合模型：虽然训练误差接近 0，泛化误差非常大。</p><p>欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。</p><h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。</p><ul><li>偏差 bias，与真实值的<strong>偏离程度</strong>；</li><li>方差 variance，该随机变量在其期望值附近的<strong>波动程度</strong>。</li></ul><p><img src="/IMAGE/模型评估与选择/Untitled.png" alt="Untitled"></p><h1 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h1><p><strong>评估：对学习器的泛化误差进行评估并进而做出选择。</strong></p><h2 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h2><p>以一定比例划分训练集和测试集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 导入包</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"># 加载数据集</span><br><span class="line">def load_pts(): </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    return: 返回随机生成 200 个点的坐标</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    dots = 200  # 样本数</span><br><span class="line">    dim = 2 # 数据维度</span><br><span class="line">    X = np.random.randn(dots,dim) # 建立数据集，shape(200,2)</span><br><span class="line">    # 建立样本 X 的类别</span><br><span class="line">    Y = np.zeros(dots, dtype=&#x27;int&#x27;)      </span><br><span class="line">    for i in range(X.shape[0]):</span><br><span class="line">            Y[i] = 1           </span><br><span class="line">    return X, Y</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">X,Y = load_pts()</span><br><span class="line"></span><br><span class="line"># 使用train_test_split划分训练集和测试集</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0)</span><br></pre></td></tr></table></figure><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。</p><p><img src="/IMAGE/模型评估与选择/Untitled 1.png" alt="Untitled"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集，随机生成40个点</span></span><br><span class="line">data = np.random.randn(<span class="number">40</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证法</span></span><br><span class="line">kf = KFold(n_splits = <span class="number">4</span>, shuffle = <span class="literal">False</span>, random_state = <span class="literal">None</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(data):</span><br><span class="line">    <span class="built_in">print</span>(train)</span><br><span class="line">    <span class="built_in">print</span>(test,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h2><p>有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ：</p><ul><li>每次随机从 D 中挑选一个样本；</li><li>将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中；</li><li>重复执行 m 次该过程；</li><li>最后得到包含 m 个样本数据集 D’。</li></ul><p>由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任意设置一个数据集</span></span><br><span class="line">X = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">23</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">45</span>,<span class="number">67</span>,<span class="number">89</span>,<span class="number">34</span>,<span class="number">54</span>,<span class="number">76</span>,<span class="number">98</span>,<span class="number">43</span>,<span class="number">52</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过产生的随机数获得抽取样本的序号 </span></span><br><span class="line">bootstrapping = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">    bootstrapping.append(np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(X),(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过序号获得原始数据集中的数据</span></span><br><span class="line">D_1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">int</span>(bootstrapping[i]))</span><br><span class="line">    D_1.append(X[<span class="built_in">int</span>(bootstrapping[i])])</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(D_1)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="table-container"><table><thead><tr><th></th><th>采样方法</th><th>与原始数据集的分布是否相同</th><th>相比原始数据集的容量</th><th>是否适用小数据集</th><th>是否适用大数据集</th><th>是否存在估计偏差</th></tr></thead><tbody><tr><td>留出法</td><td>分层抽样</td><td>否</td><td>变小</td><td>否</td><td>是</td><td>是</td></tr><tr><td>交叉验证法</td><td>分层抽样</td><td>否</td><td>变小</td><td>否</td><td>是</td><td>是</td></tr><tr><td>自助法</td><td>放回抽样</td><td>否</td><td>不变</td><td>是</td><td>否</td><td>是</td></tr></tbody></table></div><h1 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h1><p>性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。</p><p>性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。</p><p>回归任务常用性能度量：MSE mean square error，均方差。</p><p>分类任务常用性能度量：acc accuracy，精度；错误率</p><h2 id="二分类-查准率、查全率、F1"><a href="#二分类-查准率、查全率、F1" class="headerlink" title="二分类 - 查准率、查全率、F1"></a>二分类 - 查准率、查全率、F1</h2><p>对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为：</p><ul><li>真正例 true positive</li><li>假正例 false positive</li><li>真反例 true negative</li><li>假反例 false negative</li></ul><p><img src="/IMAGE/模型评估与选择/Untitled 2.png" alt="Untitled"></p><script type="math/tex; mode=display">P(\text { Precision })=\frac{T P}{T P+F P} \\R(\text { Recall })=\frac{T P}{T P+F N}</script><p>Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。</p><p>Precision，查准率：在预测为正的样本中，实际为正的概率。</p><p>P-R 曲线，BRP，Break Even Point：平衡单 P = R。</p><p><img src="/IMAGE/模型评估与选择/Untitled 3.png" alt="Untitled"></p><p>由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。<br>BRP 还是过于简单，更常用的是 F1 度量。</p><script type="math/tex; mode=display">F 1=\frac{2 \times P \times R}{P+R}=\frac{2 T P}{n+T P-T N}</script><p>F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。</p><p>当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。</p><script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script><p>β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。</p><p>如果使用了类似交叉验证法，我们会得到多个 confusion matrix：</p><ul><li>宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1；</li><li>微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">random_state=<span class="number">2021</span></span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果</span></span><br><span class="line"><span class="string">            Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    noise_rate = <span class="number">0.1</span> <span class="comment"># 噪声比例</span></span><br><span class="line">    sample_num = <span class="number">4096</span>  <span class="comment"># 总样本数</span></span><br><span class="line">    noise_sample_num = <span class="built_in">int</span>(sample_num*noise_rate) <span class="comment"># 噪声样本数</span></span><br><span class="line">    np.random.seed(random_state)</span><br><span class="line">    Pred_Score = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,sample_num)</span><br><span class="line">    GT_label = (Pred_Score&gt;<span class="number">0.5</span>).astype(np.<span class="built_in">int</span>)</span><br><span class="line">    noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=<span class="literal">False</span>, p=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> noise_ids:</span><br><span class="line">        GT_label[index] = <span class="number">1</span> <span class="keyword">if</span> GT_label[index] == <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> GT_label, Pred_Score</span><br><span class="line"></span><br><span class="line">GT_label, Pred_Score = generate_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请你补全以下代码，计算查准率与查全率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_PR</span>(<span class="params">GT_label, Pred_Score, threshold, random_state=<span class="number">2021</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算错误率和精度</span></span><br><span class="line"><span class="string">    :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果</span></span><br><span class="line"><span class="string">    :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    :threshold: 评估阈值</span></span><br><span class="line"><span class="string">    :random_state: 随机种子</span></span><br><span class="line"><span class="string">    :返回值: P: 查准率</span></span><br><span class="line"><span class="string">            R: 查全率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Pred_Label = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>, Pred_Score))</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line">    P = precision_score(GT_label, Pred_Label)</span><br><span class="line">    R = recall_score(GT_label, Pred_Label)</span><br><span class="line">    <span class="string">&quot;&quot;&quot; TODO &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> P, R</span><br><span class="line">    </span><br><span class="line">P, R = get_PR(GT_label, Pred_Score, <span class="number">0.55</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查准率P ：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(P))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查全率R ：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(R))</span><br></pre></td></tr></table></figure><h2 id="ROC-与-AUC-原理"><a href="#ROC-与-AUC-原理" class="headerlink" title="ROC 与 AUC 原理"></a>ROC 与 AUC 原理</h2><p>ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。</p><script type="math/tex; mode=display">\begin{aligned}T P R(\text { Precision }) &=\frac{T P}{T P+F N} \\F P R(\text { Precision }) &=\frac{F P}{F P+T N}\end{aligned}</script><p>TPR 真正率，真正样本与实际为正的样本的比率；</p><p>FPR 假正率，加正样本与实际为负的样本的比率。</p><p>若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两  个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC  Area Under ROC Curve。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_pts</span>(): </span><br><span class="line">    dots = <span class="number">200</span>  <span class="comment"># 点数</span></span><br><span class="line">    X = np.random.randn(dots,<span class="number">2</span>) * <span class="number">15</span>  <span class="comment"># 建立数据集，shape(200,2)，坐标放大15倍</span></span><br><span class="line">    <span class="comment"># 建立 X 的类别</span></span><br><span class="line">    y = np.zeros(dots, dtype=<span class="string">&#x27;int&#x27;</span>)      </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> X[i,<span class="number">0</span>] &gt; -<span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">0</span>] &lt; <span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">1</span>] &gt; -<span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">1</span>] &lt; <span class="number">15</span>:  <span class="comment"># 矩形框内的样本都是目标类（正例）</span></span><br><span class="line">            y[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> == np.random.randint(i+<span class="number">1</span>) % <span class="number">10</span>:  <span class="comment"># 对数据随机地插入错误，20 个左右</span></span><br><span class="line">            y[i] = <span class="number">1</span> - y[i]  </span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 数据集可视化          </span></span><br><span class="line">    plt.scatter(X[np.argwhere(y==<span class="number">0</span>).flatten(),<span class="number">0</span>], X[np.argwhere(y==<span class="number">0</span>).flatten(),<span class="number">1</span>],s = <span class="number">20</span>, color = <span class="string">&#x27;blue&#x27;</span>, edgecolor = <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.scatter(X[np.argwhere(y==<span class="number">1</span>).flatten(),<span class="number">0</span>], X[np.argwhere(y==<span class="number">1</span>).flatten(),<span class="number">1</span>],s = <span class="number">20</span>, color = <span class="string">&#x27;red&#x27;</span>, edgecolor = <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.xlim(-<span class="number">40</span>,<span class="number">40</span>)</span><br><span class="line">    plt.ylim(-<span class="number">40</span>,<span class="number">40</span>)</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    plt.tick_params(</span><br><span class="line">    axis=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">    which=<span class="string">&#x27;both&#x27;</span>,</span><br><span class="line">    bottom=<span class="literal">False</span>,</span><br><span class="line">    top=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">X, y = load_pts()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">### 训练模型 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集拆分成训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型 </span></span><br><span class="line">clf1 = DecisionTreeClassifier(max_depth=<span class="number">5</span>, min_samples_leaf=<span class="number">4</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">clf2 = GradientBoostingClassifier(max_depth=<span class="number">8</span>, min_samples_leaf=<span class="number">10</span>, min_samples_split=<span class="number">10</span>)</span><br><span class="line">clf3 = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">0.001</span>, probability=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">clf1.fit(X_train, y_train)</span><br><span class="line">clf2.fit(X_train, y_train)</span><br><span class="line">clf3.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 评估模型 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_score1 = clf1.predict_proba(X_test)</span><br><span class="line">y_score2 = clf2.predict_proba(X_test)</span><br><span class="line">y_score3 = clf3.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得 FPR、TPR 值</span></span><br><span class="line">fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,<span class="number">1</span>])</span><br><span class="line">fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,<span class="number">1</span>])</span><br><span class="line">fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">### 绘制 ROC 曲线 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 ROC 函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_roc_curve</span>(<span class="params">fpr, tpr, c, name</span>):</span><br><span class="line">    lw = <span class="number">2</span></span><br><span class="line">    roc_auc = auc(fpr,tpr)</span><br><span class="line">    plt.plot(fpr, tpr, color=c,lw=lw, </span><br><span class="line">             label= name +<span class="string">&#x27; (area = %0.2f)&#x27;</span> % roc_auc)</span><br><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], color=<span class="string">&#x27;navy&#x27;</span>, lw=lw, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">1.0</span>])</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1.05</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">    <span class="comment">#plt.title(&#x27;&#x27;)</span></span><br><span class="line">    plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">    </span><br><span class="line">plot_roc_curve(fpr1, tpr1, <span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;DecisionTreeClassifier &#x27;</span>)   </span><br><span class="line">plot_roc_curve(fpr2, tpr2, <span class="string">&#x27;navy&#x27;</span>,<span class="string">&#x27;GradientBoostingClassifier &#x27;</span>)   </span><br><span class="line">plot_roc_curve(fpr3, tpr3, <span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;SVC &#x27;</span>) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="比较检验（TODO）"><a href="#比较检验（TODO）" class="headerlink" title="比较检验（TODO）"></a>比较检验（TODO）</h1><p>模型性能比较的重要因素：</p><ul><li>实验评估得到的性能不等于泛化性能；</li><li>测试集上的性能与测试集本身的选择有很大关系；</li><li>很多机器学习算法本身有一定的随机性。</li></ul><p>统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。</p><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><p>由样本推测总体的方法。</p><h3 id="交叉验证-t-检验"><a href="#交叉验证-t-检验" class="headerlink" title="交叉验证 t 检验"></a>交叉验证 t 检验</h3><h3 id="McNemar-检验"><a href="#McNemar-检验" class="headerlink" title="McNemar 检验"></a>McNemar 检验</h3><h3 id="Friedman-检验与-Nemenyi-后续检验"><a href="#Friedman-检验与-Nemenyi-后续检验" class="headerlink" title="Friedman 检验与 Nemenyi 后续检验"></a>Friedman 检验与 Nemenyi 后续检验</h3><h1 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h1><p>在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。</p><p>如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。</p><p>解决不平衡分类问题的策略可以分为两大类：</p><ul><li>从数据层面入手 , 通过改变训练集样本分布降低不平衡程度；</li><li>从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。</li></ul><h2 id="数据层面解决类别不平衡"><a href="#数据层面解决类别不平衡" class="headerlink" title="数据层面解决类别不平衡"></a>数据层面解决类别不平衡</h2><p><strong>扩大数据样本</strong>。</p><p><strong>重采样</strong>：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。</p><ul><li><p>过采样：复制稀有样本；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> RandomOverSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">5000</span>, n_features=<span class="number">2</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">                           n_redundant=<span class="number">0</span>, n_repeated=<span class="number">0</span>, n_classes=<span class="number">3</span>,</span><br><span class="line">                           n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">                           weights=[<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.94</span>],</span><br><span class="line">                           class_sep=<span class="number">0.8</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(Counter(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过采样</span></span><br><span class="line">ros = RandomOverSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = ros.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印过采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成新的稀有样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过采样</span></span><br><span class="line">sm = SMOTE(random_state=<span class="number">42</span>)</span><br><span class="line">X_res, y_res = sm.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印过采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Resampled dataset shape %s&#x27;</span> % Counter(y_res))</span><br></pre></td></tr></table></figure></li><li><p>欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> RandomUnderSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 欠采样</span></span><br><span class="line">rus = RandomUnderSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = rus.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印欠采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br></pre></td></tr></table></figure></li><li><p>过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。</p><p>因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.combine <span class="keyword">import</span> SMOTEENN</span><br><span class="line"><span class="comment"># 过采样与欠采样结合</span></span><br><span class="line">smote_enn = SMOTEENN(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = smote_enn.fit_resample(X, y)</span><br><span class="line"><span class="comment"># 打印采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h2 id="算法层面解决类别不平衡"><a href="#算法层面解决类别不平衡" class="headerlink" title="算法层面解决类别不平衡"></a>算法层面解决类别不平衡</h2><p><strong>惩罚项方法</strong>：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，<strong>错分稀有类的样本需要付出更大的代价</strong>。</p><p>通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。</p><p>例如，调整 SVM 以惩罚稀有类别的错误分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LABEL 0 4000</span></span><br><span class="line"><span class="comment"># LABEL 1 200</span></span><br><span class="line"><span class="comment"># 导入相关包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加惩罚项</span></span><br><span class="line">clf = SVC(C=<span class="number">0.8</span>, probability=<span class="literal">True</span>, class_weight=&#123;<span class="number">0</span>:<span class="number">0.25</span>, <span class="number">1</span>:<span class="number">0.75</span>&#125;)</span><br></pre></td></tr></table></figure><p><strong><strong>特征选择方法</strong></strong></p><p>样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。</p><p>选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;错误率和精度&quot;&gt;&lt;a href=&quot;#错误率和精度&quot; class=&quot;headerlink&quot; title=&quot;错误率和精度&quot;&gt;&lt;/a&gt;错误率和精度&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gu</summary>
      
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯算法</title>
    <link href="https://wingowen.github.io/2022/07/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>https://wingowen.github.io/2022/07/29/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</id>
    <published>2022-07-29T10:24:58.000Z</published>
    <updated>2022-07-29T10:33:34.976Z</updated>
    
    <content type="html"><![CDATA[<h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><p>P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(A \cap B)}{P(B)}</script><p>P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率：</p><script type="math/tex; mode=display">P(B \mid A)=\frac{P(A \cap B)}{P(A)}</script><p>那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(B \mid A) \times P(A)}{P(B)}{\scriptsize }</script><h1 id="贝叶斯理论"><a href="#贝叶斯理论" class="headerlink" title="贝叶斯理论"></a>贝叶斯理论</h1><p><strong>基础思想：</strong></p><ul><li>已知类条件概率密度参数表达式和先验概率；</li><li>利用贝叶斯公式转换成后验概率；</li><li>根据后验概率大小进行决策分类。</li></ul><p>根据以上基本思想，可以得到贝叶斯概率计算公式表达为<strong>：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）</strong>。</p><h1 id="贝叶斯推断：广告邮件自动识别的代码实现"><a href="#贝叶斯推断：广告邮件自动识别的代码实现" class="headerlink" title="贝叶斯推断：广告邮件自动识别的代码实现"></a><strong>贝叶斯推断：广告邮件自动识别的代码实现</strong></h1><p>若邮件包含某个关键词，求此邮件是广告的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广告邮件数量</span></span><br><span class="line">ad_number = <span class="number">4000</span></span><br><span class="line"><span class="comment"># 正常邮件数量</span></span><br><span class="line">normal_number = <span class="number">6000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有广告邮件中，出现 “红包” 关键词的邮件的数量</span></span><br><span class="line">ad_hongbao_number = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 所有正常邮件中，出现 “红包” 关键词的邮件的数量</span></span><br><span class="line">normal_hongbao_number = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 广告的先验概率 P(A)</span></span><br><span class="line">P_ad = ad_number / (ad_number + normal_number)</span><br><span class="line"><span class="comment"># 包含红包的先验概率 P(B)</span></span><br><span class="line">P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)</span><br><span class="line"><span class="comment"># 广告 包含红包的似然概率 P(B|A)</span></span><br><span class="line">P_hongbao_ad = ad_hongbao_number / ad_number</span><br><span class="line"><span class="comment"># 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)</span></span><br><span class="line">P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbao</span><br><span class="line"><span class="built_in">print</span>(P_ad_hongbao)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9940357852882705</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;条件概率&quot;&gt;&lt;a href=&quot;#条件概率&quot; class=&quot;headerlink&quot; title=&quot;条件概率&quot;&gt;&lt;/a&gt;条件概率&lt;/h1&gt;&lt;p&gt;P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率：&lt;/p&gt;
&lt;script type=&quot;math/tex; m</summary>
      
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>考研</title>
    <link href="https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/"/>
    <id>https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/</id>
    <published>2022-04-21T06:19:23.000Z</published>
    <updated>2022-07-29T10:13:08.337Z</updated>
    
    <content type="html"><![CDATA[<p>报考专业 <a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/4/114/085410">深大 - 人工智能与金融科技</a></p><p>初试科目</p><ul><li><p><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/101">101 思想政治理论</a></p></li><li><p><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/201">201 英语一</a></p></li><li><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/301">301 数学一</a></li><li><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/408">408 计算机学科专业基础综合</a></li></ul><p>复试科目 <a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/FSX8">FSX8 机器学习</a></p><p>计算机考研 408 包括（150）</p><ul><li>数据结构 45</li><li>计算机组成原理 45</li><li>操作系统 35</li><li>计算机网络 25</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;报考专业 &lt;a href=&quot;http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/4/114/085410&quot;&gt;深大 - 人工智能与金融科技&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;初试科目&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a hr</summary>
      
    
    
    
    <category term="考研" scheme="https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
    <category term="考研" scheme="https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
</feed>
