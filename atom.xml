<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WINGO&#39;S BLOG</title>
  
  
  <link href="https://wingowen.github.io/atom.xml" rel="self"/>
  
  <link href="https://wingowen.github.io/"/>
  <updated>2022-09-01T05:58:03.222Z</updated>
  <id>https://wingowen.github.io/</id>
  
  <author>
    <name>Wingo Wen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>网络相关</title>
    <link href="https://wingowen.github.io/2022/09/01/%E8%BF%90%E7%BB%B4/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"/>
    <id>https://wingowen.github.io/2022/09/01/%E8%BF%90%E7%BB%B4/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/</id>
    <published>2022-09-01T02:45:43.000Z</published>
    <updated>2022-09-01T05:58:03.222Z</updated>
    
    <content type="html"><![CDATA[<p>iptables / netfilter</p><span id="more"></span><h1 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h1><p>iptables 是 Linux 防火墙的管理工具而已，位于 /sbin/iptables；真正实现防火墙功能的是 netfilter，它是 Linux 内核中实现包过滤的内部结构。</p><p><strong>iptables 传输数据包的过程：</strong></p><ul><li>当一个数据包进入网卡时，它首先进入 PREROUTING 链，内核根据数据包目的 IP 判断是否需要转送出去。 </li><li>如果数据包就是进入本机的，它就会沿着图向下移动，到达 INPUT 链。数据包到了 INPUT 链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过 OUTPUT 链，然后到达 POSTROUTING 链输出。 </li><li>如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过 FORWARD 链，然后到达 POSTROUTING 链输出。</li></ul><p><strong>iptables的规则表和链：</strong></p><p>表（tables）提供特定的功能，iptables 内置了 4 个表</p><ul><li>filter 表，包过滤；</li><li>nat 表，网络地址转换；</li><li>mangle 表，包重构、修改；</li><li>raw 表，数据跟踪处理。</li></ul><p>链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一 条或数条规则。当一个数据包到达一个链时，iptables 就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则 iptables 将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。</p><p>五个链为：</p><ul><li>PREROUTING：路由选择前；</li><li>INPUT：路由选择后，进入到主机中；</li><li>FORWARD：路由选择后，转发；</li><li>OUTPUT：路由选择后（判断用哪张网卡发出包）,流出；</li><li>POSTROUTING：最后的数据流出。</li></ul><p><strong>常用命令：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看规则</span></span><br><span class="line">iptables -t 表名 -L</span><br><span class="line">iptables -t nat --line -nvL PREROUTING </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">--line 显示规则的行号</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-n 不解析IP</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-v 显示详细内容</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加规则</span></span><br><span class="line">iptables -t filter -A INPUT -s 192.168.1.146 -j DROP</span><br><span class="line">iptables -t filter -I INPUT -s 192.168.1.146 -j ACCEPT</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定位置</span></span><br><span class="line">iptables -t filter -I INPUT 5 -s 192.168.1.146 -j REJECT </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置指定表的指定链的默认策略（默认动作），并非添加规则。</span></span><br><span class="line">iptables -t filter -P FORWARD ACCEPT</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-I 插入到第一行</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-A 插入到最后</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"> 删除规则</span></span><br><span class="line">iptables -t filter -D INPUT 3</span><br><span class="line">iptables -t filter -D INPUT -s 192.168.1.146 -j DROP</span><br><span class="line">iptables -t filter -F INPUT</span><br><span class="line">iptables -t filter -F</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">-F 清空</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除自定义链</span></span><br><span class="line">iptables -X WEB</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改规则</span></span><br><span class="line">iptables -t filter -R INPUT 3 -s 192.168.1.146 -j ACCEPT</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清除包的计数</span></span><br><span class="line">iptabls -t nat -Z PREROUTING</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清除nat表所有链的计数</span></span><br><span class="line">iptabls -t nat -Z</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存</span></span><br><span class="line">service iptables save</span><br><span class="line">iptables-save &gt; /etc/sysconfig/iptables</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;iptables / netfilter&lt;/p&gt;</summary>
    
    
    
    <category term="运维" scheme="https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="网络相关" scheme="https://wingowen.github.io/tags/%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>服务安装</title>
    <link href="https://wingowen.github.io/2022/08/31/%E8%BF%90%E7%BB%B4/%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/"/>
    <id>https://wingowen.github.io/2022/08/31/%E8%BF%90%E7%BB%B4/%E6%9C%8D%E5%8A%A1%E5%AE%89%E8%A3%85/</id>
    <published>2022-08-31T02:41:25.000Z</published>
    <updated>2022-09-01T02:44:34.288Z</updated>
    
    <content type="html"><![CDATA[<p>JDK</p><span id="more"></span><h1 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wget &#x27;https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.rpm&#x27;</span><br><span class="line">yum install jdk-8u202-linux-x64.rpm -y</span><br><span class="line"></span><br><span class="line">cat &gt;&gt; ~/.bash_profile &lt;&lt; EOF</span><br><span class="line">JAVA_HOME=/usr/java/jdk1.8.0_202-amd64/</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure><h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># mysql 服务下载启动</span><br><span class="line">yum install -y http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm</span><br><span class="line"></span><br><span class="line">yum install mysql-community-server.x86_64 -y --nogpgcheck</span><br><span class="line"></span><br><span class="line">systemctl start mysqld</span><br><span class="line">systemctl enable mysqld</span><br><span class="line"></span><br><span class="line"># mysql 服务配置</span><br><span class="line"></span><br><span class="line"># 获取初始密码</span><br><span class="line">cat /var/log/mysqld.log | grep password</span><br><span class="line"># 登陆控制台</span><br><span class="line">mysql -p</span><br><span class="line"># 设置密码及权限</span><br><span class="line">&gt; set global validate_password_policy=0;</span><br><span class="line">&gt; SET PASSWORD = PASSWORD(&#x27;wefe2022&#x27;);</span><br><span class="line">&gt; grant all privileges on *.* to root@&quot;%&quot; IDENTIFIED BY &quot;wefe2022&quot;;</span><br><span class="line">&gt; flush privileges;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;JDK&lt;/p&gt;</summary>
    
    
    
    <category term="运维" scheme="https://wingowen.github.io/categories/%E8%BF%90%E7%BB%B4/"/>
    
    
    <category term="部署" scheme="https://wingowen.github.io/tags/%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>网站收集</title>
    <link href="https://wingowen.github.io/2022/08/29/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"/>
    <id>https://wingowen.github.io/2022/08/29/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/</id>
    <published>2022-08-29T08:57:30.000Z</published>
    <updated>2022-08-29T09:06:24.800Z</updated>
    
    <content type="html"><![CDATA[<p>收集一些有用的网站。</p><span id="more"></span><h1 id="JAVA"><a href="#JAVA" class="headerlink" title="JAVA"></a>JAVA</h1><h2 id="Sring"><a href="#Sring" class="headerlink" title="Sring"></a>Sring</h2><p><a href="https://github.com/wuyouzhuguli/SpringAll">https://github.com/wuyouzhuguli/SpringAll</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;收集一些有用的网站。&lt;/p&gt;</summary>
    
    
    
    <category term="日常" scheme="https://wingowen.github.io/categories/%E6%97%A5%E5%B8%B8/"/>
    
    
    <category term="网站收集" scheme="https://wingowen.github.io/tags/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>TED 第一期</title>
    <link href="https://wingowen.github.io/2022/08/27/TED/TED%20%E7%AC%AC%E4%B8%80%E6%9C%9F/"/>
    <id>https://wingowen.github.io/2022/08/27/TED/TED%20%E7%AC%AC%E4%B8%80%E6%9C%9F/</id>
    <published>2022-08-27T00:28:40.000Z</published>
    <updated>2022-08-29T09:05:21.479Z</updated>
    
    <content type="html"><![CDATA[<p>Keep Learngin.</p><span id="more"></span><h1 id="Keep-your-goal-to-yourself"><a href="#Keep-your-goal-to-yourself" class="headerlink" title="Keep your goal to yourself"></a>Keep your goal to yourself</h1><p>Everyone, please think of your biggest personal goal. For real — you can take a second. You’ve got to feel this to learn it. Take a few seconds and think of your personal biggest goal, okay? Imagine deciding right now that you’re going to do it. Imagine telling someone that you meet today what you’re going to do. Imagine their congratulations, and their high image of you. Doesn’t it feel good to say it out loud?Don’t you feel one step closer already, like it’s already becoming part of your identity?</p><p>Well, bad news: you should have kept your mouth shut,because that good feeling now will make you less likely to do it. The repeated psychology tests have proven that telling someone your goal makes it less likely to happen. Any time you have a goal, there are some steps that need to be done, some work that needs to be done in order to achieve it. Ideally you would not be satisfied until you’d actually done the work. But when you tell someone your goal and they acknowledge it, psychologists have found that it’s called a “social reality”. The mind is kind of tricked into feeling that it’s already done. And then because you’ve felt taht satisfaction, you’re less motivated to do the actual hard work necessary.</p><p>So this goes against conventianal wisdom that we should tell our friends our goals, right? So they hold us to it.</p><p>So, let’s look at the proof. 1926: Kurt Lewin, founder of social psychology, called this “subsituation”. 1933: Wera Mahler found when it was acknowledged by others, it felt real in the mind. 1982, Peter Gollwitzer wrote a whole book about this, and in 2009, he did some new tests that were published.</p><p>It goes like this: 163 people across four separate tests. Everyone wrote down their personal goal. Then half of them announced their commitment to this goal to the room, and half didn’t. Then everyone was given 45 minutes of work that would directly lead them towards their goal, but they were told that they could stop at any time. Now, those who kept their mouths shut worked the entire 45 minutes on average, and when asked afterward, said that they felt that they had a long way to go still to achieve their goal. But those who had announced it quit after only 33 minutes, on average, and when asked afterward, said that they felt much closer to achieving their goal.</p><p>So if this is true, what can we do? Well, you could resist the temptation to announce your goal. You can delay the gratification that the social acknowledgement brings, and you can understand that your mind mistakes the talking for the doing. But if you do need to talk about something, you can state it in a way that gives you no satisfication, such as, “I really want to run this marathon, so I need to train five times a week and kick my ass if I don’t, okay?”</p><p>So audience, next time you’re tempted to tell someone your goal, what will you say? </p><p>Exactly! Well done.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Keep Learngin.&lt;/p&gt;</summary>
    
    
    
    <category term="英语" scheme="https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
    <category term="英语演讲" scheme="https://wingowen.github.io/tags/%E8%8B%B1%E8%AF%AD%E6%BC%94%E8%AE%B2/"/>
    
  </entry>
  
  <entry>
    <title>计算机科学</title>
    <link href="https://wingowen.github.io/2022/08/15/%E7%BC%96%E7%A8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"/>
    <id>https://wingowen.github.io/2022/08/15/%E7%BC%96%E7%A8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/</id>
    <published>2022-08-15T09:16:17.000Z</published>
    <updated>2022-08-29T09:05:56.912Z</updated>
    
    <content type="html"><![CDATA[<p>协程。</p><span id="more"></span><h1 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h1><p>操作系统在线程等待 IO 的时候，会阻塞当前线程，切换到其它线程，这样在当前线程等待 IO 的过程中，其它线程可以继续执行。当系统线程较少的时候没有什么问题，但是当线程数量非常多的时候，却产生了问题。一是系统线程会占用非常多的内存空间，二是过多的线程切换会占用大量的系统时间。</p><p>协程刚好可以解决上述 2 个问题。协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多。</p><p>协程只有在等待 IO 的过程中才能重复利用线程。</p><p>假设协程运行在线程之上，并且协程调用了一个阻塞 IO 操作，这时候会发生什么？实际上操作系统并不知道协程的存在，它只知道线程，因此在协程调用阻塞 IO 操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的。</p><p>因此在协程中不能调用导致线程阻塞的操作。也就是说，协程只有和异步 IO 结合起来，才能发挥其作用。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;协程。&lt;/p&gt;</summary>
    
    
    
    <category term="编程" scheme="https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="计算机科学" scheme="https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>降维</title>
    <link href="https://wingowen.github.io/2022/08/14/%E7%AE%97%E6%B3%95/%E9%99%8D%E7%BB%B4/"/>
    <id>https://wingowen.github.io/2022/08/14/%E7%AE%97%E6%B3%95/%E9%99%8D%E7%BB%B4/</id>
    <published>2022-08-14T08:48:21.000Z</published>
    <updated>2022-08-29T09:04:45.843Z</updated>
    
    <content type="html"><![CDATA[<p>数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效<br>信息的提取综合及无用信息的摈弃。</p><p>数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。</p><span id="more"></span><h1 id="低维嵌入介绍"><a href="#低维嵌入介绍" class="headerlink" title="低维嵌入介绍"></a>低维嵌入介绍</h1><p>在很多时候，人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个低维嵌入 embedding。</p><p>缓解维数灾难的一个重要途径是降维 dimension reduction。它是通过某种数学变换将原始高纬度属性空间转变为一个低维子空间，在这个子空间中样本密度大幅提高，计算距离也变得更为容易。低维嵌入的目的是解决 k 邻近学习方法可操作性弱的问题。</p><p>将一个三维问题垂直投影，变成一个二维问题。 这种方法叫做多维缩放 Multiple Dimensional Scaling，简称 MDS，这是一种经典的降维方法。</p><p><strong>MDS</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 导入包</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn import datasets,manifold</span><br><span class="line">from collections import Counter</span><br><span class="line"></span><br><span class="line">def load_data():</span><br><span class="line">    # 使用 scikit-learn 自带的 iris 数据集</span><br><span class="line">    iris=datasets.load_iris()</span><br><span class="line">    return  iris.data,iris.target</span><br><span class="line"></span><br><span class="line"># 产生用于降维的数据集</span><br><span class="line">X, y=load_data()</span><br><span class="line"></span><br><span class="line">print(X.shape)</span><br><span class="line">print(Counter(y))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;数据降维的目的：数据降维，直观地好处是维度降低了，便于计算和可视化，其更深层次的意义在于有效&lt;br&gt;信息的提取综合及无用信息的摈弃。&lt;/p&gt;
&lt;p&gt;数据降维的好处：降维可以方便数据可视化，数据分析，数据压缩，数据提取等。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="https://wingowen.github.io/2022/08/13/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <id>https://wingowen.github.io/2022/08/13/%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</id>
    <published>2022-08-13T14:41:12.000Z</published>
    <updated>2022-08-29T09:05:47.711Z</updated>
    
    <content type="html"><![CDATA[<p>特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。</p><span id="more"></span><h1 id="过滤式选择"><a href="#过滤式选择" class="headerlink" title="过滤式选择"></a>过滤式选择</h1><p>先对数据集进行特征选择，然后再训练分类器，特征选择过程与后续训练无关。这相当于先用特征选择过程对初始特征进行过滤，再用过滤后的特征来训练模型。</p><h2 id="Relief-选择法"><a href="#Relief-选择法" class="headerlink" title="Relief 选择法"></a>Relief 选择法</h2><p>Relief Relevant Features，该方法设计了一个相关统计量来度量特征的重要性，并且其是一个向量，其每个分量分别对应一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。最终只需要确定一个阈值 r，然后选择比 r 大的相关统计量分量所对应的特征即可。也可以指定选取相关统计量分量最大的前 k 个特征。</p><h2 id="FInsher-选择法"><a href="#FInsher-选择法" class="headerlink" title="FInsher 选择法"></a>FInsher 选择法</h2><p>计算数据集中每个类别样本的类内特征方差与类间特征方差。<br>类内特征方差越小，类间特征方差越大，越有利于后续的分类训练，即该特征需要保留，反之该特征需要滤除。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">#导入包</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">#创建示例样本</span><br><span class="line">sample = [[1,1,5],[10,0,1],[2,5,6]]</span><br><span class="line">label = [1, 0, 1]</span><br><span class="line">print(&quot;sample:&quot;,sample)</span><br><span class="line">print(&quot;label:&quot;,label)</span><br><span class="line"></span><br><span class="line">#判断样本长度与类标长度是否匹配</span><br><span class="line">if len(sample) != len(label):</span><br><span class="line">    print(&#x27;Sample does not match label&#x27;)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">#创建并保存计算过程中的变量</span><br><span class="line">df1 = pd.DataFrame(sample)</span><br><span class="line">df2 = pd.DataFrame(label, columns=[&#x27;label&#x27;])</span><br><span class="line">data = pd.concat([df1, df2], axis=1)  # 合并成为一个dataframe</span><br><span class="line">print(&quot;data:\n&quot;,data,&#x27;\n&#x27;)</span><br><span class="line">data0 = data[data.label == 0]#对标签分类，分成包含0和1的两个dataframe</span><br><span class="line">data1 = data[data.label == 1]</span><br><span class="line">n = len(label)#标签长度</span><br><span class="line">n1 = sum(label)#1类标签的个数</span><br><span class="line">n0 = n - n1#0类标签的个数</span><br><span class="line">lst = []#用于返回的列表</span><br><span class="line">features_list = list(data.columns)[:-1]</span><br><span class="line">print(&quot;特征维数:&quot;)</span><br><span class="line">print(features_list)</span><br><span class="line"></span><br><span class="line">#fisher score计算</span><br><span class="line">for feature in features_list:</span><br><span class="line">    print(&#x27;\nfeature&#x27;,feature,&#x27;:&#x27;)</span><br><span class="line">    # 算关于类标0</span><br><span class="line">    m0_feature_mean = data0[feature].mean()  # 0 类标签在第 m 维上的均值</span><br><span class="line">    print(&quot;m0_feature_mean&quot;,m0_feature_mean)</span><br><span class="line">    m0_SW=sum((data0[feature] -m0_feature_mean )**2) # 0类在第 m 维上的类内方差</span><br><span class="line">    print(&quot;m0_SW&quot;,m0_SW)</span><br><span class="line">    # 算关于类标1</span><br><span class="line">    m1_feature_mean = data1[feature].mean()  # 1 类标签在第 m 维上的均值</span><br><span class="line">    print(&quot;m1_feature_mean&quot;,m1_feature_mean)</span><br><span class="line">    m1_SW=sum((data1[feature] -m1_feature_mean )**2)# 1 类标签在第 m 维上的类内方差</span><br><span class="line">    print(&quot;m1_SW&quot;,m1_SW)</span><br><span class="line"></span><br><span class="line">    # 算关于 data</span><br><span class="line">    m_all_feature_mean = data[feature].mean()  # 所有类标签在第 m 维上的均值</span><br><span class="line">    print(&quot;m_all_feature_mean&quot;,m_all_feature_mean)</span><br><span class="line">    m0_SB = n0 / n * (m0_feature_mean - m_all_feature_mean) ** 2  # 0 类标签在第 m 维上的类间方差</span><br><span class="line">    print(&quot;m0_SB&quot;,m0_SB)</span><br><span class="line">    m1_SB = n1 / n * (m1_feature_mean - m_all_feature_mean) ** 2  # 1 类标签在第 m 维上的类间方差</span><br><span class="line">    print(&quot;m1_SB&quot;,m1_SB)</span><br><span class="line"></span><br><span class="line">    m_SB = m1_SB + m0_SB   # 计算SB</span><br><span class="line">    print(&quot;m_SB&quot;,m_SB)</span><br><span class="line"></span><br><span class="line">    m_SW = (m0_SW + m1_SW) / n   # 计算 SW</span><br><span class="line">    print(&quot;m_SW&quot;,m_SW)</span><br><span class="line"></span><br><span class="line">    if m_SW == 0:</span><br><span class="line">        # 0/0类型也是返回nan</span><br><span class="line">        m_fisher_score = np.nan</span><br><span class="line">    else:</span><br><span class="line">        # 计算Fisher score</span><br><span class="line">        m_fisher_score = m_SB / m_SW  #计算第m维特征的Fisher score</span><br><span class="line">    #Fisher score值添加进列表</span><br><span class="line">    print(&quot;m_fisher_score&quot;,m_fisher_score)</span><br><span class="line">    lst.append(m_fisher_score)</span><br></pre></td></tr></table></figure><h1 id="包裹式选择"><a href="#包裹式选择" class="headerlink" title="包裹式选择"></a>包裹式选择</h1><p>包裹式特征选择直接将最终要使用的学习器的性能作为特征子集的评价准则，包裹式特征选择的目的就是为给定的学习器选择最有利于其性能而量身定做的特征子集。</p><p>一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，比过滤式特征选择更好，但由于在特征选择过程中要多次训练学习器，因此包裹式特征选择的计算开销比过滤式特征选择大得多。</p><p>LVW 是一个经典的包裹式特征选择方法，它在拉斯维加斯方法框架下使用随机策略进行子集搜索，以最终分类器误差作为特征子集评价标准。</p><p>除了 LVW 包裹式特征选择之外，RFE(递归特征消除)也是一种常见的包裹式特征选择方法，RFE特征选择使用模型准确率来判断哪些特征（或特征组合）对预测结果贡献较大，并且递归地去除贡献小的特征。</p><p>除了 RFE 之外，还有一种选择算法称为 RFECV，其是以 RFE 为基础进行改进得到的。</p><p>RFECV 通过交叉验证的方式执行 RFE，以此来选择最佳数量的特征，即不手动设置保留的特征数量。对于一个数量为 d 的 feature 的集合，他的所有的子集的个数是 2 的 d 次方减 1 (包含空集)。指定一个外部的学习算法，比如 SVM 之类。通过该算法计算所有子集的validation error。选择 error 最小的那个子集作为所挑选的特征。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#导入包</span><br><span class="line">from sklearn.feature_selection import RFE,RFECV</span><br><span class="line">from sklearn.svm import LinearSVC</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn import model_selection</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">class RFE(BaseEstimator, MetyuanaEstimatorMixin, SelectorMixin):</span><br><span class="line">    参数：</span><br><span class="line">    BaseEstimator: 基模型</span><br><span class="line">    n_features_to_select：目标特征数量</span><br><span class="line">    return：经过选择后的特征</span><br><span class="line"></span><br><span class="line">    比较经过特征选择和未经特征选择的数据集，对 LinearSVC 的预测性能的区别</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">### 加载数据</span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">### 特征提取</span><br><span class="line">estimator = LinearSVC()</span><br><span class="line">selector = RFE(estimator=estimator, n_features_to_select=2)</span><br><span class="line">X_t = selector.fit_transform(X, y)  #对样本进行特征选择，最终保留n_features_to_select个特征。</span><br><span class="line">print(&quot;\n特征选择结果显示:&quot;)</span><br><span class="line">print(&quot;原数据样本X：&quot;,X[1])</span><br><span class="line">print(&quot;特征选择后样本X_t：&quot;,X_t[1])</span><br><span class="line"></span><br><span class="line">#### 切分测试集与验证集</span><br><span class="line">X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,</span><br><span class="line">                                            test_size=0.25, random_state=0, stratify=y)</span><br><span class="line">X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y,</span><br><span class="line">                                            test_size=0.25, random_state=0,</span><br><span class="line">                                            stratify=y)</span><br><span class="line">print(&quot;测试集与验证集切分完成&quot;)</span><br><span class="line"></span><br><span class="line">### 测试与验证</span><br><span class="line">clf = LinearSVC()</span><br><span class="line">clf_t = LinearSVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(&quot;\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))</span><br><span class="line">clf_t.fit(X_train_t, y_train_t)</span><br><span class="line">print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t)))</span><br></pre></td></tr></table></figure><h1 id="嵌入式选择"><a href="#嵌入式选择" class="headerlink" title="嵌入式选择"></a>嵌入式选择</h1><p>嵌入式特征选择是将特征选择过程与学习器训练过程融合为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。</p><p><strong>正则化嵌入式选择</strong></p><p>$L1$ 范数与 $L2$ 范数都有利于降低过拟合，但前者还会带来一个额外的好处，即 $L1$ 范数比 $L2$ 范数更容易获得稀疏解，即它求得的 $w$ 会有更少的非零分量。</p><p>其中，基于 $L1$ 正则化的学习方法是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#导入包</span><br><span class="line">from sklearn.svm import LinearSVC</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.feature_selection import SelectFromModel</span><br><span class="line">from sklearn import model_selection</span><br><span class="line"></span><br><span class="line">#导入数据集并打印示例</span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">print(&quot;原始数据特征维数：&quot;,len(X[1]))  # (150, 4)</span><br><span class="line">print(&quot;原始数据样本：&quot;,X[1])</span><br><span class="line"></span><br><span class="line">#特征选择</span><br><span class="line">lsvc = LinearSVC(C=0.01, penalty=&quot;l1&quot;, dual=False).fit(X, y)  #设置分类器</span><br><span class="line">model = SelectFromModel(lsvc, prefit=True)  #设置模型为特征选择</span><br><span class="line">X_t = model.transform(X)  #获取经过筛选的数据</span><br><span class="line">print(&quot;特征选择数据特征维数&quot;,len(X_t[1]))  #(150, 3)</span><br><span class="line">print(&quot;特征选择后数据样本&quot;,X_t[1])</span><br><span class="line"></span><br><span class="line">#### 切分测试集与验证集</span><br><span class="line">X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,</span><br><span class="line">                                                                    test_size=0.25, random_state=0, stratify=y)</span><br><span class="line">X_train_t, X_test_t, y_train_t, y_test_t = model_selection.train_test_split(X_t, y,</span><br><span class="line">                                                                            test_size=0.25, random_state=0,stratify=y)</span><br><span class="line">print(&quot;测试集与验证集切分完成&quot;)</span><br><span class="line"></span><br><span class="line">### 测试与验证</span><br><span class="line">clf = LinearSVC()</span><br><span class="line">clf_t = LinearSVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf_t.fit(X_train_t, y_train_t)</span><br><span class="line">print(&quot;\n原始数据集: test score=%s&quot; % (clf.score(X_test, y_test)))</span><br><span class="line">print(&quot;特征选择后的数据集: test score=%s&quot; % (clf_t.score(X_test_t, y_test_t)))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;特征选择也称特征子集选择或属性选择。从已有的 M 个特征中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高学习算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。对于一个学习算法来说，好的学习样本是训练模型的关键。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性回归与逻辑回归</title>
    <link href="https://wingowen.github.io/2022/08/12/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>https://wingowen.github.io/2022/08/12/%E7%AE%97%E6%B3%95/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</id>
    <published>2022-08-12T02:35:16.000Z</published>
    <updated>2022-08-13T14:34:12.821Z</updated>
    
    <content type="html"><![CDATA[<p>监督学习。</p><p>线性回归 Linear Regress 是回归问题的基础。</p><p>逻辑回归 Logistic Regress 是分类问题的基础。</p><p>损失函数与梯度下降法。</p><p>过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等</p><span id="more"></span><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>线性回归分析 Linear Regression Analysis 是确定两种或两种以上<strong>变量间相互依赖的定量关系</strong>的一种<strong>统计分析方法</strong>。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。</p><p>所以线性的定义为：<strong>自变量之间只存在线性关系</strong>，即自变量只能通过相加、或者相减进行组合。</p><p><strong>监督学习</strong></p><p>如果现在有一个房子 H1，面积是 S，监督学习如何估算它的价格？</p><ol><li><del>监督学习从训练集中找到面积最接近 S 的房子 H2，预测 H1 的价格等于 H2 的价格。</del></li><li>监督学习根据训练集，找到一个数学表达式，对任意的面积的房子都可以估算出其价格。</li></ol><p>h 代表假设函数：Training Set → Learning Algorithm → h；Size of House  + h → Estimated Price。</p><p><strong>线性回归的假设模型</strong></p><script type="math/tex; mode=display">h_{\theta}(x)=\theta_{0}+\theta_{1} x</script><p>如何求解模型，有以下两种思路。</p><ul><li>尝试一些 $\theta<em>{0}$ 和 $\theta</em>{1}$ 的组合，选择能使得画出的直线正好穿过训练集的 $\theta<em>{0}$ 和 $\theta</em>{1}$ 。</li><li>尝试一些  $\theta<em>{0}$ 和 $\theta</em>{1}$ 的组合，然后在训练集上进行预测，选能使得预测值与真实的房子价格最接近的  $\theta<em>{0}$ 和 $\theta</em>{1}$ 。</li></ul><p>选择最佳的 $\theta<em>{0}$ 和 $\theta</em>{1}$，使得 $h_{\theta}(x)$ 对所有的训练样本 $(x, y)$ 来说，尽可能的接近 $y$。</p><p><strong>损失函数</strong></p><p>Train Set: $\left{\left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \cdots,\left(x^{(m)}, y^{(m)}\right)\right}$</p><script type="math/tex; mode=display">\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>最小化损失函数，得到的 $\theta<em>{0}$ 和 $\theta</em>{1}$ 是最佳的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 房屋的价格和面积数据</span><br><span class="line">import numpy as np</span><br><span class="line">data = np.array([[2104, 460], [1416, 232], [1534, 315], [852,178]])</span><br><span class="line"></span><br><span class="line"># 使用线性回归模型计算预测值</span><br><span class="line">def get_predict(x, theta0, theta1):</span><br><span class="line">    h = theta0 + theta1 * x</span><br><span class="line">    #todo </span><br><span class="line">    return h</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。</p><p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合 $(\theta<em>{0},\theta</em>{1},……,\theta_{n})$ 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，  可能会找到不同的局部最小值。</p><p>实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，需要同时更新 $\theta<em>{0}$ 和 $\theta</em>{1}$，实现方法是：你应该计算公式右边的部分，通过那一部分计算出 $\theta<em>{0}$ 和 $\theta</em>{1}$的值，然后同时更新 $\theta<em>{0}$ 和 $\theta</em>{1}$。</p><script type="math/tex; mode=display">\text { temp0 }:=\theta_{0}-\alpha \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right) \\\text { temp1 }:=\theta_{1}-\alpha \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right) \\\theta_{0}:=\text { temp0 } \\\theta_{1}:=\text { temp1 }</script><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>二分类问题下，采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。它适用于标签 y 取值离散的情况。</p><p>逻辑函数 Logistic Function，一个最常用的逻辑函数是 Sigmoid Function，以 Z=0 为决策界限，公式如下：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return 1 / (1 + np.exp(-z))</span><br></pre></td></tr></table></figure><p><strong>逻辑回归模型假设</strong></p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^TX)</script><p>$h<em>\theta(x)$ 的作用是，对于给定的输入变量，根据选择的参数计算输出变量为 1 的可能性 （estimated probablity），即 $ h</em>\theta(x) = P(y=1|x;\theta)$。 </p><p>例如，如果对于给定的 x，通过已经确定的参数计算得出 $h_\theta(x)$=0.7，则表示有 70% 的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7 = 0.3。</p><p><strong>损失函数</strong></p><p>线性回归模型的代价函数是所有模型误差的平方和，若逻辑回归的假设模型沿用这个定义，得到的函数将是一个非凸函数 non-convex function。这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p><script type="math/tex; mode=display">J(\theta)= \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})</script><script type="math/tex; mode=display">\operatorname{Cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\-\log \left(1-h_{\theta}(x)\right) & \text { if } y=0\end{aligned}\right.</script><script type="math/tex; mode=display">Cost(h_\theta(x), y)=-y\times{log(h_\theta(x))}-(1-y)\times{log(1-h_\theta(x))}</script><script type="math/tex; mode=display">J(\theta) = -\frac{1}{m}\sum^m_{i=1}[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def cost(theta, X, y):</span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))</span><br><span class="line">    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))</span><br><span class="line">    return np.sum(first - second) / (len(X))</span><br></pre></td></tr></table></figure><p>当实际的 y=1 且 $h<em>\theta(x)$ 也为1 时误差为 0，当  y=1 但 $h</em>\theta(x)$ 不为 1 时误差随着 $h_\theta(x)$ 的变小而变大；</p><p>当实际的 y=0 且 $h<em>\theta(x)$ 也为 0 时代价为 0，当 y=0 但 $h</em>\theta(x)$ 不为 0 时误差随着 $h_\theta(x)$ 的变大而变大。 </p><p>同样使用梯度下降法对参数进行更新：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j</script><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 返回某一轮训练中的梯度</span><br><span class="line">def _gradient(X, Y_label, theta):</span><br><span class="line">    # _f用来计算 y 的值</span><br><span class="line">    y_pred = _f(X, theta, b)</span><br><span class="line">    pred_error = Y_label - y_pred</span><br><span class="line">    w_grad = -np.sum(pred_error * X.T, 1)</span><br><span class="line">    return w_grad</span><br></pre></td></tr></table></figure><p><strong>多元分类</strong></p><p>我们将多个类中的一个类标记为正向类 y=1，然后将其他所有类都标记为负向类，这个模型记作 $h^{(1)<em>\theta(x)}$。接着，类似地第我们选择另一个类标记为正向类 y=2，再将其它类都标记为负向类，将这个模型记作 $h^{(2)</em>\theta(x)}$，依此类推。 最后我们得到一系列的模型简记为：</p><script type="math/tex; mode=display">h^{(i)_\theta(x)} = p(y=i|x;\theta)</script><p>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h^{(i)<em>\theta(x)}$， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 $h^{(i)</em>\theta(x)}$ 最大的 i，即 $\max<em>ih^{(i)</em>\theta(x)}$。 </p><h1 id="过拟合化和正则化"><a href="#过拟合化和正则化" class="headerlink" title="过拟合化和正则化"></a>过拟合化和正则化</h1><p>过拟合在训练数据上的表现非常好；对于非训练的数据点，过拟合的模型表现与我们的期望有较大的偏。</p><p>减少拟合化的方法：</p><ul><li>减少选取变量的数量：选取最重要的参数；</li><li>正则化：一种减小参数大小的办法。</li></ul><p><strong>正则化</strong></p><p>回归：岭回归。</p><p>分类：L1 正则化，L2 正则化。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;监督学习。&lt;/p&gt;
&lt;p&gt;线性回归 Linear Regress 是回归问题的基础。&lt;/p&gt;
&lt;p&gt;逻辑回归 Logistic Regress 是分类问题的基础。&lt;/p&gt;
&lt;p&gt;损失函数与梯度下降法。&lt;/p&gt;
&lt;p&gt;过拟合与正则化，正则化方式包括：1）减少项数；2）岭回归，L1，L2 等&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>英语单词</title>
    <link href="https://wingowen.github.io/2022/08/02/%E8%80%83%E7%A0%94/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/"/>
    <id>https://wingowen.github.io/2022/08/02/%E8%80%83%E7%A0%94/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/</id>
    <published>2022-08-02T01:16:03.000Z</published>
    <updated>2022-09-12T05:19:46.505Z</updated>
    
    <content type="html"><![CDATA[<p>第一次学习时间：2022年08月02日。</p><span id="more"></span><h1 id="UNIT-01"><a href="#UNIT-01" class="headerlink" title="UNIT 01"></a>UNIT 01</h1><p>state n. 状态 情况 国 州 v. 陈述 说明</p><p>The current state of affairs may have been encouraged — though not justified — by the lack of legal penalty (in American, bur not in Europe) for data leakage.</p><p>As a condition of receiving state approval for the sale, the company agreed to seek permission from state regulators to operate past 2012.</p><p>statute n. 法令 法规 manifestation n. 显示 表现 示威运动 statistic n. 统计数值 statistical adj. 统计学的</p><p>a statistical population distribution among age peers.</p><p>stationary adj. 固定的 静止的 不动的 statement n. 陈述 声明 表达</p><p>A string of accidents, including the partial collapse of a colling tower in 2007 and the discovery of an underground pipe system leakage, raised serious questions about both A’s safety and B’s management especically after company made misleading statements about the pipe.</p><p>understatement n. 保守陈述 轻描淡写 overstate v. 夸大陈述</p><p>It was banks that were on the wrong planet, with account that vastly overvalued assets. Today they argue that market prices overstate losses, because they largely reflect the temporary illiquidity of markets, not the likely extent of bad debts.</p><p>statesman n. 政治家 estate n. 房地产 身份 财产 devastate v. 毁灭 毁坏 devastating adj. 毁灭的 极具破坏力的</p><p>The wear-and-tear that come from these longer relationships can be quite devastating.</p><p>workstation n. 工作台 status n. 地位 情形 状态 assert v. 坚称 断言 表明</p><p>The administration was in essence asserting that because it didn’t want to carry out Congress’s immigration wishes, no state should be allowed to do so either.</p><p>affirm v. 断言 肯定 证实 public adj. 公众的 公共的 n. 民众 publication n. 出版物 republican adj. 共和国的 共和党的 n. 共和主义者 publicity n. 公众信息 宣传 公之于众的状况 publicized adj. 公开的 publicly adv. 公共得 in public 公开地 mass n. 民众 大量 civil adj. 公民的 civil servant 公务员</p><p>law n. 法律 法规 claw n. 爪 爪形器具 v. 用爪挖 lawful adj. 合法的 法定的 lawsuit n. 诉讼 诉讼案件 flaw n. 缺点 v. 使破裂 lawyer n. 律师 legislation n. 法律 法规</p><p>That may change fast: lots of proposed data-security legislation is now doing the rounds in Washington.D.C.</p><p>mean v. 意味着 adj. 吝啬的 刻薄的 n. 平均数</p><p>Clearly, intelligence encompasses more than a score on a test, Just What does it mean to be smart?</p><p>The development of “cloud computing”, meanwhile, means that police officers could conceivably access even more information with a few swipes on a touchscreen.</p><p>meaning n. 意义 含义  well-meaning adj. 好心的 善意的 meaningless adj. 没有意义的 不重要的 无价值的 meaningfully adv. 有意义地</p><p>And the best way to learn how to encoding information meaningfully, Tom determined. was a process konwn as deliberate pracitce. Deliberate practice entails more than simply repeating a task. Rather, it involves setting specific goles,obtaining immediate feedback and concentrating as much on technique as on outcome.</p><p>means n. 方法 手段 meanwhile adv. 同时 其间</p><p>Meanwhile, as the recession is looming large, people are getting anxious. They tend to keep a tighter hold on their purse and considereating at home a realistic alternative.</p><p>by means of 用 依靠 by no means 绝不 一点也不 indicate v. 表明 暗示 象征 反映 implication n. 可能引发的后果 暗示 含义</p><p>Scholars, policymakers, and critics of all stripes have debated the social implications of these changes.</p><p>influence n. 影响 权势 v. 影响 influential adj. 有影响力的 n. 影响者 有势力的人 impact n. 撞击 碰撞 影响 v. 撞击 对…产生影响 碰撞</p><p>Though serveral fast-fashion companies have made efforts to curb their impact on labor and the environment - including H&amp;M, with its green Conscious Collection Line - Tom believes lasting change can only be effected by the customer.</p><p>live v. 居住 n. 生命 adj. 活的 现场演出的</p><p>Devoted concertgoers who reply that recordings are no subtitute for live performance are missing the point.</p><p>alive adj. 有生气的 活着的 deliver v. 发表 递送 接生 delivery n. 分娩 投递 讲话方式 outlive v. 比…活的久</p><p>Steelworkers, airline employees, and now thoes in ths auto industry are joining millions of families who must worry about interest rates, stock market fluctuation, and the hash reality that they may outlive their retirement money.</p><p>liveliness n. 活力 livelihood n. 生计 生活 live off 依赖…生活 dewell v. 居住于 reside v. 定居于 survive v. 幸存 比…活得长 survival n. 生存 幸存 inhabit v. 居住于 settle v. 使定居</p><p>federal adj. 联邦的</p><p>A few premiers are suspicious of any federal-provincial deal-making.</p><p>federation n. 联邦</p><p>The idea is to create a federation of private online identity systems.</p><p>FBI Federal Bureau of Investigation</p><p>CIA Central Intelligence Agency</p><p>large adj. 大的 大规模的 largely adv. 主要地 大体的 enlarge v. 扩大 放大 at large 一般 普遍地 bulky adj. 庞大的 体积大的 outsize adj. 特大的 huge adj. 庞大的</p><p>mark v. 做标记 n. 痕迹</p><p>America American 人</p><p>America’s new plan to buy up toxic assets will not work unless banks mark assets to levels which buyers find attractive.</p><p>market n. 市场 行情 v. 推销 营销</p><p>The rough guide to marketing success used to be that you got what you paid for.</p><p>marketplace n. 市场 商场 市集 marked adj. 显著的 明显的 有记号的</p><p>There is a marked different between the education which every one gets from living with others, and the deliberate educating of the young.</p><p>marketer n. 市场商人 市场营销人员</p><p>we media 自媒体 paid and owned media 付费媒体与自有媒体</p><p>Paid and owned media are controlled by marketers promoting their own products.</p><p>remark v. 评论 n. 评论 remarkable adj. 卓越的 非凡的 辉煌的 landmark n. 地标 里程碑 转折点 adj. 有重大意义的</p><p>blot n. 污点 spot n. 点 stain n. 污渍 污迹 symbol n. 象征 标志</p><p>system n. 系统 体制 systematic adj. 有系统的 系统化的 systematically adv. 系统地 有条理地</p><p>Databases used by some companies don’t rely on data conllected systematically but rather lump together information from different research projects.</p><p>regime n. 政权 政体 管理制度 organization n. 组织 机构 团体 structure n. 结构 构造 组织</p><p>stress v. 强调 着重 n. 压力</p><p>They show kone how to deal with setbacks, stresses and feelings of inadequacy.</p><p>Because representative government presupposes an informed citizency, the report supports full literacy; stresses the study of history and government, particularly American history and American government; and encourages the use of new digital technologies.</p><p>stressed-out 因心理紧张而被压垮的 highlight v. 强调 突出 emphasize v. 强调 starin v. 扭伤 不堪重负 n. 压力 负担 重负</p><h1 id="UNIT-02"><a href="#UNIT-02" class="headerlink" title="UNIT 02"></a>UNIT 02</h1><p>peer n. 同龄人 同等地位的人 贵族 v. 仔细看 费力地看 contemporary n. 同代人</p><p>Several massive leakages of customer and employee data this year … have left managers hurriedly peering into their intricate IT systems and business processes in search of potential vulnerabilities.</p><p>For less certain, however, is how successfully experts and bureaucrats can select our peer groups and steer their activities in virtuous direction.</p><p>gaze at 凝视 注视 start at 盯 glare at 瞪着 怒视</p><p>issue v. 发行 发表 n. 问题 争端 报刊号 tissue n. 组织 面巾纸</p><p>claim v. 主张 要求 断言 n. 主张 要求 proclaim v. 宣布 明确表示 disclaim v. 放弃 否认 acclaim v. 向…欢呼 为…喝彩</p><p>At any rate, this change will ultimately be acclaimed by an ever-growing number of both domestic and international customers, regardless of how long the current consumer pattern will take hold.</p><p>lay claim to 对…提出所有权要求 allege v. 断言 声称 contend v. 声称 主张 assert v. 声称 断言</p><p>patent v, 授予专利 adj. 专利的 n. 专利</p><p>Over the past decade, thousands of patents have been granted for what are called business methods.</p><p>intellectual property 知识产权</p><p>court n. 法院 球场 庭院 courteous adj. 有礼貌的</p><p>line n. 行 路线 界限 队伍 v. 排队 hardline adj. 强硬的 underline v. 在…下划线 强调 突出 decline v. n. 下降 减少 衰退 online adj. 在线的</p><p>Only if the jobless arrive at the jobcentre with a CV, register for a online job search, and start looking for work will they be eligible for benefit - and then they should report weekly rather than fortnightly.</p><p>offline adj. 离线的 coastline n. 海岸线 deadline n. 最后期限 截止日期</p><p>Assign responsibilities around the house and make sure homework deadlines are met.</p><p>incline v. 倾向 倾斜 易于 baseline n. 基线 基准 airline n. 航线 航空公司 outline n. 轮廓 大纲 概要</p><p>Be flexible, Your outline should smoothly conduct you from one point to next, but do not permit it to railroad you. If a relevant and important idea occurs to you now, work it into the draft.</p><p>lineage n. 血统 家系 家族 pipeline n. 管线 管道 in the pipeline 在筹备中 guideline n. 指导原则 in line with 和… 成直线 与…一致 按照 series n. 一连串 一系列</p><p>His first experiment, nearly 30 years ago, invilved momory: training a person to hear and then repeat a radom series iof numbers.</p><p>row n. 排 行 boundary n. 边界 分界线 bound n. 界限 限制 route n. 线路 succession n. 继承 连续 procession n. 队伍 队列</p><p>value n. 价格 实用性 重要性 v. 评价 估计 重视 overvalue v. 对…估价过高 过分重视</p><p>It was banks that were on the wrong planet, with accounts that vastly overvalued assets.</p><p>devalued adj. 贬值的 worth n. 价值 财产 significance n. 意义 重要性</p><p>view v. 观察 看 n. 观点 景色 眼界 interview n. 面试 采访 v. 采访 viewer n. 电视观众 观看者 指示器 review n. 评论 评审 v. 回顾 复习</p><p>reviewer n. 批评家 评论家 worldview n. 世界观 in view of 由于 考虑到 in view 在考虑中 在能看见的范围内 on view 在展出 在容易看见的地方 take sth. in view = take sth. into account 考虑 perspective n. 观点 思考方法 角度</p><p>individual adj. 个人的 单独的 独特的 n. 个人 个体 individually adv. 个别地 单独地 collective adj. 集体的</p><p>But it takes collective scrutiny and acceptance to transform a discovery claim into a mature discovery.</p><p>economic adj. 经济的 经济学的</p><p>We have to suspect that continuing economic growth promotes the development of education even when government don’t force it.</p><p>uneconomic adj. 不经济的 不赢利的 economics n. 经济学 economically adv. 节俭地 socioeconomic adj. 社会经济的 fiscal cliff 财政悬崖 IMF International Monetary Fund 国际货币基金组织 financial adj. 金融的 财政的</p><p>A smartphone may contain an arrestee’s reading history, financial history, medical history and comprehensive records of recent correspondence.</p><p>fiscal adj. 财政的 国库的 monetary adj. 货币的</p><p>create v. 创造 创作 建立 creature n. 人 生物 动物 creative adj. 创造的 有创造力的 recreate v. 重现 再建 creativity n. 创造力 creation n. 创造物 物品</p><p>So it seems paradoxical to talk about habits in the same context as creativity and innovation.</p><p>creationism n. 创世说 creationist n. 创始者 procreation n. 生育 生殖</p><p>In a society that so persistently celebrates procreations, is it any wonder that admitting you regret having children is equivalent to admitting you support kitten-killing?</p><p>legal adj. 法律的 合法的 illegal adj. 非法的 违法的 n. 非法移民 lawful adj. 合法的 法定的 legitimate adj. 合法的 v. 使合法化</p><p>In effect, the White House claimed that it could invalidate any otherwise legitimate state law that it disagrees with.</p><p>official adj. 官方的 法定的 n. 官员 行政人员</p><p>consider v. 认为 把…看作 consideration n. 体贴 关心 reconsider v. 重新 考虑 considerable adj. 相当多的 considering prep. 考虑到 就…而言 鉴于 consider…as… 把…当作…</p><p>No shock there, considering how much work it is to raise a kid withour a partner to lean on.</p><h1 id="UNIT-03"><a href="#UNIT-03" class="headerlink" title="UNIT 03"></a>UNIT 03</h1><p>subject v. 使遭受 使服从 n. 主题 学科 adj. 受…支配</p><p>At the very least, the court should make itself subject to the code of conduct that applied to the rest of the federal judiciary.</p><p>subjective adj. 主观的 be subject to 遭受 承受</p><p>In addition, the computer programs a company uses to estimate relationships may be patented and not subject to peer review or outside evaluation.</p><p>lead v. 领导 带来 促进 n. 带领</p><p>Science and technology would cure all the ills of humanity, leading to lives of fulfillment and opportunity for all.</p><p>leadership n. 领导能力 mislead v. 误导 leader n. 领导者</p><p>Research has found IQ predicted leadership skills when the tests were given under low-stress conditions, but under high-stress conditions, IQ was negatively correlated with leadership - that is, it predicted the opposite.</p><p>plead v. 请求 提出…为理由</p><p>First, the object of our study pleads for definition.</p><p>“Dare to be different, please don’t smoke!” pleads one billboard campaign aimed at reducing smoking among teenagers - teenagers, who desire nothing more than fitting in.</p><p>leading adj. 卓越的 最重要的</p><p>However, many leading American universities want their undergraduates to have a grounding in the basic canon of ideas that every educated person should possess.</p><p>lead someone on 误导某人 蒙骗某人 lead up to 导致  转向 为…作准备 take the lead 带头 领导</p><p>intend v. 想要 打算 企图 intend to do / inteng doing sth.</p><p>sector n. 部门</p><p>But many within the public sector suffer under the current system, too.</p><p>account v. 说明 解释 把…视作 n. 账目 账户 描述 解释</p><p>It is painful to read these roundabout accounts today.</p><p>It was bank that were on the wrong planet, with accounts that vastly overvalued assets.</p><p>Many, like the Fundamental Physics Prize, are funded from the telephone-number-sized banks accounts of Internet entrepreneurs.</p><p>accountant n. 会计师 会计的 account for 占据 解释 说明</p><p>What might account for this strange phenomenon.</p><p>take into account 考虑到 体谅 leave sth. out of account 不考虑 on account of 因为 由于</p><p>The need of training is too evident;  the pressure to accomplish a change in their attitude and habits is too urgent to leave these consequency wholly out of account.</p><p>description n. 描述 形容 explanation n. 解释 说明</p><p>consumer n. 顾客 消费者</p><p>These labels encourage style-conscious consumer to see clothes as disposable - meant to last only a wash or two, although they don’t advertise taht - and to renew their wardrobe every few weeks.</p><p>consumption n. 消耗 消费</p><p>But despite some claim to the contrary, laughing probably has little influence on physical fitness. Laughter does produce short-term changes in the function of the heart and its blood vessels, boosting heart rate and oxygen consupmtion. But because hard laughter is difficult to sustain, a good laugh is unlikely to have manageable benefits the way, say, walking or jogging does.</p><p>consuming adj. 消耗的 消费的 consumerism n. 消费主义 消费者权益保护 client n. 客户 顾客 委托人</p><p>But there are few places where clients have more grounds for complaint than American.</p><p>environmental adj. 环境的 有关环境的 environmentally adv. 有关环境地 environmentallist n. 环境保护论者 surroundings n. 周边 周围环境 atmosphere n. 大气层 氛围 situation n. 状况 形势 essay n. 论文 article n. 文章 论文 matter n. 物质 v. 有关系</p><p>Anyone who has toiled through SAT will testify that test-taking skill also matters, whether it’s knowing when to guess or what questions to skip.</p><p>as a matter of fact 其实 实际上 substance n. 物质 重要性 实质 stuff n. 材料 本质 要素 material n. 物质 材料 原料</p><p>fund v. 拨款 为… 提供资金 n. 专款 基金 资金</p><p>Many, like the Fundamental Physics Prize, are funded from the telephone-number-sized bank accounts of internet entrepreneurs.</p><p>fundamental adj. 基本的 根本的 fundamentally adv. 从根本上地 基础地</p><p>The traditional rule was it’s safer to stay where you are, but that’s been fundamentally inverted, says one headhunter.</p><p>underfund v. 对…资金提供不足</p><p>hold v. 保持 n. 船舱</p><p>Home prices are holding steady in most regions.</p><p>household n. 家庭 一家人 同住在一所房子里 adj. 家庭的 家喻户晓的 众人皆知的</p><p>My salary barely covered her household expendes.</p><p>holder n. 支持物 支持者 持有人</p><p>The Federal Circuit’s action comes in the wake of a series of recent decisions by the Supreme Court that has narrowed  the scope of protections for patent holders.</p><p>withhold v. 扣留 拒绝 给予 shareholder n. 股东</p><p>As boards scrutinize succession plans in response to shareholder pressure, executives who don’t get the node also may wish to move on.</p><p>shareholding n. 股权 stockholder n. 股权持有者 股东 stakeholder n. 利益相关人 股东</p><p>Such hijacked media are the opposite of earned media: an asset or campaign becomes hostage to consumer, other stakeholders, or activists who make negative allegations about a brand or product.</p><p>hold back 退缩 阻止 抑制 hold on 等一等 别挂电话 坚持 hold out 伸出 递出 hold up 举起 支持 拦截</p><p>cling v. 紧紧抓住 抱紧不放 withstand v. 经守住 承受 顶住</p><p>Moreover, even though humans have been upright for millions of years, our feet and back continnue to struggle with bipedal posture and cannot easily withstand repeated strain inflicted by over size limbs.</p><p>seize v. 抓住 控制</p><p>function v. 起作用 n. 功能 作用 functional adj. 功能性的 运转正常的 work v. 使工作 employ v. 使用 利用 雇用</p><p>His function is analogous to that of a judge, who must accept the obligation of revealing in as abvious a manner as possible the course of reasoning which lead him to this dicision.</p><p>evidence n. 证据 迹象 evident adj. 明显的</p><p>Even though there is plenty of evidence that the quality of the teachers is the most important variable, teachers’ unions have fought against getting rid of bad ones and promoting good ones.</p><p> The need of training is too evident; the pressure to accomplish a change in their attitude and habits is too urgent to leave these consequences wholly out of account.</p><p>proof n. 证明 confirmation n. 确认 证实</p><p>Little reward accompanies duplication and confirmation of what is already known and believed.</p><p>practice v. / n. 练习 训练 in practice 在实践中 实际上 事实上 perform v. 执行 履行</p><p>But in the everyday practice of science, discovery frequently follows an ambiguous an complicated route.</p><p>perform an impressive variety of interesting compositions.</p><p>note v. 记下 n. 笔记 denote v. 代表 表示 northworthy 值得注意的 显著的 重要的 take / make notes 作笔记</p><p>If you were to examine in the birth certificates of every soccer player in 2006’s World Cup tournament you would most likely find a noteworthy quirk.</p><h1 id="UNIT-04"><a href="#UNIT-04" class="headerlink" title="UNIT 04"></a>UNIT 04</h1><p>degree n. 程度 读书 学位 登记</p><p>I struggled a lot to get the college degree.</p><p>agreement n. 协定 一致 同意</p><p>There is the so-called big deal, where institutional subscribers pay for access to a collection of online journal titles through sitelicensing agreement.</p><p>Greek n. 希腊人 希腊文 adj. 希腊的 希腊人的</p><p>Ancient Greek philosopher Aristotle viewed laughter as “a bodily exercise precious to health.”</p><p>concern n. 关心 v. 涉及</p><p>Certainly, there are valid concerns about the patchwork regulations that could result if every state sets its own rules.</p><p>As Nature has pointed out before, there are some legitimate concerns about how science prizes - both new and old - are distribute.</p><p>concerned adj. 关心的 担心的 认为重要的 unconcerned adj. 不关心的 无忧虑的 concern oneslef with 关心 as / so far as … be concerned 就…来说</p><p>allow v. 允许</p><p>In the last decade or so, advances in technology have allowed mass-market labels such as Zara, H&amp;M, and Uniqlo to react to trends more quickly and anticipate demand more precisely. </p><p>However, the Justices said that Arizona police would be allowed to verify the legal status of people who come in contact with law enforcement.</p><p>shallow v. 变浅 adj. 浅的 allow for 考虑到 估计 允许有</p><p>product n. 产品 production n. 产品 作品 productivity n. 生产力 生产率 生产能力 invention n. 发明物</p><p>Only when humanity began to get its food in a more productive way was there time for other things.</p><p>As education improved humanity’s productivity protential, they could in turn afford more education.</p><p>level n. 水平 adj. 水平的 v. 弄平 low-level adj. 低水平的 次要的</p><p>At the state level their influence can be even more fearsome.</p><p>Left, until now, to odd, low-level IT staff to put right，and seen as a concern only of data-rich industries such as banking, telecoms and air travel, information protection is now hight on the boss’s agenda in business of every variety.</p><p>effort n. 努力 艰难的尝试</p><p>Though serveral fast-fashion companies have made efforts to curb their compact on labor and environment - including H&amp;M, with its green conscious collection line - cline believes lasting change can only be effected by the customer.</p><p>effortless adj. 不费力气的</p><p>infer v. 推论 推断 inferiority n. 劣势 下等 次级 自卑感 inferior adj. 低等的 n. 部下 inference n. 推论 推断 deduce v. 推论 推断 conclude v. 得出结论 推断出</p><p>This seems a justification for neglect of those in need,and a rationalization of exploitation, of the superiority of those at the top and the inferiority of those at the bottom.</p><p>professional n.  专业人士 adj. 职业的</p><p>If you then examined the European national youth teams that feed the World Cup and professional ranks, you would find this strange phenomenon to be even more pronounced.</p><p>professionalize v. 使…职业化 </p><p>Besides professionalizing the professions by this separation, top American universities have professionlized the professor.</p><p>professionalization n. 职业化 professionlism n. 敬业精神</p><p>Professionlism has turned the acquisition of a doctoral degress into a prerequisite for a successful academic career: as late as 1696, a third of American professors did not possess one.</p><p>provide v. 提供 provided conj. 倘若 假如 provider n. 提供者 供应者 provision n. 供应 条款</p><p>The program keep track of your progress and provides detailed feedback on your performance amd improvement.</p><p>The same drematic technological changes that have provide marketers with more conmunications choices have also increased the risk that passionate consumers will voice thier opinions in quicker, more visible, and much more damaging ways.</p><p>This trend, which we believe is still in its infancy, effectively began with retailers and travel providers such as airlines and hotels will no doubt go further.</p><p>supply v. 供给 补充 offer v. 提供 给予</p><p>You are now not wanted; you are now exclued from the work environmrent that offers purpose and structure in your life.</p><p>challenge n. / v. 挑战 怀疑 unchallended adj. 没有挑战的 没有异议的</p><p>deal n. 数量 v. 处理</p><p>What we need is a package deal.</p><p>From the middle-class family perspective, much of this, understanably, looks far less like an opportunity to exercise more financial reponsibility, and a good deal more like a frightening acceleration of the wholesale shift of financial risk onto their already overburdened shoulders.</p><p>ideal n. 理想 adj. 理想的 非常合适的 dealing n. 行为 交易 dealership n. 代理权 deal with 处理 应付 安排 a great deal of 大量的 contract n.  合同 契约 v. 缩小</p><p>If humanity has made some headway in realizing that ultimate value of every institution is its distinctively huaman effect, we may well believe that this lesson has been learned largely through dealings with the younth.</p><p>Those forced to exercise their smiling muscles reacted more exuberantly to funny cartoons than did those whose mouths were contracted in a frown, suggesting that expressions may influence emotions rather than  just the other way other.</p><p>structure v. 构造 n. 建筑物</p><p>The other reason why costs are so high is the retrictive guild-like ownership structure of the business.</p><p>restructuring n. 改组 重新组织 调整 structural adj. 结构的 建筑的 infrastructure n. 基础措施</p><p>All in all, this clearly seems to be a market in which big retailers could profitably apply their scale, existing infrastructure and proven skills in the management of product ranges, logistics, and marketing intelligence.</p><p>define v. 解释 给…下定义</p><p>The define term of intelligence in humans still seems to be the IQ score, even though IQ tests are not given as they used to be.</p><p>content v. 使满足 n. 容量 内容 满足 adj. 满足的 满意的</p><p>Even if a job’s starting salary seems too small to satisfy an emerging adult’s need for rapid content, the transition from school to work can be less of a setback if the start-up adult is ready for the move.</p><p>The commission ignores that for serveral decades America’s colleges and universities have produced graduates who don’t know the content and character of liberal education and thus deprived of its benefits.</p><p>contention n. 争吵 看法 观点</p><p>Part of the fame of Allen’s book is its contention that “Circumstances do not make a person, they reveal him.”</p><p>discontent n. 不满意 adj. 不满的 v. 使…不满 self-contented 自满的 沾沾自喜的</p><p>immigrant adj. 移民的 n. 移民 immigration n. 移民 emigrate v. 移居国外 移民</p><p>Immigrants are quickly fitting into this common culture, which may not be altoghter elevating but is hardly poisonous.</p><p>On a five to three vote, the Supreme Court knocked out much of Arizona’s immigration law Monday - a modest  policy victory for the Obama Administration.</p><p>comprehension n. 理解 领悟 comprehensive adj. 全面的 能充分了解的 n. 综合中学</p><p>They would try to decide what intelligence in humans is really for, not merely how much of it there is. Above all, they would hope to study a fundamental question: Are humans actually aware of the world they live in?</p><p>A smartphone may contain an arrestee’s reading history, financial history, medical history and comprehensive records of recent correspondence.</p><p>beyond one’s comprehension 不可理解</p><p>potential n. 潜能 潜力 adj. 潜在的 可能的</p><p>Basic eonomics suggests that greater the potential consumers, the higher the likelihood of a better price.</p><p>possible adj. 可能的 潜在的 probable adj. 可能的 probably adv.</p><p>mass adj. 大规模的 群众的 大量的 n. 大量 民众 massive adj. 大量的</p><p>The mass media, advertising and sports are other forces for homogenization.</p><p>Several massive leakages of customer and employee data this year - from organizations as diverse as Time Warner.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第一次学习时间：2022年08月02日。&lt;/p&gt;</summary>
    
    
    
    <category term="英语" scheme="https://wingowen.github.io/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
    <category term="英语单词" scheme="https://wingowen.github.io/tags/%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>Python 编程</title>
    <link href="https://wingowen.github.io/2022/08/01/%E7%BC%96%E7%A8%8B/Python-%E7%BC%96%E7%A8%8B/"/>
    <id>https://wingowen.github.io/2022/08/01/%E7%BC%96%E7%A8%8B/Python-%E7%BC%96%E7%A8%8B/</id>
    <published>2022-08-01T03:17:02.000Z</published>
    <updated>2022-09-09T11:27:08.781Z</updated>
    
    <content type="html"><![CDATA[<p>Python 编程开发查漏补缺。</p><p>gRPC</p><p>Redis</p><span id="more"></span><h1 id="GRPC"><a href="#GRPC" class="headerlink" title="GRPC"></a>GRPC</h1><p>Google 开发的基于 HTTP/2 和 Protocol Buffer 3 的 RPC 框架。</p><p>Protocol Buffers, protobuf：结构数据序列化机制。</p><p>gRPC 默认使用 Brotocol Buffers，用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。</p><p>定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 GRPC 服务器来处理客户端调用。在客户端拥有一个存根 Stub，存根负责接收本地方法调用，并将它们委派给各自的具体实现对象（在远程服务器上）。</p><p><img src="http://wingowen.gitee.io/image/2022/Python-编程/gRPC.png" alt="Concept Diagram"></p><h2 id="简单实现"><a href="#简单实现" class="headerlink" title="简单实现"></a>简单实现</h2><p>实现一个简单的 gRPC HelloWorld。</p><h3 id="proto-file"><a href="#proto-file" class="headerlink" title="proto file"></a>proto file</h3><p>定义 Protocol Buffers 规则文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">syntax = &quot;proto3&quot;;</span><br><span class="line"></span><br><span class="line">package helloworld;</span><br><span class="line"></span><br><span class="line">service Greeter &#123;</span><br><span class="line">    // 定义方法参数和返回类型</span><br><span class="line">    rpc SayHello (HelloRequest) returns (HelloResponse) &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 请求结构声明</span><br><span class="line">message HelloRequest &#123;</span><br><span class="line">    string name = 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 响应结构声明</span><br><span class="line">message HelloResponse &#123;</span><br><span class="line">    string message = 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 grpc_tools 工具。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. helloworld.proto</span><br></pre></td></tr></table></figure><p> 生成 python 代码。</p><p><code>helloworld_pb2.py</code> 为 Protocol Buffers 的 Python 实现。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># helloworld_pb2_grpc.py 用于 gRPC 实现的 Python 方法实现</span><br><span class="line"></span><br><span class="line"># 客户端存根</span><br><span class="line">class GreeterStub(object):</span><br><span class="line">    def __init__(self, channel):</span><br><span class="line">        self.SayHello = channel.unary_unary(</span><br><span class="line">                &#x27;/helloworld.Greeter/SayHello&#x27;,</span><br><span class="line">                request_serializer=helloworld__pb2.HelloRequest.SerializeToString,</span><br><span class="line">                response_deserializer=helloworld__pb2.HelloResponse.FromString,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line"># 服务端服务</span><br><span class="line">class GreeterServicer(object):</span><br><span class="line">    def SayHello(self, request, context):</span><br><span class="line">        context.set_code(grpc.StatusCode.UNIMPLEMENTED)</span><br><span class="line">        context.set_details(&#x27;Method not implemented!&#x27;)</span><br><span class="line">        raise NotImplementedError(&#x27;Method not implemented!&#x27;)</span><br><span class="line">        </span><br><span class="line">def add_GreeterServicer_to_server(servicer, server):</span><br><span class="line"># ......</span><br></pre></td></tr></table></figure><h3 id="server"><a href="#server" class="headerlink" title="server"></a>server</h3><p>自定义 gRPC 服务端。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import grpc</span><br><span class="line">import random</span><br><span class="line">from concurrent import futures</span><br><span class="line">import helloworld_pb2</span><br><span class="line">import helloworld_pb2_grpc</span><br><span class="line"></span><br><span class="line"># 实现定义的方法，继承并实现方法</span><br><span class="line">class Greeter(helloworld_pb2_grpc.GreeterServicer):</span><br><span class="line">    def SayHello(self, request, context):</span><br><span class="line">        return helloworld_pb2.HelloResponse(message=&#x27;Hello &#123;msg&#125;&#x27;.format(msg=request.name))</span><br><span class="line"></span><br><span class="line">def serve():</span><br><span class="line">    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))</span><br><span class="line">    # 绑定处理器</span><br><span class="line">    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server)</span><br><span class="line"># 未使用 SSL，所以是不安全的</span><br><span class="line">    server.add_insecure_port(&#x27;[::]:50054&#x27;)</span><br><span class="line">    server.start()</span><br><span class="line">    print(&#x27;gRPC 服务端已开启，端口为 50054...&#x27;)</span><br><span class="line">    server.wait_for_termination()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    serve()</span><br></pre></td></tr></table></figure><h3 id="client"><a href="#client" class="headerlink" title="client"></a>client</h3><p>自定义客户端。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import grpc</span><br><span class="line">import helloworld_pb2, helloworld_pb2_grpc</span><br><span class="line"></span><br><span class="line">def run():</span><br><span class="line">    # 本次不使用 SSL，所以 channel 是不安全的</span><br><span class="line">    channel = grpc.insecure_channel(&#x27;localhost:50054&#x27;)</span><br><span class="line">    # 客户端实例</span><br><span class="line">    stub = helloworld_pb2_grpc.GreeterStub(channel)</span><br><span class="line">    # 调用服务端方法</span><br><span class="line">    response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#x27;World&#x27;))</span><br><span class="line">    print(&quot;Greeter client received: &quot; + response.message)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure><h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><p>REmote DIctionary Server, Redis 是一个 key-value 存储系统，是跨平台的非关系型数据库。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pip install redis</span><br><span class="line"></span><br><span class="line">import redis   # 导入redis 模块</span><br><span class="line"></span><br><span class="line"># 获取连接</span><br><span class="line">r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)  </span><br><span class="line"># Redis 实例会维护一个自己的连接池，建立连接池，从连接池获取连接</span><br><span class="line">pool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, decode_responses=True)</span><br><span class="line">r = redis.Redis(connection_pool=pool)</span><br><span class="line"></span><br><span class="line">r.set(&#x27;name&#x27;, &#x27;runoob&#x27;, nx, xx)  # 设置 name 对应的值, 当 nx = Ture 则只有 Key 不存在才执行插入; xx 相反</span><br><span class="line"></span><br><span class="line">print(r[&#x27;name&#x27;])</span><br><span class="line">print(r.get(&#x27;name&#x27;), px, ex)  # 取出键 name 对应的值, px 毫秒 ex 秒 为过期时间</span><br><span class="line">print(type(r.get(&#x27;name&#x27;)))  # 查看类型</span><br></pre></td></tr></table></figure><p>在使用中，Redis 存储可分为两大类：</p><ul><li>set 即 k v，这里的 v 通常是一个字符串。</li><li>hset 即 k Hash-v，这里的 v 是一个 Redis Hash，是一个 string 类型的 field（字段）和 value（值）的映射表。</li></ul><h1 id="缓存技术"><a href="#缓存技术" class="headerlink" title="缓存技术"></a>缓存技术</h1><p>缓存就是利用编程技术将数据存储在临时位置，而不是每次都从源数据去检索。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Python 编程开发查漏补缺。&lt;/p&gt;
&lt;p&gt;gRPC&lt;/p&gt;
&lt;p&gt;Redis&lt;/p&gt;</summary>
    
    
    
    <category term="编程" scheme="https://wingowen.github.io/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
    <category term="Python" scheme="https://wingowen.github.io/tags/Python/"/>
    
    <category term="gRPC" scheme="https://wingowen.github.io/tags/gRPC/"/>
    
  </entry>
  
  <entry>
    <title>决策树算法</title>
    <link href="https://wingowen.github.io/2022/07/31/%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/"/>
    <id>https://wingowen.github.io/2022/07/31/%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95/</id>
    <published>2022-07-31T02:58:15.000Z</published>
    <updated>2022-08-10T15:03:29.129Z</updated>
    
    <content type="html"><![CDATA[<p>基本概念。</p><span id="more"></span><h1 id="决策树的基本概念"><a href="#决策树的基本概念" class="headerlink" title="决策树的基本概念"></a>决策树的基本概念</h1><p>决策树节点：</p><ul><li>叶节点表示一份类别或者一个值；</li><li>非叶节点表示一个属性划分。</li></ul><p>决策树的有向边代表了属性划分的不同取值：</p><ul><li>当属性是离散时，可将属性的每一个取值用一条边连接到子结点；</li><li>当属性是连续时，需要特殊处理。</li></ul><p>决策树是一种描述实例进行分类的树形结构。</p><p>对于某个样本，决策树模型将从根结点开始，对样本的某个属性进行测试，根据结果将其划分到子结点中，递归进行，直至将其划分到叶结点的类中。这个过程产生了从根结点到叶结点的一条路径，对应了一个测试序列。</p><p><img src="/IMAGE/决策树算法/决策树的学习过程.png" alt="img"></p><p>决策树学习的目的是为了产生一根泛化能力强的决策树，其基本流程遵循了分而治之策略；决策树的学习过程本质上是从训练数据中寻找一组分类规则；决策树学习也可以看做是由训练数据集估计条件概率模型。</p><p>由上述描述可以得知，决策树学习是一个递归过程，有三种情况会导致递归返回：</p><ul><li><p>当前结点包含的所有样本属于同一类别。</p></li><li><p>当前属性结合为空，或所有样本在所有属性上的取值都相同：将当前结点标记为叶结点，其类别为该节点包含的样本最多的类别。</p></li><li><p>当前结点包含的样本基本为空：将当前结点标记为叶结点，其类别为父节点包含样本最多的类别</p></li></ul><p>决策树的学习结果为：树结构 + 叶节点的取值（类别）</p><h1 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h1><p>熵，又称信息熵，是信息论的重要概念。熵是度量样本集合纯度的指标，熵越大，样本的纯度越低。假设当前样本集合 $D$ 中第 $i$ 类样本所占比例的为 $p_i(i = 1,2,…,C)$，则 $D$ 的熵定义为：</p><script type="math/tex; mode=display">H(D)=-\sum_{i=1}^{C} p_{i} \log _{2} p_{i}</script><p>信息增益表示特征对于当前样本集纯度提升的程度。某属性的信息增益越大，说明使用改属性进行划分获得的纯度提升越大。因此使用信息增益进行决策树属性选择时，选择属性信息增益最大的作为当前节点。</p><script type="math/tex; mode=display">G(D, a)=H(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} H\left(D^{v}\right)</script><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 信息增益计算</span><br><span class="line"></span><br><span class="line">def get_G(data, index, clss_idx):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    求样本集某个属性的信息增益</span><br><span class="line">    :param data: 数据集, type:pandas.DataFrame</span><br><span class="line">    :param index: 属性索引, type:int, e.g.: 1</span><br><span class="line">    :param clss_idx: 样本类别的索引, type:int, e.g.:4</span><br><span class="line">    :return: index属性的信息增益, type:float, e.g.:0.32</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    H = 0</span><br><span class="line">    for value in np.unique(data.iloc[:,clss_idx]):</span><br><span class="line">        p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0]</span><br><span class="line">        H = H - p * np.log2(p)</span><br><span class="line">    E = 0</span><br><span class="line">    for v in np.unique(data.iloc[:,index]):</span><br><span class="line">        new_data = data[data.iloc[:,index] == v]</span><br><span class="line">        # new_data 中所有样本属于同一类，由于 xlnx 在 x = 1和 x-&gt;0 是都为0，故无需计算该项</span><br><span class="line">        if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1:</span><br><span class="line">            continue</span><br><span class="line">        TE = 0</span><br><span class="line">        for value in np.unique(data.iloc[:,clss_idx]):</span><br><span class="line">            p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0]</span><br><span class="line">            TE = TE - p * np.log2(p)</span><br><span class="line">        E = E  + new_data.shape[0] / data.shape[0] * TE</span><br><span class="line">    return H - E</span><br><span class="line"></span><br><span class="line">print(&#x27;各个属性的信息增益为&#x27;)</span><br><span class="line">for i in range(len(data.columns)-1):</span><br><span class="line">     print(data.columns[i],get_G(data,i,len(data.columns)-1))</span><br></pre></td></tr></table></figure><h1 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h1><p>以信息增益作为划分数据集的特征，会导致对可取值数目较多的属性有所偏好。为了缓解这种不良影响，采用信息增益比作为选择属性的准则。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def get_GR(data, index, clss_idx):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    求样本集某个属性的信息增益比</span><br><span class="line">    :param data: 数据集, type:pandas.DataFrame</span><br><span class="line">    :param index: 属性索引, type:int, e.g.: 1</span><br><span class="line">    :param clss_idx: 样本类别的索引, type:int, e.g.:4</span><br><span class="line">    :return: index属性的信息增益比, type:float, e.g.:0.32</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    H = 0</span><br><span class="line">    for value in np.unique(data.iloc[:,clss_idx]):</span><br><span class="line">        p = np.sum(data.iloc[:,clss_idx] == value) / data.shape[0]</span><br><span class="line">        H = H - p * np.log2(p)</span><br><span class="line">    E = 0</span><br><span class="line">    IV = 0</span><br><span class="line">    for v in np.unique(data.iloc[:,index]):</span><br><span class="line">        new_data = data[data.iloc[:,index] == v]</span><br><span class="line">        # new_data中所有样本属于同一类，由于xlnx 在x = 1和x-&gt;0是都为0，故无需计算该项</span><br><span class="line">        if np.unique(new_data.iloc[:,clss_idx]).shape[0] == 1:</span><br><span class="line">            continue</span><br><span class="line">        TE = 0</span><br><span class="line">        for value in np.unique(data.iloc[:,clss_idx]):</span><br><span class="line">            p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0]</span><br><span class="line">            TE = TE - p * np.log2(p)</span><br><span class="line">        E = E  + new_data.shape[0] / data.shape[0] * TE</span><br><span class="line">        IV = IV - new_data.shape[0] / data.shape[0] * (np.log2(new_data.shape[0] / data.shape[0]))</span><br><span class="line">    G = H - E</span><br><span class="line">    return G / IV</span><br><span class="line">print(&#x27;各个属性的信息增益比为&#x27;)</span><br><span class="line">for i in range(len(data.columns)-1):</span><br><span class="line">    print(data.columns[i],get_GR(data,i,len(data.columns)-1))</span><br></pre></td></tr></table></figure><h1 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h1><p>纯度使用基尼指数来度量，在使用基尼系数作为指标时，应该选择基尼指数最小的属性。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def get_Gini(data, index, clss_idx):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    求样本集某个属性的基尼系数</span><br><span class="line">    :param data: 数据集, type:pandas.DataFrame</span><br><span class="line">    :param index: 属性索引, type:int, e.g.: 1</span><br><span class="line">    :param clss_idx: 样本类别的索引, type:int, e.g.:4</span><br><span class="line">    :return: index属性的基尼系数, type:float, e.g.:0.32</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    Gini = 0</span><br><span class="line"></span><br><span class="line">    for v in np.unique(data.iloc[:,index]):</span><br><span class="line">        new_data = data[data.iloc[:,index] == v]</span><br><span class="line">        Gini_v = 1</span><br><span class="line"></span><br><span class="line">        for value in np.unique(data.iloc[:,clss_idx]):</span><br><span class="line">            p = np.sum(new_data.iloc[:,clss_idx] == value) / new_data.shape[0]</span><br><span class="line">            Gini_v = Gini_v - p * p</span><br><span class="line"></span><br><span class="line">        Gini = Gini + new_data.shape[0] / data.shape[0] * Gini_v</span><br><span class="line">    return Gini</span><br><span class="line">print(&#x27;各个属性的基尼指数为&#x27;)</span><br><span class="line">for i in range(len(data.columns)-1):</span><br><span class="line">    print(data.columns[i],get_Gini(data,i,len(data.columns)-1))</span><br></pre></td></tr></table></figure><h1 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h1><p>ID3 算法的核心是在决策树各结点上使用信息增益准则选择特征：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，根据特征的不同取值建立子结点。递归地调用以上方法，构建决策树。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">def build_tree_id3(data, fa, ppt_list, clss_idx):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    使用 ID3 算法在 data 数据集上建立决策树</span><br><span class="line">    :param data: 数据集, type:pandas.DataFrame</span><br><span class="line">    :param fa: 父结点, type:pandas.DataFrame</span><br><span class="line">    :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2]</span><br><span class="line">    :param clss_idx: 样本类别的索引, type:int, e.g.:4</span><br><span class="line">    :return: 决策树的根结点, type:Node</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    nu = Node(data, fa, ppt_list)</span><br><span class="line"></span><br><span class="line">    if len(np.unique(data.iloc[:, clss_idx])) == 1:</span><br><span class="line">        kind = data.iloc[:, clss_idx].value_counts().keys()[0]</span><br><span class="line">        nu.set_leaf(kind)</span><br><span class="line">        return nu</span><br><span class="line"></span><br><span class="line">    if len(ppt_list) == 0:</span><br><span class="line">        kind = data.iloc[:, clss_idx].value_counts().keys()[0]</span><br><span class="line">        nu.set_leaf(kind)</span><br><span class="line">        return nu</span><br><span class="line"></span><br><span class="line">    best = -10000000</span><br><span class="line">    best_ppt = -1</span><br><span class="line">    for ppt in ppt_list:</span><br><span class="line">        G = get_G(data, ppt, clss_idx)</span><br><span class="line">        if G &gt; best:</span><br><span class="line">            best = G</span><br><span class="line">            best_ppt = ppt</span><br><span class="line"></span><br><span class="line">    new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt))</span><br><span class="line"></span><br><span class="line">    for v in np.unique(data.iloc[:, best_ppt]):</span><br><span class="line">        new_data = data[data.iloc[:, best_ppt] == v]</span><br><span class="line"></span><br><span class="line">        ch_node = build_tree_id3(new_data, nu, new_ppt_list, clss_idx)</span><br><span class="line">        nu.add_child(ch_node)</span><br><span class="line"></span><br><span class="line">        if ch_node.is_leaf:</span><br><span class="line">            nu.add_leaf_ch(ch_node)</span><br><span class="line">        else :</span><br><span class="line">            for nd in ch_node.leaf_ch:</span><br><span class="line">                nu.add_leaf_ch(nd)</span><br><span class="line">    return nu</span><br><span class="line"></span><br><span class="line">ori_ppt = np.arange(len(data.columns)-1)</span><br><span class="line">root_id3 = build_tree_id3(data, None, ori_ppt, len(data.columns)-1)</span><br><span class="line"># 可视化</span><br><span class="line">createPlot(root_id3)</span><br></pre></td></tr></table></figure><h1 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h1><p>C4.5 算法对 ID3 算法进行了改进，即使用信息增益比来选择特征，其余和 ID3 算法基本相同。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def build_tree_c45(data, fa, ppt_list, clss_idx):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    使用C4.5算法在data数据集上建立决策树</span><br><span class="line">    :param data: 数据集, type:pandas.DataFrame</span><br><span class="line">    :param fa: 父结点, type:pandas.DataFrame</span><br><span class="line">    :param ppt_list: 属性索引列表, type:list, e.g.: [0,1,2]</span><br><span class="line">    :param clss_idx: 样本类别的索引, type:int, e.g.:4</span><br><span class="line">    :return: 决策树的根结点, type:Node</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    nu = Node(data, fa, ppt_list)</span><br><span class="line"></span><br><span class="line">    if len(np.unique(data.iloc[:, clss_idx])) == 1:</span><br><span class="line">        kind = data.iloc[:, clss_idx].value_counts().keys()[0]</span><br><span class="line">        nu.set_leaf(kind)</span><br><span class="line">        return nu</span><br><span class="line"></span><br><span class="line">    if len(ppt_list) == 0:</span><br><span class="line">        kind = data.iloc[:, clss_idx].value_counts().keys()[0]</span><br><span class="line">        nu.set_leaf(kind)</span><br><span class="line">        return nu</span><br><span class="line"></span><br><span class="line">    best = -10000000</span><br><span class="line">    best_ppt = -1</span><br><span class="line">    for ppt in ppt_list:</span><br><span class="line">        G = get_GR(data, ppt, clss_idx)</span><br><span class="line">        if G &gt; best:</span><br><span class="line">            best = G</span><br><span class="line">            best_ppt = ppt</span><br><span class="line"></span><br><span class="line">    new_ppt_list = np.delete(ppt_list, np.where(ppt_list == best_ppt))</span><br><span class="line"></span><br><span class="line">    for v in np.unique(data.iloc[:, best_ppt]):</span><br><span class="line">        new_data = data[data.iloc[:, best_ppt] == v]</span><br><span class="line"></span><br><span class="line">        ch_node = build_tree_c45(new_data, nu,new_ppt_list, clss_idx)</span><br><span class="line">        nu.add_child(ch_node)</span><br><span class="line"></span><br><span class="line">        if ch_node.is_leaf:</span><br><span class="line">            nu.add_leaf_ch(ch_node)</span><br><span class="line">        else :</span><br><span class="line">            for nd in ch_node.leaf_ch:</span><br><span class="line">                nu.add_leaf_ch(nd)</span><br><span class="line">    return nu</span><br><span class="line">ori_ppt = np.arange(len(data.columns)-1)</span><br><span class="line"># print(data)</span><br><span class="line">root_c45 = build_tree_c45(data, None ,ori_ppt, len(data.columns)-1)</span><br><span class="line">createPlot(root_c45)</span><br></pre></td></tr></table></figure><h1 id="损失函数与剪枝"><a href="#损失函数与剪枝" class="headerlink" title="损失函数与剪枝"></a>损失函数与剪枝</h1><p>决策树的剪枝往往通过最小化决策树的损失函数实现。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def cal_loss(root, alpha):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    计算以root为根结点的决策树的损失值</span><br><span class="line">    :param root: 根结点, type:Node</span><br><span class="line">    :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3</span><br><span class="line">    :return: 以root为根结点的决策树的损失值, type:float, e.g.:0.24</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    loss = 0</span><br><span class="line">    for leaf in root.leaf_ch:</span><br><span class="line">        data = leaf.data</span><br><span class="line">        for v in np.unique(data.iloc[:,len(data.columns)-1]):</span><br><span class="line">            ntk = data[data.iloc[:,len(data.columns)-1] == v].shape[0]</span><br><span class="line">            loss = loss - ntk * np.log2(ntk / data.shape[0])</span><br><span class="line"></span><br><span class="line">    loss = loss + alpha * len(root.leaf_ch)</span><br><span class="line"></span><br><span class="line">    return loss</span><br><span class="line">ori_ppt = np.arange(len(data.columns)-1)</span><br><span class="line">root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)</span><br><span class="line">cal_loss(root_c45, 0.3)</span><br></pre></td></tr></table></figure><p>决策树生成算法递归地产生决策树，直到无法继续。这种做法会带来过拟合问题。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，构建过于复杂的决策树。因此，一种解决方法是考虑决策树的复杂程度，从而对决策树进行简化。对决策树进行简化的过程称为剪枝。即从决策树中裁掉一些子树或叶结点，将其根节点或父节点作为新的叶结点。</p><p><strong>剪枝算法的实现</strong></p><p>计算每个节点的经验熵。</p><p>递归地从树的叶结点向上回缩，若回缩后的损失值 &gt; 回缩前的损失值，则进行剪枝，父节点变为叶节点。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def tree_pruning(root, leaf, alpha):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    决策树剪枝</span><br><span class="line">    :param root: 根结点, type:Node</span><br><span class="line">    :param leaf: 叶结点, type:Node</span><br><span class="line">    :param alpha: 损失函数中定义的参数, type:float, e.g.:0.3</span><br><span class="line">    :return: 剪枝后的决策树根结点, type:Node</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    new_root = copy.deepcopy(root)</span><br><span class="line">    pre_loss = cal_loss(root, alpha)</span><br><span class="line">    flag = 1</span><br><span class="line"></span><br><span class="line">    for nl in leaf.fa.leaf_ch.copy():</span><br><span class="line">        nl.can_delete = 1</span><br><span class="line"></span><br><span class="line">    new_set = set()</span><br><span class="line">    for leaf_ch in root.leaf_ch:</span><br><span class="line">        if leaf_ch.can_delete != 1:</span><br><span class="line">            new_set.add(leaf_ch)</span><br><span class="line"></span><br><span class="line">    root.leaf_ch = new_set</span><br><span class="line">    leaf.fa.set_leaf(leaf.fa.data.iloc[:,len(root.data.columns)-1].value_counts().keys()[0])</span><br><span class="line">    root.add_leaf_ch(leaf.fa)</span><br><span class="line">    after_loss = cal_loss(root, alpha)</span><br><span class="line"></span><br><span class="line">    if after_loss &gt;= pre_loss:</span><br><span class="line">        #不剪枝</span><br><span class="line">        root = new_root</span><br><span class="line">        flag = 0</span><br><span class="line">    return root, flag</span><br><span class="line">    </span><br><span class="line">ori_ppt = np.arange(len(data.columns)-1)</span><br><span class="line">root_c45 = build_tree_c45(data, None, ori_ppt, len(data.columns)-1)</span><br><span class="line"># createPlot(root_c45)</span><br><span class="line"></span><br><span class="line">#设置超参数</span><br><span class="line">alpha = 0.3</span><br><span class="line">update = 1</span><br><span class="line">while update == 1:</span><br><span class="line">    update = 0</span><br><span class="line">    for leaf in root_c45.leaf_ch.copy():</span><br><span class="line">        root_c45,flag = tree_pruning(root_c45, leaf, alpha)</span><br><span class="line">        if flag:</span><br><span class="line">            update = 1</span><br><span class="line"># root_c45</span><br><span class="line">createPlot(root_c45)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>连续值处理</p><p>缺失值处理</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;基本概念。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>计算机组成</title>
    <link href="https://wingowen.github.io/2022/07/30/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/"/>
    <id>https://wingowen.github.io/2022/07/30/%E8%80%83%E7%A0%94/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90/</id>
    <published>2022-07-30T07:15:43.000Z</published>
    <updated>2022-09-12T05:28:05.492Z</updated>
    
    <content type="html"><![CDATA[<p>北大计算机组成课程。</p><p>计算机基本结构：冯诺依曼结构，计算机执行指令的过程。</p><p>系统总线</p><span id="more"></span><h1 id="计算机系统概论"><a href="#计算机系统概论" class="headerlink" title="计算机系统概论"></a>计算机系统概论</h1><h2 id="计算机系统层次结构"><a href="#计算机系统层次结构" class="headerlink" title="计算机系统层次结构"></a>计算机系统层次结构</h2><ul><li><p>微程序机器 M0 微指令系统： 由硬件直接执行微命令；</p></li><li><p>实际机器 M1 机器语言机器：用微程序解释机器指令；</p></li><li><p>虚拟机器 M2 操作系统机器：用机器语言解释操作系统；</p></li><li><p>虚拟机器 M3 汇编语言机器：用汇编程序翻译成机器语言程序；</p></li><li><p>虚拟机器 M4 高级语言机器：用编译程序翻译成汇编语言程序或其它中间语言程序。</p></li></ul><h2 id="计算机的基本组成"><a href="#计算机的基本组成" class="headerlink" title="计算机的基本组成"></a>计算机的基本组成</h2><p>冯·诺依曼提出<strong>存储程序</strong>的概念，以此概念为基础的计算机通称为冯·诺依曼计算机，其具有如下特点：</p><ul><li><p>计算机由运算器、存储器、控制器、输入设备和输出设备五大部件组成；</p></li><li><p>指令和数据以同地位存放于存储器内，可按地址寻访；</p></li><li><p>指令和数据均用二进制数表示；</p></li><li><p>指令由操作码和地址码组成，操作码用来表示操作的性质，地址码用来表示操作数在存储器中的位置；</p></li><li><p>指令在存储器内按顺序存放。通常，指令是顺序执行的，在特定条件下，可根据运算结果或根据设定的条件改变执行顺序。</p></li><li><p>机器以运算器为中心，输入输出设备与存储器间的数据传送通过运算器完成。</p></li></ul><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/计算机结构框图.png" alt=""></p><p>各部件的功能如下：</p><ul><li>运算器用来完成算术运算和逻辑运算，并将运算的中间结果暂存在运算器内；</li><li>存储器用来存放数据和程序；</li><li>控制器用来控制、指挥程序的运行、程序的输入输出以及处理运算结果；</li></ul><p>运算器和控制器在逻辑关系和电路结构上联系十分紧密，两大部件往往集成在同一芯片上，因此通常将它们合起来统称为中央处理器 CPU, Central Processing Unit。</p><p>现代计算机组成：CPU, I/O 以及 主存储器 Main Memory, MM。 </p><p>CPU + MM 称为主机；I/O 有称为外部设备。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/现代计算机的组成框图.png" alt="image-20220828111757686"></p><p>Arithmetic Logic Unit, ALU 算术逻辑部件，用来完成算术逻辑运算；Control Unit, CU 控制单元，用来解释存储器中的指令，并发出各种操作命令来执行指令。</p><p>ALU 和 CU 是 CPU 的核心部件；</p><p>I/O 设备也受 CU控制，用来完成相应的输入、输出操作。</p><h2 id="计算机的工作步骤"><a href="#计算机的工作步骤" class="headerlink" title="计算机的工作步骤"></a>计算机的工作步骤</h2><p>TODO</p><h1 id="系统总线"><a href="#系统总线" class="headerlink" title="系统总线"></a>系统总线</h1><p>计算机系统的五大部件之间的互连方式有两种：</p><ul><li>各部件之间使用单独的连线，成为分散连接；</li><li>另一种是各部件连到一组公共信息传输线上，成为总线连接。</li></ul><p>总线是连接多个部件的信息传输线，是各部件共享的传输介质。当多个部件与总线相连时，如果出现两个或两个以上部件同时向总线发送信息，势必导致信号冲突，传输无效。因此，在某一时刻，只允许有一个部件向总线发送信息，而多个部件可以同时从总线上接收相同信息。</p><p>以运算器为中心的结构。I/O 设备与主存交换信息时仍然要占用 CPU，因此还会影响 CPU 的工作效率。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/面向 CPU 的双总线结构框图.png" alt="image-20220828120738527"></p><p>单总线（系统总线）。I/O 设备与主存交换信息时，原则上不影响 CPU 的工作，CPU 仍可继续处理不访问主存或 I/O 设备的操作，提高了 CPU 的工作效率。</p><p>但当某一时刻各部件都要占用总线时，就会发生冲突，必须设置总线判优逻辑，让各部件按优先级高低来占用总线，这也会影响整机的工作速度。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/单总线.png" alt="image-20220828125143622"></p><p>以存储器为中心的双总线结构框图。存储中线只供主存与 CPU 之间传输信息，即提高了传输效率，又减轻了系统总线的负担，还保留了 I/O 设备与存储器交换信息不经过 CPU 的特点。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/以存储器为中心的双总线结构框.png" alt="image-20220828125237500"></p><h2 id="总线分类"><a href="#总线分类" class="headerlink" title="总线分类"></a>总线分类</h2><p>片内总线：芯片内部总线；</p><p>系统总线：各大部件之间的信息数据信息；按系统总线传输信息的不同，又可分为三类<strong>：数据总线、地址总线和控制总线</strong>。</p><p>通信总线：计算机系统之间或与其他系统之间的通信。</p><h2 id="总线特性及性能指标"><a href="#总线特性及性能指标" class="headerlink" title="总线特性及性能指标"></a>总线特性及性能指标</h2><p>TODO</p><h2 id="总线结构"><a href="#总线结构" class="headerlink" title="总线结构"></a>总线结构</h2><p>TODO</p><h2 id="总线控制"><a href="#总线控制" class="headerlink" title="总线控制"></a>总线控制</h2><p>判优控制（仲裁逻辑）和通信控制。</p><p><strong>总线判优控制</strong></p><p>主设备：对总线有控制选；从设备：只能响应从主设备发来的总线命令，对总线没有控制权。</p><p>若多个主设备同时要使用总线时，由总线的判优、仲裁逻辑按一定的优先等级顺序确定哪个主设备能使用总线，只有获得总线使用权的主设备才能开始传送数据。</p><p>总线判优控制可分集中式和分布式两种：</p><ul><li>将控制逻辑集中在一处；</li><li>将控制逻辑分散在与总线连接的各个部件或设备上。</li></ul><h3 id="集中控制优先仲裁方式"><a href="#集中控制优先仲裁方式" class="headerlink" title="集中控制优先仲裁方式"></a>集中控制优先仲裁方式</h3><p><strong>链式查询方式</strong></p><p>控制总线中有 3 根线用于总线控制：BS 总线忙、BR 总线请求、BG 总线同意。其中 BG 是串行从一个 I/O 接口送到下一个 I/O 接口。如果 BG 到达的接口有总线请求 BR，BG 信号就不再往下传，意味着该接口获得了总线使用权，并建立总线忙 BS 信号，表示它占用了总线。</p><p>离总线控制部件最近的设备具有最高优先级。</p><p>只需很少几根线就能按一定有限次序实现总线控制，并且很容易扩充设备，但对电路故障很敏感，且优先级别低的设备可能很难获得请求。</p><p><strong>计数器定时查询方式</strong></p><p>与链式查询方式相比，多了一组设备地址线，少了一根总线同意线 BG。</p><p>总线控制部件接到由 BR 送来的总线请求信号后，在 BS=0 时，总线控制部件中的计数器开始计数，并通过设备地址线向各设备发出一组地址信号。当某个请求占用总线的设备地址与计数值一致时，便获得总线使用权，此时终止计数查询。</p><p>初始值可由程序设置；终止计数后可以重头开始，也可以从上一次计数终点开始。</p><p>对电路故障敏感度小于链式查询方式，但增加了控制线数（设备地址）目，控制也较复杂。</p><p><strong>独立请求方式</strong></p><p>每一台设备均有一对总线请求线和总线同意线。当设备要求使用总线时，便发出改设备的请求信号。总线控制部件中有一排队电路，可根据优先次序确定响应哪一台设备的请求。</p><p>响应快，优先次序控制灵活（根据程序改变），控制线数量多，总线控制更复杂。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/集中控制的优先仲裁方式.png" alt="image-20220830231905034"></p><h3 id="通信控制"><a href="#通信控制" class="headerlink" title="通信控制"></a>通信控制</h3><p>通常将完成一次总线操作的时间称为总线周期，可分为以下 4 各阶段。</p><p>申请分配阶段：主模块提出申请，总线仲裁机构决定下一传数周期的总线使用权授予某一申请者；</p><p>寻址阶段：取得了使用权的主模块通过总线发出本次要访问的从模块的地址及有关命令。启动参与本次传数的从模块；</p><p>传数阶段：主模块与从模块进行数据交换，数据由源模块发出，经数据总线流入目的模块；</p><p>结束阶段：主模块的有关信息均从系统总线上撤除，让出总线使用权。</p><p><strong>总线通信控制主要解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调如何配合。</strong></p><p><strong>同步通信</strong></p><p>读命令：CPU 在 T1 上升沿发出地址信息；在 T2 的上升沿发出读命令；与地址信号相符合的输入设备按命令进行一系列内部操作，且必须在 T3 的上升沿到来之前将 CPU 所需数据送到数据总线上；CPU 在 T3 时钟周期内将数据上的信息传送到其内部寄存器；CPU 在 T4 上升沿撤销读命令，输入设备不再向数据总线上传送数据，撤销它对数据总线的驱动。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/同步式数据输入传输.png" alt="image-20220903234653231"></p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/同步式数据输出传输.png" alt="image-20220904002529984"></p><p>规定明确、统一，模板间配合简单一致。</p><p>缺点是主、从模块时间配合属于强制性“同步”，必须在限定时间内完成规定的要求。并且对所有从模块都用统一时限，各模块速度不同，必须以最慢速度的部件来设计公共时钟，严重影响总线工作效率，给设计带来局限性，缺乏灵活性。</p><p><strong>异步通信</strong></p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/异步通讯中请求与回答的互锁.png" alt="image-20220904121139069"></p><p>(a) 不互锁：主模块发出请求信号后，不必等待接到从模块的回答信号，而是经过一段时间，确认从模块已收到请求信号后，便撤销其请求信号；从模块接到请求信号后，在条件允许时发出回答信号，并且经过一段时间确认主模块已收到回答信号后，自动撤销回答信号。</p><p>(b) 半互锁：主模块发出请求信号，必须接待到从模块的回答信号后再撤销其请求信号，有旧互锁关系；而从模块接到请求信号后发出回答信号，但不必等待获知主模块的请求信号，而是隔一段时间后自动撤销其回答信号，无互锁关系。</p><p>(c) 全互锁：皆需获得回答信号后撤销。在网络通信中，通信双方采用的就是全互锁方式。</p><p><strong>半同步通信</strong></p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/半同步通信数据输入过程.png" alt="image-20220904143256997"></p><p>保留了同步通信的基本特点，同时又像异步通信那样允许不同速度的模块和谐地工作。</p><p>增设了一条等待响应信号线，采用插入等待周期的措施来协调通信双方的配合问题。</p><p><strong>分离式通信</strong></p><p>将一个传输周期（总线周期）分解为两个子周期，两个传输子周期都只有单方面的信息流，每个模块都变成了主模块。</p><h1 id="存储器"><a href="#存储器" class="headerlink" title="存储器"></a>存储器</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="存储器分类"><a href="#存储器分类" class="headerlink" title="存储器分类"></a>存储器分类</h3><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/存储器分类.png" alt="image-20220904220629761"></p><p>a) 按存储介质分类。</p><p><strong>半导体存储器</strong></p><p>由半导体器件组成，用超大规模集成电路工艺制成芯片，体积小、功耗低、存取时间短。当电源消失时，所存信息也随即像丢失，是一种易失性存储器。</p><p>非挥发性材料制成的半导体存储器，克服了信息易失的弊病。</p><p>双极型 TTL 半导体存储器：高速。</p><p>MOS 半导体存储器：高集成度，制造简单，成本低，故被广泛应用。</p><p><strong>磁表面存储器</strong></p><p>在金属或塑料基体的表面上涂一层磁性材料作为记录介质，工作时磁层随载磁体高速运转，用磁头在磁层上进行读、写操作。</p><p>用具有矩形磁滞回线特性的材料作次表面物质，按其剩磁状态的不同而区分 0 或 1，而且剩磁状态不会轻易丢失，故这类存储器具有非易失性的特点。</p><p><strong>磁芯存储器</strong></p><p>被半导体存储器取代。</p><p><strong>光盘存储器</strong></p><p>用激光在磁光材料上进行读、写的存储器，具有非易失性的特点。</p><p>记录密度高、耐用性好、可靠性高和可互换性强等特点。</p><p>b) 按存取方式分类</p><p><strong>随机存储器 Random Access Memory RAM</strong></p><p>是一种可读、写存储器，其特点是存储器的任何一个存储单元的内容都可以随机存取，且存取时间与存储单位的物理位置无关。计算机系统中的主存都采用这种随机存储器。</p><p>静态 RAM，以触发器原理寄存信息。</p><p>动态 RAM，以电容充放电原理寄存信息。</p><p><strong>只读存储器 Read Only Memory ROM</strong></p><p>掩模型只读存储器 Masked ROM，MROM；可编程只读存储器 Programmable  ROM，PROM；可擦除可编程只读存储器 Erasable Programmable  ROM，EPROM；用电可擦除可编程只读存储器 Electrically Erasable Programmable  ROM， EEPROM；Flash Memory。</p><p><strong>串行访问存储器</strong></p><p>对存储单元进行读写操作时，需按其物理位置的先后顺序寻找地址，则这种存储器称为串行访问存储器。</p><p>由于信息所在位置不同，读写时间均不相同。</p><p>c) 按在计算机中的作用分类</p><p><strong>主存储器</strong></p><p>可以和 CPU 直接交换信息。</p><p>速度快、容量小、每位价格高。</p><p><strong>辅助存储器</strong></p><p>是主存储器的后援存储器，用来存放当时暂时不用的程序和书，不能与 CPU 直接交换信息。</p><p>速度慢、容量大、每位价格低。</p><p><strong>缓冲存储器</strong></p><p>用在两个速度不同的部件之中。</p><h3 id="层次结构"><a href="#层次结构" class="headerlink" title="层次结构"></a>层次结构</h3><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/存储器速度、容量和价位的关系.png" alt="image-20220904220938783"></p><p>由上至下，位价越来越低，速度越来越慢，容量越来越大。</p><p>寄存器中的数直接在 CPU 内部参与运算。</p><p>主存用来存放将要参与运行的程序和数据，其速度与 CPU 速度差距较大，为使它们匹配，在主存与 CPU 之间插入了一种比主存速度更快、容量跟更小的高速缓冲存储器 Cache。</p><p>寄存器、缓存、主存这三类存储器都是由速度不同、位价不等的半导体存储材料制成的，它们都设在主机内。</p><p>磁盘、磁带属于辅助存储器，其容量比主存大得多，大都用来存放暂时未用到的程序和数据文件。</p><p>CPU 不能直接访问辅存，辅存只能与主存交换信息，因此辅存的速度可以比主存慢很多。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/层次交换.png" alt="image-20220904225701286"></p><p>缓存 - 主存层次主要解决 CPU 与主存速度不匹配的问题。</p><p>主存 - 辅存层次主要解决存储系统的容量问题。形成了虚拟存储系统，在这个系统中，程序员变成的地址范围与虚拟存储器的地址空间相对应。</p><p>程序员编程时，可用的地址空间远远大于主存空间，使程序员以为自己占有一个容量极大的主存（虚拟存储器）。其逻辑地址转变为物理地址的工作由计算机系统的硬件和操作系统自动完成的，对程序员是透明的。</p><p>当虚地址的内容在主存时，机器便可立即使用；若虚地址的内容不在主存，则必须先将此虚地址内容传递到主存的合适单元后再为机器所用。</p><h2 id="主存储器"><a href="#主存储器" class="headerlink" title="主存储器"></a>主存储器</h2><p>主存的基本组成。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/主存的基本组成.png" alt="image-20220906220311036"></p><p>根据 MAR 储存器地址寄存器中的地址访问某个存储单元时，还需要经过地址译码、驱动等电路，才能找到所需要访问的单元。</p><p>读出时，需经过读出放大器，才能将被选中单元的存储字送到 MDR 主存数据寄存器。</p><p>写入时，MDR 中的数据也必须经过写入电路才能真正写入到被选中的单元中。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/主存和 CPU 的联系.png" alt="image-20220906221437792"></p><p>现代计算机的主存都由半导体集成电路构成，驱动器、译码器和读写电路均制作在存储芯片中，而 MAR 和 MDR 制作在 CPU 芯片中。存储芯片和 CPU 芯片可通过总线连接。</p><p>当要从存储器读出来某一信息字时，首先由 CPU 将该字的地址送到 MAR，经地址总线送至主存，然后发出读命令，主存接到读命令后将该单元内容读至数据总线上。</p><p>若要向主存存入一个信息字时，首先 CPU 将该字所在主存单元的地址经 MAR 送到地址总线，并将信息字送入 MDR，然后向主存发出写命令，主存接到写命令后，便将数据线上的信息写入到对应地址线指出的主存单元中。</p><p><strong>主存中存储单元地址的分配</strong></p><p>不同的机器存储字长不同，常用 8 位二进制数代表一个字节，因此存储字长都取 8 的倍数。</p><p>通常计算机系统即可按字寻址，也可按字节寻址。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/字节寻址的主存地址分配.png" alt="image-20220909202215235"></p><p><strong>主存的技术指标</strong></p><p>存储容量：存储单元 x 存储字长；1 字长 = 8 字节。</p><p>存储速度：由存取时间和存取周期来表示。</p><ul><li>存取时间、访问时间 Memory Access Time，是指启动一次存储器操作（读或写）到完成该操作所需的全部时间。存取时间分为读出时间和写入时间两种。</li><li>存取周期 Memory Cycle Time 是指存储器进行连续两次独立的存储器操作所需的最小间隔时间，通常存储周期大于存储时间。</li></ul><p><strong>存储器带宽</strong></p><p>与存储周期密切相关，表示单位时间内存储器存取的信息量。通过以下方式提高存储器带宽：</p><ul><li>缩短存储周期；</li><li>增加存储字长，使每个存取周期可读 / 写更多的二进制数；</li><li>增加存储体。</li></ul><h3 id="半导体存储芯片简介"><a href="#半导体存储芯片简介" class="headerlink" title="半导体存储芯片简介"></a>半导体存储芯片简介</h3><p><strong>基本结构</strong></p><p>半导体存储芯片采用超大规模集成电路制造工艺，在一个芯片内集成具有记忆功能的存储矩阵、译码驱动电路和读写电路等。</p><p>译码驱动能把地址总线送来的地址信号翻译成对应存储单元的选择信号，该信号在读 / 写电路的配合下完成对被选中单元的读写操作。</p><p>读写电路包括读出放大器和写入电路，用来完成读写操作。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/存储芯片的基本结构.png" alt="image-20220910012033349"></p><p>地址线是单向输入的，数据线是双向的，位数与芯片容量有关。</p><p>地址线和数据线位数共同反映存储芯片的容量。例如，地址线为 10 根，数据线为 4 根，则芯片容量为 </p><p>2<sup>10</sup>x4 K 位。</p><p>控制线主要包括读写控制线和片选线两种（不同存储芯片不同，可共用一根或分用两根）。由于半导体存储器是由许多芯片组成的，为此需要用片选信号来确定哪个芯片被选中。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/64K x 8 位的存储器.png" alt="image-20220910014103742"></p><p><strong>译码驱动方式</strong></p><p>线选法：用一根字选择线直接选中一个存储单元的各位。</p><p>重合法：</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/译码驱动方式.png" alt="image-20220910014206795"></p><h3 id="随机存取存储器"><a href="#随机存取存储器" class="headerlink" title="随机存取存储器"></a>随机存取存储器</h3><p><strong>静态 RAM、Static RAM、SRAM</strong></p><p>存储器用于寄存 0 和 1 代码的电路成为存储器的基本单元电路。</p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/静态 RAM 的基本单元电路.png" alt="image-20220910212750097"></p><p><img src="http://wingowen.gitee.io/image/2022/计算机组成/2114 RAM 芯片结构示意图.png" alt="image-20220910215559548"></p><p><img src="http://wingowen.gitee.io/IMAGE/计算机组成/2114 RAM 的读周期时序.png" alt="image-20220910215840000"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;北大计算机组成课程。&lt;/p&gt;
&lt;p&gt;计算机基本结构：冯诺依曼结构，计算机执行指令的过程。&lt;/p&gt;
&lt;p&gt;系统总线&lt;/p&gt;</summary>
    
    
    
    <category term="考研" scheme="https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
    <category term="计算机科学" scheme="https://wingowen.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>模型评估与选择</title>
    <link href="https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"/>
    <id>https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/</id>
    <published>2022-07-29T10:24:58.000Z</published>
    <updated>2022-08-02T01:19:57.843Z</updated>
    
    <content type="html"><![CDATA[<p>错误率和精度，误差，偏差和方差。</p><p>评估方法：留出法，交叉验证，自助法。</p><p>二分类任务性能度量：查准率，查全率，F1，ROC，AUC。</p><p>数据层面解决类别不平衡：欠采样，过采样，~结合。</p><p>算法层面解决类别不平衡：惩罚项。</p><span id="more"></span><h1 id="错误率和精度"><a href="#错误率和精度" class="headerlink" title="错误率和精度"></a>错误率和精度</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实的数据标签</span></span><br><span class="line">real_label = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>])                                         \                              \   \</span><br><span class="line"><span class="comment"># 分类器的预测标签</span></span><br><span class="line">classifier_pred = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">compare_result = (real_label == classifier_pred)</span><br><span class="line">compare_result = compare_result.astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># m 为样本数量 b 为预测错误样本</span></span><br><span class="line">m = <span class="built_in">len</span>(real)</span><br><span class="line">b = m - np.<span class="built_in">sum</span>(cmp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 错误率</span></span><br><span class="line">error_rate = (b / m)*<span class="number">100</span></span><br><span class="line"><span class="comment"># 精确度 acc</span></span><br><span class="line">accuracy = (<span class="number">1</span> - b / m)*<span class="number">100</span></span><br></pre></td></tr></table></figure><h1 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h1><p>模型在训练样本上的误差称为<strong>训练误差</strong>或<strong>经验误差</strong>；模型在新样本上的误差称为<strong>泛化误差。</strong></p><p>过拟合模型：虽然训练误差接近 0，泛化误差非常大。</p><p>欠拟合的模型无论是在训练集中还是在新样本上，表现都很差，即经验误差和泛化误差都很大。</p><h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差-方差分解 bias-variance decomposition， 是解释学习算法泛化性能的一种重要工具。</p><ul><li>偏差 bias，与真实值的<strong>偏离程度</strong>；</li><li>方差 variance，该随机变量在其期望值附近的<strong>波动程度</strong>。</li></ul><p><img src="/IMAGE/模型评估与选择/Untitled.png" alt="Untitled"></p><h1 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h1><p><strong>评估：对学习器的泛化误差进行评估并进而做出选择。</strong></p><h2 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h2><p>以一定比例划分训练集和测试集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 导入包</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"># 加载数据集</span><br><span class="line">def load_pts(): </span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    return: 返回随机生成 200 个点的坐标</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    dots = 200  # 样本数</span><br><span class="line">    dim = 2 # 数据维度</span><br><span class="line">    X = np.random.randn(dots,dim) # 建立数据集，shape(200,2)</span><br><span class="line">    # 建立样本 X 的类别</span><br><span class="line">    Y = np.zeros(dots, dtype=&#x27;int&#x27;)      </span><br><span class="line">    for i in range(X.shape[0]):</span><br><span class="line">            Y[i] = 1           </span><br><span class="line">    return X, Y</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">X,Y = load_pts()</span><br><span class="line"></span><br><span class="line"># 使用train_test_split划分训练集和测试集</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=0)</span><br></pre></td></tr></table></figure><h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>交叉验证法 cross validation，先将数据集 D 划分为 k 个大小相似的互斥子集。</p><p><img src="/IMAGE/模型评估与选择/Untitled 1.png" alt="Untitled"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集，随机生成40个点</span></span><br><span class="line">data = np.random.randn(<span class="number">40</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉验证法</span></span><br><span class="line">kf = KFold(n_splits = <span class="number">4</span>, shuffle = <span class="literal">False</span>, random_state = <span class="literal">None</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> kf.split(data):</span><br><span class="line">    <span class="built_in">print</span>(train)</span><br><span class="line">    <span class="built_in">print</span>(test,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h2><p>有放回抽样，给定包含 m 个样本的数据集 D，我们对它进行采样产生数据集 D’ ：</p><ul><li>每次随机从 D 中挑选一个样本；</li><li>将该样本拷贝放入 D’，然后再将该样本放回初始数据集 D 中；</li><li>重复执行 m 次该过程；</li><li>最后得到包含 m 个样本数据集 D’。</li></ul><p>由上述表达式可知，初始数据集与自助采样数据集 D1’，自助采样数据集 D2’ 的概率分布不一样，且自助法采样的数据集正负类别比例与原始数据集不同。因此用自助法采样的数据集代替初始数据集来构建模型存在估计偏差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 任意设置一个数据集</span></span><br><span class="line">X = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">23</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">45</span>,<span class="number">67</span>,<span class="number">89</span>,<span class="number">34</span>,<span class="number">54</span>,<span class="number">76</span>,<span class="number">98</span>,<span class="number">43</span>,<span class="number">52</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过产生的随机数获得抽取样本的序号 </span></span><br><span class="line">bootstrapping = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">    bootstrapping.append(np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(X),(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过序号获得原始数据集中的数据</span></span><br><span class="line">D_1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">int</span>(bootstrapping[i]))</span><br><span class="line">    D_1.append(X[<span class="built_in">int</span>(bootstrapping[i])])</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(D_1)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="table-container"><table><thead><tr><th></th><th>采样方法</th><th>与原始数据集的分布是否相同</th><th>相比原始数据集的容量</th><th>是否适用小数据集</th><th>是否适用大数据集</th><th>是否存在估计偏差</th></tr></thead><tbody><tr><td>留出法</td><td>分层抽样</td><td>否</td><td>变小</td><td>否</td><td>是</td><td>是</td></tr><tr><td>交叉验证法</td><td>分层抽样</td><td>否</td><td>变小</td><td>否</td><td>是</td><td>是</td></tr><tr><td>自助法</td><td>放回抽样</td><td>否</td><td>不变</td><td>是</td><td>否</td><td>是</td></tr></tbody></table></div><h1 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h1><p>性能度量：对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量。</p><p>性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。这意味着模型的好坏是相对的，什么样的模型是好的? 这不仅取决于算法和数据，还决定于任务需求。</p><p>回归任务常用性能度量：MSE mean square error，均方差。</p><p>分类任务常用性能度量：acc accuracy，精度；错误率</p><h2 id="查准率、查全率、F1"><a href="#查准率、查全率、F1" class="headerlink" title="查准率、查全率、F1"></a>查准率、查全率、F1</h2><p>对于二分类问题，可将样例根据真实值与学习器预测类别组合划分为：</p><ul><li>真正例 true positive</li><li>假正例 false positive</li><li>真反例 true negative</li><li>假反例 false negative</li></ul><p><img src="/IMAGE/模型评估与选择/Untitled 2.png" alt="Untitled"></p><script type="math/tex; mode=display">P(\text { Precision })=\frac{T P}{T P+F P} \\R(\text { Recall })=\frac{T P}{T P+F N}</script><p>Recall，查全率、召回率：计算实际为正的样本中，预测正确的样本比例。</p><p>Precision，查准率：在预测为正的样本中，实际为正的概率。</p><p>P-R 曲线，BRP，Break Even Point：平衡单 P = R。</p><p><img src="/IMAGE/模型评估与选择/Untitled 3.png" alt="Untitled"></p><p>由 P-R 曲线可以看出，查全率与准确率是成反比的，这里可以理解为为了获取所有正样本而牺牲了准确性，即广撒网。<br>BRP 还是过于简单，更常用的是 F1 度量。</p><script type="math/tex; mode=display">F 1=\frac{2 \times P \times R}{P+R}=\frac{2 T P}{n+T P-T N}</script><p>F1 的核心思想在于，在尽可能的提高 P 和 R 的同时，也希望两者之间的差异尽可能小。</p><p>当对 P 和 R 有所偏向时，则需要 F1 更泛性的度 Fβ。</p><script type="math/tex; mode=display">F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}</script><p>β &gt; 1时更偏向 R，β &lt; 1 更偏向 P。</p><p>如果使用了类似交叉验证法，我们会得到多个 confusion matrix：</p><ul><li>宏观 macroF1 对于每个 confusion matrix 先计算出P、R，然后求得平均并带入公式求 macroF1；</li><li>微观 microF1 先求 confusion matrix 各元素的平均值，然后计算 P、R。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_data</span>(<span class="params">random_state=<span class="number">2021</span></span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :返回值: GT_label: 数据集的真实标签，0表示非苹果，1表示苹果</span></span><br><span class="line"><span class="string">            Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    noise_rate = <span class="number">0.1</span> <span class="comment"># 噪声比例</span></span><br><span class="line">    sample_num = <span class="number">4096</span>  <span class="comment"># 总样本数</span></span><br><span class="line">    noise_sample_num = <span class="built_in">int</span>(sample_num*noise_rate) <span class="comment"># 噪声样本数</span></span><br><span class="line">    np.random.seed(random_state)</span><br><span class="line">    Pred_Score = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,sample_num)</span><br><span class="line">    GT_label = (Pred_Score&gt;<span class="number">0.5</span>).astype(np.<span class="built_in">int</span>)</span><br><span class="line">    noise_ids = np.random.choice(a=sample_num, size=noise_sample_num, replace=<span class="literal">False</span>, p=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> noise_ids:</span><br><span class="line">        GT_label[index] = <span class="number">1</span> <span class="keyword">if</span> GT_label[index] == <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> GT_label, Pred_Score</span><br><span class="line"></span><br><span class="line">GT_label, Pred_Score = generate_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请你补全以下代码，计算查准率与查全率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_PR</span>(<span class="params">GT_label, Pred_Score, threshold, random_state=<span class="number">2021</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算错误率和精度</span></span><br><span class="line"><span class="string">    :GT_label: 数据集的真实标签，0表示非苹果，1表示苹果</span></span><br><span class="line"><span class="string">    :Pred_Score: 模型对数据样本预测为苹果的分数，取值范围为[0,1]</span></span><br><span class="line"><span class="string">    :threshold: 评估阈值</span></span><br><span class="line"><span class="string">    :random_state: 随机种子</span></span><br><span class="line"><span class="string">    :返回值: P: 查准率</span></span><br><span class="line"><span class="string">            R: 查全率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    Pred_Label = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>, Pred_Score))</span><br><span class="line">    <span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line">    P = precision_score(GT_label, Pred_Label)</span><br><span class="line">    R = recall_score(GT_label, Pred_Label)</span><br><span class="line">    <span class="string">&quot;&quot;&quot; TODO &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> P, R</span><br><span class="line">    </span><br><span class="line">P, R = get_PR(GT_label, Pred_Score, <span class="number">0.55</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查准率P ：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(P))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查全率R ：&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(R))</span><br></pre></td></tr></table></figure><h2 id="ROC-与-AUC-原理"><a href="#ROC-与-AUC-原理" class="headerlink" title="ROC 与 AUC 原理"></a>ROC 与 AUC 原理</h2><p>ROC 全称是受试者工作特征 Receiver Operating Characteristic) 。与 P-R 曲线不同的是，ROC使用了真正例率和假正例率。</p><script type="math/tex; mode=display">\begin{aligned}T P R(\text { Precision }) &=\frac{T P}{T P+F N} \\F P R(\text { Precision }) &=\frac{F P}{F P+T N}\end{aligned}</script><p>TPR 真正率，真正样本与实际为正的样本的比率；</p><p>FPR 假正率，加正样本与实际为负的样本的比率。</p><p>若一个学习器的 ROC 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者； 若两  个学习器的 ROC 曲线发生交叉，则难以一般性地断言两者孰优孰劣。此时如果一定要进行比较，则较为合理的判据是比较 ROC 曲线下的面积，即 AUC  Area Under ROC Curve。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_pts</span>(): </span><br><span class="line">    dots = <span class="number">200</span>  <span class="comment"># 点数</span></span><br><span class="line">    X = np.random.randn(dots,<span class="number">2</span>) * <span class="number">15</span>  <span class="comment"># 建立数据集，shape(200,2)，坐标放大15倍</span></span><br><span class="line">    <span class="comment"># 建立 X 的类别</span></span><br><span class="line">    y = np.zeros(dots, dtype=<span class="string">&#x27;int&#x27;</span>)      </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> X[i,<span class="number">0</span>] &gt; -<span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">0</span>] &lt; <span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">1</span>] &gt; -<span class="number">15</span> <span class="keyword">and</span> X[i,<span class="number">1</span>] &lt; <span class="number">15</span>:  <span class="comment"># 矩形框内的样本都是目标类（正例）</span></span><br><span class="line">            y[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> == np.random.randint(i+<span class="number">1</span>) % <span class="number">10</span>:  <span class="comment"># 对数据随机地插入错误，20 个左右</span></span><br><span class="line">            y[i] = <span class="number">1</span> - y[i]  </span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 数据集可视化          </span></span><br><span class="line">    plt.scatter(X[np.argwhere(y==<span class="number">0</span>).flatten(),<span class="number">0</span>], X[np.argwhere(y==<span class="number">0</span>).flatten(),<span class="number">1</span>],s = <span class="number">20</span>, color = <span class="string">&#x27;blue&#x27;</span>, edgecolor = <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.scatter(X[np.argwhere(y==<span class="number">1</span>).flatten(),<span class="number">0</span>], X[np.argwhere(y==<span class="number">1</span>).flatten(),<span class="number">1</span>],s = <span class="number">20</span>, color = <span class="string">&#x27;red&#x27;</span>, edgecolor = <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    plt.xlim(-<span class="number">40</span>,<span class="number">40</span>)</span><br><span class="line">    plt.ylim(-<span class="number">40</span>,<span class="number">40</span>)</span><br><span class="line">    plt.grid(<span class="literal">False</span>)</span><br><span class="line">    plt.tick_params(</span><br><span class="line">    axis=<span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">    which=<span class="string">&#x27;both&#x27;</span>,</span><br><span class="line">    bottom=<span class="literal">False</span>,</span><br><span class="line">    top=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">X, y = load_pts()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">### 训练模型 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集拆分成训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型 </span></span><br><span class="line">clf1 = DecisionTreeClassifier(max_depth=<span class="number">5</span>, min_samples_leaf=<span class="number">4</span>,min_samples_split=<span class="number">4</span>)</span><br><span class="line">clf2 = GradientBoostingClassifier(max_depth=<span class="number">8</span>, min_samples_leaf=<span class="number">10</span>, min_samples_split=<span class="number">10</span>)</span><br><span class="line">clf3 = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, gamma=<span class="number">0.001</span>, probability=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">clf1.fit(X_train, y_train)</span><br><span class="line">clf2.fit(X_train, y_train)</span><br><span class="line">clf3.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 评估模型 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_score1 = clf1.predict_proba(X_test)</span><br><span class="line">y_score2 = clf2.predict_proba(X_test)</span><br><span class="line">y_score3 = clf3.predict_proba(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得 FPR、TPR 值</span></span><br><span class="line">fpr1, tpr1, _ = roc_curve(y_test, y_score1[:,<span class="number">1</span>])</span><br><span class="line">fpr2, tpr2, _ = roc_curve(y_test, y_score2[:,<span class="number">1</span>])</span><br><span class="line">fpr3, tpr3, _ = roc_curve(y_test, y_score3[:,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">### 绘制 ROC 曲线 ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> auc</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 ROC 函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_roc_curve</span>(<span class="params">fpr, tpr, c, name</span>):</span><br><span class="line">    lw = <span class="number">2</span></span><br><span class="line">    roc_auc = auc(fpr,tpr)</span><br><span class="line">    plt.plot(fpr, tpr, color=c,lw=lw, </span><br><span class="line">             label= name +<span class="string">&#x27; (area = %0.2f)&#x27;</span> % roc_auc)</span><br><span class="line">    plt.plot([<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">1</span>], color=<span class="string">&#x27;navy&#x27;</span>, lw=lw, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">1.0</span>])</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1.05</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">    <span class="comment">#plt.title(&#x27;&#x27;)</span></span><br><span class="line">    plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">    </span><br><span class="line">plot_roc_curve(fpr1, tpr1, <span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;DecisionTreeClassifier &#x27;</span>)   </span><br><span class="line">plot_roc_curve(fpr2, tpr2, <span class="string">&#x27;navy&#x27;</span>,<span class="string">&#x27;GradientBoostingClassifier &#x27;</span>)   </span><br><span class="line">plot_roc_curve(fpr3, tpr3, <span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;SVC &#x27;</span>) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h1 id="比较检验（TODO）"><a href="#比较检验（TODO）" class="headerlink" title="比较检验（TODO）"></a>比较检验（TODO）</h1><p>模型性能比较的重要因素：</p><ul><li>实验评估得到的性能不等于泛化性能；</li><li>测试集上的性能与测试集本身的选择有很大关系；</li><li>很多机器学习算法本身有一定的随机性。</li></ul><p>统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出：哪个学习器更优秀，并且成立的把我有多大。</p><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><p>由样本推测总体的方法。</p><h3 id="交叉验证-t-检验"><a href="#交叉验证-t-检验" class="headerlink" title="交叉验证 t 检验"></a>交叉验证 t 检验</h3><h3 id="McNemar-检验"><a href="#McNemar-检验" class="headerlink" title="McNemar 检验"></a>McNemar 检验</h3><h3 id="Friedman-检验与-Nemenyi-后续检验"><a href="#Friedman-检验与-Nemenyi-后续检验" class="headerlink" title="Friedman 检验与 Nemenyi 后续检验"></a>Friedman 检验与 Nemenyi 后续检验</h3><h1 id="类别不平衡"><a href="#类别不平衡" class="headerlink" title="类别不平衡"></a>类别不平衡</h1><p>在分类任务中，当不同类别的训练样本数量差别很大时，训练得到的模型往往泛化性很差 ，这就是类别不平衡。如在风控系统识别中，欺诈的样本应该是很少部分。</p><p>如果类别不平衡比例超过 4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。</p><p>解决不平衡分类问题的策略可以分为两大类：</p><ul><li>从数据层面入手 , 通过改变训练集样本分布降低不平衡程度；</li><li>从算法层面入手 , 根据算法在解决不平衡问题时的缺陷，适当地修改算法使之适应不平衡分类问题。</li></ul><h2 id="数据层面解决类别不平衡"><a href="#数据层面解决类别不平衡" class="headerlink" title="数据层面解决类别不平衡"></a>数据层面解决类别不平衡</h2><p><strong>扩大数据样本</strong>。</p><p><strong>重采样</strong>：通过过增加稀有类训练样本数的过采样和减少大类样本数的欠采样使不平衡的样本分布变得比较平衡 ，从而提高分类器对稀有类的识别率。</p><ul><li><p>过采样：复制稀有样本；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> RandomOverSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成样本集，用于分类算法：3 类，5000 个样本，特征维度为 2</span></span><br><span class="line">X, y = make_classification(n_samples=<span class="number">5000</span>, n_features=<span class="number">2</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">                           n_redundant=<span class="number">0</span>, n_repeated=<span class="number">0</span>, n_classes=<span class="number">3</span>,</span><br><span class="line">                           n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">                           weights=[<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.94</span>],</span><br><span class="line">                           class_sep=<span class="number">0.8</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(Counter(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过采样</span></span><br><span class="line">ros = RandomOverSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = ros.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印过采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成新的稀有样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过采样</span></span><br><span class="line">sm = SMOTE(random_state=<span class="number">42</span>)</span><br><span class="line">X_res, y_res = sm.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印过采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Resampled dataset shape %s&#x27;</span> % Counter(y_res))</span><br></pre></td></tr></table></figure></li><li><p>欠采样：保存所有稀有类样本，并在丰富类别中随机选择与稀有类别样本相等数量的样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> RandomUnderSampler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 欠采样</span></span><br><span class="line">rus = RandomUnderSampler(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = rus.fit_resample(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印欠采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br></pre></td></tr></table></figure></li><li><p>过采样与欠采样结合：在之前的SMOTE方法中, 生成无重复的新的稀有类样本, 也很容易生成一些噪音数据。</p><p>因此, 在过采样之后需要对样本进行清洗。常见的有两种方法：SMOTETomek、SMOTEENN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> imblearn.combine <span class="keyword">import</span> SMOTEENN</span><br><span class="line"><span class="comment"># 过采样与欠采样结合</span></span><br><span class="line">smote_enn = SMOTEENN(random_state=<span class="number">0</span>)</span><br><span class="line">X_resampled, y_resampled = smote_enn.fit_resample(X, y)</span><br><span class="line"><span class="comment"># 打印采样后每个类别样本数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sorted</span>(Counter(y_resampled).items()))</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h2 id="算法层面解决类别不平衡"><a href="#算法层面解决类别不平衡" class="headerlink" title="算法层面解决类别不平衡"></a>算法层面解决类别不平衡</h2><p><strong>惩罚项方法</strong>：在大部分不平衡分类问题中，稀有类是分类的重点，在这种情况下正确识别出稀有类的样本比识别大类的样本更有价值，反过来说，<strong>错分稀有类的样本需要付出更大的代价</strong>。</p><p>通过设计一个代价函数来惩罚稀有类别的错误分类而不是分类丰富类别，可以设计出许多自然泛化为稀有类别的模型。</p><p>例如，调整 SVM 以惩罚稀有类别的错误分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LABEL 0 4000</span></span><br><span class="line"><span class="comment"># LABEL 1 200</span></span><br><span class="line"><span class="comment"># 导入相关包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加惩罚项</span></span><br><span class="line">clf = SVC(C=<span class="number">0.8</span>, probability=<span class="literal">True</span>, class_weight=&#123;<span class="number">0</span>:<span class="number">0.25</span>, <span class="number">1</span>:<span class="number">0.75</span>&#125;)</span><br></pre></td></tr></table></figure><p><strong>特征选择方法</strong></p><p>样本数量分布很不平衡时，特征的分布同样也会不平衡。 大类中经常出现的特征也许在稀有类中根本不出现，这样的特征是冗余的。</p><p>选取最具有区分能力的特征，有利于提高稀有类的识别率。特征选择比较不错的方法是决策树，如 C4.5、C5.0、CART 和随机森林。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;错误率和精度，误差，偏差和方差。&lt;/p&gt;
&lt;p&gt;评估方法：留出法，交叉验证，自助法。&lt;/p&gt;
&lt;p&gt;二分类任务性能度量：查准率，查全率，F1，ROC，AUC。&lt;/p&gt;
&lt;p&gt;数据层面解决类别不平衡：欠采样，过采样，~结合。&lt;/p&gt;
&lt;p&gt;算法层面解决类别不平衡：惩罚项。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯算法</title>
    <link href="https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    <id>https://wingowen.github.io/2022/07/29/%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</id>
    <published>2022-07-29T10:24:58.000Z</published>
    <updated>2022-09-09T11:25:24.065Z</updated>
    
    <content type="html"><![CDATA[<p>条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。</p><p>朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。</p><span id="more"></span><h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><p>P(A|B) 表示事件 B 发生的前提下，事件 A 发生的概率：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(A \cap B)}{P(B)}</script><p>P(B|A) 表示事件 A 发生的前提下，事件 B 发生的概率：</p><script type="math/tex; mode=display">P(B \mid A)=\frac{P(A \cap B)}{P(A)}</script><p>那么，就有 P(A|B) x P(B) = P(B|A) x P(A)，即可推导出<strong>贝叶斯公式</strong>：</p><script type="math/tex; mode=display">P(A \mid B)=\frac{P(B \mid A) \times P(A)}{P(B)}{\scriptsize }</script><h1 id="贝叶斯"><a href="#贝叶斯" class="headerlink" title="贝叶斯"></a>贝叶斯</h1><p><strong>基础思想：</strong></p><ul><li>已知类条件概率密度参数表达式和先验概率；</li><li>利用贝叶斯公式转换成后验概率；</li><li>根据后验概率大小进行决策分类。</li></ul><p>根据以上基本思想，可以得到贝叶斯概率计算公式表达为<strong>：后验概率 = 先验概率 × 似然概率（即新增信息所带来的调节程度）</strong>。</p><p><strong>优点：</strong></p><ul><li>贝叶斯决策能对信息的价值或是否需要采集新的信息做出科学的判断；</li><li>它能对调查结果的可能性加以数量化的评价，而不是像一般的决策方法那样，对调查结果或者是完全相信,或者是完全不相信；</li><li>如果说任何调查结果都不可能完全准确，先验知识或主观概率也不是完全可以相信的，那么贝叶斯决策则巧妙地将这两种信息有机地结合起来了；</li><li>它可以在决策过程中根据具体情况下不断地使用，使决策逐步完善和更加科学。</li></ul><p><strong>缺点：</strong></p><ul><li>它需要的数据多,分析计算比较复杂,特别在解决复杂问题时,这个矛盾就更为突出；</li><li>有些数据必须使用主观概率，有些人不太相信，这也妨碍了贝叶斯决策方法的推广使用。</li></ul><p><strong>扩展阅读：</strong></p><ul><li><a href="https://www.jiqizhixin.com/articles/2019-11-21">一文读懂概率论学习：贝叶斯理论</a></li><li><a href="https://zhuanlan.zhihu.com/p/5056240">贝叶斯决策论&amp;朴素贝叶斯算法</a></li><li><a href="https://www.bilibili.com/video/av57126177?from=search&amp;seid=1588787263892359481">朴素贝叶斯法讲解</a></li><li><a href="https://scikit-learn.org/stable/modules/naive_bayes.html">sklearn 贝叶斯方法</a></li></ul><h2 id="贝叶斯推断：广告邮件自动识别的代码实现"><a href="#贝叶斯推断：广告邮件自动识别的代码实现" class="headerlink" title="贝叶斯推断：广告邮件自动识别的代码实现"></a>贝叶斯推断：广告邮件自动识别的代码实现</h2><p>若邮件包含某个关键词，求此邮件是广告的概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 广告邮件数量</span></span><br><span class="line">ad_number = <span class="number">4000</span></span><br><span class="line"><span class="comment"># 正常邮件数量</span></span><br><span class="line">normal_number = <span class="number">6000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有广告邮件中，出现 “红包” 关键词的邮件的数量</span></span><br><span class="line">ad_hongbao_number = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 所有正常邮件中，出现 “红包” 关键词的邮件的数量</span></span><br><span class="line">normal_hongbao_number = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 广告的先验概率 P(A)</span></span><br><span class="line">P_ad = ad_number / (ad_number + normal_number)</span><br><span class="line"><span class="comment"># 包含红包的先验概率 P(B)</span></span><br><span class="line">P_hongbao = (normal_hongbao_number + ad_hongbao_number) / (ad_number + normal_number)</span><br><span class="line"><span class="comment"># 广告 包含红包的似然概率 P(B|A)</span></span><br><span class="line">P_hongbao_ad = ad_hongbao_number / ad_number</span><br><span class="line"><span class="comment"># 求包含红包且是广告的概率 P(A|B) = P(B|A) x P(A) / P(B)</span></span><br><span class="line">P_ad_hongbao = P_hongbao_ad * P_ad / P_hongbao</span><br><span class="line"><span class="built_in">print</span>(P_ad_hongbao)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9940357852882705</span><br></pre></td></tr></table></figure><h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><p>极大似然估计方法 ，Maximum Likelihood Estimate，MLE，也称为最大概似估计或最大似然估计，是求估计的另一种方法，用部分已知数据去预测整体的分布。</p><p>极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：<strong>“模型已定，参数未知”</strong>。 通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p><blockquote><p><strong>极大似然估计</strong>与<strong>贝叶斯推断</strong>是统计中两种对模型的参数确定的方法，两种参数估计方法使用不同的思想。后者属于贝叶斯派，认为参数也是服从某种概率分布的，已有的数据只是在这种参数的分布下产生的；前者来自于频率派，认为参数是固定的，需要根据已经掌握的数据来估计这个参数。</p></blockquote><h2 id="极大似然估计的简单计算"><a href="#极大似然估计的简单计算" class="headerlink" title="极大似然估计的简单计算"></a>极大似然估计的简单计算</h2><p>一个硬币被抛了100次，有61次正面朝上，计算最大似然估计。</p><script type="math/tex; mode=display">\begin{array}{c}\frac{d}{d p}\left(\begin{array}{c}100 \\61\end{array}\right) p^{61}(1-p)^{39}=\left(\begin{array}{c}100 \\61\end{array}\right)\left(61 p^{60}(1-p)^{39}-39 p^{61}(1-p)^{38}\right) \\=\left(\begin{array}{c}100 \\61\end{array}\right) p^{60}(1-p)^{38}(61(1-p)-39 p) \\=\left(\begin{array}{c}100 \\61\end{array}\right) p^{60}(1-p)^{38}(61-100 p) \\=0\end{array}</script><p>当 $P = \frac{61}{100}, 0$ 时，导数为零。因为 1 &lt; P &lt; 0，所以 $P = \frac{61}{100}$。</p><h2 id="极大似然估计的简单应用"><a href="#极大似然估计的简单应用" class="headerlink" title="极大似然估计的简单应用"></a>极大似然估计的简单应用</h2><p>求极大似然估计 MLE 的一般步骤：</p><ul><li>由总体分布导出样本的联合概率函数（或联合密度）；</li><li>把样本联合概率函数（或联合密度）中自变量看成已知常数，而把参数 $θ$ 看作自变量，得到似然函数 $l(θ)$；</li><li>求似然函数 $l(θ)$ 的最大值点，常常转化为求 $lnl(θ)$ 的最大值点，即 $θ$ 的 MLE；</li><li>在最大值点的表达式中，用样本值带入就得到参数的极大似然估计。</li></ul><p>若随机变量 $x$ 服从一个数学期望为 $μ$、方差为 $σ^2$ 的正态分布，记为 $N(μ,σ^2)$，假设 $μ=30, σ=2$。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from scipy.stats import norm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">μ = 30  # 数学期望</span><br><span class="line">σ = 2  # 方差</span><br><span class="line">x = μ + σ * np.random.randn(10000)  # 正态分布</span><br><span class="line"></span><br><span class="line">plt.hist(x, bins=100)  # 直方图显示</span><br><span class="line">plt.show()</span><br><span class="line">print(norm.fit(x))  # 返回极大似然估计，估计出参数约为 30 和 2</span><br></pre></td></tr></table></figure><h1 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h1><p>朴素贝叶斯分类器是一系列假设特征之间<strong>强（朴素）独立</strong>条件下以贝叶斯定理为基础的简单概率分类器，该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。<strong>所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。</strong></p><p>朴素贝叶斯的思想基础是：<strong>对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</strong></p><p>对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够获取相当好的效果。</p><p><img src="http://wingowen.gitee.io/image/2022/贝叶斯算法/贝叶斯算法流程.png" alt="img"></p><h2 id="MNIST-手写体数字识别"><a href="#MNIST-手写体数字识别" class="headerlink" title="MNIST 手写体数字识别"></a>MNIST 手写体数字识别</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line"># numpy 库</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># tensorflow 库中的 mnist 数据集</span><br><span class="line">import tensorflow as tf</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line"># sklearn 库中的 BernoulliNB</span><br><span class="line">from sklearn.naive_bayes import BernoulliNB</span><br><span class="line"></span><br><span class="line"># 绘图工具库 plt</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">print(&quot;读取数据中 ...&quot;)</span><br><span class="line"></span><br><span class="line"># 载入数据</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line"># 将 (28,28) 图像数据变形为一维的 (1,784) 位的向量</span><br><span class="line">train_images = train_images.reshape(len(train_images),784)</span><br><span class="line">test_images =  test_images.reshape(len(test_images),784)</span><br><span class="line"></span><br><span class="line">print(&#x27;读取完毕!&#x27;)</span><br><span class="line"></span><br><span class="line">def plot_images(imgs):</span><br><span class="line">    &quot;&quot;&quot;绘制几个样本图片</span><br><span class="line">    :param show: 是否显示绘图</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sample_num = min(9, len(imgs))</span><br><span class="line">    img_figure = plt.figure(1)</span><br><span class="line">    img_figure.set_figwidth(5)</span><br><span class="line">    img_figure.set_figheight(5)</span><br><span class="line">    for index in range(0, sample_num):</span><br><span class="line">        ax = plt.subplot(3, 3, index + 1)</span><br><span class="line">        ax.imshow(imgs[index].reshape(28, 28), cmap=&#x27;gray&#x27;)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">    plt.margins(0, 0)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_images(train_images)</span><br><span class="line"></span><br><span class="line">print(&quot;初始化并训练贝叶斯模型...&quot;)</span><br><span class="line"></span><br><span class="line"># 定义 朴素贝叶斯模型</span><br><span class="line">classifier_BNB = BernoulliNB()</span><br><span class="line"></span><br><span class="line"># 训练模型</span><br><span class="line">classifier_BNB.fit(train_images,train_labels)</span><br><span class="line"></span><br><span class="line">print(&#x27;训练完成!&#x27;)</span><br><span class="line"></span><br><span class="line">print(&quot;测试训练好的贝叶斯模型...&quot;)</span><br><span class="line"></span><br><span class="line"># 分类器在测试集上的预测值</span><br><span class="line">test_predict_BNB = classifier_BNB.predict(test_images)</span><br><span class="line"></span><br><span class="line">print(&quot;预测完成!&quot;)</span><br><span class="line"></span><br><span class="line"># 计算准确率</span><br><span class="line">accuracy = classifier_BNB.score(test_images, test_labels)</span><br><span class="line"></span><br><span class="line">print(&#x27;贝叶斯分类模型在测试集上的准确率为 :&#x27;,accuracy)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对结果进行统计比较分析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 记录每个类别的样本的个数，例如 &#123;0：100&#125; 即 数字为 0 的图片有 100 张 </span><br><span class="line">class_num = &#123;&#125;</span><br><span class="line"># 每个类别预测为 0-9 类别的个数，</span><br><span class="line">predict_num = []</span><br><span class="line"># 每个类别预测的准确率</span><br><span class="line">class_accuracy = &#123;&#125;</span><br><span class="line"></span><br><span class="line">for i in range(10):</span><br><span class="line">    # 找到类别是 i 的下标</span><br><span class="line">    class_is_i_index = np.where(test_labels == i)[0]</span><br><span class="line">    # 统计类别是 i 的个数</span><br><span class="line">    class_num[i] = len(class_is_i_index)</span><br><span class="line"></span><br><span class="line">    # 统计类别 i 预测为 0-9 各个类别的个数</span><br><span class="line">    predict_num.append(</span><br><span class="line">        [sum(test_predict_BNB[class_is_i_index] == e) for e in range(10)])</span><br><span class="line"></span><br><span class="line">    # 统计类别 i 预测的准确率</span><br><span class="line">    class_accuracy[i] = round(predict_num[i][i] / class_num[i], 3) * 100</span><br><span class="line"></span><br><span class="line">    print(&quot;数字 %s 的样本个数：%4s，预测正确的个数：%4s，准确率：%.4s%%&quot; % (</span><br><span class="line">    i, class_num[i], predict_num[i][i], class_accuracy[i]))</span><br></pre></td></tr></table></figure><p>用热力图对结果进行分析。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">sns.set(rc=&#123;&#x27;figure.figsize&#x27;: (12, 8)&#125;, font_scale=1.5)</span><br><span class="line">sns.set_style(&#x27;whitegrid&#x27;,&#123;&#x27;font.sans-serif&#x27;:[&#x27;simhei&#x27;,&#x27;sans-serif&#x27;]&#125;) </span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line">uniform_data = predict_num</span><br><span class="line">ax = sns.heatmap(uniform_data, cmap=&#x27;YlGnBu&#x27;, vmin=0, vmax=150)</span><br><span class="line">ax.set_xlabel(&#x27;真实值&#x27;)</span><br><span class="line">ax.set_ylabel(&#x27;预测值&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;条件概率 ，贝叶斯，极大似然估计，朴素贝叶斯分类器。&lt;/p&gt;
&lt;p&gt;朴素贝叶斯分类器 BernoulliNB：MNIST 手写识别案例。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://wingowen.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://wingowen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>考研</title>
    <link href="https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/%E8%80%83%E7%A0%94/"/>
    <id>https://wingowen.github.io/2022/04/21/%E8%80%83%E7%A0%94/%E8%80%83%E7%A0%94/</id>
    <published>2022-04-21T06:19:23.000Z</published>
    <updated>2022-08-03T05:41:56.366Z</updated>
    
    <content type="html"><![CDATA[<p>考研院校信息整理汇总。</p><span id="more"></span><p>报考专业 <a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/4/114/085410">深大 - 人工智能与金融科技</a></p><p>初试科目</p><ul><li><p><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/101">101 思想政治理论</a></p></li><li><p><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/201">201 英语一</a></p></li><li><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/301">301 数学一</a></li><li><a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/408">408 计算机学科专业基础综合</a></li></ul><p>复试科目 <a href="http://ehall.szu.edu.cn/gsapp/sys/zsjzapp/index.do#/2022/2/FSX8">FSX8 机器学习</a></p><p>计算机考研 408 包括（150）</p><ul><li>数据结构 45</li><li>计算机组成原理 45</li><li>操作系统 35</li><li>计算机网络 25</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;考研院校信息整理汇总。&lt;/p&gt;</summary>
    
    
    
    <category term="考研" scheme="https://wingowen.github.io/categories/%E8%80%83%E7%A0%94/"/>
    
    
    <category term="考研" scheme="https://wingowen.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
</feed>
