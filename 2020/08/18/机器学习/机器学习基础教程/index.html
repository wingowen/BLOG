<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>机器学习基础教程 | WINGO BLOG</title>
  <meta name="description" content="深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础教程">
<meta property="og:url" content="https://wingowen.github.io/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/index.html">
<meta property="og:site_name" content="WINGO&#39;S BLOG">
<meta property="og:description" content="深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200818214503512.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200820210552194.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200820231618577.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200821222155374.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200821225715794.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200821233630557.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200822001304469.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200822123541273.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200822124604426.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200822143223061.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200822145645813.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200822152548464.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200823093719567.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200823095611385.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200823104855860.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200823113137757.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200823151447998.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200823170440945.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200830124602354.png">
<meta property="og:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200830131252233.png">
<meta property="article:published_time" content="2020-08-18T12:27:37.000Z">
<meta property="article:modified_time" content="2023-09-20T07:35:51.893Z">
<meta property="article:author" content="Wingo Wen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wingowen.github.io/img//2020/08/ML/image-20200818214503512.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://wingowen.github.io/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/index.html">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="WINGO'S BLOG" type="application/atom+xml">
</head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">WINGO.WEN</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md"></h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav menu-highlight">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
      </ul>
      
    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>得失从缘 心无增减</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9F%BA%E7%A1%80%E7%A7%91%E5%AD%A6/">基础科学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E9%A1%B9/">杂项</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6/">计算机科学</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%BB%B4/">运维</a><span class="category-list-count">5</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Aysnc/" style="font-size: 13px;">Aysnc</a> <a href="/tags/Cache/" style="font-size: 13px;">Cache</a> <a href="/tags/Druid/" style="font-size: 13px;">Druid</a> <a href="/tags/Dubbo/" style="font-size: 13px;">Dubbo</a> <a href="/tags/ElasticSearch/" style="font-size: 13px;">ElasticSearch</a> <a href="/tags/Email/" style="font-size: 13px;">Email</a> <a href="/tags/Flume/" style="font-size: 13px;">Flume</a> <a href="/tags/Git/" style="font-size: 13px;">Git</a> <a href="/tags/HBase/" style="font-size: 13px;">HBase</a> <a href="/tags/Hadoop/" style="font-size: 13px;">Hadoop</a> <a href="/tags/Hive/" style="font-size: 13.25px;">Hive</a> <a href="/tags/Impala/" style="font-size: 13px;">Impala</a> <a href="/tags/InooDB/" style="font-size: 13px;">InooDB</a> <a href="/tags/JDBC/" style="font-size: 13px;">JDBC</a> <a href="/tags/JPA/" style="font-size: 13px;">JPA</a> <a href="/tags/Kafka/" style="font-size: 13px;">Kafka</a> <a href="/tags/Kudu/" style="font-size: 13px;">Kudu</a> <a href="/tags/Linux/" style="font-size: 13px;">Linux</a> <a href="/tags/Log/" style="font-size: 13px;">Log</a> <a href="/tags/MQ/" style="font-size: 13.25px;">MQ</a> <a href="/tags/MyBatis/" style="font-size: 13px;">MyBatis</a> <a href="/tags/MySQL/" style="font-size: 13px;">MySQL</a> <a href="/tags/Netty/" style="font-size: 14px;">Netty</a> <a href="/tags/Pandas/" style="font-size: 13px;">Pandas</a> <a href="/tags/SQL/" style="font-size: 13px;">SQL</a> <a href="/tags/Scheduled/" style="font-size: 13px;">Scheduled</a> <a href="/tags/Security/" style="font-size: 13px;">Security</a> <a href="/tags/Shiro/" style="font-size: 13px;">Shiro</a> <a href="/tags/Spring-Boot/" style="font-size: 14px;">Spring Boot</a> <a href="/tags/Web/" style="font-size: 13px;">Web</a> <a href="/tags/WebSocket/" style="font-size: 13px;">WebSocket</a> <a href="/tags/demo/" style="font-size: 13.5px;">demo</a> <a href="/tags/gRPC/" style="font-size: 13px;">gRPC</a> <a href="/tags/peewee/" style="font-size: 13px;">peewee</a> <a href="/tags/sklearn/" style="font-size: 13px;">sklearn</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 13px;">爬虫</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 13.75px;">算法</a> <a href="/tags/%E7%BD%91%E7%BB%9C/" style="font-size: 13px;">网络</a> <a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 13.5px;">考研</a> <a href="/tags/%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4/" style="font-size: 13px;">脚本命令</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 13px;">设计模式</a> <a href="/tags/%E9%83%A8%E7%BD%B2/" style="font-size: 13px;">部署</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">九月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">七月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
  <aside class="sidebar sidebar-toc collapse   in  " id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    <nav id="toc" class="article-toc">
      <h3 class="toc-title">文章目录</h3>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text"> 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">1.1.</span> <span class="toc-text"> 环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#numpy"><span class="toc-number">1.1.1.</span> <span class="toc-text"> NumPy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#scipy"><span class="toc-number">1.1.2.</span> <span class="toc-text"> SciPy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#matplotlib"><span class="toc-number">1.1.3.</span> <span class="toc-text"> matplotlib</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pandas"><span class="toc-number">1.1.4.</span> <span class="toc-text"> pandas</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.</span> <span class="toc-text"> 鸢尾花分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%96%E5%BE%97%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text"> 取得数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text"> 划分数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%82%E5%AF%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.3.</span> <span class="toc-text"> 观察数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">1.2.4.</span> <span class="toc-text"> 模型构建</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text"> 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.</span> <span class="toc-text"> 监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">2.1.</span> <span class="toc-text"> 基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text"> 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#knn"><span class="toc-number">2.2.1.</span> <span class="toc-text"> KNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.2.</span> <span class="toc-text"> 回归线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.2.2.1.</span> <span class="toc-text"> 线性回归</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">2.2.2.2.</span> <span class="toc-text"> 岭回归</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#lasso"><span class="toc-number">2.2.2.3.</span> <span class="toc-text"> Lasso</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.3.</span> <span class="toc-text"> 分类线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#lr-svc"><span class="toc-number">2.2.3.1.</span> <span class="toc-text"> LR &amp; SVC</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">2.2.3.2.</span> <span class="toc-text"> 多分类</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93"><span class="toc-number">2.2.4.</span> <span class="toc-text"> 线性模型总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">2.2.5.</span> <span class="toc-text"> 朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">2.2.6.</span> <span class="toc-text"> 决策树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E9%9B%86%E6%88%90"><span class="toc-number">2.2.7.</span> <span class="toc-text"> 决策树集成</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">2.2.7.1.</span> <span class="toc-text"> 随机森林</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E6%A0%91"><span class="toc-number">2.2.7.2.</span> <span class="toc-text"> 梯度提升回归树</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">2.2.8.</span> <span class="toc-text"> 核支持向量机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.2.9.</span> <span class="toc-text"> 神经网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E4%B8%8D%E7%A1%AE%E5%AE%9A%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="toc-number">2.3.</span> <span class="toc-text"> 分类器的不确定度估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93%E4%B8%8E%E5%B1%95%E6%9C%9B"><span class="toc-number">2.4.</span> <span class="toc-text"> 小结与展望</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text"> 无监督学习与预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text"> 无监督学习类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">3.2.</span> <span class="toc-text"> 无监督学习的挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%BC%A9%E6%94%BE"><span class="toc-number">3.3.</span> <span class="toc-text"> 预处理与缩放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.</span> <span class="toc-text"> 降维、特征提取与流形学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-number">3.4.1.</span> <span class="toc-text"> 主成分分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="toc-number">3.4.2.</span> <span class="toc-text"> 非负矩阵分解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8-t-sne-%E8%BF%9B%E8%A1%8C%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.4.3.</span> <span class="toc-text"> 用 t-SNE 进行流形学习</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB"><span class="toc-number">3.5.</span> <span class="toc-text"> 聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#k-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB"><span class="toc-number">3.5.1.</span> <span class="toc-text"> k 均值聚类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%9D%E8%81%9A%E8%81%9A%E7%B1%BB"><span class="toc-number">3.5.2.</span> <span class="toc-text"> 凝聚聚类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dbscan-%E8%81%9A%E7%B1%BB"><span class="toc-number">3.5.3.</span> <span class="toc-text"> DBSCAN 聚类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="toc-number">3.6.</span> <span class="toc-text"> 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">4.</span> <span class="toc-text"> 数据表示与特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#one-hot-%E7%BC%96%E7%A0%81%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F"><span class="toc-number">4.1.</span> <span class="toc-text"> one-hot 编码（虚拟变量）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%AE%B1-%E7%A6%BB%E6%95%A3%E5%8C%96-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%A0%91"><span class="toc-number">4.2.</span> <span class="toc-text"> 分箱、离散化、线性模型与树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E4%BA%92%E7%89%B9%E5%BE%81%E4%B8%8E%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81"><span class="toc-number">4.3.</span> <span class="toc-text"> 交互特征与多项式特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="toc-number">4.4.</span> <span class="toc-text"> 单变量非线性变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%8C%96%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">4.5.</span> <span class="toc-text"> 自动化特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BB%9F%E8%AE%A1"><span class="toc-number">4.5.1.</span> <span class="toc-text"> 单变量统计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">4.5.2.</span> <span class="toc-text"> 基于模型的特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">4.5.3.</span> <span class="toc-text"> 迭代特征选择</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%94%B9%E8%BF%9B"><span class="toc-number">5.</span> <span class="toc-text"> 模型评估与改进</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.1.</span> <span class="toc-text"> 交叉验证</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#k-%E6%8A%98%E4%BA%A4%E5%8F%89"><span class="toc-number">5.1.1.</span> <span class="toc-text"> K 折交叉</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B1%82-k-%E6%8A%98%E4%BA%A4%E5%8F%89"><span class="toc-number">5.1.2.</span> <span class="toc-text"> 分层 K 折交叉</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%95%99%E4%B8%80%E6%B3%95%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.1.3.</span> <span class="toc-text"> 留一法交叉验证</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%93%E4%B9%B1%E5%88%92%E5%88%86%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.1.4.</span> <span class="toc-text"> 打乱划分交叉验证</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.1.5.</span> <span class="toc-text"> 分组交叉验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="toc-number">5.2.</span> <span class="toc-text"> 网格搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%A6%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E7%9A%84%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2"><span class="toc-number">5.2.1.</span> <span class="toc-text"> 带交叉验证的网格搜索</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E4%B8%8E%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E7%9A%84%E5%B5%8C%E5%A5%97"><span class="toc-number">5.2.2.</span> <span class="toc-text"> 交叉验证与网格搜索的嵌套</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E4%B8%8E%E8%AF%84%E5%88%86"><span class="toc-number">5.2.3.</span> <span class="toc-text"> 评估指标与评分</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87"><span class="toc-number">5.2.3.1.</span> <span class="toc-text"> 二分类指标</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%86%E7%A1%AE%E7%8E%87-%E5%8F%AC%E5%9B%9E%E7%8E%87%E6%9B%B2%E7%BA%BF"><span class="toc-number">5.2.3.1.1.</span> <span class="toc-text"> 准确率-召回率曲线</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#roc-%E4%B8%8E-auc"><span class="toc-number">5.2.3.1.2.</span> <span class="toc-text"> ROC 与 AUC</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87"><span class="toc-number">5.2.3.2.</span> <span class="toc-text"> 多分类指标</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%8C%87%E6%A0%87"><span class="toc-number">5.2.3.3.</span> <span class="toc-text"> 回归指标</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%8C%87%E6%A0%87%E9%80%89%E6%8B%A9"><span class="toc-number">5.2.3.4.</span> <span class="toc-text"> 模型指标选择</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E9%93%BE%E5%92%8C%E7%AE%A1%E9%81%93"><span class="toc-number">6.</span> <span class="toc-text"> 算法链和管道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E7%AE%A1%E9%81%93"><span class="toc-number">6.1.</span> <span class="toc-text"> 构建管道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%AE%A1%E9%81%93"><span class="toc-number">6.2.</span> <span class="toc-text"> 网格搜索中使用管道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E7%AE%A1%E9%81%93%E6%8E%A5%E5%8F%A3"><span class="toc-number">6.3.</span> <span class="toc-text"> 通用管道接口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">6.4.</span> <span class="toc-text"> 网格搜索&amp;预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">6.5.</span> <span class="toc-text"> 网格搜索&amp;模型选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE"><span class="toc-number">7.</span> <span class="toc-text"> 处理文本数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A2%8B"><span class="toc-number">7.1.</span> <span class="toc-text"> 词袋</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-idf-%E7%BC%A9%E6%94%BE%E6%95%B0%E6%8D%AE"><span class="toc-number">7.2.</span> <span class="toc-text"> tf-idf 缩放数据</span></a></li></ol></li></ol>
    </nav>
  </div>
</aside>

<main class="main" role="main">
  <div class="content">
  <article id="post-机器学习/机器学习基础教程" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      机器学习基础教程
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/" class="article-date">
	  <time datetime="2020-08-18T12:27:37.000Z" itemprop="datePublished">2020-08-18</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </span>

        

        

        <!-- <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/#comments" class="article-comment-link">评论</a></span> -->
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p>深度学习是一种实现机器学习的技术，机器学习是实现人工智能的方法。本文记录为《Python 机器学习基础教程》的阅读笔记。目标是整理出大纲，在后面的实际应用中持续的查漏补缺。</p>
<span id="more"></span>
<h2 id="引言"><a class="markdownIt-Anchor" href="#引言"></a> 引言</h2>
<p>机器学习（machine learning）是从数据中提取知识。它是统计学、人工智能和计算机科学交叉的研究领域，也被称为预测分析（predictive analytics）或统计学习（statistical learning）。</p>
<p>最成功的机器学习算法是能够将决策过程自动化的那些算法，这些决策过程是从已知示例中泛化得出的。</p>
<p>机器学习可以分为两大类：</p>
<ul>
<li>监督学习算法：从输入 / 输出对中进行学习的机器学习算法。</li>
<li>无监督学习算法：只有输入数据是已知的，没有为算法提供输出数据。</li>
</ul>
<p>无论是监督学习任务还是无监督学习任务，将输入数据表征为计算机可以理解的形式都是十分重要的。在机器学习中，这里的每个实体或每一行被称为一个样本（sample）或数据点，而每一列（用来描述这些实体的属性）则被称为特征（feature）。</p>
<p>构建良好的数据表征的方法称为特征提取（feature extraction）或特征工程（feature engineering）。</p>
<h3 id="环境搭建"><a class="markdownIt-Anchor" href="#环境搭建"></a> 环境搭建</h3>
<p>Anaconda 管理 Python 环境，已经预先安装好了 Python 科学计算包。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">conda --version</span><br><span class="line">conda create -n python37 python=3.7</span><br><span class="line">conda env list</span><br><span class="line">conda activate python37</span><br><span class="line">conda remove --name python36 --all</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">第三方包</span></span><br><span class="line">pip install requests</span><br><span class="line">pip uninstall requests</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看环境包信息</span></span><br><span class="line">conda list</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">若直接安装 Python 可用 pip 安装将要用到的所有包</span></span><br><span class="line">pip install numpy scipy matplotlib ipython scikit-learn pandas</span><br></pre></td></tr></table></figure>
<p>必要的库和工具的使用。</p>
<h4 id="numpy"><a class="markdownIt-Anchor" href="#numpy"></a> NumPy</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Numpy 数组</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x:</span></span><br><span class="line"><span class="string">[[1 2 3]</span></span><br><span class="line"><span class="string"> [4 5 6]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="scipy"><a class="markdownIt-Anchor" href="#scipy"></a> SciPy</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Scipy 是用于科学计算的函数集合</span></span><br><span class="line"><span class="comment"># 最重要的是 scipy.sparse 给出稀疏矩阵</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个二维 NumPy 数组，对角线为 1，其余都为 0</span></span><br><span class="line">eye = np.eye(<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy array:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(eye))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">NumPy array:</span></span><br><span class="line"><span class="string">[[ 1. 0. 0. 0.]</span></span><br><span class="line"><span class="string"> [ 0. 1. 0. 0.]</span></span><br><span class="line"><span class="string"> [ 0. 0. 1. 0.]</span></span><br><span class="line"><span class="string"> [ 0. 0. 0. 1.]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 NumPy 数组转换为CSR格式的SciPy稀疏矩阵</span></span><br><span class="line"><span class="comment"># 只保存非零元素</span></span><br><span class="line">sparse_matrix = sparse.csr_matrix(eye)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nSciPy sparse CSR matrix:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(sparse_matrix))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">SciPy sparse CSR matrix:</span></span><br><span class="line"><span class="string"> (0, 0) 1.0</span></span><br><span class="line"><span class="string"> (1, 1) 1.0</span></span><br><span class="line"><span class="string"> (2, 2) 1.0</span></span><br><span class="line"><span class="string"> (3, 3) 1.0</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">## 直接创建稀疏矩阵 COO 格式</span></span><br><span class="line"><span class="comment"># 创建 [1. 1. 1. 1.]</span></span><br><span class="line">data = np.ones(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 创建 [0 1 2 3]</span></span><br><span class="line">row_indices = np.arange(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 创建 [0 1 2 3]</span></span><br><span class="line">col_indices = np.arange(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 直接创建稀疏矩阵，仅提供非 0 的格子与存储的数据</span></span><br><span class="line">eye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;COO representation:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(eye_coo))</span><br></pre></td></tr></table></figure>
<h4 id="matplotlib"><a class="markdownIt-Anchor" href="#matplotlib"></a> matplotlib</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用魔法命令来显示图像 否则需要 plt.show</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 -10 和 10 之间生成一个数列，共 100 个数</span></span><br><span class="line">x = np.linspace(-<span class="number">10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># 用正弦函数创建每个 x 的 y 值</span></span><br><span class="line">y = np.sin(x)</span><br><span class="line"><span class="comment"># plot 函数绘制一个数组关于另一个数组的折线图 (marker=&quot;x&quot; 看起来是用字母 x 在曲线上标记一下而已)</span></span><br><span class="line">plt.plot(x, y, marker=<span class="string">&quot;x&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/img//2020/08/ML/image-20200818214503512.png" alt="image-20200818214503512" style="zoom:50%;" />
<h4 id="pandas"><a class="markdownIt-Anchor" href="#pandas"></a> pandas</h4>
<p>pandas 是用于处理和分析数据的 Python 库。它基于一种叫作 DataFrame 的数据结构，这种数据结构模仿了 R 语言中的 DataFrame。</p>
<p>简单来说，一个 pandas DataFrame 是一张表格，类似于 Excel 表格。pandas 中包含大量用于修改表格和操作表格的方法，尤其是可以像 SQL 一样对表格进行查询和连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="comment">## 利用字典创建 DataFrame</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;Name&#x27;</span>: [<span class="string">&quot;John&quot;</span>, <span class="string">&quot;Anna&quot;</span>, <span class="string">&quot;Peter&quot;</span>, <span class="string">&quot;Linda&quot;</span>],</span><br><span class="line">    <span class="string">&#x27;Location&#x27;</span> : [<span class="string">&quot;New York&quot;</span>, <span class="string">&quot;Paris&quot;</span>, <span class="string">&quot;Berlin&quot;</span>, <span class="string">&quot;London&quot;</span>],</span><br><span class="line">    <span class="string">&#x27;Age&#x27;</span> : [<span class="number">24</span>, <span class="number">13</span>, <span class="number">53</span>, <span class="number">33</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表格</span></span><br><span class="line">data_pandas = pd.DataFrame(data)</span><br><span class="line"><span class="comment"># 打印表格</span></span><br><span class="line">display(data_pandas)</span><br><span class="line"><span class="comment"># 选择年龄大于 30 的所有行</span></span><br><span class="line">display(data_pandas[data_pandas.Age &gt; <span class="number">30</span>])</span><br></pre></td></tr></table></figure>
<h3 id="鸢尾花分类"><a class="markdownIt-Anchor" href="#鸢尾花分类"></a> 鸢尾花分类</h3>
<p>前提：有一些鸢尾花的测量数据，这些花之前已经被植物学专家鉴定为属于 setosa、versicolor 或 virginica 三个品种之一。另外，这些测量数据可以确定每朵鸢尾花所属的品种。</p>
<p>目标：构建一个机器学习模型，可以从这些已知品种的鸢尾花测量数据中进行学习，从而能够预测新鸢尾花的品种。</p>
<p>分析：因为我们有已知品种的鸢尾花的测量数据，所以这是一个监督学习问题。我们要在多个选项中预测其中一个（鸢尾花的品种）。这是一个分类（classification）问题，可能的输出（鸢尾花的不同品种）叫作类别（class）。数据集中的每朵鸢尾花都属于三个类别之一，所以这是一个三分类问题。单个数据点（一朵鸢尾花）的预期输出是这朵花的品种。对于一个数据点来说，它的品种叫作标签（label）。</p>
<h4 id="取得数据"><a class="markdownIt-Anchor" href="#取得数据"></a> 取得数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">iris_dataset = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集包含了哪些信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Keys of iris_dataset: \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset.keys()))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Keys of iris_dataset:</span></span><br><span class="line"><span class="string">dict_keys([&#x27;target_names&#x27;, &#x27;feature_names&#x27;, &#x27;DESCR&#x27;, &#x27;data&#x27;, &#x27;target&#x27;])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 数据集的描述信息 DESCR</span></span><br><span class="line"><span class="built_in">print</span>(iris_dataset[<span class="string">&#x27;DESCR&#x27;</span>][:<span class="number">193</span>] + <span class="string">&quot;\n...&quot;</span>)</span><br><span class="line"><span class="comment"># 数据集包含了 3 个品种，也就是分类</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Target names: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;target_names&#x27;</span>]))</span><br><span class="line"><span class="comment"># 数据集的特征列表</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Feature names: \n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;feature_names&#x27;</span>]))</span><br><span class="line"><span class="comment"># 样本数据在 data 里，是 numpy 的 array，其中 shape 记录了有多少样本，每行样本有几个属性</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Type of data: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(iris_dataset[<span class="string">&#x27;data&#x27;</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape of data: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;data&#x27;</span>].shape))</span><br><span class="line"><span class="comment"># 样本数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First five rows of data:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;data&#x27;</span>][:<span class="number">5</span>]))</span><br><span class="line"><span class="comment"># 分类数据在 target 里，也是 numpy array</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape of target: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(iris_dataset[<span class="string">&#x27;target&#x27;</span>].shape))</span><br></pre></td></tr></table></figure>
<h4 id="划分数据"><a class="markdownIt-Anchor" href="#划分数据"></a> 划分数据</h4>
<p>我们想要利用这些数据构建一个机器学习模型，用于预测新测量的鸢尾花的品种。但在将模型应用于新的测量数据之前，我们需要知道模型是否有效，也就是说，我们是否应该相信它的预测结果。</p>
<p>我们不能将用于构建模型的数据用于评估模型。因为我们的模型会一直记住整个训练集，所以对于训练集中的任何数据点总会预测正确的标签。这种“记忆”无法告诉我们模型的泛化（generalize）能力如何（换句话说，在新数据上能否正确预测）。</p>
<p>通常的做法是将收集好的带标签数据（此例中是 150 朵花的测量数据 分成两部分。一部分数据用于构建机器学习模型，叫作训练数据（training data）或训练集（training set）。其余的数据用来评估模型性能，叫作测试数据（test data）、测试集（test set）或留出集（hold-out set）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切分样本</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">iris_dataset = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_test_split 函数利用伪随机数生成器将数据集打乱</span></span><br><span class="line"><span class="comment"># 为保证多次运行输出一致，这里指定 random_state = 0（随机生成器种子）</span></span><br><span class="line">X_train, X_test, y_train, y_test = \</span><br><span class="line">	train_test_split(iris_dataset[<span class="string">&#x27;data&#x27;</span>], iris_dataset[<span class="string">&#x27;target&#x27;</span>], random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 75% 25%</span></span><br><span class="line"><span class="built_in">print</span>(X_train.shape, y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(X_test.shape, y_test.shape)</span><br></pre></td></tr></table></figure>
<h4 id="观察数据"><a class="markdownIt-Anchor" href="#观察数据"></a> 观察数据</h4>
<p>在构建机器学习模型之前，通常最好检查一下数据，看看如果不用机器学习能不能轻松完成任务，或者需要的信息有没有包含在数据中。此外，检查数据也是发现异常值和特殊值的好方法。</p>
<p>检查数据的最佳方法之一就是将其可视化。一种可视化方法是绘制散点图（scatter plot）。数据散点图将一个特征作为 x 轴，另一个特征作为 y 轴，将每一个数据点绘制为图上的一个点。用这种方法难以对多于 3 个特征的数据集作图。解决这个问题的一种方法是绘制散点图矩阵（pair plot），从而可以两两查看所有的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line">iris_dataset = load_iris()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = \</span><br><span class="line">	train_test_split(iris_dataset[<span class="string">&#x27;data&#x27;</span>], iris_dataset[<span class="string">&#x27;target&#x27;</span>], random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 X_train 中的数据创建 DataFrame</span></span><br><span class="line"><span class="comment"># 利用 iris_dataset.feature_names 中的字符串对数据列进行标记</span></span><br><span class="line">iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)</span><br><span class="line"><span class="comment"># 利用 DataFrame 创建散点图矩阵，按 y_train 着色</span></span><br><span class="line"><span class="comment"># mglearn 为自定义图像美化库</span></span><br><span class="line">grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(<span class="number">15</span>, <span class="number">15</span>), \</span><br><span class="line">	marker=<span class="string">&#x27;o&#x27;</span>, hist_kwds=&#123;<span class="string">&#x27;bins&#x27;</span>: <span class="number">20</span>&#125;, s=<span class="number">60</span>, alpha=<span class="number">.8</span>, cmap=mglearn.cm3)</span><br></pre></td></tr></table></figure>
<p>这个画散点矩阵图的函数说实话我看不太懂。</p>
<h4 id="模型构建"><a class="markdownIt-Anchor" href="#模型构建"></a> 模型构建</h4>
<p>好了，现在让我们开始构建真实的机器学习模型了。让我们先从一个比较容易理解的算法：KNN 分类器算法（k 近邻分类器）。</p>
<p>k 近邻算法中 k 的含义是，我们可以考虑训练集中与新数据点最近的任意 k 个邻居（比如说，距离最近的 3 个或 5 个邻居），而不是只考虑最近的那一个。然后，我们可以用这些邻居中数量最多的类别做出预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">iris_dataset = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分 75% 的训练集和 25% 的测试集, 随机打乱数据的种子固定为 0</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_dataset[<span class="string">&#x27;data&#x27;</span>], iris_dataset[<span class="string">&#x27;target&#x27;</span>], random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># K 邻近分类器, 算法取最近 1 个点的标签作为分类 即 k=1</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造一个测试的数据, 需要作为一行放在矩阵里</span></span><br><span class="line">X_new = np.array([[<span class="number">5</span>, <span class="number">2.9</span>, <span class="number">1</span>, <span class="number">0.2</span>]])</span><br><span class="line"><span class="comment"># 预测分类</span></span><br><span class="line">y_new = knn.predict(X_new)</span><br><span class="line"><span class="comment"># 分类是整形 0, 1, 2 分别对应相应的类型</span></span><br><span class="line"><span class="built_in">print</span>(y_new)</span><br><span class="line"><span class="comment"># 对应成分类的名字</span></span><br><span class="line"><span class="built_in">print</span>(iris_dataset.target_names[y_new])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集做预测</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"><span class="comment"># 计算预测的精度, 其中 y_pred==y_test 得到的是一个 bool 向量, np.mean 返回 True 的比例</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(y_pred == y_test)))</span><br><span class="line"><span class="comment"># 也可以直接用模型的 score 方法来完成精度计算</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<h3 id="小结"><a class="markdownIt-Anchor" href="#小结"></a> 小结</h3>
<ul>
<li>监督学习与非监督学习</li>
<li>可能的品种叫做分类，具体每个样本的品种就叫做标签</li>
<li>数据集需划分为训练集和测试集</li>
<li>数据集包括了 X 和 y，其中 X 是特征的二维数组，y 是标签的一维数组</li>
<li>本章用到了 scikit-learn 中任何机器学习算法的核心方法：fit、predict、score</li>
</ul>
<h2 id="监督学习"><a class="markdownIt-Anchor" href="#监督学习"></a> 监督学习</h2>
<p>记住，每当想要根据给定输入预测某个结果，并且还有输入 / 输出对的示例时，都应该使用监督学习。</p>
<h3 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h3>
<p><strong>分类与回归</strong></p>
<p>简单来说，分类是预测标签，包括二分类与多分类。回归是预测连续值，比如预测收入、房价。</p>
<p><strong>泛化、过拟合与欠拟合</strong></p>
<p>随着模型算法逐渐复杂，其在训练集上的预测精度将提高，但在测试集上的预测精度将降低，因此模型的复杂度需要折衷。</p>
<p>模型过于复杂，将导致模型泛化能力差，即过拟合。 模型过于简单，将导致模型精度在训练集表现就很差，更不用说测试集的表现了，此时即欠拟合。</p>
<img src="/img//2020/08/ML/image-20200820210552194.png" alt="image-20200820210552194" style="zoom:50%;" />
<p><strong>模型复杂度与数据集大小的关系</strong></p>
<p>数据点的值变化范围越大，则可以应用更加复杂的模型，预测的表现也会越好。更多的训练数据往往伴随着更大范围的特征值变化，因此可以应用更复杂的模型算法。</p>
<p>但注意，如果是非常类似的数据点，无论数据集多大也是无济于事的。</p>
<h3 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h3>
<p>现在开始介绍最常用的机器学习算法，并解释这些算法如何从数据中学习以及如何预测。</p>
<blockquote>
<p>许多算法都有分类和回归两种形式。</p>
</blockquote>
<h4 id="knn"><a class="markdownIt-Anchor" href="#knn"></a> KNN</h4>
<p>前面的鸢尾花的例子已经简单的介绍过 KNN 的原理，这里我们先用一张图来简单的复习一下，这张图表示的是 <code>n_neighbors=3</code> 的情况（3 个邻居）。</p>
<img src="/img//2020/08/ML/image-20200820231618577.png" alt="image-20200820231618577" style="zoom: 67%;" />
<p>好了，我们将在现实世界的乳腺癌数据集上进行研究。先将数据集分成训练集和测试集，然后用不同的邻居个数对训练集和测试集的性能进行评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">66</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录不同 n_neighbors 情况下，模型的训练集精度与测试集精度的变化</span></span><br><span class="line">training_accuracy = [] </span><br><span class="line">test_accuracy = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># n_neighbors 取值从 1 到 10 </span></span><br><span class="line">neighbors_settings = <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>)</span><br><span class="line"><span class="keyword">for</span> n_neighbors <span class="keyword">in</span> neighbors_settings:</span><br><span class="line">    <span class="comment"># 模型对象</span></span><br><span class="line">    clf = KNeighborsClassifier(n_neighbors=n_neighbors) </span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    <span class="comment"># 记录训练集精度</span></span><br><span class="line">    training_accuracy.append(clf.score(X_train, y_train)) </span><br><span class="line">    <span class="comment"># 记录测试集精度</span></span><br><span class="line">    test_accuracy.append(clf.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出 2 条曲线，横坐标是邻居个数，纵坐标分别是训练集精度和测试集精度</span></span><br><span class="line">plt.plot(neighbors_settings, training_accuracy, label=<span class="string">&quot;training accuracy&quot;</span>)</span><br><span class="line">plt.plot(neighbors_settings, test_accuracy, label=<span class="string">&quot;test accuracy&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Accuracy&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;n_neighbors&quot;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<img src="/img//2020/08/ML/image-20200821222155374.png" alt="image-20200821222155374" style="zoom: 67%;" />
<p>通过前面的介绍，我们可以知道是否有乳腺癌是一个二分类的问题，这里的 KNN 属于分类算法形式。但其实 KNN 也可以用于回归算法形式。</p>
<p>我们先看看 wave 数据集在 <code>n_neighbors = 3</code> 的图示情况。在使用多个近邻时，预测结果为这些近邻的平均值。</p>
<img src="/img//2020/08/ML/image-20200821225715794.png" alt="image-20200821225715794" style="zoom: 67%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集 （自定义的 wave 数据集）</span></span><br><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 wave 数据集分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型实例化 并将邻居个数设为 3 这里用的是回归模型</span></span><br><span class="line">reg = KNeighborsRegressor(n_neighbors=<span class="number">3</span>) </span><br><span class="line"><span class="comment"># 利用训练数据和训练目标值来拟合模型 </span></span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># predict 测试集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set predictions:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(reg.predict(X_test)))</span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set R^2: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(reg.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p>到这里，我们不难发现，KNN 在 <code>n_neighbors</code> 的值越大时，模型越简单，约适合简单的数据。在测试 KNN 分类形式时，我们做了 <code>n_neighbors</code> 从 1~10 的测试精确度与预测精确度的变化对比，从上面的图中可以很好的看出这一规律。其实也很好理解，毕竟判断所需要的邻居越多，那模型的鲁棒性肯定是越强的，那表现大概率也是趋于稳定的。</p>
<p>虽然 k 近邻算法很容易理解，但由于预测速度慢且不能处理具有很多特征的数据集，所以在实践中往往不会用到。接下来我们来看看实践中常用的模型。</p>
<h4 id="回归线性模型"><a class="markdownIt-Anchor" href="#回归线性模型"></a> 回归线性模型</h4>
<p>线性模型利用输入特征的线性函数（linear function）进行预测。</p>
<p>对于回归问题，线性模型预测的一般公式是：<code>ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b</code>。有许多种不同的线性回归模型，区别在于模型如何学习到参数 <code>w</code> 和 <code>b</code>，以及如何控制模型复杂度。</p>
<h5 id="线性回归"><a class="markdownIt-Anchor" href="#线性回归"></a> 线性回归</h5>
<p>线性回归，或者普通最小二乘法（ordinary least squares，OLS），是回归问题最简单也最经典的线性方法。线性回归寻找参数 <code>w</code> 和 <code>b</code>，使得对训练集的预测值与真实的回归目标值 <code>y</code> 之间的均方误差最小。</p>
<p>均方误差（mean squared error）是预测值与真实值之差的平方和除以样本数。线性回归没有参数，这是一个优点，但也因此无法控制模型的复杂度。</p>
<p>来看看 wave 数据集的线性回归模型预测的图示。</p>
<img src="/img//2020/08/ML/image-20200821233630557.png" alt="image-20200821233630557" style="zoom:67%;" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 60 个样本数据, 一维特征</span></span><br><span class="line">X, y = mglearn.datasets.make_wave(n_samples=<span class="number">60</span>)</span><br><span class="line"><span class="comment"># 切分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 训练线性回归模型</span></span><br><span class="line">lr = LinearRegression().fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coef_ 就是斜率 w 即每个特征对应一个权重</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;lr.coef_: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr.coef_))</span><br><span class="line"><span class="comment"># intercept_ 是截距 b</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;lr.intercept_: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr.intercept_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_train, y_train)))</span><br><span class="line"><span class="comment"># 测试机精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.67</span></span><br><span class="line"><span class="string">Test set score: 0.66</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>你可能注意到了 <code>coef_</code> 和 <code>intercept_</code> 结尾处奇怪的下划线。sklearn 总是将从训练数据中得出的值保存在以下划线结尾的属性中。这是为了将其 与用户设置的参数区分开。</p>
<p>从这个模拟的小数据集 wave 的训练中得到的训练精确度和测试精确度非常接近，这表示可能存在欠拟合的问题。对于简单的数据（比如本例子中的一维数据集）来说，过拟合的风险很小，因为模型非常简单。</p>
<p>然而，对于更高维的数据集（即有大量特征的数据集），线性模型将变得更加强大，过拟合的可能性也会变大。接下来让我我们来看一下 LinearRegression 在更复杂的数据集上的表现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 波士顿 extended 数据集</span></span><br><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">lr = LinearRegression().fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.95</span></span><br><span class="line"><span class="string">Test set score: 0.61</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在这个维度较高的数据集中，我们在训练集上的预测非常准确，但测试集上的精确度就要低很多。训练集和测试集之间的性能差异是过拟合的明显标志，因此我们应该试图找到一个可以控制复杂度的模型。标准线性回归最常用的替代方法之一就是岭回归（ridge regression），下面来看一下。</p>
<h5 id="岭回归"><a class="markdownIt-Anchor" href="#岭回归"></a> 岭回归</h5>
<p>采用线性回归同样的公式，但是模型约束学习得到的 <code>w</code> 系数尽可能的接近于 0，即每个特征对输出的影响尽可能小，从而避免过拟合。这个约束叫做正则化，岭回归用到的是 L2 正则化。</p>
<blockquote>
<p>L1 正则化是指权值向量 <code>w</code> 中各个元素的绝对值之和，通常表示为 ||w||<sub>1</sub>。</p>
<p>L2 正则化是指权值向量 <code>w</code> 中各个元素的平方和然后再求平方根，通常表示为 ||w||<sub>2</sub>。</p>
<img src="/img//2020/08/ML/image-20200822001304469.png" alt="image-20200822001304469" style="zoom:50%;" />
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Tx411j7tJ?from=search&amp;seid=3771131606128307897">B 站视频详细讲解</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 波士顿 extended 数据集</span></span><br><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认值 alpha=1</span></span><br><span class="line">ridge = Ridge(alpha=<span class="number">1</span>).fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge.score(X_test, y_test)))</span><br><span class="line">```</span><br><span class="line">Training <span class="built_in">set</span> score: <span class="number">0.89</span></span><br><span class="line">Test <span class="built_in">set</span> score: <span class="number">0.75</span></span><br><span class="line">```</span><br></pre></td></tr></table></figure>
<p>Ridge 在训练集上的分数要低于 LinearRegression，但在测试集上的分数更高。 这和我们的预期一致。</p>
<p>岭回归泛化能力优于线性回归，带来的就是训练集精度下降，测试集精度上升。其基本原理是对影响力大的  <code>w</code> 项进行了惩罚。</p>
<p>该模型支持 alpha 参数，该参数默认为 1，调大 alpha 增大约束会进一步下降训练集精度，可能加强泛化能力；相反，调小 alpha 则减少了约束，训练集精度上升，可能降低泛化能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用上面的数据尝试 alpha=10 的情况</span></span><br><span class="line">ridge10 = Ridge(alpha=<span class="number">10</span>).fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge10.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge10.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.79</span></span><br><span class="line"><span class="string">Test set score: 0.64</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># alpha=0.1 的情况</span></span><br><span class="line">ridge01 = Ridge(alpha=<span class="number">0.1</span>).fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge10.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(ridge10.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.93</span></span><br><span class="line"><span class="string">Test set score: 0.77</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>岭回归会对模型的进行约束，所以在训练集较少的时候表现要优于线性回归。但需要注意的是，当数据量越来越多的时候，正则化变得不那么重要，并且岭回归和线性回归将具有相同的性能。</p>
<h5 id="lasso"><a class="markdownIt-Anchor" href="#lasso"></a> Lasso</h5>
<p>除了 Ridge，还有一种正则化的线性回归是 Lasso，不同之处在于 Lasso 用的方法为 L1 正规化。L1 正则化的结果是，使用 Lasso 时某些系数刚好为 0。这说明某些特征被模型完全忽略。这可以看作是一种自动化的特征选择。某些系数刚好为 0，这样模型更容易解释，也可以呈现模型最重要的特征。</p>
<p>同样的的，我们将 Lasso 应用在波士顿房价数据集上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 波士顿 extended 数据集</span></span><br><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">lasso = Lasso().fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lasso.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lasso.score(X_test, y_test)))</span><br><span class="line"><span class="comment"># lasso.coef_ 是 w 斜率向量 数一下有几个特征的系数不为 0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of features used: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.<span class="built_in">sum</span>(lasso.coef_ != <span class="number">0</span>)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.29</span></span><br><span class="line"><span class="string">Test set score: 0.21</span></span><br><span class="line"><span class="string">Number of features used: 4</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>该模型只用到了 105 个特征中的 4 个，其他的 <code>w</code> 系数都是 0。从以上这个结果来看，该模型预测精度很差，属于欠拟合，需要减少模型的 alpha 参数，即放松正则化 L1。这么做的同时，我们还需要增加 <code>max_iter</code> 的值（运行迭代的最大次数）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 波士顿 extended 数据集</span></span><br><span class="line">X, y = mglearn.datasets.load_extended_boston()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 增大 max_iter 的值 否则模型会警告我们 说应该增大 max_iter</span></span><br><span class="line">lasso001 = Lasso(alpha=<span class="number">0.01</span>, max_iter=<span class="number">100000</span>).fit(X_train, y_train) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lasso001.score(X_train, y_train))) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lasso001.score(X_test, y_test))) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of features used: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.<span class="built_in">sum</span>(lasso001.coef_ != <span class="number">0</span>)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.90</span></span><br><span class="line"><span class="string">Test set score: 0.77</span></span><br><span class="line"><span class="string">Number of features used: 33</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在实践中，在两个模型中一般首选岭回归。但如果特征很多，你认为只有其中几个是重要的，那么选择 Lasso 可能更好。同样，如果你想要一个容易解释的模型，Lasso 可以给出更容易理解的模型，因为它只选择了一部分输入特征。scikit-learn 还提供了 ElasticNet 类，结合了 Lasso 和 Ridge 的惩罚项。在实践中，这种结合的效果最好，不过代价是要调节两个参数：一个用于 L1 正则化，一个用于 L2 正则化。</p>
<h4 id="分类线性模型"><a class="markdownIt-Anchor" href="#分类线性模型"></a> 分类线性模型</h4>
<p>线性模型也广泛应用于分类问题。我们首先来看二分类。这时可以利用下面的公式进行预测：</p>
<p><code>ŷ = w[0] * x[0] + w[1] * x[1] + ...+ w[p] * x[p] + b &gt; 0</code></p>
<p>设置了阈值 0，y 小于 0 则预测为类别 -1，大于 0 则预测为类类别 +1。</p>
<p>不同的线性分类算法的区别包括 2 点：</p>
<ul>
<li><code>w</code> 和 <code>b</code> 对训练集拟合好坏的度量方式（损失函数）</li>
<li>是否使用正则化以及使用哪种正则化</li>
</ul>
<p>常见线性分类算法包括：</p>
<ul>
<li>LogisticRegression：Logistic 回归分类器（注意只是名字叫回归，但是分类算法）</li>
<li>LinearSVC：线性支持向量机分类器</li>
</ul>
<blockquote>
<p>这两个分类算法默认应用的是 L2 正则化。</p>
<p>SVM=Support Vector Machine 支持向量机</p>
<p>SVC=Support Vector Classification 支持向量机用于分类</p>
<p>SVC=Support Vector Regression 支持向量机用于回归分析</p>
</blockquote>
<h5 id="lr-svc"><a class="markdownIt-Anchor" href="#lr-svc"></a> LR &amp; SVC</h5>
<p>让我们在构造的小数据集上尝试着对比这两种线性分类算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">X, y = mglearn.datasets.make_forge()</span><br><span class="line"></span><br><span class="line"><span class="comment"># subplots(m,n,figsize)函数 把 n 个图画在 m 行里，每个图片的长宽由 figsize 指定</span></span><br><span class="line"><span class="comment"># 返回的第二个值是每个图的绘制位置</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 zip 组合：让 LinearSVC 画在第一个图片中，LogisticRegression 画在第二个图片中</span></span><br><span class="line"><span class="comment"># zip() 函数用于将可迭代的对象作为参数</span></span><br><span class="line"><span class="comment"># 将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</span></span><br><span class="line"><span class="comment"># 权衡参数默认 C=1</span></span><br><span class="line"><span class="keyword">for</span> model, ax <span class="keyword">in</span> <span class="built_in">zip</span>([LinearSVC(C=<span class="number">1</span>, penalty=<span class="string">&quot;l2&quot;</span>), LogisticRegression(C=<span class="number">1</span>, penalty=<span class="string">&quot;l2&quot;</span>)], axes):</span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    clf = model.fit(X, y)</span><br><span class="line">    <span class="comment"># 画出了这个线性 model 的图像 是一个斜线</span></span><br><span class="line">    mglearn.plots.plot_2d_separator(clf, X, fill=<span class="literal">False</span>, eps=<span class="number">0.5</span>, ax=ax, alpha=<span class="number">.7</span>)</span><br><span class="line">    <span class="comment"># 特征一 特征二 输入的标签 图像位置 是一个离散点图</span></span><br><span class="line">    mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y, ax=ax)</span><br><span class="line">    ax.set_title(<span class="string">&quot;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(clf.__class__.__name__))</span><br><span class="line">    ax.set_xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].legend()</span><br></pre></td></tr></table></figure>
<img src="/img//2020/08/ML/image-20200822123541273.png" alt="image-20200822123541273" style="zoom:80%;" />
<p>两个模型得到了相似的决策边界。注意，两个模型中都有两个点的分类是错误的。两个模型都默认使用 L2 正则化，就像 Ridge 对回归所做的那样。</p>
<p>对于 LogisticRegression 和 LinearSVC，决定正则化强度的权衡参数叫作 C。C 值越大，对应的正则化越弱。换句话说，如果参数 C 值较大，那么 LogisticRegression 和 LinearSVC 将尽可能将训练集拟合到最好，而如果 C 值较小，那么模型更强调使系数向量（w）接近于 0。</p>
<p>参数 C 的作用还有另一个有趣之处。较小的 C 值可以让算法尽量适应大多数数据点，而较大的 C 值更强调每个数据点都分类正确的重要性。C 越大越有可能导致模型的过拟合。</p>
<img src="/img//2020/08/ML/image-20200822124604426.png" alt="image-20200822124604426" style="zoom:80%;" />
<p>与回归的情况类似，用于分类的线性模型在低维空间中看起来可能非常受限，决策边界只能是直线或平面。同样，在高维空间中，用于分类的线性模型变得非常强大，当考虑更多特征时，避免过拟合变得越来越重要。</p>
<p>然我们在乳腺癌数据集上详细分析逻辑回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">42</span>)</span><br><span class="line">logreg = LogisticRegression().fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.955</span></span><br><span class="line"><span class="string">Test set score: 0.958</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在训练集和测试集上的精度（性能）都很好，而且基本一样，这种情况可以尝试加强对训练集拟合看是否能带来进一步提升。将 <code>C</code> 调大以减弱正则化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logreg = LogisticRegression(C=<span class="number">100</span>).fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(logreg.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Training set score: 0.974</span></span><br><span class="line"><span class="string">Test set score: 0.965</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>从以上结果可以发现，模型的精度得到了进一步的提升。</p>
<p>如果想要一个可解释性更强的模型，使用 L1 正则化可能更好，因为它约束模型只使用少数几个特征</p>
<h5 id="多分类"><a class="markdownIt-Anchor" href="#多分类"></a> 多分类</h5>
<p>许多线性分类只适用于二分类问题，不能轻易推广到多类别问题（除了 LR）。将二分类算法推广到多分类算法的一种常见方法是一对其余 <code>one-vs.-rest</code> 方法。</p>
<blockquote>
<p><code>one-vs.rest</code>：对每个类别都学习一个二分类模型，将这个类别与所有其他类别尽量分开，这样就生成了与类别个数一样多的二分类模型。在测试点上运行所有二类分类器来进行预测。在对应类别上分数最高的分类器胜出，将这个类别标签返回作为预测结果。</p>
</blockquote>
<p>我们用一个简单的自定义数据集来体验一下 <code>one-vs.-rest</code> 方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line">X, y = make_blobs(random_state=<span class="number">42</span>)</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Class 0&quot;</span>, <span class="string">&quot;Class 1&quot;</span>, <span class="string">&quot;Class 2&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>这个由高斯分布中采样得出的数据如下图所示。</p>
<img src="/img//2020/08/ML/image-20200822143223061.png" alt="image-20200822143223061" style="zoom:50%;" />
<p>现在，在这个数据集上训练一个 LinearSVC 分类器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">linear_svm = LinearSVC().fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficient shape: &quot;</span>, linear_svm.coef_.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Intercept shape: &quot;</span>, linear_svm.intercept_.shape)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Coefficient shape: (3, 2)</span></span><br><span class="line"><span class="string">Intercept shape: (3,)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><code>coef_</code> 的形状是 (3, 2)，说明 <code>coef_</code> 每行包含三个类别之一的系数向量，每列包含某个特征（这个数据集有 2 个特征）对应的系数值。现在 <code>intercept_</code> 是一维数组，保存每个类别的截距。</p>
<p>接下来让我们将这 3 个二分类器所代表的直线进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mglearn.plots.plot_2d_classification(linear_svm, X, fill=<span class="literal">True</span>, alpha=<span class="number">.7</span>)</span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line"><span class="comment"># 生成数列</span></span><br><span class="line">line = np.linspace(-<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line"><span class="keyword">for</span> coef, intercept, color <span class="keyword">in</span> <span class="built_in">zip</span>(linear_svm.coef_, linear_svm.intercept_, [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;g&#x27;</span>]):</span><br><span class="line">	plt.plot(line, -(line * coef[<span class="number">0</span>] + intercept) / coef[<span class="number">1</span>], c=color)</span><br><span class="line">plt.ylim(-<span class="number">10</span>, <span class="number">15</span>)</span><br><span class="line">plt.xlim(-<span class="number">10</span>, <span class="number">8</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Class 0&#x27;</span>, <span class="string">&#x27;Class 1&#x27;</span>, <span class="string">&#x27;Class 2&#x27;</span>, <span class="string">&#x27;Line class 0&#x27;</span>, <span class="string">&#x27;Line class 1&#x27;</span>, <span class="string">&#x27;Line class 2&#x27;</span>], loc=(<span class="number">1.01</span>, <span class="number">0.3</span>))</span><br></pre></td></tr></table></figure>
<img src="/img//2020/08/ML/image-20200822145645813.png" alt="image-20200822145645813" style="zoom: 80%;" />
<h4 id="线性模型总结"><a class="markdownIt-Anchor" href="#线性模型总结"></a> 线性模型总结</h4>
<p>线性模型主要参数就是正则化参数，包括 L1 / L2，以及回归的 <code>alpha</code> 以及分类的 <code>C</code> 值。</p>
<p><code>alpha</code> 越大或者 <code>C</code> 越小，则正则化越强，可以理解为 <code>w</code> 系数都很小，模型很简单，对训练集精度也会下降。</p>
<p>线性模型无论训练还是预测都很快，但是大数据集需要考虑 <code>solver='sag'</code> 加速训练。</p>
<p>L1 正则化因为会让很多 <code>w</code> 系数为 0，使模型更简单，更容易理解。</p>
<h4 id="朴素贝叶斯分类器"><a class="markdownIt-Anchor" href="#朴素贝叶斯分类器"></a> 朴素贝叶斯分类器</h4>
<p>与线性模型相似，但速度更快，泛化能力较弱的一种分类器。</p>
<p>朴素贝叶斯模型如此高效的原因在于，它通过单独查看每个特征来学习参数，并从每个特征中收集简单的类别统计数据。</p>
<p>一共有三种类型的模型。</p>
<ul>
<li>GaussianNB: 特征可以是任意连续数据。</li>
<li>BernoulliNB：特征必须是 2 分类的数据。</li>
<li>MultinomialNB：特征是计数性质的数据。</li>
</ul>
<p>GaussianNB 适合高维数据，后两者适合文本领域的稀疏数据。后两个模型支持 <code>alpha</code> 参数，调大该值可以略微提高精度。</p>
<h4 id="决策树"><a class="markdownIt-Anchor" href="#决策树"></a> 决策树</h4>
<p>让我们来看一个简单的 <code>deep=2</code> 的树的决策边界与相应的树的图示。</p>
<p><img src="/img//2020/08/ML/image-20200822152548464.png" alt="image-20200822152548464" /></p>
<p>随着树的深度的增加，模型肯定会越来越复杂。那我们该如何控制决策树的复杂度呢？</p>
<p>防止过拟合有两种常见的策略：一种是及早停止树的生长，也叫预剪枝（pre-pruning）；另一种是先构造树，但随后删除或折叠信息量很少的结点，也叫后剪枝（post-pruning）或剪枝（pruning）。预剪枝的限制条件可能包括限制树的最大深度、限制叶结点的最大数目，或者规定一个结点中数据点的最小数目来防止继续划分。</p>
<blockquote>
<p>sklearn 只实现了预剪枝，没有实现后剪枝。</p>
</blockquote>
<p>让我们在乳腺癌数据集上更详细地看一下预剪枝的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, stratify=cancer.target, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树分类</span></span><br><span class="line">tree = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tree.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(tree.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(tree.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 1.000</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.937</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>我们用数据进行了建模，并没有进行预剪枝的处理，这个训练精度为 100% 的结果是可以预测到的，这是因为叶结点都是纯的，树的深度很大，足以完美地记住训练数据的所有标签。测试集精度比之前讲过的线性模型略低，线性模型的精度约为 95%。</p>
<p>如果我们不限制树的深度，其复杂度和深度将可以无限大。故未进行预剪枝的树很容易过拟合，对新数据的泛化能力不高。接下来，我们将树的深度控制在 4 来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策树分类</span></span><br><span class="line">tree = DecisionTreeClassifier(max_depth=<span class="number">4</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">tree.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(tree.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(tree.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 0.988</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.951</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>对于决策树模型，我们可以利用 tree 模块的 export_graphviz 函数来将树可视化。这个函数会生成一个 <code>.dot</code> 格式的文件，这是一种用于保存图形的文本文件格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导出</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line">export_graphviz(tree, out_file=<span class="string">&quot;tree.dot&quot;</span>, class_names=[<span class="string">&quot;malignant&quot;</span>,<span class="string">&quot;benign&quot;</span>],</span><br><span class="line">                feature_names=cancer.feature_names, impurity=<span class="literal">False</span>, filled=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 读入</span></span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;tree.dot&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    dot_graph = f.read()</span><br><span class="line">graphviz.Source(dot_graph)</span><br></pre></td></tr></table></figure>
<p>查看整个树是非常费劲的，我们常用特征重要性（feature importance）来为每个特征对树的决策的重要性进行排序。对于每个特征来说，它都是一个介于 0 和 1 之间的数字，其中 0 表示根本没用到，1 表示完美预测目标值。特征重要性的求和始终为 1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Feature importances:\n&#123;&#125;&quot;</span>.<span class="built_in">format</span>(tree.feature_importances_))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Feature importances:</span></span><br><span class="line"><span class="string">[ 0.    0. 	0. 	  0.    0.    0.    0.    0. 0. 0.    0.01</span></span><br><span class="line"><span class="string">  0.048 0. 	0. 	  0.002 0.    0.    0.    0. 0. 0.727 0.046</span></span><br><span class="line"><span class="string">  0.    0. 	0.014 0.    0.018 0.122 0.012 0. ]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>回归的决策树与分类的决策树是很相似的，但在将基于树的模型用于回归时，都是不能外推（extrapolate）的，也不能在训练数据范围之外进行预测。</p>
<p>这是因为回归是为了预测一个连续值，但是树的叶节点只能保存训练集内出现过的目标值，因此对于训练集外的数据是无法进行目标预测的，只能得到一个训练集内出现过的结果。</p>
<h4 id="决策树集成"><a class="markdownIt-Anchor" href="#决策树集成"></a> 决策树集成</h4>
<p>集成（ensemble）是合并多个机器学习模型来构建更强大模型的方法。现已证明的对大量分类和回归的数据集都是有效的模型有：是随机森林（random forest）和梯度提升决策树（gradient boosted decision tree），二者都是以决策树为基础的。</p>
<h5 id="随机森林"><a class="markdownIt-Anchor" href="#随机森林"></a> 随机森林</h5>
<p>前面我们说过，决策树的一个主要缺点在于经常对训练数据过拟合。随机森林是解决这个问题的一种方法。随机森林本质上是许多决策树的集合，其中每棵树都和其他树略有不同。</p>
<p>随机森林背后的思想是，每棵树的预测可能都相对较好，但可能对部分数据过拟合。如果构造很多树，并且每棵树的预测都很好，但都以不同的方式过拟合，那么我们可以对这些树的结果取平均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。</p>
<p>随机森林的名字来自于将随机性添加到树的构造过程中，以确保每棵树都各不相同。随机森林中树的随机化方法有两种：</p>
<ul>
<li>
<p>一种是通过选择用于构造树的数据点。</p>
</li>
<li>
<p>另一种是通过选择每次划分测试的特征。</p>
</li>
</ul>
<p>想要构造一棵树，首先要对数据进行自助采样（bootstrap sample）。也就是说，从 n_samples 个数据点中有放回地（即同一样本可以被多次抽取）重复随机抽取一个样本，共抽取 n_samples 次。这样会创建一个与原数据集大小相同的数据集，但有些数据点会缺失（大约三分之一），有些会重复。</p>
<p>想要利用随机森林进行预测，算法首先对森林中的每棵树进行预测。对于回归问题，我们可以对这些结果取平均值作为最终预测。对于分类问题，则用到了软投票（soft voting）策略。也就是说，每个算法做出软预测，给出每个可能的输出标签的概率。对所有树的预测概率取平均值，然后将概率最大的类别作为预测结果。</p>
<p>让我们来看一个 5 个树的随机森林分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">100</span>, noise=<span class="number">0.25</span>, random_state=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 依据标签 y 按原数据 y 中各类比例 分配给 train 和 test</span></span><br><span class="line"><span class="comment"># 使得 train 和 test 中各类数据的比例与原数据集一样</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=<span class="number">42</span>)</span><br><span class="line">forest = RandomForestClassifier(n_estimators=<span class="number">5</span>, random_state=<span class="number">2</span>)</span><br><span class="line">forest.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(forest.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(forest.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<p>作为随机森林的一部分，树被保存在 <code>estimator_</code> 属性中。</p>
<p>与决策树类似，随机森林也可以给出特征重要性，计算方法是将森林中所有树的特征重要性求和并取平均。一般来说，随机森林给出的特征重要性要比单棵树给出的更为可靠。是随机森林比单棵树更能从总体把握 数据的特征。</p>
<h5 id="梯度提升回归树"><a class="markdownIt-Anchor" href="#梯度提升回归树"></a> 梯度提升回归树</h5>
<p>梯度提升回归树是另一种集成方法，通过合并多个决策树来构建一个更为强大的模型。名字中含有回归，但这个模型既可以用于回归也可以用于分类。与随机森林方法不同，梯度提升采用连续的方式构造树，每棵树都试图纠正前一棵树的错误。默认情况下，梯度提升回归树中没有随机化，而是用到了强预剪枝。梯度提升树通常使用深度很小（1到 5 之间）的树，这样模型占用的内存更少，预测速度也更快。</p>
<p>梯度提升背后的主要思想是合并许多简单的模型（在这个语境中叫作弱学习器），比如深度较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越来越多，可以不断迭代提高性能。</p>
<p>与随机森林相比，它通常对参数设置更为敏感，但如果参数设置正确的话，模型精度更高。除了预剪枝与集成中树的数量之外，梯度提升的另一个重要参数是 <code>learning_rate</code>（学习率），用于控制每棵树纠正前一棵树的错误的强度。较高的学习率意味着每棵树都可以做出较强的修正，这样模型更为复杂。通过增大 <code>n_estimators</code> 来向集成中添加更多树，也可以增加模型复杂度，因为模型有更多机会纠正训练集上的错误。</p>
<p>让我们在乳腺癌数据集上应用 GradientBoostingClassifier 试试看。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">gbrt = GradientBoostingClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 1.000</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.958</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 训练精度太高 可能过拟合</span></span><br><span class="line"><span class="comment"># 调小学习率从而降低迭代过程中的修正强度 默认 learning_rate=0.1</span></span><br><span class="line"><span class="comment"># gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)</span></span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 0.988</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.965</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 可以看到泛化能力提高了</span></span><br><span class="line"><span class="comment"># 我们也可以通过预剪枝来提升泛化能力 默认 max_depth=3</span></span><br><span class="line">gbrt = GradientBoostingClassifier(max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</span><br><span class="line">gbrt.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(gbrt.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 0.991</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.972</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>由于梯度提升和随机森林两种方法在类似的数据上表现得都很好，因此一种常用的方法就是先尝试随机森林，它的鲁棒性很好。如果随机森林效果很好，但预测时间太长，或者机器学习模型精度小数点后第二位的提高也很重要，那么切换成梯度提升通常会有用。</p>
<p>如果你想要将梯度提升应用在大规模问题上，可以研究一下 xgboost 包及其 Python 接口。这个库在许多数据集上的速度都比 scikit-learn 对梯度提升的实现要快。</p>
<h4 id="核支持向量机"><a class="markdownIt-Anchor" href="#核支持向量机"></a> 核支持向量机</h4>
<p>线性模型在特征少的情况下非常受限，比如二维特征情况下可能很难利用一条线区分 2 个分类。</p>
<p>为了继续使用之前讲过的线性模型，可以通过基于已有特征进行组合或者变换添加非线性特征（比如对某个特征求平方作为新特征），更高的维度可以解决线性模型的限制，达到不错的效果。</p>
<p>但问题是我们不知道对已有特征如何进行变换与组合对模型是有效的。</p>
<p>总之，能够将已有数据向更高维变换的话，模型就能够表现的更好。</p>
<p><strong>核技巧</strong></p>
<ul>
<li>多项式核：在一 定阶数内计算原始特征所有可能的多项式（比如 feature1 ** 2 * feature2 ** 5）。</li>
<li>高斯核：对应无限维特征空间。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">X, y = mglearn.tools.make_handcrafted_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练，用 RBF 核(高斯核)完成高维映射</span></span><br><span class="line"><span class="comment"># gamma 控制宽度 决定点于点的靠近指多大距离 越小则核半径越大</span></span><br><span class="line"><span class="comment"># C 参数是正则化参数 限制每个点的重要性</span></span><br><span class="line"><span class="comment"># 默认情况下，C=1，gamma=1/n_features</span></span><br><span class="line">svm = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, C=<span class="number">10</span>, gamma=<span class="number">0.1</span>).fit(X, y) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 画分类的分界线</span></span><br><span class="line">mglearn.plots.plot_2d_separator(svm, X, eps=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画样本点</span></span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出支持向量，支持向量的类别标签由 dual_coef_ 的正负号给出</span></span><br><span class="line">sv = svm.support_vectors_</span><br><span class="line">sv_labels = svm.dual_coef_.ravel() &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画支持向量点</span></span><br><span class="line">mglearn.discrete_scatter(sv[:, <span class="number">0</span>], sv[:, <span class="number">1</span>], sv_labels, s=<span class="number">15</span>, markeredgewidth=<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/img//2020/08/ML/image-20200823093719567.png" alt="image-20200823093719567" style="zoom:80%;" />
<p>由图可以看出 SVM 是非线性的。上图中较大的点代表的是支持向量（位于类别之间边界上的那些点），想要对新样本点进行预测，需要测量它与每个支持向量之间的距离。分类决策是基于它与支持向量之间的距离以及在训练过程中学到的支持向量重要性（保存在 SVC 的 <code>dual_coef_</code> 属性中）来做出的。</p>
<p>让我们通过图片来理解一下 SVC 的两个参数的作用。</p>
<img src="/img//2020/08/ML/image-20200823095611385.png" alt="image-20200823095611385"  />
<p>左侧的图决策边界非常平滑，越向右的图决策边界更关注单个点。小的 gamma 值表示决策边界变化很慢，生成的是复杂度较低的模型，而大的 gamma 值则会生成更为复杂的模型。</p>
<p>左上角的图中，决策边界看起来几乎是线性的，误分类的点对边界几乎没有任何影响。再看左下角的图，增大 C 之后这些点对模型的影响变大，使得决策边界发生弯曲来将这些点正确分类。</p>
<p>虽然 SVM 的表现通常都很好，但它对参数的设定和数据的缩放非常敏感。特别地，它要求所有特征有相似的变化范围。所以在使用 SVM 时我们通常要进行数据预处理：每个特征进行缩放，使其大致都位于同一范围。核 SVM 常用的缩放方法就是将所有特征缩放到 0 和 1 之间。</p>
<p>（较复杂，待整理）</p>
<h4 id="神经网络"><a class="markdownIt-Anchor" href="#神经网络"></a> 神经网络</h4>
<p>一类被称为神经网络的算法最近以深度学习的名字再度流行。虽然深度学习在许多机器学习应用中都有巨大的潜力，但深度学习算法往往经过精确调整，只适用于特定的使用场景。</p>
<p>这里只讨论一些相对简单的方法，即用于分类和回归的多层感知机（multilayer perceptron，MLP），它可以作为研究更复杂的深度学习方法的起点。MLP 也被称为（普通）前馈神经网络，有时也简称为神经网络。</p>
<p>MLP 可以被视为广义的线性模型。输入特征经过多次线性变换得到输出。每一个隐层包含多个隐单元，每个隐单元是由前一层的特征经过线性计算后，应用一个非线性函数（叫做激活函数）得到的。计算出前一个隐层内的所有隐单元，作为下一个隐层的特征输入，如此往复。</p>
<p><a target="_blank" rel="noopener" href="https://welab-wingo.gitee.io/2020/08/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/">深度学习入门</a></p>
<p>我们先用一个小的数据集来尝试一下神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">100</span>, noise=<span class="number">0.25</span>, random_state=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练神经网络 默认使用 100 个隐结点</span></span><br><span class="line"><span class="comment"># solver 参数指定神经网络如何学习 w 系数</span></span><br><span class="line"><span class="comment"># 默认 adam 对数据缩放敏感 lbfgs 对数据缩放不敏感 sgd 有大量参数需要调节</span></span><br><span class="line">mlp = MLPClassifier(solver=<span class="string">&#x27;lbfgs&#x27;</span>, random_state=<span class="number">0</span>, hidden_layer_sizes=[<span class="number">100</span>]).fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制模型分类边界</span></span><br><span class="line">mglearn.plots.plot_2d_separator(mlp, X_train, fill=<span class="literal">True</span>, alpha=<span class="number">.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画样本点</span></span><br><span class="line">mglearn.discrete_scatter(X_train[:, <span class="number">0</span>], X_train[:, <span class="number">1</span>], y_train)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(mlp.score(X_train, y_train))</span><br><span class="line"><span class="built_in">print</span>(mlp.score(X_test, y_test))</span><br></pre></td></tr></table></figure>
<img src="/img//2020/08/ML/image-20200823104855860.png" alt="image-20200823104855860" style="zoom:80%;" />
<p>MLP 的可以利用 L2 惩罚使权重趋向于零，从而降低拟合与模型的复杂度。MLPClassifier 中调节 L2 惩罚的参数是 alpha（与线性回归模型中的相同），它的默认值很小（弱正则化）。</p>
<p>我们还是用图示的方式来理解这两个参数对 MLP 模型训练的影响。</p>
<p><img src="/img//2020/08/ML/image-20200823113137757.png" alt="image-20200823113137757" /></p>
<p>从图中我们可以看出，参数 <code>n_hidden</code> 和 <code>alpha</code> 越大，模型越简单，曲线越平滑。</p>
<p>注意，神经网络对特征的范围也很敏感，也要求所有输入特征的变化范围相似，最理想的情况是均值为 0、方差为 1。</p>
<p>接下里我们用 cancer 这个比较大的数据集来对模型进行训练。首先用默认参数进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练神经网络</span></span><br><span class="line">mlp = MLPClassifier(random_state=<span class="number">0</span>).fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(mlp.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(mlp.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 0.91</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.91</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>精度不错，但没达到预期（这里看起来也有点欠拟合了的样子），因为神经网络对输入特征要求范围相似，最理想情况是均值为 0，方差为 1，需要进行数据预处理，对数据进行缩放：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算训练集中每个特征的平均值 </span></span><br><span class="line">mean_on_train = X_train.mean(axis=<span class="number">0</span>) </span><br><span class="line"><span class="comment"># 计算训练集中每个特征的标准差 </span></span><br><span class="line">std_on_train = X_train.std(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 减去平均值，然后乘以标准差的倒数 如此运算之后 mean=0 std=1</span></span><br><span class="line">X_train_scaled = (X_train - mean_on_train) / std_on_train </span><br><span class="line"><span class="comment"># 对测试集做相同的变换(使用训练集的平均值和标准差) </span></span><br><span class="line">X_test_scaled = (X_test - mean_on_train) / std_on_train</span><br><span class="line">     </span><br><span class="line"><span class="comment"># 训练 这里我们提高一下最大迭代次数 否则模型会有警告</span></span><br><span class="line">mlp = MLPClassifier(random_state=<span class="number">0</span>, max_iter=<span class="number">1000</span>)</span><br><span class="line">mlp.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on training set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(mlp.score(X_train_scaled, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy on test set: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(mlp.score(X_test_scaled, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Accuracy on training set: 0.995</span></span><br><span class="line"><span class="string">Accuracy on test set: 0.965</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="分类器的不确定度估计"><a class="markdownIt-Anchor" href="#分类器的不确定度估计"></a> 分类器的不确定度估计</h3>
<p>分类器能够给出预测的不确定度估计。一般来说，你感兴趣的不仅是分类器会预测一个测试点属于哪个类别，还包括它对这个预测的置信程度。</p>
<p>举个很经典的实际案例：不同类型的错误会在现实应用中导致非常不同的结果。想象一个用于测试癌症的医疗应用。假阳性预测可能只会让患者接受额外的测试，但假阴性预测却可能导致重病没有得到治疗。</p>
<p>sklearn 中有两个函数可用于获取分类器的不确定度估计：decision_function 和 predict_proba。大多数分类器（但不是全部）都至少有其中一个函数，很多分类器两个都有。</p>
<p>接下来我们来看这两个函数对一个模拟的二维数据的作用。</p>
<h3 id="小结与展望"><a class="markdownIt-Anchor" href="#小结与展望"></a> 小结与展望</h3>
<p>先从线性模型、朴素贝叶斯、最邻近等简单模型开始，对数据有了解后再使用随机森林、梯度提升决策树、SVM、神经网络。</p>
<ul>
<li>K 邻近（KNN）：适用于小型数据集，是很好的基准模型，很容易解释。</li>
<li>线性模型：非常可靠的首选算法，适用于非常大的数据集，也适用于高维数据。</li>
<li>朴素贝叶斯：只适用于分类问题。比线性模型速度还快，适用于非常大的数据集和高维数据。精度通常要低于线性模型。</li>
<li>决策树：速度很快，不需要数据缩放，可以可视化，很容易解释。</li>
<li>随机森林：几乎总是比单棵决策树的表现要好，鲁棒性很好，非常强大。不需要数据缩放。不适用于高维稀疏数据。</li>
<li>梯度提升决策树：精度通常比随机森林略高。与随机森林相比，训练速度更慢，但预测速度更快，需要的内存也更少。比随机森林需要更多的参数调节。</li>
<li>支持向量机：对于特征含义相似的中等大小的数据集很强大。需要数据缩放，对参数敏感。</li>
<li>神经网络：可以构建非常复杂的模型，特别是对于大型数据集而言。对数据缩放敏感，对参数选取敏感。大型网络需要很长的训练时间。</li>
</ul>
<h2 id="无监督学习与预处理"><a class="markdownIt-Anchor" href="#无监督学习与预处理"></a> 无监督学习与预处理</h2>
<p>训练算法时只有输入数据，没有已知输出。因为训练数据没有分类标签，所以很难用精度去评估模型有效。因此，无监督算法通常用于探索数据的规律，而不是自动化系统。</p>
<h3 id="无监督学习类型"><a class="markdownIt-Anchor" href="#无监督学习类型"></a> 无监督学习类型</h3>
<p>无监督变换（unsupervised transformation）：创建数据新的表示的算法，与数据的原始表示相比，新的表示可能更容易被人或其他机器学习算法所理解。</p>
<ul>
<li>常用于降维（dimensionality reduction），它接受包含许多特征的数据的高维表示，并找到表示该数据的一种新方法，用较少的特征就可以概括其重要特性。降维的一个常见应用是为了可视化将数据降为二维。</li>
<li>无监督变换的另一个应用是找到构成数据的各个组成部分。这方面的一个例子就是对文本文档集合进行主题提取。这里的任务是找到每个文档中讨论的未知主题，并学习每个文档中出现了哪些主题。这可以用于追踪社交媒体上的话题讨论，比如选举、枪支管制或 流行歌手等话题。</li>
</ul>
<p>聚类算法（clustering algorithm）：将数据划分成不同的组，每组包含相似的物项。</p>
<ul>
<li>思考向社交媒体网站上传照片的例子。为了方便你整理照片，网站可能想要将同一个人的照片分在一组。但网站并不知道每张照片是谁，也不知道你的照片集中出现了多少个 人。明智的做法是提取所有的人脸，并将看起来相似的人脸分在一组。但愿这些人脸对应同一个人，这样图片的分组也就完成了。</li>
</ul>
<h3 id="无监督学习的挑战"><a class="markdownIt-Anchor" href="#无监督学习的挑战"></a> 无监督学习的挑战</h3>
<p>无监督学习的一个主要挑战就是评估算法是否学到了有用的东西。通常来说，评估无监督算法结果的唯一方法就是人工检查。</p>
<p>因此，如果数据科学家想要更好地理解数据，那么无监督算法通常可用于探索性的目的，而不是作为大型自动化系统的一部分。无监督算法的另一个常见应用是作为监督算法的预处理步骤。学习数据的一种新表示，有时可以提高监督算法的精度，或者可以减少内存占用和时间开销。</p>
<h3 id="预处理与缩放"><a class="markdownIt-Anchor" href="#预处理与缩放"></a> 预处理与缩放</h3>
<p>让我们先来看看对数据集缩放核预处理的各种方法的处理结果。</p>
<p><img src="/img//2020/08/ML/image-20200823151447998.png" alt="image-20200823151447998" /></p>
<p>StandardScaler 确保每个特征的平均值为 0，方差为 1，使所有特征位于同一量级。</p>
<p>RobustScaler 使用的是中位数和四分位数 ，而不是平均值和方差。这样 RobustScaler 会忽略与其他点有很大不同的数据点（异常值，outlier）（比如测量误差）。</p>
<p>MinMaxScaler 移动数据，使所有特征都刚好位于 0 到 1 之间。对于二维数据集来说，所有的数据都包含在 x 轴 0 到 1 与 y 轴 0 到 1 组成的矩形中。</p>
<p>Normalizer 用到一种完全不同的缩放方法。它对每个数据点进行缩放，使得特征向量的欧式长度等于 1。换句话说，它将一个数据点投射到半径为 1 的圆上（对于更高维度的情况，是球面）。这意味着每个数据点的缩放比例都不相同（乘以其长度的倒数）。如果只有数据的方向（或角度）是重要的，而特征向量的长度无关紧要，那么通常会使用这种归一化。</p>
<p><strong>对训练数据和测试数据进行相同的缩放</strong></p>
<p>注意！！！所有的缩放器总是对训练集核测试集应用完全相同的变换。也就是说，transform 方法总是减去训练集的最小值，然后除以训练集的范围，而这两个值可能与测试集的最小值和范围并不相同。为了让监督模型能够在测试集上运行，对训练集和测试集应用完全相同的变换是很重要的。</p>
<h3 id="降维-特征提取与流形学习"><a class="markdownIt-Anchor" href="#降维-特征提取与流形学习"></a> 降维、特征提取与流形学习</h3>
<p>利用无监督学习进行数据变换最简单也最常用的一种算法就是主成分分析（principal component analysis，PCA）。</p>
<h4 id="主成分分析"><a class="markdownIt-Anchor" href="#主成分分析"></a> 主成分分析</h4>
<p>主成分分析（principal component analysis，PCA）是一种旋转数据集的方法，旋转后的特征在统计上不相关。在做完这种旋转之后，通常是根据新特征对解释数据的重要性来选择它的一个子集。</p>
<p>我们对 cancer 数据集利用 PCA 从 30 个维降到 2 个维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">#  数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaler.fit(cancer.data)</span><br><span class="line">X_scaled = scaler.transform(cancer.data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 降维到 2 的PCA</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 训练 PCA</span></span><br><span class="line">pca.fit(X_scaled)</span><br><span class="line"><span class="comment"># 进行特征提取变换</span></span><br><span class="line">X_pca = pca.transform(X_scaled)</span><br><span class="line"><span class="comment"># 降维前的特征个数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(X_scaled.shape))) </span><br><span class="line"><span class="comment"># 降维后的特征个数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reduced shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(X_pca.shape)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Original shape: (569, 30)</span></span><br><span class="line"><span class="string">Reduced shape: (569, 2)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>因为 PCA 后只有 2 个维度，所以可以绘图做可视化分析。</p>
<p>重要的是要注意，PCA 是一种无监督方法，在寻找旋转方向时没有用到任何类别信息。它只是观察数据中的相关性。</p>
<p>PCA 的一个缺点在于，通常不容易对图中的两个轴做出解释。主成分对应于原始数据中的方向，所以它们是原始特征的组合，但这些组合往往非常复杂。</p>
<p>来看一个人脸识别的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据 </span></span><br><span class="line"><span class="comment"># 一共有 3023 张图像，每张大小为 87 像素 ×65 像素，分别属于 62 个不同的人</span></span><br><span class="line">people = fetch_lfw_people(min_faces_per_person=<span class="number">20</span>, resize=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3023 张人脸照片作为样本输入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;people.images.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(people.images.shape))</span><br><span class="line"><span class="comment"># 3023 张图片对应的人名</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of classes: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(people.target_names)))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">people.images.shape: (3023, 87, 65)</span></span><br><span class="line"><span class="string">Number of classes: 62</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3023 列的 false 向量</span></span><br><span class="line">mask = np.zeros(people.target.shape, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有的分类(也就是人名，target 取值是 0~61)</span></span><br><span class="line"><span class="comment"># 每个分类保留 50 个样本 防止某个 target 的数据集过多造成倾斜</span></span><br><span class="line"><span class="keyword">for</span> target <span class="keyword">in</span> np.unique(people.target):</span><br><span class="line">    mask[np.where(people.target == target)[<span class="number">0</span>][:<span class="number">50</span>]] = <span class="literal">True</span></span><br><span class="line">X_people = people.data[mask]  <span class="comment"># data 是已经把每张图片 n*m 的 2 维压成 1 维的数据格式</span></span><br><span class="line">y_people = people.target[mask]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在留下的样本，每个人不会超过50张</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征缩放到 0~1 之间, 因为是RGB颜色</span></span><br><span class="line">X_people = X_people / <span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据</span></span><br><span class="line"><span class="comment"># print(X_people, y_people)</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_people, y_people, stratify=y_people, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KNN分类, 最近邻分类(只考虑最近的节点)</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score of 1-nn: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Test set score of 1-nn: 0.27</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>这个结果对于一个 62 分类问题来说也不算太大，但还是由改进的空间。</p>
<p>这里就可以用到 PCA。想要度量人脸的相似度，计算原始像素空间中的距离是一种相当糟糕的方法。用像素表示来比较两张图像时，我们比较的是每个像素的灰度值与另一张图像对应位置的像素灰度值。这种表示与人们对人脸图像的解释方式有很大不同，使用这种原始表示很难获取到面部特征。</p>
<p>例如，如果使用像素距离，那么将人脸向右移动一个像素将会发生巨大的变化，得到一个完全不同的表示。</p>
<p>我们希望，使用沿着主成分方向的距离可以提高精度。这里我们启用 PCA 的白化（whitening）选项，它将主成分缩放到相同的尺度。</p>
<p>PCA 的白化（whitening）选项，它将主成分缩放到相同的尺度。变换后的结果与使用 StandardScaler 相同。</p>
<img src="/img//2020/08/ML/image-20200823170440945.png" alt="image-20200823170440945" style="zoom: 67%;" />
<p>我们可以考虑利用 PCA 提取主成分，作为 100 个新特征输入到模型。对数据做了特征提取后再用 KNN 进行建模。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">people = fetch_lfw_people(min_faces_per_person=<span class="number">20</span>, resize=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3023 张人脸照片作为样本输入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;people.images.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(people.images.shape))</span><br><span class="line"><span class="comment"># 3023 张图片对应的人名</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Number of classes: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(people.target_names)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3023 列的 false 向量</span></span><br><span class="line">mask = np.zeros(people.target.shape, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有的分类(也就是人名，target 取值是 0~61)</span></span><br><span class="line"><span class="comment"># 每个分类保留 50 个样本</span></span><br><span class="line"><span class="keyword">for</span> target <span class="keyword">in</span> np.unique(people.target):</span><br><span class="line">    mask[np.where(people.target == target)[<span class="number">0</span>][:<span class="number">50</span>]] = <span class="literal">True</span></span><br><span class="line">X_people = people.data[mask]  <span class="comment"># data 是已经把每张图片 n*m 的 2 维压成 1 维的数据格式</span></span><br><span class="line">y_people = people.target[mask]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在留下的样本，每个人不会超过50张</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征缩放到 0~1 之间, 因为是 RGB 颜色</span></span><br><span class="line">X_people = X_people / <span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据</span></span><br><span class="line"><span class="comment"># print(X_people, y_people)</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_people, y_people, stratify=y_people, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA 提取 100 个主成分作为新的特征, 基于训练集 fit, 应用到训练集和测试集</span></span><br><span class="line">pca = PCA(n_components=<span class="number">100</span>, whiten=<span class="literal">True</span>, random_state=<span class="number">0</span>).fit(X_train)</span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_pca.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_pca.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># KNN 分类, 最近邻分类(只考虑最近的节点)</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">1</span>)</span><br><span class="line">knn.fit(X_train_pca, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(knn.score(X_test_pca, y_test)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Test set accuracy: 0.36</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在对数据进行 PCA 处理后，KNN 模型的精度确实得到了提升。</p>
<h4 id="非负矩阵分解"><a class="markdownIt-Anchor" href="#非负矩阵分解"></a> 非负矩阵分解</h4>
<p>非负矩阵分解（non-negative matrix factorization，NMF）是另一种无监督学习算法，其目的在于提取有用的特征。它的工作原理类似于 PCA，也可以用于降维。与 PCA 相同，我们试图将每个数据点写成一些分量的加权求和。</p>
<p>但在 PCA 中，我们想要的是正交分量，并且能够解释尽可能多的数据方差；而在 NMF 中，我们希望分量和系数均为非负，也就是说，我们希望分量和系数都大于或等于 0。因此，这种方法只能应用于每个特征都是非负的数据，因为非负分量的非负求和不可能变为负值。</p>
<p>（常用的降维方法为 PCA，NMF 待整理）</p>
<h4 id="用-t-sne-进行流形学习"><a class="markdownIt-Anchor" href="#用-t-sne-进行流形学习"></a> 用 t-SNE 进行流形学习</h4>
<p>虽然 PCA 通常是用于变换数据的首选方法，使你能够用散点图将其可视化，但这一方法的性质（先旋转然后减少方向）限制了其有效性。有一类用于可视化的算法叫作流形学习算法（manifold learning algorithm），它允许进行更复杂的映射，通常也可以给出更好的可视化。其中特别有用的一个就是 t-SNE 算法。</p>
<p>新特征能够根据原始数据中数据点之间的远近程度将不同类比明确分开。在可视化用途比 PCA 更有效，但只能用于做可视化，无法像 PCA 一样应用到测试集上。</p>
<h3 id="聚类"><a class="markdownIt-Anchor" href="#聚类"></a> 聚类</h3>
<p>聚类（clustering）是将数据集划分成组的任务，这些组叫做簇。聚类算法给每个数据点分配一个数字，表示数据点属于哪个簇。</p>
<h4 id="k-均值聚类"><a class="markdownIt-Anchor" href="#k-均值聚类"></a> k 均值聚类</h4>
<p>k 均值聚类是最简单也最常用的聚类算法之一。它试图找到代表数据特定区域的簇中心（cluster center）。算法交替执行以下两个步骤：将每个数据点分配给最近的簇中心，然后将每个簇中心设置为所分配的所有数据点的平均值。如果簇的分配不再发生变化（收敛），那么算法结束。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成模拟的二维数据</span></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建聚类模型, 指定 3 个簇中心</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>) </span><br><span class="line">kmeans.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个数据所属的簇标签, 因为 n_clusters=3 所以就是 0~2</span></span><br><span class="line"><span class="built_in">print</span>(kmeans.labels_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也支持 predict 方法来计算一个新数据点属于哪个簇标签</span></span><br><span class="line"><span class="built_in">print</span>(kmeans.predict(X))</span><br></pre></td></tr></table></figure>
<p>簇标签没有先验意义，我们并不知道每一个簇代表什么，只能人为观察。</p>
<p>kmeans 类似于 PCA 可以进行特征变换，首先 kmeans 进行 fit 找到所有簇中心后，进而对训练集 / 测试集特征进行 transform，从而将原始数据点变换为到各个簇中心的距离，作为新的特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">200</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分成10簇</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">kmeans.fit(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换原有特征为到簇中心的距离</span></span><br><span class="line">train_distance_features = kmeans.transform(X_train)</span><br><span class="line">test_distance_features = kmeans.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 跑模型</span></span><br><span class="line">gbdt = GradientBoostingClassifier()</span><br><span class="line">gbdt.fit(train_distance_features, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(gbdt.score(test_distance_features, y_test))</span><br></pre></td></tr></table></figure>
<p>kmeans 最初的簇中心是随机产生的，算法输出依赖于随机种子，sklearn 会默认跑 10 次选最好的 1 次。另外，kmeans 对簇形状有假设，人工确定簇个数是很难琢磨的。</p>
<h4 id="凝聚聚类"><a class="markdownIt-Anchor" href="#凝聚聚类"></a> 凝聚聚类</h4>
<p>算法首先声明每个点是自己的簇，然后不断合并相似的簇，直到簇数量达到目标。因为算法是不断合并簇的逻辑，所以它对新数据无法做出所属簇的预测。</p>
<p>scikit-learn 中实现了以下三种选项。</p>
<ul>
<li>ward ：默认选项。ward 挑选两个簇来合并，使得所有簇中的方差增加最小。这通常会得到大小差不多相等的簇。</li>
<li>average：average 链接将簇中所有点之间平均距离最小的两个簇合并。</li>
<li>complete： complete 链接（也称为最大链接）将簇中点之间最大距离最小的两个簇合并。</li>
</ul>
<p>ward 适用于大多数数据集。如果簇中的成员个数非常不同（比如其中一个比其他所有都大得多），那么 average 或 complete 可能效果更好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">X, y = make_blobs(random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 凝聚聚类为 3 个簇</span></span><br><span class="line">agg = AgglomerativeClustering(n_clusters=<span class="number">3</span>)</span><br><span class="line">assignment = agg.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出数据点分布以及所属簇</span></span><br><span class="line">mglearn.discrete_scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], assignment)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Feature 0&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Feature 1&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>凝聚聚类生成了所谓的层次聚类（hierarchical clustering）。聚类过程迭代进行，每个点都从一个单点簇变为属于最终的某个簇。每个中间步骤都提供了数据的一种聚类（簇的个数也不相同）。</p>
<img src="/img//2020/08/ML/image-20200830124602354.png" alt="image-20200830124602354" style="zoom:50%;" />
<p>可以将层次聚类可视化为树状图进行分析。但 sklearn 目前没有绘制树状图的功能，这里我们利用 Scipy 库。</p>
<img src="/img//2020/08/ML/image-20200830131252233.png" alt="image-20200830131252233" style="zoom:50%;" />
<h4 id="dbscan-聚类"><a class="markdownIt-Anchor" href="#dbscan-聚类"></a> DBSCAN 聚类</h4>
<p>DBSCAN（density-based spatial clustering of applications with noise，即具有噪声的基于密度的空间聚类应用）的主要优点是它不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇，还可以找出不属于任何簇的点，DBSCAN 也不允许对新的测试数据进行预测。</p>
<p>DBSCAN 有两个参数：min_samples 和 eps。如果在距一个给定数据点 eps 的距离内至少有 minsamples 个数据点，那么这个数据点就是核心样本。DBSCAN 将彼此距离小于 eps 的核心样本放到同一个簇中。</p>
<h3 id="小结-2"><a class="markdownIt-Anchor" href="#小结-2"></a> 小结</h3>
<p>无监督学习可以用于探索性的数据分析与预处理。</p>
<p>预处理以及分解方法在数据准备中具有重要作用。</p>
<p>通常来说，很难量化无监督算法的有用性，但这不应该妨碍你使用它们来深入理解数据。</p>
<h2 id="数据表示与特征工程"><a class="markdownIt-Anchor" href="#数据表示与特征工程"></a> 数据表示与特征工程</h2>
<p>到现在，我们已经知道了模型的形成是依赖数据的特征的，并且不同的算法对特征的敏感度不同。特征可以分为两大类型：分类特征与离散特征。</p>
<p>对于某个特定应用来说，如何找到最佳数据表示，这个问题被称为特征工程（feature engineering），它是数据科学家和机器学习从业者在尝试解决现实世界问题时的主要任务之一。用正确的方式表示数据，对监督模型性能的影响比所选择的精确参数还要大。</p>
<h3 id="one-hot-编码虚拟变量"><a class="markdownIt-Anchor" href="#one-hot-编码虚拟变量"></a> one-hot 编码（虚拟变量）</h3>
<p>如果特征不是连续值，而是一些分类特征，来自一系列固定的可能取值（非数字），那么直接用于类似 Logistic 回归分类模型是没有意义。显然，在应用 Logistic 回归时，我们需要换一种方式来表示数据。下一节将会说明我们如何解决这一问题。</p>
<p>将 1 个特征的多种取值，改为多个特征的 0 / 1 取值，其中有一个特征为 1，其他为 0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit 训练 one hot, 返回非稀疏矩阵</span></span><br><span class="line"><span class="comment"># 当 transform 时遇到没见过的特征值则对应 one-hot 编码全部为 0</span></span><br><span class="line">enc = OneHotEncoder(sparse=<span class="literal">False</span>, handle_unknown=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="comment"># 每一列代表一种特征的可能性</span></span><br><span class="line"><span class="comment"># 第一列：[0, 1, 0, 1] 只有 0 和 1 两种情况 one-hot 占两列</span></span><br><span class="line"><span class="comment"># 第二列：[0, 1, 2, 0] 有 0 1 2 三种情况 one-hot 占三列</span></span><br><span class="line"><span class="comment"># 第三列：[3, 0, 1, 2] 有 0 1 2 3 四种情况 one-hot 占四列</span></span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">enc.fit(X)    </span><br><span class="line"></span><br><span class="line"><span class="comment"># one-hot 编码新数据</span></span><br><span class="line">X_one_hot = enc.transform([[<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>]])    </span><br><span class="line"><span class="built_in">print</span>(X_one_hot)</span><br><span class="line"><span class="comment"># OUT：[[0. 1. 0. 0. 0. 0. 0. 1. 0.]]</span></span><br></pre></td></tr></table></figure>
<p>也可指定对哪些 one-hot，而对其他连续特征可以不做处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定了只有前 2 个特征需要离散化</span></span><br><span class="line">enc = OneHotEncoder(categorical_features=[<span class="number">0</span>,<span class="number">1</span>], sparse=<span class="literal">False</span>, handle_unknown=<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">enc.fit(X)    </span><br><span class="line"></span><br><span class="line"><span class="comment"># one-hot 编码新数据</span></span><br><span class="line">X_one_hot = enc.transform([[<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>]])    </span><br><span class="line"><span class="built_in">print</span>(X_one_hot)</span><br></pre></td></tr></table></figure>
<p>我们也可以利用 ColumnTransformer 来作为统一的特征处理方法，将字符串编码为整形。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = [[<span class="string">&#x27;male&#x27;</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="string">&#x27;male&#x27;</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="string">&#x27;female&#x27;</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="string">&#x27;female&#x27;</span>, <span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串编码为整形</span></span><br><span class="line">sex_enc = OrdinalEncoder(dtype = np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 独热编码</span></span><br><span class="line">one_hot_enc = OneHotEncoder(sparse=<span class="literal">False</span>, handle_unknown=<span class="string">&#x27;ignore&#x27;</span>, dtype=np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对第 0 列的字符串做整形转换, 然后对所有列做 one-hot</span></span><br><span class="line">col_transformer = ColumnTransformer(transformers = [(<span class="string">&#x27;sex_enc&#x27;</span>, sex_enc, [<span class="number">0</span>]), (<span class="string">&#x27;one_hot_enc&#x27;</span>, one_hot_enc, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>,<span class="number">3</span>)))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练编码</span></span><br><span class="line">col_transformer.fit(X)</span><br><span class="line">X_trans = col_transformer.transform(X)</span><br><span class="line"><span class="built_in">print</span>(X_trans)</span><br></pre></td></tr></table></figure>
<h3 id="分箱-离散化-线性模型与树"><a class="markdownIt-Anchor" href="#分箱-离散化-线性模型与树"></a> 分箱、离散化、线性模型与树</h3>
<p>对于只有 1 个连续特征的线性模型，它的表现就是 <code>y=w*x</code> 的线，效果不好。这时候可以考虑将特征取值划分范围，这就是分箱，每个箱子的数值区间不同，提高了线性模型的表现力。</p>
<p>这样就把 1 个连续特征变成了离线特征，也就是在哪个区间里，可以继续通过 one-hot 编码成多个 0/1 特征。</p>
<p>分箱对线性模型有提升效果，对树模型效果不会很好可能还会下降。</p>
<h3 id="交互特征与多项式特征"><a class="markdownIt-Anchor" href="#交互特征与多项式特征"></a> 交互特征与多项式特征</h3>
<p>把原始特征之间进行组合和扩充，对线性模型有提升效果，比如：添加特征的平方或立方，或者把两两特征相乘。添加交互 / 多项式特征之后的线性模型，与没有交互特征的树模型 / 复杂模型的性能就比较相近了。</p>
<h3 id="单变量非线性变换"><a class="markdownIt-Anchor" href="#单变量非线性变换"></a> 单变量非线性变换</h3>
<p>对于简单模型（线性、朴素贝叶斯）来说，如果数据集的数据分布存在大量的小值以及个别非常大的值，会导致线性模型很难处理。这种情况出现在一些计数性质的特征上，比如点赞次数。</p>
<p>此时对该特征应用 <code>log(x+1)</code> 或者 <code>exp</code> （对数 / 指数）来调节特征值得比例，可以改进线性模、SVM、神经网络的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对跨度较大的特征做 log</span></span><br><span class="line">X_train_log = np.log(X_train + <span class="number">1</span>)</span><br><span class="line">X_test_log = np.log(X_test + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>对线性模型提升最明显，对树模型意义不大，对 SVM / KNN / 神经网络有可能受益。</p>
<h3 id="自动化特征选择"><a class="markdownIt-Anchor" href="#自动化特征选择"></a> 自动化特征选择</h3>
<p>前面提到的方法都是增加特征（线性 👉 非线性），现在是通过分析数据来减少特征，得到一个泛化更好，更简单的模型，也就是特征选择。</p>
<h4 id="单变量统计"><a class="markdownIt-Anchor" href="#单变量统计"></a> 单变量统计</h4>
<p>每次考虑一个特征，观察特征与目标值之间是否存在统计显著性，但是没法综合考虑多个特征。算法可以指定保留一定数量的重要特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectPercentile</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, random_state=<span class="number">0</span>, test_size=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_train.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动选择 50% 的特征留下来</span></span><br><span class="line">select = SelectPercentile(percentile=<span class="number">50</span>)</span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集提取特征</span></span><br><span class="line">X_train_selected = select.transform(X_train)</span><br><span class="line"><span class="built_in">print</span>(X_train_selected.shape)</span><br><span class="line"><span class="comment"># 测试集提取特征</span></span><br><span class="line">X_test_selected = select.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始特征模型，看精度</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score with all features: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br><span class="line">                    </span><br><span class="line"><span class="comment"># 特征选择的模型, 看精度</span></span><br><span class="line">lr.fit(X_train_selected, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Score with only selected features: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test_selected, y_test)))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">out：</span></span><br><span class="line"><span class="string">(284, 30)</span></span><br><span class="line"><span class="string">(284, 15)</span></span><br><span class="line"><span class="string">Score with all features: 0.954</span></span><br><span class="line"><span class="string">Score with only selected features: 0.954</span></span><br><span class="line"><span class="string">特征删了一半 但对模型的精度丝毫没有产生影响</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="基于模型的特征选择"><a class="markdownIt-Anchor" href="#基于模型的特征选择"></a> 基于模型的特征选择</h4>
<p>线性模型、决策树模型在训练的过程中自然的完成了对特征重要程度的学习。所以可以基于这些模型进行特征选择，再将选择后的特征作为另外一个模型的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, random_state=<span class="number">0</span>, test_size=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从随机森林模型的训练的结果中进行特征选择, 取中位数偏上重要的特征，也就是保留一半最重要的特征</span></span><br><span class="line"><span class="comment"># threshold：滤波器</span></span><br><span class="line">select = SelectFromModel(</span><br><span class="line">                        RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>),</span><br><span class="line">                        threshold=<span class="string">&quot;median&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征选择训练</span></span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留下选择后的特征</span></span><br><span class="line">X_train_l1 = select.transform(X_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X_train_l1.shape: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train_l1.shape))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择出来的特征完成训练</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train_l1, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集特征选择</span></span><br><span class="line">X_test_l1 = select.transform(X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test_l1, y_test)))</span><br></pre></td></tr></table></figure>
<p>模型综合度量了所有特征的重要性，所以比单变量统计强大的多。</p>
<h4 id="迭代特征选择"><a class="markdownIt-Anchor" href="#迭代特征选择"></a> 迭代特征选择</h4>
<p>（RFE，Recursive Feature Elimination）利用模型进行多轮特征选择，每轮筛掉1个最不重要的特征。</p>
<p>下面用随机森林做特征选择，选择出来的特征用线性 LR 做训练，发现线性模型精度很好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    cancer.data, cancer.target, random_state=<span class="number">0</span>, test_size=<span class="number">.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于迭代的特征选择, 保留15个特征</span></span><br><span class="line">select = RFE(RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>),</span><br><span class="line">                   n_features_to_select=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练特征选择</span></span><br><span class="line">select.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集特征选择</span></span><br><span class="line">X_train_rfe = select.transform(X_train)</span><br><span class="line"><span class="comment"># 测试集特征选择</span></span><br><span class="line">X_test_rfe = select.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择后的特征训练模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train_rfe, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test_rfe, y_test) ))</span><br></pre></td></tr></table></figure>
<h2 id="模型评估与改进"><a class="markdownIt-Anchor" href="#模型评估与改进"></a> 模型评估与改进</h2>
<p>到目前为止，为了评估我们的监督模型，我们使用 train_test_split 函数将数据集划分为 、训练集和测试集，在训练集上调用 fit 方法来构建模型，并且在测试集上用 score 方法来评估这个模型，对于分类问题而言，就是计算正确分类的样本所占的比例。</p>
<p>请记住，之所以将数据划分为训练集和测试集，是因为我们想要度量模型对前所未见的新数据的泛化性能。我们对模型在训练集上的拟合效果不感兴趣，而是想知道模型对于训练过程中没有见过的数据的预测能力。</p>
<h3 id="交叉验证"><a class="markdownIt-Anchor" href="#交叉验证"></a> 交叉验证</h3>
<p>单次划分数据集并不稳定和全面，因此我们需要对数据集进行多次划分，训练多个模型进行综合评估，这叫交叉验证。</p>
<h4 id="k-折交叉"><a class="markdownIt-Anchor" href="#k-折交叉"></a> K 折交叉</h4>
<p>这是最常见的交叉验证方式，将数据均匀划分成 K 份，每次用 1 份做测试集，剩余做训练集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># K折交叉验证</span></span><br><span class="line">scores = cross_val_score(logreg, iris.data, iris.target, cv=<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cross-validation scores: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(scores))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average cross-validation score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(scores.mean()))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Cross-validation scores: [0.96078431 0.92156863 0.95833333]</span></span><br><span class="line"><span class="string">Average cross-validation score: 0.95</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="分层-k-折交叉"><a class="markdownIt-Anchor" href="#分层-k-折交叉"></a> 分层 K 折交叉</h4>
<p>K 折交叉划分数据的方式是从头开始均分成 K 份，如果样本数据的分类分布不均匀，那么就会导致 K 折交叉策略失效。</p>
<p>在分层交叉验证中，我们划分数据，使每个折中类别之间的比例与整个数据集中的比例相同。sklearn 会根据模型是回归还是分类决定使用标准 K 折还是分层 K 折，不需我们关心，只需要了解。</p>
<h4 id="留一法交叉验证"><a class="markdownIt-Anchor" href="#留一法交叉验证"></a> 留一法交叉验证</h4>
<p>可以将留一法交叉验证看作是每折只包含单个样本的 k 折交叉验证。对于每次划分，你选择单个数据点作为测试集。这种方法可能非常耗时，特别是对于大型数据集来说，但在小型数据集上有时可以给出更好的估计结果。</p>
<h4 id="打乱划分交叉验证"><a class="markdownIt-Anchor" href="#打乱划分交叉验证"></a> 打乱划分交叉验证</h4>
<p>在打乱划分交叉验证中，每次划分为训练集取样 train_size 个点，为测试集取样 test_size 个 （不相交的）点。将这一划分方法重复 n_iter 次。</p>
<h4 id="分组交叉验证"><a class="markdownIt-Anchor" href="#分组交叉验证"></a> 分组交叉验证</h4>
<p>利用分组保证测试集与训练集的样本不同。（为了准确评估模型对新的人脸的泛化能力，我们必须确保训练集和测试集中包含不同人的图像。）</p>
<h3 id="网格搜索"><a class="markdownIt-Anchor" href="#网格搜索"></a> 网格搜索</h3>
<p>利用网格搜索，实现模型的自动化调参，找到最佳泛化性能的参数。</p>
<p>在尝试调参之前， 重要的是要理解参数的含义。找到一个模型的重要参数（提供最佳泛化性能的参数）的取值是一项棘手的任务，但对于几乎所有模型和数据集来说都是必要的。由于这项任务如此常见，所以 sklearn 中有一些标准方法可以帮你完成。最常用的方法就是网格搜索（grid search），它主要是指尝试我们关心的参数的所有可能组合。</p>
<h4 id="带交叉验证的网格搜索"><a class="markdownIt-Anchor" href="#带交叉验证的网格搜索"></a> 带交叉验证的网格搜索</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         iris.data, iris.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 种网格</span></span><br><span class="line">param_grid = [</span><br><span class="line">    <span class="comment"># （高斯）径向基函数核（英語：Radial basis function kernel）</span></span><br><span class="line">    <span class="comment"># gamma 越大，支持向量越少，gamma 值越小，支持向量越多。支持向量的个数影响训练与预测的速度</span></span><br><span class="line">    <span class="comment"># 第1个网格</span></span><br><span class="line">    &#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;rbf&#x27;</span>], <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;,</span><br><span class="line">    <span class="comment"># 第2个网格</span></span><br><span class="line">    &#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>],<span class="string">&#x27;C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 在 2 个网格中, 找到 SVC 模型的最佳参数, 这里 cv=5 表示每一种参数组合进行 5 折交叉验证计算得分</span></span><br><span class="line">grid_search = GridSearchCV(SVC(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit 找到最佳泛化的参数</span></span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;泛化精度:&quot;</span>, grid_search.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最佳参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br></pre></td></tr></table></figure>
<h4 id="交叉验证与网格搜索的嵌套"><a class="markdownIt-Anchor" href="#交叉验证与网格搜索的嵌套"></a> 交叉验证与网格搜索的嵌套</h4>
<p>可以采用先交叉划分数据集，再进行 K 折网格搜索，这就是嵌套的意思。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score, train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line">param_grid = [</span><br><span class="line">    &#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;rbf&#x27;</span>], <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>],<span class="string">&#x27;C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line">]</span><br><span class="line">grid_search = GridSearchCV(SVC(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 外层 K 折</span></span><br><span class="line">scores = cross_val_score(grid_search, iris.data, iris.target, cv=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 打印精度</span></span><br><span class="line"><span class="built_in">print</span>(scores)</span><br></pre></td></tr></table></figure>
<h4 id="评估指标与评分"><a class="markdownIt-Anchor" href="#评估指标与评分"></a> 评估指标与评分</h4>
<p>首先，需要牢记的一点，精度不是唯一目标！！！我们需要考虑商业指标，对商业的影响。</p>
<h5 id="二分类指标"><a class="markdownIt-Anchor" href="#二分类指标"></a> 二分类指标</h5>
<p>二分类一共有 2 种类型，一种称为正类（positive），一种称为反类（negative）。</p>
<p>根据样本分类与模型预测分类，可以产生 4 种组合：</p>
<ul>
<li>TP：预测是正类，样本是正类</li>
<li>FP：预测是正类，样本是反类</li>
<li>TN：预测是反类，样本是反类</li>
<li>FN：预测是反类，样本是正类</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">digits = load_digits()</span><br><span class="line"><span class="comment"># 转换成 2 分类, 即目标数字是否等于 9</span></span><br><span class="line">y = digits.target == <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         digits.data, y, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印 f-score</span></span><br><span class="line"><span class="built_in">print</span>(f1_score(y_test, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印所有指标</span></span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Out:</span></span><br><span class="line"><span class="string">[[399   4]</span></span><br><span class="line"><span class="string"> [  7  40]]</span></span><br><span class="line"><span class="string">0.8791208791208791</span></span><br><span class="line"><span class="string">              precision    recall  f1-score   support</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       False       0.98      0.99      0.99       403</span></span><br><span class="line"><span class="string">        True       0.91      0.85      0.88        47</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   micro avg       0.98      0.98      0.98       450</span></span><br><span class="line"><span class="string">   macro avg       0.95      0.92      0.93       450</span></span><br><span class="line"><span class="string">weighted avg       0.98      0.98      0.98       450</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>混淆矩阵的分布：</p>
<table>
<thead>
<tr>
<th></th>
<th>预测为反类</th>
<th>预测为正类</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>实际为反类</strong></td>
<td>TN</td>
<td>FP</td>
</tr>
<tr>
<td><strong>实际为正类</strong></td>
<td>FN</td>
<td>TP</td>
</tr>
</tbody>
</table>
<p>正确率 Accuracy：<code>Accuracy = (TN+TP) / (TN+TP+FN+FP)</code></p>
<p>精确率 Precision：<code>Precision = TP / (TP + FP)</code></p>
<blockquote>
<p>精确率的商业目标就是限制假正例的数量，可能因为假正例会带来很严重的影响。</p>
</blockquote>
<p>召回率 Recall：<code>Recall = TP / (TP + FN)</code></p>
<p>精确率和召回率是矛盾的，如果模型预测所有都是正类，那么召回率就是100%；此时，就会出现很多假正类，精确率就很差。因此需要综合 2 个指标进行折衷，就是 <code>f-score</code> 或 <code>f-measure</code>。</p>
<p><code>F = 2 * precision * recall / (precision + recall)</code></p>
<p>阈值：阈值（默认为 50%）越高，意味着模型需要更加确信才能做出正类的判断。</p>
<h6 id="准确率-召回率曲线"><a class="markdownIt-Anchor" href="#准确率-召回率曲线"></a> 准确率-召回率曲线</h6>
<p>曲线越靠近右上角，则分类器越好。右上角的点表示对于同一个阈值，准确率和召回率都很高。曲线从左上角开始，这里对应于非常低的阈值，将所有样本都划为正类。提高阈值可以让曲线向准确率更高的方向移动，但同时召回率降低。继续增大阈值，大多数被划为正类的点都是真正例，此时准确率很高，但召回率更低。随着准确率的升高，模型越能够保持较高的召回率，则模型越好。</p>
<h6 id="roc-与-auc"><a class="markdownIt-Anchor" href="#roc-与-auc"></a> ROC 与 AUC</h6>
<p>受试者工作特征曲线（receiver operating characteristics curve），简称为 ROC 曲线（ROC curve）。与准确率 - 召回率曲线类似，ROC 曲线考虑了给定分类器的所有可能的阈值，但它显示的是假正例率（false positive rate，FPR）和真正例率（true positive rate，TPR），而不是报告准确率和召回率。</p>
<p>与准确率 - 召回率曲线一样，我们通常希望使用一个数字来总结 ROC 曲线，即曲线下的面积［通常被称为 AUC（area under the curve），这里的曲线指的就是 ROC 曲线］。</p>
<h5 id="多分类指标"><a class="markdownIt-Anchor" href="#多分类指标"></a> 多分类指标</h5>
<p>用分类报告来观察各个分类的指标就很不错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         digits.data, digits.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">lr = LogisticRegression().fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">pred = lr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Accuracy: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br><span class="line"><span class="comment"># 精确度、召回率、f1 指标</span></span><br><span class="line"><span class="built_in">print</span>(classification_report(pred, y_test))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Out:</span></span><br><span class="line"><span class="string">Accuracy: 0.953</span></span><br><span class="line"><span class="string">              precision    recall  f1-score   support</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">           0       1.00      1.00      1.00        37</span></span><br><span class="line"><span class="string">           1       0.91      0.89      0.90        44</span></span><br><span class="line"><span class="string">           2       0.93      0.95      0.94        43</span></span><br><span class="line"><span class="string">           3       0.96      0.90      0.92        48</span></span><br><span class="line"><span class="string">           4       1.00      0.97      0.99        39</span></span><br><span class="line"><span class="string">           5       0.98      0.98      0.98        48</span></span><br><span class="line"><span class="string">           6       1.00      0.96      0.98        54</span></span><br><span class="line"><span class="string">           7       0.94      1.00      0.97        45</span></span><br><span class="line"><span class="string">           8       0.90      0.93      0.91        46</span></span><br><span class="line"><span class="string">           9       0.94      0.96      0.95        46</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   micro avg       0.95      0.95      0.95       450</span></span><br><span class="line"><span class="string">   macro avg       0.95      0.95      0.95       450</span></span><br><span class="line"><span class="string">weighted avg       0.95      0.95      0.95       450</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>宏（macro）平均：计算未加权的按类别 f- 分数。它对所有类别给出相同的权重，无论类别中的样本量大小。</p>
<p>加权（weighted）平均：以每个类别的支持作为权重来计算按类别 f- 分数的平均值。分类报告中给出的就是这个值。</p>
<p>微（micro）平均：计算所有类别中假正例、假反例和真正例的总数，然后利用这些计数来计算准确率、召回率和 f- 分数。</p>
<p>如果你对每个样本等同看待，那么推荐使用微平均 f1- 分数；如果你对每个类别等同看待，那么推荐使用宏平均 f1- 分数。</p>
<h5 id="回归指标"><a class="markdownIt-Anchor" href="#回归指标"></a> 回归指标</h5>
<p>对于回归问题来说，使用 score 方法评估即可，因为他没有分类的正反问题。score底层使用的是 R<sup>^2</sup>，它是评估回归模型的很好的指标。</p>
<h5 id="模型指标选择"><a class="markdownIt-Anchor" href="#模型指标选择"></a> 模型指标选择</h5>
<p>网格搜索评估和 K 折交叉验证最佳模型参数默认是基于精度评判的，我们可以指定基于其他指标（精确度、召回率、f1）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score, train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切分成 2 分类问题, 数字是否等于 9</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         digits.data, digits.target == <span class="number">9</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 种网格</span></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line"><span class="comment"># 在 2 个网格中, 找到 SVC 模型的最佳参数, 每一组参数进行 3 折评估, 使用 f1-score 作为评估依据</span></span><br><span class="line">grid_search = GridSearchCV(SVC(), param_grid, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;f1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索最佳参数</span></span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最佳参数</span></span><br><span class="line"><span class="built_in">print</span>(grid_search.best_params_)</span><br><span class="line"><span class="comment"># 打印最佳参数的 f1-score</span></span><br><span class="line"><span class="built_in">print</span>(grid_search.best_score_)</span><br><span class="line"><span class="comment"># 打印在测试集上的各种指标</span></span><br><span class="line"><span class="built_in">print</span>(classification_report(grid_search.predict(X_test), y_test))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Out:</span></span><br><span class="line"><span class="string">&#123;&#x27;gamma&#x27;: 0.001&#125;</span></span><br><span class="line"><span class="string">0.9771729298313027</span></span><br><span class="line"><span class="string">              precision    recall  f1-score   support</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       False       1.00      0.99      1.00       405</span></span><br><span class="line"><span class="string">        True       0.94      0.98      0.96        45</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   micro avg       0.99      0.99      0.99       450</span></span><br><span class="line"><span class="string">   macro avg       0.97      0.99      0.98       450</span></span><br><span class="line"><span class="string">weighted avg       0.99      0.99      0.99       450</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="算法链和管道"><a class="markdownIt-Anchor" href="#算法链和管道"></a> 算法链和管道</h2>
<p>大多数机器学习应用不仅需要应用单个算法，而且还需要将许多不同的处理步骤和机器学习模型链接在一起。我们一般使用 Pipeline 类来简化构建变换和模型链，Pipeline 类最常见的用例是将预处理步骤（比如数据缩放）与一个监督模型（比如分类器）链接在一起。</p>
<h3 id="构建管道"><a class="markdownIt-Anchor" href="#构建管道"></a> 构建管道</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">pipe = Pipeline([(<span class="string">&quot;scaler&quot;</span>, MinMaxScaler()), (<span class="string">&quot;svm&quot;</span>, SVC())])</span><br><span class="line">pipe.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(pipe.score(X_test, y_test)))</span><br><span class="line"><span class="comment"># Test score: 0.95</span></span><br></pre></td></tr></table></figure>
<p>利用管道，我们减少了预处理 + 分类过程 所需要的代码量。但是，使用管道的主要优点在于，现在我们可以在 cross_val_score 或 GridSearchCV 中使用这个估计器。</p>
<h3 id="网格搜索中使用管道"><a class="markdownIt-Anchor" href="#网格搜索中使用管道"></a> 网格搜索中使用管道</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并划分数据</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先缩放再跑模型的pipeline</span></span><br><span class="line">pipe = Pipeline([(<span class="string">&quot;scaler&quot;</span>, MinMaxScaler()), (<span class="string">&quot;svm&quot;</span>, SVC())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5折网格搜索</span></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;svm__C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],<span class="string">&#x27;svm__gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line">grid = GridSearchCV(pipe, param_grid=param_grid, cv=<span class="number">5</span>)</span><br><span class="line">grid.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best cross-validation accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(grid.best_score_))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(grid.score(X_test, y_test)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best parameters: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(grid.best_params_))</span><br></pre></td></tr></table></figure>
<p>这里采用 5 折网格参数搜索，但是在 pipeline 情况下，需要把搜索参数增加对应步骤的名字作为前缀，这样才会被 pipeline 中的某个步骤使用，因此，要想搜索 SVC 的 C 参数，必须使用 svm__C 作为参数网格字典的键，对 gamma 参数也是同理。</p>
<h3 id="通用管道接口"><a class="markdownIt-Anchor" href="#通用管道接口"></a> 通用管道接口</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并划分数据</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先缩放再跑模型的 pipeline</span></span><br><span class="line">pipe = Pipeline([(<span class="string">&quot;scaler&quot;</span>, MinMaxScaler()), (<span class="string">&quot;svm&quot;</span>, SVC())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5 折网格搜索</span></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;svm__C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],<span class="string">&#x27;svm__gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line">grid = GridSearchCV(pipe, param_grid=param_grid, cv=<span class="number">5</span>)</span><br><span class="line">grid.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 最佳泛化的训练结果</span></span><br><span class="line"><span class="built_in">print</span>(grid.best_estimator_)</span><br><span class="line"><span class="comment"># 最佳结果中的 svm 步骤</span></span><br><span class="line"><span class="built_in">print</span>(grid.best_estimator_.named_steps[<span class="string">&#x27;svm&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Out:</span></span><br><span class="line"><span class="string">Pipeline(memory=None,</span></span><br><span class="line"><span class="string">     steps=[(&#x27;scaler&#x27;, MinMaxScaler(copy=True, feature_range=(0, 1))), (&#x27;svm&#x27;, SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,</span></span><br><span class="line"><span class="string">  decision_function_shape=&#x27;ovr&#x27;, degree=3, gamma=1, kernel=&#x27;rbf&#x27;,</span></span><br><span class="line"><span class="string">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span></span><br><span class="line"><span class="string">  tol=0.001, verbose=False))])</span></span><br><span class="line"><span class="string">SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,</span></span><br><span class="line"><span class="string">  decision_function_shape=&#x27;ovr&#x27;, degree=3, gamma=1, kernel=&#x27;rbf&#x27;,</span></span><br><span class="line"><span class="string">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span></span><br><span class="line"><span class="string">  tol=0.001, verbose=False)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>可以看到具有最佳泛化的 pipeline，其 2 个步骤的训练结果也对应的保存了起来，我们可以取出其中的某一步骤的训练结果。</p>
<h3 id="网格搜索预处理"><a class="markdownIt-Anchor" href="#网格搜索预处理"></a> 网格搜索&amp;预处理</h3>
<p>添加交互多项式特征的预处理步骤。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并划分数据</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         boston.data, boston.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缩放数据 + 生成多项式特征 + 岭回归</span></span><br><span class="line">pipe = Pipeline([(<span class="string">&quot;scaler&quot;</span>, StandardScaler()), (<span class="string">&quot;ploy&quot;</span>, PolynomialFeatures()), (<span class="string">&#x27;ridge&#x27;</span>, Ridge())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前缀指定各个步骤的搜索参数</span></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;ploy__degree&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;ridge__alpha&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网格搜索</span></span><br><span class="line">grid = GridSearchCV(pipe, param_grid=param_grid, cv=<span class="number">5</span>)</span><br><span class="line">grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(grid.score(X_test, y_test))</span><br><span class="line"><span class="comment"># 最佳泛化的一组参数</span></span><br><span class="line"><span class="built_in">print</span>(grid.best_params_)</span><br></pre></td></tr></table></figure>
<h3 id="网格搜索模型选择"><a class="markdownIt-Anchor" href="#网格搜索模型选择"></a> 网格搜索&amp;模型选择</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并划分数据</span></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">         cancer.data, cancer.target, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一下 pipeline 的 2 个步骤</span></span><br><span class="line">pipe = Pipeline([(<span class="string">&#x27;preprocessing&#x27;</span>, StandardScaler()), (<span class="string">&#x27;classifier&#x27;</span>, SVC())])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对 SVC 模型进行 gamma 参数搜索、以及是否预处理的比较</span></span><br><span class="line"><span class="comment"># 对随机森林分类器进行 max_features 参数搜索、并且不进行预处理</span></span><br><span class="line">param_grid = [</span><br><span class="line">         &#123;<span class="string">&#x27;classifier&#x27;</span>: [SVC()], <span class="string">&#x27;preprocessing&#x27;</span>: [StandardScaler(), <span class="literal">None</span>],</span><br><span class="line">          <span class="string">&#x27;classifier__gamma&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],</span><br><span class="line">          <span class="string">&#x27;classifier__C&#x27;</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</span><br><span class="line">         &#125;,</span><br><span class="line">         &#123;<span class="string">&#x27;classifier&#x27;</span>: [RandomForestClassifier(n_estimators=<span class="number">100</span>)],</span><br><span class="line">          <span class="string">&#x27;preprocessing&#x27;</span>: [<span class="literal">None</span>], <span class="string">&#x27;classifier__max_features&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">         &#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网格搜索</span></span><br><span class="line">grid = GridSearchCV(pipe, param_grid, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">grid.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 精度</span></span><br><span class="line"><span class="built_in">print</span>(grid.score(X_test, y_test))</span><br><span class="line"><span class="comment"># 最佳泛化的一组参数</span></span><br><span class="line"><span class="built_in">print</span>(grid.best_params_)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">out:</span></span><br><span class="line"><span class="string">0.9790209790209791</span></span><br><span class="line"><span class="string">&#123;&#x27;classifier&#x27;: SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,</span></span><br><span class="line"><span class="string">  decision_function_shape=&#x27;ovr&#x27;, degree=3, gamma=0.01, kernel=&#x27;rbf&#x27;,</span></span><br><span class="line"><span class="string">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span></span><br><span class="line"><span class="string">  tol=0.001, verbose=False), &#x27;classifier__C&#x27;: 10, &#x27;classifier__gamma&#x27;: 0.01, &#x27;preprocessing&#x27;: StandardScaler(copy=True, with_mean=True, with_std=True)&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看出，StandardScaler 缩放 + SVC 分类模型的效果要好于随机森林分类。</p>
<h2 id="处理文本数据"><a class="markdownIt-Anchor" href="#处理文本数据"></a> 处理文本数据</h2>
<p>在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。这些术语来自于信息检索（information retrieval，IR）和自然语言处理（natural language processing，NLP）的社区，它们主要针对文本数据。</p>
<h3 id="词袋"><a class="markdownIt-Anchor" href="#词袋"></a> 词袋</h3>
<p>下面利用 CountVectorizer 对原始本文输入进行词袋统计，从而转换成稀疏的特征向量作为模型输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 行文本数据</span></span><br><span class="line">bards_words =[<span class="string">&quot;The fool doth think he is wise,&quot;</span>, <span class="string">&quot;but the wise man knows himself to be a fool&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个文档分词, 生成样本中所有词的表, 但是忽略掉那些只出现在 1 个样本中的单词, 并且忽略掉停词</span></span><br><span class="line">vect = CountVectorizer(min_df=<span class="number">2</span>, stop_words=<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line">vect.fit(bards_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印词表</span></span><br><span class="line"><span class="built_in">print</span>(vect.vocabulary_)</span><br><span class="line"><span class="comment"># 打印特征向量的构成</span></span><br><span class="line"><span class="built_in">print</span>(vect.get_feature_names())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本数据转换成词袋特征向量</span></span><br><span class="line">bag_of_words = vect.transform(bards_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转成稠密矩阵输出 2 行特征向量</span></span><br><span class="line"><span class="built_in">print</span>(bag_of_words.toarray())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">out:</span></span><br><span class="line"><span class="string">&#123;&#x27;fool&#x27;: 0, &#x27;wise&#x27;: 1&#125;</span></span><br><span class="line"><span class="string">[&#x27;fool&#x27;, &#x27;wise&#x27;]</span></span><br><span class="line"><span class="string">[[1 1]</span></span><br><span class="line"><span class="string"> [1 1]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>min_df 令词表仅保留了在不同文档中出现过至少 2 次的单词，另外 stop_words 指定忽略掉英文中的停词，其实上述过程就是舍弃我们认为不重要的单词。</p>
<h3 id="tf-idf-缩放数据"><a class="markdownIt-Anchor" href="#tf-idf-缩放数据"></a> tf-idf 缩放数据</h3>
<p>（term frequency–inverse document frequency）即词频-逆向文档频率，如果一个单词在某个特定文档中经常出现，但在许多文档中却不常出现，那么这个单词可能是对文档的很好的描述。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 行文本数据</span></span><br><span class="line">bards_words =[<span class="string">&quot;The fool doth think he is wise,&quot;</span>, <span class="string">&quot;but the wise man knows himself to be a fool&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本统计 tf-idf</span></span><br><span class="line">vect = TfidfVectorizer()</span><br><span class="line">vect.fit(bards_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印词表</span></span><br><span class="line"><span class="built_in">print</span>(vect.vocabulary_)</span><br><span class="line"><span class="comment"># 打印特征向量的构成</span></span><br><span class="line"><span class="built_in">print</span>(vect.get_feature_names())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入文本分词，并用每个分词的 tf-idf 作为特征值</span></span><br><span class="line">tfidf_X = vect.transform(bards_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转成稠密矩阵输出</span></span><br><span class="line"><span class="built_in">print</span>(tfidf_X.toarray())</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#123;&#x27;the&#x27;: 9, &#x27;fool&#x27;: 3, &#x27;doth&#x27;: 2, &#x27;think&#x27;: 10, &#x27;he&#x27;: 4, &#x27;is&#x27;: 6, &#x27;wise&#x27;: 12, &#x27;but&#x27;: 1, &#x27;man&#x27;: 8, &#x27;knows&#x27;: 7, &#x27;himself&#x27;: 5, &#x27;to&#x27;: 11, &#x27;be&#x27;: 0&#125;</span></span><br><span class="line"><span class="string">[&#x27;be&#x27;, &#x27;but&#x27;, &#x27;doth&#x27;, &#x27;fool&#x27;, &#x27;he&#x27;, &#x27;himself&#x27;, &#x27;is&#x27;, &#x27;knows&#x27;, &#x27;man&#x27;, &#x27;the&#x27;, &#x27;think&#x27;, &#x27;to&#x27;, &#x27;wise&#x27;]</span></span><br><span class="line"><span class="string">[[0.         0.         0.42567716 0.30287281 0.42567716 0.</span></span><br><span class="line"><span class="string">  0.42567716 0.         0.         0.30287281 0.42567716 0.</span></span><br><span class="line"><span class="string">  0.30287281]</span></span><br><span class="line"><span class="string"> [0.36469323 0.36469323 0.         0.25948224 0.         0.36469323</span></span><br><span class="line"><span class="string">  0.         0.36469323 0.36469323 0.25948224 0.         0.36469323</span></span><br><span class="line"><span class="string">  0.25948224]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
      
    </div>
    <!-- <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://wingowen.github.io/2020/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/" title="机器学习基础教程" target="_blank" rel="external">https://wingowen.github.io/2020/08/18/机器学习/机器学习基础教程/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">WINGO.WEN</span><small class="ml-1x"></small></a></h3>
        <div>一个疯子。</div>
      </div>
    </figure>
  </div>
</div>


    </div> -->
  </article>
  
    
  <section id="comments">
  	
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2020/09/07/Python/python-%E8%BF%9B%E9%98%B6/" title="python 进阶"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2020/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" title="深度学习入门"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
    <li class="toggle-toc">
      <a class="toggle-btn " data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button">    <span>[&nbsp;</span><span>文章目录</span>
        <i class="text-collapsed icon icon-anchor"></i>
        <i class="text-in icon icon-close"></i>
        <span>]</span>
      </a>
    </li>
    
  </ul>
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
    <div class="copyright">
    	
        <div class="publishby">
        	<!-- Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>. -->
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   






</body>
</html>